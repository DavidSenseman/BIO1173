{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEEW13uCtiJ"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/master/Class_05_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIO 1173: Intro Computational Biology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module 5: Regularization and Dropout**\n",
    "\n",
    "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
    "\n",
    "### Module 5 Material\n",
    "\n",
    "* Part 5.1: Part 5.1: Introduction to Regularization: Ridge and Lasso\n",
    "* **Part 5.2: Using K-Fold Cross Validation with Keras**\n",
    "* Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting\n",
    "* Part 5.4: Drop Out for Keras to Decrease Overfitting\n",
    "* Part 5.5: Benchmarking Keras Deep Learning Regularization Techniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKQylnEiLDUM"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n",
    "  Running the following code will map your GDrive to ```/content/drive```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seXFCYH4LDUM",
    "outputId": "c05015aa-871e-4779-9265-5ad07e8bf617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Setup\n",
    "\n",
    "Run the next code cell to load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current working directory is : C:\\Users\\David\\BIO1173\\Class_05_2\n",
      "Disk usage(total=4000108531712, used=995134951424, free=3004973580288)\n"
     ]
    }
   ],
   "source": [
    "# You MUST run this code cell first\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "path = '/'\n",
    "memory = shutil.disk_usage(path)\n",
    "dirpath = os.getcwd()\n",
    "print(\"Your current working directory is : \" + dirpath)\n",
    "print(\"Disk\", memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **README FIRST**\n",
    "\n",
    "Class_05_2 is setup differently than your other lessons. Instead of pairs of Examples and Exercises, there are only Examples to run. There are no **Exercises** because simply running the 3 examples below will take a considerable length of time. I didn't want to give you a lesson the would take over an hour to run, not counting the additional time you would need to debug your code.  \n",
    "\n",
    "How long it will take you to simply run this lesson will of course depend on what hardware (computer/laptop) you use. Using Google COLAB, Class_05_2 will take approximately 35 minutes to run. It takes even longer on my home workstation which is fairly powerful.   \n",
    "\n",
    "**1. So, what is the point of this lesson?**\n",
    "\n",
    "You might be asking yourself, what is the point of a lesson that doesn't require working Exercises. The answer is that you will **need** to understand how and why K-Fold Cross-validation works for you next assignment. Instead of spending time writing code, I want you to spend your time trying to _understand_ how the code examples below work.\n",
    "\n",
    "**2. Could I save time if I paid for a Pro membership to Google COLAB?**\n",
    "\n",
    "As you probably, know, Google COLAB allows you to work for free, but you are limited in what resources you can use. If you want more more powerful processors with faster memory, you need to buy a subscription. For $9.99 per month, Google offers a [Colab Pro](https://colab.research.google.com/signup) version. When you upgrade to the \"Pro\" level, you get \n",
    "\n",
    "* **100 compute units per month** <br>\n",
    "Compute units expire after 90 days. Purchase more as you need them.\n",
    "* **Faster GPUs** <br>\n",
    "Upgrade to more powerful GPUs.\n",
    "* **More memory** <br>\n",
    "Access our highest memory machines.\n",
    "* **Terminal** <br>\n",
    "Ability to use a terminal with the connected VM.\n",
    "* **AI-enabled autocompletions**<br>\n",
    "Intelligent multi-line suggestions automatically rendered while you type.\n",
    "* **Code generation**<br>\n",
    "Generate code with natural language, including an integrated chatbot. \n",
    "\n",
    "So you might be asking, if I upgrades to COLAB Pro, and used the more powerful processors, could I get this lesson finished sooner? At least when it comes to this lesson, the answer is definitely **NO!**\n",
    "\n",
    "While this lesson takes a fair amount of time to run, due to training time of the neural networks, this particular lesson does **not** run any faster if you use the \"fancy\" (i.e. expensive) runtime options like \"A100\" or \"VT100 GPU\", or even the free \"T4 GPU\". For example, the first neural network in this lesson required 11 minutes and 8 seconds to run with the expensive \"TPU\" runtime, but only 11 minutes and 1 second using the default (i.e. free) \"CPU\".  \n",
    "\n",
    "So why doesn't using a GPU or TPU (tensor processing unit) help? Simple 3 layer neural networks, like those used in this lesson, may not run faster with GPU support because the network is not complex enough to fully utilize the parallel processing power of a GPU. GPUs are best suited for handling very large and complex neural networks with multiple layers and thousands of neurons. In this case, the overhead of transferring data between the CPU and GPU may outweigh the potential speedup gained from GPU parallelism, resulting in minimal performance improvement or even slower execution times.\n",
    "\n",
    "While having access to more powerful processors will definitely help you run your code faster in some of the future lessons, there is no reason to upgrade for this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5.2: Using K-Fold Cross-validation with Keras\n",
    "\n",
    "**_K-fold validation_** is a technique used in machine learning to evaluate the performance and generalization ability of a model. In K-fold validation, the original dataset is randomly partitioned into K equal-sized subsets. The model is trained and evaluated K times, with each iteration using a different subset as the validation set and the remaining subsets as the training set. This allows for a more robust evaluation of the model's performance as it reduces the variance that may result from using a single train-test split. The final performance metric is typically averaged over the K iterations for a more reliable estimation of the model's performance.\n",
    "\n",
    "You can use cross-validation for a variety of purposes in predictive modeling:\n",
    "\n",
    "* Generating out-of-sample predictions from a neural network\n",
    "* Estimate a good number of epochs to train a neural network for (early stopping)\n",
    "* Evaluate the effectiveness of certain hyperparameters, such as activation functions, neuron counts, and layer counts\n",
    "\n",
    "Cross-validation uses several folds and multiple models to provide each data segment a chance to serve as both the validation and training set. Figure 5.CROSS shows cross-validation.\n",
    "\n",
    "**Figure 5.CROSS: K-Fold Crossvalidation**\n",
    "![K-Fold Crossvalidation](https://biologicslab.co/BIO1173/images/class_1_kfold.png \"K-Fold Crossvalidation\")\n",
    "\n",
    "It is important to note that each fold will have one model (neural network). To generate predictions for new data (not present in the training set), predictions from the fold models can be handled in several ways:\n",
    "\n",
    "* Choose the model with the highest validation score as the final model.\n",
    "* Preset new data to the five models (one for each fold) and average the result (this is an [ensemble](https://en.wikipedia.org/wiki/Ensemble_learning)).\n",
    "* Retrain a new model (using the same settings as the cross-validation) on the entire dataset. Train for as many epochs and with the same hidden layer structure.\n",
    "\n",
    "Generally, I prefer the last approach and will retrain a model on the entire data set once I have selected hyper-parameters. Of course, I will always set aside a final holdout set for model validation that I do not use in any aspect of the training process.\n",
    "\n",
    "## Regression vs Classification K-Fold Cross-Validation\n",
    "\n",
    "Regression and classification are handled somewhat differently concerning cross-validation. Regression is the simpler case where you can break up the data set into K folds with little regard for where each item lands. For regression, the data items should fall into the folds as randomly as possible. It is also important to remember that not every fold will necessarily have the same number of data items. It is not always possible for the data set to be evenly divided into K folds. For regression cross-validation, we will use the Scikit-Learn class **KFold**.\n",
    "\n",
    "Cross-validation for classification could also use the **KFold** object; however, this technique would not ensure that the class balance remains the same in each fold as in the original. The balance of classes that a model was trained on must remain the same (or similar) to the training set. Drift in this distribution is one of the most important things to monitor after a trained model has been placed into actual use. Because of this, we want to make sure that the cross-validation itself does not introduce an unintended shift. This technique is called stratified sampling and is accomplished by using the Scikit-Learn object **StratifiedKFold** in place of **KFold** whenever you use classification. In summary, you should use the following two objects in Scikit-Learn:\n",
    "\n",
    "* **KFold** When dealing with a regression problem.\n",
    "* **StratifiedKFold** When dealing with a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Performance dataset\n",
    "\n",
    "For this lesson we will be using the [Body Performance dataset](https://www.kaggle.com/datasets/kukuroo3/body-performance-data).\n",
    "This is data that confirmed the grade of performance with age and some exercise performance data. This is a relatively large dataset with 12 categories of information about 13,303 individuals.\n",
    "\n",
    "The 12 categories are:\n",
    "* **age:** 20 ~64\n",
    "* **gender:** M,F\n",
    "* **height_cm:** (If you want to convert to feet, divide by 30.48)\n",
    "* **weight_kg:**\n",
    "* **body fat_%:**\n",
    "* **diastolic:** diastolic blood pressure (min)\n",
    "* **systolic:** systolic blood pressure (min)\n",
    "* **gripForce:**\n",
    "* **sit and bend forward_cm:**\n",
    "* **sit-ups counts:**\n",
    "* **broad jump_cm:**\n",
    "* **class:** A,B,C,D ( A: best) / stratified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>...</th>\n",
       "      <th>sit and bend forward_cm</th>\n",
       "      <th>sit-ups counts</th>\n",
       "      <th>broad jump_cm</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>M</td>\n",
       "      <td>172.3</td>\n",
       "      <td>75.24</td>\n",
       "      <td>...</td>\n",
       "      <td>18.4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.0</td>\n",
       "      <td>M</td>\n",
       "      <td>165.0</td>\n",
       "      <td>55.80</td>\n",
       "      <td>...</td>\n",
       "      <td>16.3</td>\n",
       "      <td>53.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.0</td>\n",
       "      <td>M</td>\n",
       "      <td>179.6</td>\n",
       "      <td>78.00</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.0</td>\n",
       "      <td>M</td>\n",
       "      <td>174.5</td>\n",
       "      <td>71.10</td>\n",
       "      <td>...</td>\n",
       "      <td>15.2</td>\n",
       "      <td>53.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13389</th>\n",
       "      <td>21.0</td>\n",
       "      <td>M</td>\n",
       "      <td>179.7</td>\n",
       "      <td>63.90</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13390</th>\n",
       "      <td>39.0</td>\n",
       "      <td>M</td>\n",
       "      <td>177.2</td>\n",
       "      <td>80.50</td>\n",
       "      <td>...</td>\n",
       "      <td>16.4</td>\n",
       "      <td>45.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13391</th>\n",
       "      <td>64.0</td>\n",
       "      <td>F</td>\n",
       "      <td>146.1</td>\n",
       "      <td>57.70</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13392</th>\n",
       "      <td>34.0</td>\n",
       "      <td>M</td>\n",
       "      <td>164.0</td>\n",
       "      <td>66.10</td>\n",
       "      <td>...</td>\n",
       "      <td>7.1</td>\n",
       "      <td>51.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13393 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age gender  height_cm  weight_kg  ...  sit and bend forward_cm  \\\n",
       "0      27.0      M      172.3      75.24  ...                     18.4   \n",
       "1      25.0      M      165.0      55.80  ...                     16.3   \n",
       "2      31.0      M      179.6      78.00  ...                     12.0   \n",
       "3      32.0      M      174.5      71.10  ...                     15.2   \n",
       "...     ...    ...        ...        ...  ...                      ...   \n",
       "13389  21.0      M      179.7      63.90  ...                      1.1   \n",
       "13390  39.0      M      177.2      80.50  ...                     16.4   \n",
       "13391  64.0      F      146.1      57.70  ...                      9.2   \n",
       "13392  34.0      M      164.0      66.10  ...                      7.1   \n",
       "\n",
       "       sit-ups counts  broad jump_cm  class  \n",
       "0                60.0          217.0      C  \n",
       "1                53.0          229.0      A  \n",
       "2                49.0          181.0      C  \n",
       "3                53.0          219.0      B  \n",
       "...               ...            ...    ...  \n",
       "13389            48.0          167.0      D  \n",
       "13390            45.0          229.0      A  \n",
       "13391             0.0           75.0      D  \n",
       "13392            51.0          180.0      C  \n",
       "\n",
       "[13393 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/bodyPerformance.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Set max rows and max columns\n",
    "pd.set_option('display.max_rows', 8)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "\n",
    "# Display DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Sample Regression Predictions with K-Fold Cross-Validation\n",
    "\n",
    "The following code trains the simple dataset using a 5-fold cross-validation. The expected performance of a neural network of the type trained here would be the score for the generated out-of-sample predictions. We begin by preparing a feature vector using the **jh-simple-dataset** to predict age. This model is set up as a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read and preprocess the data\n",
    "\n",
    "# Read the data set\n",
    "dfBig = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/bodyPerformance.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Only use 15% for neural network\n",
    "df=dfBig.sample(frac=0.50)\n",
    "\n",
    "# Map gender\n",
    "mapping = {'M': 1, 'F': 0}\n",
    "df['gender'] = df['gender'].map(mapping)\n",
    "\n",
    "# Map class\n",
    "mapping =  {'A': 0,\n",
    "            'B': 1,\n",
    "            'C': 2,\n",
    "            'D': 3}\n",
    "df['class'] = df['class'].map(mapping)\n",
    "\n",
    "# Generate list of columns for x\n",
    "x_columns = df.columns.drop('class')\n",
    "\n",
    "# Standardize values with their Z-scores\n",
    "for col in x_columns:\n",
    "    df[col] = zscore(df[col])\n",
    "\n",
    "# Generate x-values as numpy array\n",
    "x = df[x_columns].values\n",
    "x = np.asarray(x).astype('float32')\n",
    "\n",
    "# Generate y-values as numpy array\n",
    "y = df['class'].values\n",
    "y = np.asarray(y).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the feature vector is created a 5-fold cross-validation can be performed to generate out-of-sample predictions.  We will assume 500 epochs and not use early stopping.  Later we will see how we can estimate a more optimal epoch count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "Fold score (RMSE): 0.5820550322532654\n",
      "Fold #2\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "Fold score (RMSE): 0.5865818858146667\n",
      "Fold #3\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "Fold score (RMSE): 0.6399576663970947\n",
      "Fold #4\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "Fold score (RMSE): 0.6411444544792175\n",
      "Fold #5\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "Fold score (RMSE): 0.5993940234184265\n",
      "Final, out of sample score (RMSE): 0.610374391078949\n",
      "Elapsed time = 0:10:14\n"
     ]
    }
   ],
   "source": [
    "# Setup KFold classification and train model\n",
    "\n",
    "# Set EPOCHS\n",
    "EPOCHS=500\n",
    "\n",
    "# Record the start time in st\n",
    "st = time.time()\n",
    "\n",
    "# Cross-Validate\n",
    "kf = KFold(5, shuffle=True, random_state=42) # Use for KFold classification\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "\n",
    "fold = 0\n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=x.shape[1], activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=0,\n",
    "              epochs=EPOCHS)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred)    \n",
    "\n",
    "    # Measure this fold's RMSE\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(f\"Fold score (RMSE): {score}\")\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print(f\"Final, out of sample score (RMSE): {score}\")    \n",
    "    \n",
    "# Write the cross-validated prediction\n",
    "oos_y = pd.DataFrame(oos_y)\n",
    "oos_pred = pd.DataFrame(oos_pred)\n",
    "oosDF = pd.concat( [df, oos_y, oos_pred],axis=1 )\n",
    "#oosDF.to_csv(filename_write,index=False)\n",
    "\n",
    "# Record the end time in et\n",
    "et = time.time()\n",
    "\n",
    "# Print out time\n",
    "seconds = int((et-st))\n",
    "seconds = seconds % (24 * 3600)\n",
    "hour = seconds // 3600\n",
    "seconds %= 3600\n",
    "minutes = seconds // 60\n",
    "seconds %= 60\n",
    "print(\"Elapsed time = %d:%02d:%02d\" % (hour, minutes, seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the above code also reports the average number of epochs needed.  A common technique is to then train on the entire dataset for the average number of epochs required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Stratified K-Fold Cross-Validation\n",
    "\n",
    "The following code trains and fits the **jh**-simple-dataset dataset with cross-validation to generate out-of-sample.  It also writes the out-of-sample (predictions on the test set) results.\n",
    "\n",
    "It is good to perform stratified k-fold cross-validation with classification data.  This technique ensures that the percentages of each class remain the same across all folds.  Use the **StratifiedKFold** object instead of the **KFold** object used in the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the data\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Read the data set\n",
    "dfBig = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/bodyPerformance.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Only use 15% for neural network\n",
    "df=dfBig.sample(frac=0.50)\n",
    "\n",
    "# Map gender\n",
    "mapping = {'M': 1, 'F': 0}\n",
    "df['gender'] = df['gender'].map(mapping)\n",
    "\n",
    "# Map class\n",
    "mapping =  {'A': 0,\n",
    "            'B': 1,\n",
    "            'C': 2,\n",
    "            'D': 3}\n",
    "df['class'] = df['class'].map(mapping)\n",
    "\n",
    "# Create list of X columns\n",
    "x_columns = df.columns.drop('class')\n",
    "\n",
    "# Standardize ranges with their Z-scores\n",
    "for col in x_columns:\n",
    "    df[col] = zscore(df[col])\n",
    "\n",
    "# Generate x-values as numpy array\n",
    "x = df[x_columns].values\n",
    "x = np.asarray(x).astype('float32')\n",
    "\n",
    "# Generate y-values as numpy array\n",
    "dummies = pd.get_dummies(df['class']) # Classification\n",
    "FitClass = dummies.columns\n",
    "y = dummies.values\n",
    "y = np.asarray(y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "Fold score (accuracy): 0.6368159203980099\n",
      "Fold #2\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "Fold score (accuracy): 0.6616915422885572\n",
      "Fold #3\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "Fold score (accuracy): 0.6492537313432836\n",
      "Fold #4\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "Fold score (accuracy): 0.6243781094527363\n",
      "Fold #5\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "Fold score (accuracy): 0.6583541147132169\n",
      "Final score (accuracy): 0.6460925833748133\n",
      "Elapsed time = 0:09:12\n"
     ]
    }
   ],
   "source": [
    "# Classification with Stratified K-Fold Cross-Validation\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# Record the start time in st\n",
    "st = time.time()\n",
    "\n",
    "# np.argmax(pred,axis=1)\n",
    "# Cross-validate\n",
    "# Use for StratifiedKFold classification\n",
    "kf = StratifiedKFold(5, shuffle=True, random_state=42) \n",
    "    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "# Must specify y StratifiedKFold for\n",
    "for train, test in kf.split(x,df['class']):  \n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Hidden 1\n",
    "    model.add(Dense(50, input_dim=x.shape[1], activation='relu')) \n",
    "    model.add(Dense(25, activation='relu')) # Hidden 2\n",
    "    model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
    "              verbose=0, epochs=EPOCHS)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    # raw probabilities to chosen class (highest probability)\n",
    "    pred = np.argmax(pred,axis=1) \n",
    "    oos_pred.append(pred)  \n",
    "\n",
    "    # Measure this fold's accuracy\n",
    "    y_compare = np.argmax(y_test,axis=1) # For accuracy calculation\n",
    "    score = metrics.accuracy_score(y_compare, pred)\n",
    "    print(f\"Fold score (accuracy): {score}\")\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "oos_y_compare = np.argmax(oos_y,axis=1) # For accuracy calculation\n",
    "\n",
    "score = metrics.accuracy_score(oos_y_compare, oos_pred)\n",
    "print(f\"Final score (accuracy): {score}\")    \n",
    "    \n",
    "# Write the cross-validated prediction\n",
    "oos_y = pd.DataFrame(oos_y)\n",
    "oos_pred = pd.DataFrame(oos_pred)\n",
    "oosDF = pd.concat( [df, oos_y, oos_pred],axis=1 )\n",
    "#oosDF.to_csv(filename_write,index=False)\n",
    "\n",
    "# Record the end time in et\n",
    "et = time.time()\n",
    "\n",
    "# Print out time\n",
    "seconds = int((et-st))\n",
    "seconds = seconds % (24 * 3600)\n",
    "hour = seconds // 3600\n",
    "seconds %= 3600\n",
    "minutes = seconds // 60\n",
    "seconds %= 60\n",
    "print(\"Elapsed time = %d:%02d:%02d\" % (hour, minutes, seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with both a Cross-Validation and a Holdout Set\n",
    "\n",
    "If you have a considerable amount of data, it is always valuable to set aside a holdout set before you cross-validate. This holdout set will be the final evaluation before using your model for its real-world use. Figure 5. HOLDOUT shows this division.\n",
    "\n",
    "**Figure 5. HOLDOUT: Cross-Validation and a Holdout Set**\n",
    "![Cross Validation and a Holdout Set](https://biologicslab.co/BIO1173/images/class_3_hold_train_val.png \"Cross-Validation and a Holdout Set\")\n",
    "\n",
    "The following program uses a holdout set and then still cross-validates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Read the data set\n",
    "dfBig = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/bodyPerformance.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Only use 20% for neural network\n",
    "df=dfBig.sample(frac=.50)\n",
    "\n",
    "# Map gender\n",
    "mapping = {'M': 1, 'F': 0}\n",
    "df['gender'] = df['gender'].map(mapping)\n",
    "\n",
    "# Map class\n",
    "mapping =  {'A': 0,\n",
    "            'B': 1,\n",
    "            'C': 2,\n",
    "            'D': 3}\n",
    "df['class'] = df['class'].map(mapping)\n",
    "\n",
    "\n",
    "# Create list of X columns\n",
    "x_columns = df.columns.drop('age')\n",
    "\n",
    "# Standardize ranges with their Z-scores\n",
    "for col in x_columns:\n",
    "    df[col] = zscore(df[col])\n",
    "\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x = df[x_columns].values\n",
    "x = np.asarray(x).astype('float32')\n",
    "y = df['age'].values\n",
    "y = np.asarray(y).astype('float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been preprocessed, we are ready to build the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Fold score (RMSE): 8.569509506225586\n",
      "Fold #2\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Fold score (RMSE): 8.620272636413574\n",
      "Fold #3\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Fold score (RMSE): 8.723272323608398\n",
      "Fold #4\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Fold score (RMSE): 8.636762619018555\n",
      "Fold #5\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Fold score (RMSE): 8.350958824157715\n",
      "\n",
      "Cross-validated score (RMSE): 8.581059455871582\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "Holdout score (RMSE): 8.075052261352539\n",
      "Elapsed time = 0:10:47\n"
     ]
    }
   ],
   "source": [
    "# Training with both a Cross-Validation and a Holdout Set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Record the start time in st\n",
    "st = time.time()\n",
    "\n",
    "# Keep a 10% holdout\n",
    "x_main, x_holdout, y_main, y_holdout = train_test_split(    \n",
    "    x, y, test_size=0.10) \n",
    "\n",
    "# Cross-validate\n",
    "kf = KFold(5)\n",
    "    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "for train, test in kf.split(x_main):        \n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train = x_main[train]\n",
    "    y_train = y_main[train]\n",
    "    x_test = x_main[test]\n",
    "    y_test = y_main[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=x.shape[1], activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
    "              verbose=0,epochs=EPOCHS)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred) \n",
    "\n",
    "    # Measure accuracy\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(f\"Fold score (RMSE): {score}\")\n",
    "\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print()\n",
    "print(f\"Cross-validated score (RMSE): {score}\")    \n",
    "    \n",
    "# Write the cross-validated prediction (from the last neural network)\n",
    "holdout_pred = model.predict(x_holdout)\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(holdout_pred,y_holdout))\n",
    "print(f\"Holdout score (RMSE): {score}\")    \n",
    "\n",
    "# Record the end time in et\n",
    "et = time.time()\n",
    "\n",
    "# Print out time\n",
    "seconds = int((et-st))\n",
    "seconds = seconds % (24 * 3600)\n",
    "hour = seconds // 3600\n",
    "seconds %= 3600\n",
    "minutes = seconds // 60\n",
    "seconds %= 60\n",
    "print(\"Elapsed time = %d:%02d:%02d\" % (hour, minutes, seconds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
