{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDmEWUold1+nA43FdL1k6G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_05_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYZVwSpdbE3Y"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExN-OzpYbE3Y"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4imk1kbE3Y"
      },
      "source": [
        "##### **Module 5: Natural Language Processing**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 5 Material\n",
        "\n",
        "* Part 5.1: Introduction to Hugging Face\n",
        "* **Part 5.2: Hugging Face Tokenizers**\n",
        "* Part 5.3: Hugging Face Datasets\n",
        "* Part 5.4: Training Hugging Face models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_-lPkxLbE3Z"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded.Make sure your GMAIL address is included as the last line in the output above."
      ],
      "metadata": {
        "id": "xG3_sXTDfyjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to Hugging Face Tokenizers**\n",
        "\n",
        "Run the next cell to see short introduction to Hugging Face Tokenizers. This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "zhPHQYX1UBTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"VFp38yj8h3A\"\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\"> </iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "afXLQcuBTJfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hugging Face Tokenizers**\n",
        "\n",
        "**Hugging Face tokenizers** are essential tools in natural language processing (NLP) that convert text into numerical data, which can be processed by machine learning models. Here's a breakdown of their importance and why they are particularly valuable for computational biologists:\n",
        "\n",
        "### What are Hugging Face Tokenizers?\n",
        "1. **Tokenization**: The process of breaking down text into smaller units called tokens. These tokens can be words, subwords, or characters.\n",
        "2. **Normalization**: Adjusting the text to a standard format, such as lowercasing or removing punctuation.\n",
        "3. **Encoding**: Converting tokens into numerical representations that models can understand.\n",
        "\n",
        "### Types of Tokenizers\n",
        "1. **Word-based Tokenizers**: Split text into words. Simple but can lead to large vocabularies.\n",
        "2. **Subword Tokenizers**: Break words into smaller units, balancing vocabulary size and representation. Examples include Byte Pair Encoding (BPE) and WordPiece.\n",
        "3. **Character-based Tokenizers**: Split text into individual characters. Useful for languages with complex morphology.\n",
        "\n",
        "### Importance for Computational Biologists\n",
        "1. **Handling Biological Texts**: Biological texts often contain specialized terminology, gene names, and sequences. Tokenizers can effectively process these texts, ensuring accurate representation and analysis.\n",
        "2. **Data Preprocessing**: Tokenizers help in preparing biological data for machine learning models, enabling tasks like gene sequence analysis, protein structure prediction, and more.\n",
        "3. **Efficiency**: Subword tokenizers, in particular, can handle rare and complex terms efficiently, reducing the need for extensive vocabularies and improving model performance.\n",
        "4. **Integration with Models**: Hugging Face tokenizers are designed to work seamlessly with pre-trained models, allowing computational biologists to leverage state-of-the-art NLP techniques for their research.\n",
        "\n",
        "You can find more detailed information about Hugging Face tokenizers [here](https://huggingface.co/docs/tokenizers/en/index).\n"
      ],
      "metadata": {
        "id": "iOOH7DgTY1Gx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hugging Face Tokenizers**\n",
        "\n",
        "Here are some of the most popular tokenizers on Hugging Face and why they are useful:\n",
        "\n",
        "1. **Byte-Pair Encoding (BPE)**:\n",
        "   - **Models**: GPT-2, RoBERTa\n",
        "   - **Usefulness**: BPE tokenizers break down words into subword units, which helps in handling rare and out-of-vocabulary words efficiently. This reduces the vocabulary size while maintaining the ability to represent complex words.\n",
        "\n",
        "2. **WordPiece**:\n",
        "   - **Models**: BERT, DistilBERT\n",
        "   - **Usefulness**: Similar to BPE, WordPiece tokenizers split words into subword units. They are particularly effective in handling morphological variations and rare words, making them suitable for a wide range of NLP tasks.\n",
        "\n",
        "3. **SentencePiece**:\n",
        "   - **Models**: T5, ALBERT\n",
        "   - **Usefulness**: SentencePiece tokenizers can handle both word and subword tokenization. They are language-agnostic and can be trained on raw text without pre-tokenization, making them versatile for different languages and scripts.\n",
        "\n",
        "4. **Unigram**:\n",
        "   - **Models**: XLNet\n",
        "   - **Usefulness**: Unigram tokenizers use a probabilistic model to generate subword units. They are effective in balancing the trade-off between vocabulary size and representation quality, making them suitable for large-scale language models.\n",
        "\n",
        "5. **Whitespace**:\n",
        "   - **Models**: Various\n",
        "   - **Usefulness**: Whitespace tokenizers split text based on spaces. They are simple and fast, making them useful for tasks where precise tokenization is not critical.\n",
        "\n",
        "These tokenizers are designed to handle different aspects of text processing, such as handling rare words, reducing vocabulary size, and maintaining the integrity of the original text. They are essential for preparing text data for machine learning models, ensuring that the input is in a format that the models can understand and process effectively.\n",
        "\n",
        "You can find more detailed information about Hugging Face tokenizers [here](https://huggingface.co/docs/tokenizers/index).\n",
        "\n"
      ],
      "metadata": {
        "id": "wVWCWSJurbIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Hugging Face\n",
        "\n",
        "Run the code in the next cell to install Hugging Face in your Colab environment."
      ],
      "metadata": {
        "id": "YlPf_OSZZK9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Hugging Face\n",
        "\n",
        "!pip install transformers > /dev/null\n",
        "!pip install transformers[sentencepiece] > /dev/null"
      ],
      "metadata": {
        "id": "evqhMPU8ZSV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1 - Step 1: Create Tokenizer\n",
        "\n",
        "In Step 1, we create a Hugging Face tokenizer using the `distilbert-base-uncased` as our tokenizer model. It is important to remember that the `distilbert-base-uncased` tokenizer is a **WordPiece** tokenizer. WordPiece tokenizers split words into subword units. They are particularly effective in handling morphological variations and rare words, making them suitable for a wide range of NLP tasks.\n",
        "\n",
        "In **Exercise 1** we will shift to a **SentencePiece** tokenizer to see how these two tokenizer types differ."
      ],
      "metadata": {
        "id": "Mm-Lhvv5Igr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Step 1: Create Tokenizer\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define model\n",
        "eg_model = \"distilbert-base-uncased\"\n",
        "\n",
        "# Setup tokenizer\n",
        "eg_tokenizer = AutoTokenizer.from_pretrained(eg_model)"
      ],
      "metadata": {
        "id": "XcLwqoeWtLS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image01F.png)"
      ],
      "metadata": {
        "id": "8Y3ul702bfMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1 - Step 2: Tokenize a Sentence\n",
        "\n",
        "The code in the cell below shows how to tokenize a sentence. For this example, we will use a quotation from Geoffrey Hinton, a pioneer in the field of deep learning:\n",
        "> \"To make a real impact on AI, we need to build systems that can learn from very large amounts of data.\"\n",
        ">\n",
        "This quote emphasizes the importance of data-driven learning in the development and advancement of neural networks and artificial intelligence"
      ],
      "metadata": {
        "id": "gpzyqVCYLZni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Step 2: Tokenize a sentence\n",
        "\n",
        "# Create sentence variable\n",
        "eg_sentence_1 = \"To make a real impact on AI, we need to build systems that can learn from very large amounts of data.\"\n",
        "\n",
        "# Tokenize sentence\n",
        "eg_encoded = eg_tokenizer(eg_sentence_1)\n",
        "\n",
        "# Print result\n",
        "print(eg_encoded)\n"
      ],
      "metadata": {
        "id": "iKQqnKlxtcNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image01A.png)"
      ],
      "metadata": {
        "id": "3cGaluTpd2fA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result of this tokenization contains two elements:\n",
        "\n",
        "* **input_ids:** The individual subword indexes, each index uniquely identifies a subword.\n",
        "\n",
        "* **attention_mask:**  Which values in input_ids are meaningful and not padding. This sentence had no padding, so all elements have an attention mask of \"1\". Later, we will request the output to be of a fixed length, introducing padding, which always has an attention mask of \"0\". Though each tokenizer can be implemented differently, the attention mask of a tokenizer is generally either \"0\" or \"1\".\n",
        "\n",
        "Due to subwords and special tokens, the number of tokens may not match the number of words in the source string. We can see the meanings of the individual tokens by converting these IDs back to strings.\n"
      ],
      "metadata": {
        "id": "P868oBVDdesp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1 - Step 3: Show Meaning of Tokens\n",
        "\n",
        "The code in the cell below shows the meaning of the tokens."
      ],
      "metadata": {
        "id": "OkZJX2EieST_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Step 3: Show meaning of tokens\n",
        "\n",
        "# Show meaning\n",
        "eg_tokenizer.convert_ids_to_tokens(eg_encoded.input_ids)"
      ],
      "metadata": {
        "id": "xYtw_zoRef-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image02A.png)"
      ],
      "metadata": {
        "id": "mUH2BNy7fgNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentence was broken down into tokens, which include special tokens **[CLS]** and **[SEP]**:\n",
        "\n",
        "**[CLS]:** This token is added at the beginning of the sequence. It stands for \"classification\" and is used in tasks like sentence classification.\n",
        "\n",
        "**Tokens:** The words and punctuation marks from your sentence have been split into individual tokens.\n",
        "\n",
        "**[SEP]:** This token is added at the end of the sequence to indicate the end of the input. It stands for \"separator\" and is used in tasks involving multiple sequences.\n",
        "\n",
        "The tokenization process is essential for preparing text data for model processing. It ensures that the text is transformed into a format that the model can understand and work with."
      ],
      "metadata": {
        "id": "ylh6vGl_e5xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1 - Step 4: Convert IDS to Tokens\n",
        "\n",
        "The cell below shows the code to convert the IDS from Example 1 - Step 3 to tokens."
      ],
      "metadata": {
        "id": "xa5szkK1n30e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Step 4: Convert ids to tokens\n",
        "\n",
        "# Convert ids back into tokens\n",
        "eg_tokenizer.convert_ids_to_tokens([0, 100, 101, 102, 103])"
      ],
      "metadata": {
        "id": "36wOqP1Tn30f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image03A.png)"
      ],
      "metadata": {
        "id": "jgW_FGZwzy95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explanation of Special Tokens in Tokenizer Output\n",
        "\n",
        "The output `['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']` consists of special tokens commonly used by tokenizers in the Hugging Face library:\n",
        "\n",
        "1. **[PAD]**: Padding token used to pad sequences to ensure they are of the same length.\n",
        "2. **[UNK]**: Unknown token used to represent tokens that are not found in the tokenizer's vocabulary.\n",
        "3. **[CLS]**: Classification token added at the beginning of the sequence, often used for tasks like sentence classification.\n",
        "4. **[SEP]**: Separator token used to indicate the end of a sequence or to separate multiple sequences.\n",
        "5. **[MASK]**: Mask token used in masked language modeling tasks, where certain tokens are masked and the model attempts to predict them.\n",
        "\n",
        "These special tokens are integral to various NLP tasks, such as sequence classification, question answering, and language modeling. They help the model understand and process input data in a structured way."
      ],
      "metadata": {
        "id": "HauY_cWPnGyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------\n",
        "\n",
        "### **Comparison of DistilBERT and ALBERT Tokenizers**\n",
        "\n",
        "For **Exercise 1** we will be comparing the `DistilBERT` tokenizer with the `ALBERT` tokenizer. Here are some of the differences we should expect to see:\n",
        "\n",
        "#### Tokenization Method\n",
        "- **DistilBERT**: Utilizes the WordPiece tokenization method. This method breaks down words into subword units and uses the `##` prefix to denote subwords that appear within a word.\n",
        "- **ALBERT**: Uses the SentencePiece tokenization method. SentencePiece tokenizers break down text into subword units and use the underscore (`▁`) character to indicate spaces before words.\n",
        "\n",
        "#### Special Tokens\n",
        "- **DistilBERT**: Uses special tokens such as `[CLS]` (classification token added at the beginning of the sequence) and `[SEP]` (separator token used to indicate the end of a sequence or to separate multiple sequences).\n",
        "- **ALBERT**: Also uses special tokens like `[CLS]` and `[SEP]`. However, the representation and tokenization of the text might differ due to the SentencePiece method.\n",
        "\n",
        "#### Vocabulary\n",
        "- **DistilBERT**: Has a distinct vocabulary file based on the WordPiece tokenization method, which includes tokens with the `##` prefix to denote subwords.\n",
        "- **ALBERT**: Has a different vocabulary file based on the SentencePiece tokenization method, which includes tokens with the underscore (`▁`) character to denote spaces before words.\n",
        "\n",
        "---------------------------"
      ],
      "metadata": {
        "id": "R8KjsQyvo_Eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1 - Step 1: Create Tokenizer**\n",
        "\n",
        "In the cell below, create a Hugging Face tokenizer using the `albert-large-v2` as your tokenizer model. Call your model `ex_model` and your tokenizer `ex_tokenizer`."
      ],
      "metadata": {
        "id": "pEZlXYhcgDge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 - Step 1 here:\n"
      ],
      "metadata": {
        "id": "Z94Dba_0hFwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image02F.png)"
      ],
      "metadata": {
        "id": "R4egHkgYh8uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1 - Step 2: Tokenize a Sentence**\n",
        "\n",
        "In the cell below use the following sentence to create `ex_sentence_1`:\n",
        "\n",
        "\"Hugging Face is on a mission to democratize artificial intelligence through open source and open science, making state-of-the-art models accessible to everyone.\"\n",
        "\n",
        "Use your `ex_tokenizer` to tokenize the sentence to create `ex_encoded`. Then print out the result of the tokenization, `ex_encoded`."
      ],
      "metadata": {
        "id": "rVjGky8mgDgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 - Step 2 here:\n",
        "\n"
      ],
      "metadata": {
        "id": "oKP9v2LYgDgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image07A.png)\n",
        "\n",
        "with additional numbers at the right."
      ],
      "metadata": {
        "id": "q2mjwwoXgDgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1 - Step 3: Show Meaning of Tokens**\n",
        "\n",
        "In the cell below, write the code to use your `ex_tokenizer` to show the meanings of the tokens in `ex_encoded`.\n",
        "\n"
      ],
      "metadata": {
        "id": "UwKyRWtcgDgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 - Step 3 here:\n",
        "\n"
      ],
      "metadata": {
        "id": "2GayYOwRgDgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image05A.png)"
      ],
      "metadata": {
        "id": "cDDDxF3fgDgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation of Tokenization Output**\n",
        "\n",
        "The output you're seeing is the result of tokenization using the `ALBERT` tokenizer. Here's what each part of the output means:\n",
        "\n",
        "1. **[CLS]**: This special token is added at the beginning of the sequence. It stands for \"classification\" and is often used in tasks like sentence classification or as a marker for the start of the sequence.\n",
        "\n",
        "2. **▁ (underscore character)**: This symbol is used in SentencePiece tokenization to indicate a space before a word. It's a way to handle whitespace in the text.\n",
        "\n",
        "3. **Tokens**: The words and subwords in your input sentence have been split into tokens. For example:\n",
        "   - `'▁hugging'` represents the word \"hugging\" with a preceding space.\n",
        "   - `'▁face'` represents the word \"face\" with a preceding space.\n",
        "   - `'▁is'`, `'▁on'`, `'▁a'`, etc., represent individual words with preceding spaces.\n",
        "   - `'▁democrat'`, `'ize'` represent the word \"democratize,\" split into two subwords.\n",
        "\n",
        "4. **Punctuation**: Punctuation marks like commas and periods are kept as separate tokens.\n",
        "\n",
        "5. **[SEP]**: This special token is added at the end of the sequence to indicate the end of the input. It stands for \"separator\" and is used in tasks involving multiple sequences.\n",
        "\n",
        "#### **Simplified Explanation of the Tokenization Process**\n",
        "- The tokenizer breaks down the sentence into smaller units called tokens.\n",
        "- Special tokens `[CLS]` and `[SEP]` are added to mark the beginning and end of the sequence.\n",
        "- Each word and punctuation mark is converted into a token, with subword tokenization applied to handle complex or rare words.\n",
        "\n",
        "Tokenization is crucial for transforming text into a format that machine learning models can process. It ensures that the input is properly segmented and encoded into numerical representations.\n",
        "\n",
        "--------------------"
      ],
      "metadata": {
        "id": "uA9KkM5OgDgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1 - Step 4: Convert IDS to Tokens**\n",
        "\n",
        "In the cell below write the code to convert the IDS in your `ex_tokenizer` back into tokens."
      ],
      "metadata": {
        "id": "jhVC6uxonLeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 - Step 4 here\n",
        "\n"
      ],
      "metadata": {
        "id": "5qRDM74Dl1vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image06A.png)"
      ],
      "metadata": {
        "id": "NHRs-sLW1lKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation of Tokenizer Output**\n",
        "\n",
        "The command `tokenizer.convert_ids_to_tokens([0, 100, 101, 102, 103])` converts a list of token IDs back into their corresponding tokens using the tokenizer's vocabulary. Here's what each part of the output means:\n",
        "\n",
        "1. **`<pad>`**: This token corresponds to the ID `0` and is typically used for padding sequences to a uniform length.\n",
        "2. **▁if**: The token with ID `100`, indicating the word \"if\" with a preceding space. The underscore (`▁`) denotes a space before the word, as seen in SentencePiece tokenization.\n",
        "3. **▁like**: The token with ID `101`, representing the word \"like\" with a preceding space.\n",
        "4. **ly**: The token with ID `102`, representing the subword \"ly\" which might be a part of a larger word.\n",
        "5. **n**: The token with ID `103`, representing the letter \"n\", which might be part of a subword or a standalone token depending on the context.\n",
        "\n",
        "These token IDs are mapped to their corresponding tokens based on the tokenizer's vocabulary. The presence of subword tokens like \"ly\" and \"n\" indicates that the tokenizer uses a subword-based method (e.g., SentencePiece, WordPiece) to handle the text.\n"
      ],
      "metadata": {
        "id": "9lsjJPNnmbC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------\n",
        "\n",
        "### **Difference in Tokenization Output: ALBERT vs. DistilBERT**\n",
        "\n",
        "The difference in output between the ALBERT tokenizer used in **Exercise 1** and the DistilBERT tokenizer used in Example 1 is due to the distinct tokenization methods used by each model. Here are the key differences:\n",
        "\n",
        "#### Tokenization Method\n",
        "- **ALBERT**: Uses SentencePiece tokenization, which breaks down text into subword units and adds the underscore (`▁`) character to indicate spaces. This helps handle complex words and out-of-vocabulary terms efficiently.\n",
        "- **DistilBERT**: Uses WordPiece tokenization, which also breaks down text into subword units but does not use the underscore character. Instead, it adds `##` before subword tokens that are not at the start of a word.\n",
        "\n",
        "#### Special Tokens\n",
        "- Both ALBERT and DistilBERT add `[CLS]` at the beginning and `[SEP]` at the end of the sequence, but their internal tokenization processes differ due to the tokenization method used.\n",
        "\n",
        "#### Vocabulary\n",
        "- The vocabulary files used by ALBERT and DistilBERT are different. Each tokenizer has its unique vocabulary that influences how text is broken down into tokens.\n",
        "\n",
        "----------------------------------"
      ],
      "metadata": {
        "id": "BFHY6IGVk9aD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenizing a List of Sequences**\n",
        "\n",
        "**Tokenizing a list of sequences** is a crucial step in preparing data for machine learning models, especially in the field of natural language processing (NLP) and bioinformatics. Here's why tokenization is important and why it might be of interest to a computational biologist:\n",
        "\n",
        "#### Importance of Tokenizing a List of Sequences\n",
        "\n",
        "1. **Uniform Data Representation**: Tokenization transforms sequences (e.g., text, DNA/RNA sequences) into numerical representations that models can process. This ensures that all sequences are uniformly represented, making it easier to feed them into machine learning algorithms.\n",
        "\n",
        "2. **Handling Variable-Length Sequences**: Biological data, such as gene sequences, can vary in length. Tokenization, combined with techniques like padding and truncation, ensures that all sequences are of a uniform length, which is necessary for batch processing.\n",
        "\n",
        "3. **Capturing Contextual Information**: Advanced tokenizers, such as those used in NLP models, can capture contextual information by breaking down sequences into meaningful subunits (e.g., words, subwords, amino acids). This helps models understand the relationships and patterns within the sequences.\n",
        "\n",
        "4. **Reducing Vocabulary Size**: Subword tokenizers (e.g., SentencePiece, WordPiece) break down rare and complex terms into smaller, more manageable units. This reduces the overall vocabulary size, making the model more efficient and capable of handling out-of-vocabulary terms.\n",
        "\n",
        "5. **Facilitating Transfer Learning**: Pre-trained models, such as BERT or ALBERT, often come with their own tokenizers. By tokenizing sequences in a manner consistent with these models, researchers can leverage transfer learning, applying pre-trained models to their specific tasks with minimal retraining.\n",
        "\n",
        "#### Why Tokenization is of Interest to Computational Biologists\n",
        "\n",
        "1. **Gene Sequence Analysis**: Tokenizing DNA/RNA sequences allows computational biologists to apply machine learning models to tasks such as gene prediction, sequence alignment, and motif discovery.\n",
        "\n",
        "2. **Protein Structure Prediction**: Tokenizing amino acid sequences helps in predicting protein structures, functions, and interactions. This can lead to advancements in drug discovery and understanding of biological processes.\n",
        "\n",
        "3. **Text Mining in Biological Literature**: Tokenizing scientific texts and literature enables computational biologists to perform text mining, extracting valuable information, identifying trends, and gaining insights from large volumes of published research.\n",
        "\n",
        "4. **Data Preprocessing**: Proper tokenization is a key step in data preprocessing, ensuring that biological data is in a suitable format for downstream analysis. This includes normalization, handling missing data, and preparing input for various bioinformatics tools.\n",
        "\n",
        "5. **Integration with NLP Models**: Tokenizing biological sequences or texts allows computational biologists to integrate their data with NLP models, enabling tasks such as entity recognition, relationship extraction, and knowledge discovery.\n",
        "\n",
        "In summary, tokenization is a vital step in transforming biological sequences into a format that can be effectively processed by machine learning models. It enhances the efficiency, accuracy, and interpretability of computational biology tasks, leading to more meaningful insights and discoveries.\n",
        "\n"
      ],
      "metadata": {
        "id": "BxOa5s-pslTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2 - Step 1: Entity Tagging\n",
        "\n",
        "The code in the cell below creates a variable, `eg_sequences` with the three DNA sequences. The top two sequences are labelled `ORF` while the bottom sequence is labelled as not an `ORF`.\n",
        "\n",
        "In molecular genetics, `ORF` stands for **Open Reading Frame**. An open reading frame is a continuous stretch of nucleotides in a DNA (or RNA) sequence that has the potential to be translated into a protein. It starts with a start codon (usually AUG, which codes for methionine) and ends with a stop codon (such as UAA, UAG, or UGA).\n",
        "\n",
        "ORFs are important because they help scientists identify protein-coding regions within a genome. In gene prediction and annotation, finding ORFs is a key step in determining which parts of the DNA sequence may encode functional proteins.\n",
        "\n",
        "The code in the cell below tokenises a batch of DNA sequences with a BERT tokenizer, padding them to the longest sequence, adding special `[CLS]`/`[SEP]` tokens, and returning PyTorch tensors. It then prints the resulting `input_ids` (integer token IDs) and `attention_mask` (1s for real tokens, 0s for padding), preparing the data for input into a BERT‑style model for downstream tasks such as ORF prediction."
      ],
      "metadata": {
        "id": "8xs1j0xLZjV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Step 1: Entity tagging\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "\n",
        "# Define DNA sequences\n",
        "eg_sequences = [\n",
        "    \"ATGCGTACCGATGCTTACGTTAGCATCGTATCGTAGCTGA\", # ORF\n",
        "    \"ATGACCGTAACTGCTGCCATCGTATGCAGTCTGATGCTAA\", # ORF\n",
        "    \"ACTGTCGACCAGTCTAGCATCGGTTACGATCGTACAGTAC\"  # Not ORF\n",
        "]\n",
        "\n",
        "# Encode Sequences\n",
        "eg_encoded = eg_tokenizer(eg_sequences, padding=True, add_special_tokens=True,\n",
        "                       return_tensors=\"pt\")  # Ensure outputs are tensors\n",
        "\n",
        "print(\"Input IDs\")\n",
        "for a in eg_encoded.input_ids:\n",
        "    print(a)\n",
        "\n",
        "print(\"**Attention Mask**\")\n",
        "for a in eg_encoded.attention_mask:\n",
        "    print(a)"
      ],
      "metadata": {
        "id": "YOVgUxDVtpzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image11A.png)"
      ],
      "metadata": {
        "id": "BByRH0z9FKL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input_ids tensors represent the tokenized and encoded version of your input sequences. Each tensor corresponds to a sequence, with special tokens and padding added as needed. Let's look at the first tensor:\n",
        "\n",
        "First Sequence:\n",
        "\n",
        "~~~text\n",
        "tensor([  101,  2012, 18195, 13512,  6305,  2278, 20697, 18195,  5946,  2278,\n",
        "         13512, 15900, 11266,  2278, 13512,  4017,  2278, 13512,  8490,  6593,\n",
        "           102,     0,     0])\n",
        "\n",
        "~~~\n",
        "\n",
        "* `101` represents the `[CLS]` token added at the beginning of the sequence.\n",
        "* The numbers like `2012`, `18195`, `13512`, ... represent the token IDs for the characters in your sequence.\n",
        "* `102` represents the `[SEP]` token added at the end of the sequence.\n",
        "* `0` represents the `[PAD]` tokens added to pad the sequence to a uniform length.\n",
        "\n",
        "#### **Attention Mask**\n",
        "\n",
        "The attention_mask tensors indicate which tokens should be attended to (1) and which tokens are just padding (0). This helps the model to focus on the relevant tokens and ignore the padding tokens.\n",
        "\n",
        "First Sequence:\n",
        "\n",
        "~~~text\n",
        "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
        "\n",
        "~~~\n",
        "All relevant tokens (including special tokens) have a value of `1`. The padding tokens have a value of `0`.\n",
        "\n",
        "#### **Summary**\n",
        "* **Input IDs:** These tensors contain the token `IDs` for the sequences, including special tokens `[CLS]` and `[SEP]`, and padding tokens `[PAD]` to ensure uniform length.\n",
        "\n",
        "* **Attention Mask:** These tensors indicate which tokens should be attended to by the model and which tokens are padding.\n",
        "\n",
        "This output ensures that the sequences are properly formatted for processing by the model, with attention focused on the relevant tokens.\n"
      ],
      "metadata": {
        "id": "3JFma0fmMAYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2 - Step 2: Analyze Tokens with Neural Network\n",
        "\n",
        "The code in the cell below loads a pretrained `bert-base-uncased` tokenizer (`eg_tokenizer`)and model (`eg_model`), tokenises a batch of sequences (`eg_sequnces`) and `attention_mask`), runs a forward pass without gradient computation to obtain the last hidden states, extracts the hidden state of the `[CLS]` token from the first position of each sequence, and prints these `[CLS]` representations as vectors which can be used for downstream classification tasks--in this case identifying `ORFs`.\n"
      ],
      "metadata": {
        "id": "UDewdqg4XP9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Step 2: Analyze tokens with neural network\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "\n",
        "# Define model and tokenizer\n",
        "eg_model_name = \"bert-base-uncased\"\n",
        "eg_tokenizer = BertTokenizer.from_pretrained(eg_model_name)\n",
        "eg_model = BertModel.from_pretrained(eg_model_name)\n",
        "\n",
        "# Prepare inputs\n",
        "input_ids = eg_encoded[\"input_ids\"]\n",
        "attention_mask = eg_encoded[\"attention_mask\"]\n",
        "\n",
        "# Perform forward pass through the model\n",
        "with torch.no_grad():\n",
        "    eg_outputs = eg_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Extract hidden states (last layer)\n",
        "eg_hidden_states = eg_outputs.last_hidden_state\n",
        "\n",
        "# Use hidden states for classification\n",
        "eg_cls_token_hidden_state = eg_hidden_states[:, 0, :]  # Select [CLS] token hidden state\n",
        "print(eg_cls_token_hidden_state)\n"
      ],
      "metadata": {
        "id": "XsATKo4ZLLFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image09A.png)\n",
        "\n",
        "You should focus on the tensor values:\n",
        "\n",
        "```text\n",
        "tensor([[-0.6225,  0.0124,  0.0010,  ...,  0.0586, -0.1268,  0.6282],\n",
        "        [-0.8602,  0.0782, -0.0059,  ...,  0.1884,  0.1436,  0.5705],\n",
        "        [-0.5524,  0.0369,  0.0565,  ...,  0.0031, -0.0707,  0.7200]])\n",
        "```\n",
        "These tensor values will be used in the next step to predict whether or not a particular DNA sequence is a `ORF`."
      ],
      "metadata": {
        "id": "IavfSeiCOUP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2 - Step 3:  Use tensors to make predictions\n",
        "\n",
        "The code in the cell below uses the tensors generated in the previous step to make predictions whether a DNA sequence is a `OFR`."
      ],
      "metadata": {
        "id": "MckqBpgj5tW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Step 3: Use tensors to make predictions\n",
        "\n",
        "import torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Assuming cls_token_hidden_state already contains the values\n",
        "\n",
        "# Y labels\n",
        "Y_labels = torch.tensor([1, 1, 0])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = eg_cls_token_hidden_state.numpy()\n",
        "y = Y_labels.numpy()\n",
        "\n",
        "# Train a logistic regression classifier\n",
        "eg_clf = LogisticRegression()\n",
        "eg_clf.fit(X, y)\n",
        "\n",
        "# Print header\n",
        "print(\"ORF Predictions: 1=True, 2=False\")\n",
        "\n",
        "# Make predictions\n",
        "eg_predictions = eg_clf.predict(X)\n",
        "print(eg_predictions)\n"
      ],
      "metadata": {
        "id": "LLjDR8mUHj50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image10A.png)"
      ],
      "metadata": {
        "id": "lREGP-0Q7E74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predicted labels [1, 1, 0] indicate how the logistic regression classifier has categorized each of the input sequences based on the hidden states from the BERT model. Specifically:\n",
        "\n",
        "* The first sequence is predicted to belong to class 1 (ORF).\n",
        "* The second sequence is predicted to belong to class 1 (ORF).\n",
        "* The third sequence is predicted to belong to class 0 (Not ORF).\n",
        "\n",
        "### **Example Scenario**\n",
        "\n",
        "Imagine you're working on classifying gene sequences into different functional groups based on their sequence patterns. The predicted labels [0, 1, 0] might represent different functional groups, such as:\n",
        "\n",
        "* Class 0: Non-coding sequences (Not ORF).\n",
        "* Class 1: Protein-coding sequences (ORF).\n",
        "\n",
        "In this scenario, the model has correctly identified the first and second sequences as protein-coding sequences (class 1), while the third sequences Was a non-coding sequences (class 0).\n",
        "\n",
        "The output demonstrates the practical application of deep learning models in sequence classification and showcases the potential of transformer-based models in extracting meaningful features from complex biological data."
      ],
      "metadata": {
        "id": "0ZBUg2caJHz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output `[0, 1, 0]` represents the predicted class labels for each of the input sequences. Here's the interpretation:\n",
        "\n",
        "1. **First Sequence:** The predicted label is 0.\n",
        "\n",
        "2. **Second Sequence:** The predicted label is 1.\n",
        "\n",
        "3. **Third Sequence:** The predicted label is 0.\n",
        "\n",
        "This means that, according to the logistic regression classifier trained on the hidden states of the `[CLS]` token:\n",
        "\n",
        "* The first sequence belongs to class 0.\n",
        "\n",
        "* The second sequence belongs to class 1.\n",
        "\n",
        "* The third sequence belongs to class 0.\n",
        "\n",
        "These class labels are based on the training data provided (in this case, the labels tensor) and the features extracted from the hidden states of the input sequences. You can use these predicted labels for various downstream tasks, such as categorizing the sequences into different classes or groups."
      ],
      "metadata": {
        "id": "_Xl_HBE7H2uT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2 - Step 1: Entity Tagging**\n",
        "\n",
        "In the cell below creates a variable, `ex_sequences` with the following three DNA sequences:\n",
        "\n",
        "```type\n",
        "\"TATAAAAGGCGTACGTAGCTAGCTAG\",  # Promotor sequence\n",
        "\"CGTAGCTAGCTAGCGCGCGCGCGCGC\",  # Not promotor sequence\n",
        "\"GCGCGTATATAAGCTAGCTAGCTAGC\"   # Promotor sequence\n",
        "```\n",
        "The top and bottom sequences are labelled `Promotor sequence` while the middle sequence is labelled `Not promotor sequence`.\n",
        "\n",
        "A **promoter sequence** is a region of DNA located upstream (before) the start of a gene that plays a crucial role in initiating transcription—the process by which RNA is synthesized from a DNA template.\n",
        "\n",
        "**Key Features of Promoter Sequences:**\n",
        "* Binding Site for RNA Polymerase: Promoters contain specific motifs where RNA polymerase and transcription factors bind to begin transcription.\n",
        "* TATA Box: A common motif in eukaryotic promoters, typically found about 25–35 bases upstream of the transcription start site. It has the sequence TATAAA and helps position RNA polymerase correctly.\n",
        "* Regulatory Elements: Promoters may also include other elements like enhancers or silencers that influence the rate of transcription.\n",
        "\n",
        "In the cell below tokenise your `ex_sequences` with a BERT tokenizer, padding them to the longest sequence, adding special `[CLS]`/`[SEP]` tokens, and returning PyTorch tensors. Then print out the resulting `input_ids` (integer token IDs) and `attention_mask`."
      ],
      "metadata": {
        "id": "7RbwwteV7jKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 - Step 1 here\n",
        "\n"
      ],
      "metadata": {
        "id": "bzqyEeKK7jKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image14A.png)"
      ],
      "metadata": {
        "id": "BYXEXDdF7jKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2 - Step 2: Analyze Tokens with Neural Network**\n",
        "\n",
        "In the cell below write the code to load the pretrained `bert-base-uncased` tokenizer as `ex_tokenizer` along with a model called `ex_model` and tokenize your DNA sequences as `ex_sequences`. Print out the Inout IDs along with the Attention mask."
      ],
      "metadata": {
        "id": "2AQQxgAY7jKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 - Step 2 here\n",
        "\n"
      ],
      "metadata": {
        "id": "V4Jm_aP_7jKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image15A.png)\n"
      ],
      "metadata": {
        "id": "ULuSo4Bj7jKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2 - Step 3:  Use Tensors to Make Predictions**\n",
        "\n",
        "In the cell below use the tensors generated in the previous step to make predictions whether a DNA sequence is a `Promotor Sequence`.\n",
        "\n",
        "You will need to change your `Y` labels to the following:\n",
        "```text\n",
        "# Y labels\n",
        "Y_labels = torch.tensor([1, 0, 1])\n",
        "```"
      ],
      "metadata": {
        "id": "pRpXV2n37jKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 - Step 3 here\n",
        "\n"
      ],
      "metadata": {
        "id": "pr4w8XWe7jKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_2_image16A.png)"
      ],
      "metadata": {
        "id": "_vZtk_LJ7jKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_05_2.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "fx2GceWzUO7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **TSMC**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/0/07/TSMC_factory_in_Taichung%27s_Central_Taiwan_Science_Park.jpg)\n",
        "\n",
        "\n",
        "**Taiwan Semiconductor Manufacturing Company Limited** (TSMC or Taiwan Semiconductor) is a Taiwanese multinational semiconductor contract manufacturing and design company. It is the world's most valuable semiconductor company, the world's largest dedicated independent (\"pure-play\") semiconductor foundry, and Taiwan's largest company, with headquarters and main operations located in the Hsinchu Science Park in Hsinchu, Taiwan. Although the central government of Taiwan is the largest individual shareholder, the majority of TSMC is owned by foreign investors. In 2023, the company was ranked 44th in the Forbes Global 2000. Taiwan's exports of integrated circuits amounted to \\$184 billion in 2022, accounted for nearly 25 percent of Taiwan's GDP. TSMC constitutes about 30 percent of the Taiwan Stock Exchange's main index.\n",
        "\n",
        "TSMC was founded in Taiwan in 1987 by Morris Chang as the world's first dedicated semiconductor foundry. It has long been the leading company in its field. When Chang retired in 2018, after 31 years of TSMC leadership, Mark Liu became chairman and C. C. Wei became Chief Executive. It has been listed on the Taiwan Stock Exchange since 1993; in 1997 it became the first Taiwanese company to be listed on the New York Stock Exchange. Since 1994, TSMC has had a compound annual growth rate (CAGR) of 17.4% in revenue and a CAGR of 16.1% in earnings.\n",
        "\n",
        "Most fabless semiconductor companies such as AMD, Apple, ARM, Broadcom, Marvell, MediaTek, Qualcomm, and Nvidia are customers of TSMC, as are emerging companies such as Allwinner Technology, HiSilicon, Spectra7, and UNISOC. Programmable logic device companies Xilinx and previously Altera also make or made use of TSMC's foundry services. Some integrated device manufacturers that have their own fabrication facilities, such as Intel, NXP, STMicroelectronics, and Texas Instruments, outsource some of their production to TSMC. At least one semiconductor company, LSI, re-sells TSMC wafers through its ASIC design services and design IP portfolio.\n",
        "\n",
        "TSMC has a global capacity of about thirteen million 300 mm-equivalent wafers per year as of 2020 and produces chips for customers with process nodes from 2 microns to 3 nanometres. TSMC was the first foundry to market 7-nanometre and 5-nanometre (used by the 2020 Apple A14 and M1 SoCs, the MediaTek Dimensity 8100, and AMD Ryzen 7000 series processors) production capabilities, and the first to commercialize ASML's extreme ultraviolet (EUV) lithography technology in high volume.\n",
        "\n",
        "**History**\n",
        "\n",
        "In 1986, Li Kwoh-ting, representing the Executive Yuan, invited Morris Chang to serve as the president of the Industrial Technology Research Institute (ITRI) and offered him a blank check to build Taiwan's chip industry. At that time, the Taiwanese government wanted to develop its semiconductor industry, but its high investment and high risk nature made it difficult to find investors. Texas Instruments and Intel turned down Chang. Only Philips was willing to sign a joint venture contract with Taiwan to put up \\$58 million, transfer its production technology, and license intellectual property in exchange for a 27.5 percent stake in TSMC. Alongside generous tax benefits, the Taiwanese government, through the National Development Fund, Executive Yuan, provided another 48 percent of the startup capital for TSMC, and the rest of the capital was raised from several of the island's wealthiest families, who owned firms that specialized in plastics, textiles, and chemicals. These wealthy Taiwanese were directly \"asked\" by the government to invest. \"What generally happened was that one of the ministers in the government would call a businessman in Taiwan,\"Chang explained, \"to get him to invest.\" From day one, TSMC was not really a private business: it was a project of the Taiwanese state. Its first CEO was James E. Dykes, who left after a year and Morris Chang became the CEO.\n",
        "Since then, the company has continued to grow, albeit subject to the cycles of demand. In 2011, the company planned to increase research and development expenditures by almost 39% to NT\\$50 billion to fend off growing competition. The company also planned to expand capacity by 30% in 2011 to meet strong market demand. In May 2014, TSMC's board of directors approved capital appropriations of US \\$568 million to increase and improve manufacturing capabilities after the company forecast higher than expected demand. In August 2014, TSMC's board of directors approved additional capital appropriations of US \\$3.05 billion.\n",
        "\n",
        "In 2011, it was reported that TSMC had begun trial production of the A5 SoC and A6 SoCs for Apple's iPad and iPhone devices. According to reports, in May 2014 Apple sourced its A8 and A8X SoCs from TSMC. Apple then sourced the A9 SoC with both TSMC and Samsung (to increase volume for iPhone 6S launch) and the A9X exclusively with TSMC, thus resolving the issue of sourcing a chip in two different microarchitecture sizes. As of 2014, Apple was TSMC's most important customer. In October 2014, ARM and TSMC announced a new multi-year agreement for the development of ARM based 10 nm FinFET processors.\n",
        "\n",
        "Over the objection of the Tsai Ing-wen administration, in March 2017, TSMC invested US\\$3 billion in Nanjing to develop a manufacturing subsidiary there.\n",
        "In 2020, TSMC became the first semiconductor company in the world to sign up for the RE100 initiative, pledging to use 100% renewable energy by 2050. TSMC accounts for roughly 5% of the energy consumption in Taiwan, even exceeding that of the capital city Taipei. This initiative was thus expected to accelerate the transformation to renewable energy in the country. For 2020, TSMC had a net income of US \\$17.60 billion on a consolidated revenue of US \\$45.51 billion, an increase of 57.5% and 31.4% respectively from the 2019 level of US \\$11.18 billion net income and US \\$34.63 billion consolidated revenue. Its market capitalization was over \\$550 billion in April 2021. TSMC's revenue in the first quarter of 2020 reached US \\$10 billion, while its market capitalization was US \\$254 billion. TSMC's market capitalization reached a value of NT \\$1.9 trillion (US \\$63.4 billion) in December 2010. It was ranked 70th in the FT Global 500 2013 list of the world's most highly valued companies with a capitalization of US \\$86.7 billion, while reaching US\\$110 billion in May 2014. In March 2017, TSMC's market capitalization surpassed that of semiconductor giant Intel for the first time, hitting NT\\$5.14 trillion (US \\$168.4 billion), with Intel's at US \\$165.7 billion. On 27 June 2020, TSMC briefly became the world's 10th most valuable company, with a market capitalization of US \\$410 billion.\n",
        "\n",
        "To mitigate business risks in the event of war between Taiwan and the People's Republic of China, since the beginning of the 2020s, TSMC has expanded its geographic operations, opening new fabs in Japan and the United States, with further plans for expansion into Germany. In July 2020, TSMC confirmed it would halt the shipment of silicon wafers to Chinese telecommunications equipment manufacturer Huawei and its subsidiary HiSilicon by 14 September. In November 2020, officials in Phoenix, Arizona in the United States approved TSMC's plan to build a \\$12 billion chip plant in the city. The decision to locate a plant in the US came after the Trump administration warned about the issues concerning the world's electronics made outside of the U.S. In 2021, news reports claimed that the facility might be tripled to roughly a \\$35 billion investment with six factories. See TSMC § Arizona for more details.\n",
        "\n",
        "In October 2024, TSMC informed the United States Department of Commerce about a potential breach of export controls in which one of its most advanced chips was sent to Huawei via another company with ties to the Chinese government.\n"
      ],
      "metadata": {
        "id": "HdwccJ0TUQ81"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bs0MR47gG6i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQQqfIE2yI01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}