{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEEW13uCtiJ",
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/master/Class_06_1_COLAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BIO 1173: Intro Computational Biology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module 6: Convolutional Neural Networks (CNN) for Computer Vision**\n",
    "\n",
    "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
    "\n",
    "### Module 6 Material\n",
    "\n",
    "* **Part 6.1: Using Convolutional Neural Networks** \n",
    "* Part 6.2: Using Pretrained Neural Networks with Keras \n",
    "* Part 6.3: Looking at Keras Generators and Image Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **WARNING--WARNING--WARNING**\n",
    "\n",
    "`Class_06_2` is the first lesson in our course in which we analyze **large** image datasets using a new type of neural network called a Convolutional Neural Network (CNN). \n",
    "\n",
    "Training a CNN on a large image dataset requires considerable amount of computational power. If your laptop does not have GPU support for Tensorflow, you will most likely want to run this lesson on Google COLAB. Otherwise, it might take many hours (days?) to run the examples in this lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEEW13uCtiJ",
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/master/Class_06_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKQylnEiLDUM"
   },
   "source": [
    "### Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n",
    "  Running the following code will map your GDrive to ```/content/drive```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seXFCYH4LDUM",
    "outputId": "c05015aa-871e-4779-9265-5ad07e8bf617",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Setup\n",
    "\n",
    "Run the next code cell to load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your LESSON_DIRECTORY is: C:\\Users\\David\\BIO1173\\Class_06_2\n",
      "Disk usage(total=4000108531712, used=993676402688, free=3006432129024)\n",
      "Tensorflow version = 2.10.0\n",
      "Available GPU acceleration = /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# You MUST run this code cell first\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "path = '/'\n",
    "memory = shutil.disk_usage(path)\n",
    "LESSON_DIRECTORY = os.getcwd()\n",
    "print(\"Your LESSON_DIRECTORY is: \" + LESSON_DIRECTORY)\n",
    "print(\"Disk\", memory)\n",
    "print(\"Tensorflow version =\", (tf.__version__))\n",
    "print(\"Available GPU acceleration =\", tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install new packages\n",
    "\n",
    "You will need to install the next two packages for this lesson if you are NOT using COLAB. \n",
    "\n",
    "You only need to install a package once, so you can comment out these cells after you have run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\david\\miniconda3\\envs\\tensorflow\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "if not COLAB:\n",
    "    !pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: patool in c:\\users\\david\\miniconda3\\envs\\tensorflow\\lib\\site-packages (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "if not COLAB:\n",
    "    !pip install patool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    !pip uninstall wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Operating System?\n",
    "\n",
    "In this lesson we will need to have your JupyterLab notebook create new file folders on your computer or laptop to store large image datasets. Image datasets can be **HUGE** and they can consume a large part of your available disk space. especially on a laptop. To help avoid problems, we will put all of your large datasets in a special **_temporary_** folder called `/temp`. That way, you only need to go to **one place** to delete them when you need more disk space. \n",
    "\n",
    "One complication is that some students in this course will be using Google COLAB, some will be using Windows and some students will be using MacOS. Each of these **_operating systems_* (`os`) handles files and folders somewhat differently. For example, my LESSON_DIRECTORY on my Windows machine is at:\n",
    "~~~text\n",
    "Your LESSON_DIRECTORY is: C:\\Users\\David\\BIO1173\\Class_06_2\n",
    "~~~\n",
    "while on my MacBook it's:\n",
    "~~~\n",
    "Your LESSON_DIRECTORY is: /home/david/BIO1173/Class_06_2\n",
    "~~~\n",
    "You should take special note of the **_opposite_ direction** of the slashes between folders and files. If the slashes are pointing in the wrong direction your machine, the lesson won't work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below uses the following system command `os.name` to see if you are running Windows:\n",
    "~~~text\n",
    "WINDOWS = False\n",
    "if os.name == \"nt\":\n",
    "    WINDOWS = True\n",
    "~~~\n",
    "If `os.name` is equal to the letters `nt` you are running a version of Microsoft Windows. The word `WINDOWS`, all in caps, is known as a ENVIRONMENTAL VARIABLE. If it is set to `True`, it tells Python that you are running Windows. If it is set to `False` Python knows that you are running MacOS or you are working on COLAB. \n",
    "\n",
    "But why `nt`? Microsoft introduced Windows NT on July 27, 1993. The initials `NT` stood for _New Technology_. The NT technology lives on today in Microsoft Windows 10 and 11, and as the os variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: Jupyterlab is running on a WINDOWS computer\n"
     ]
    }
   ],
   "source": [
    "# Detect Operating System\n",
    "\n",
    "# Start with WINDOWS flag set to False\n",
    "WINDOWS = False\n",
    "\n",
    "if os.name == \"nt\":\n",
    "    WINDOWS = True\n",
    "    print(\"\\nNote: Jupyterlab is running on a WINDOWS computer\")\n",
    "else:\n",
    "    print(\"\\n Note: Jupyterlab is not running on a WINDOWS computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a `/temp` folder\n",
    "\n",
    "Neural network requires a lot of file storage space. If you are working on Google COLAB, this shouldn't be too much of an issue. On the other hand, if you are working on your laptop computer, and your hard drive is nearly full, this could be a problem. \n",
    "\n",
    "As part of this lesson, a new folder will be created called `/temp`. This folder will be used to store large image datasets for the remainder of this course. Temporary folders, like `/temp` are ofter used in computer programming to store **_temporary files_**. In general, you should be able to delete **_all_** the folder(s) and files(s) in a temporary folder without causing any problems. \n",
    "\n",
    "Exactly where your `/temp` folder will be located in your filesystem is somewhat hard to predict. If you have been following the instructions given to you at the start of this course, `/temp` should be created in your course folder `/BIO1173`. On my Windows machine the location of my temporary folder is:\n",
    "~~~text\n",
    " C:\\Users\\David\\BIO1173\\temp\n",
    "~~~\n",
    "On my MacBook the folder is located at:\n",
    "~~~text\n",
    " /home/users/david/BIO1173/temp\n",
    "~~~\n",
    "When you run the code cell below, it will create your temporary folder in the **_same_** directory you were in, when you started this lesson (i.e. LESSON_DIRECTORY). It will also print out a log of _diagnostic_ info that can be used later to help troubleshoot any potential problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your temporary folder will be in: C:\\Users\\David\\BIO1173\n",
      "Note: \\temp already present in C:\\Users\\David\\BIO1173\n"
     ]
    }
   ],
   "source": [
    "# System commands to create a temporary folder called /temp\n",
    "\n",
    "# Change to LESSON_DIRECTORY\n",
    "os.chdir(LESSON_DIRECTORY)\n",
    "\n",
    "# Different commands for different Operating Systems \n",
    "if COLAB:\n",
    "    print(\"Note: Using COLAB, no /temp folder is needed\")\n",
    "elif WINDOWS: # Windows machine\n",
    "    os.chdir(\"..\\\\\")  # move up one level in directory \n",
    "    BASE_DIR = os.getcwd() # directory for temp folder\n",
    "    print(\"Your temporary folder will be in: \" + BASE_DIR)\n",
    "    try:\n",
    "        os.mkdir(\".\\\\temp\") \n",
    "        print(f\"Making \\\\temp in {BASE_DIR}\")\n",
    "    except:\n",
    "        print(f\"Note: \\\\temp already present in {BASE_DIR}\")\n",
    "    # Change back to LESSON_DIRECTORY\n",
    "    os.chdir(LESSON_DIRECTORY)\n",
    "else: # MacOS machine\n",
    "    os.chdir(\"../\") # move up one level in directory \n",
    "    BASE_DIR = os.getcwd()\n",
    "    print(\"Your temporary folder will be in: \" + BASE_DIR)\n",
    "    try:\n",
    "        os.mkdir(\"./temp\")\n",
    "        print(f\"Making /temp in in {BASE_DIR}\")\n",
    "    except:\n",
    "        print(f\"Note: /temp already present in {BASE_DIR}\")\n",
    "    # Change back to LESSON_DIRECTORY\n",
    "    os.chdir(LESSON_DIRECTORY)\n",
    "    \n",
    "#print(\"Your current working directory is : \" + os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the output the first time I ran the cell above, before a temporary folder had been created:\n",
    "~~~text\n",
    "Your temporary folder will be in: C:\\Users\\David\\BIO1173\n",
    "Making \\temp in C:\\Users\\David\\BIO1173\n",
    "~~~\n",
    "Here is the output when you run the cell after the temporary folder has already been created:\n",
    "~~~text\n",
    "Your temporary folder will be in: C:\\Users\\David\\BIO1173\n",
    "Note: \\temp already present in C:\\Users\\David\\BIO1173\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions\n",
    "\n",
    "The cell below creates the function(s) needed for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "k_xChO0Fkn4S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple function to print out elasped time\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf_otSJdmp8k"
   },
   "source": [
    "# Part 6.1: Keras Neural Networks for Digits and Medical MNIST\n",
    "\n",
    "This module will focus on computer vision. There are some important differences and similarities with previous neural networks.\n",
    "\n",
    "* We will usually use classification, though regression is still an option.\n",
    "* The input to the neural network is now 3D (height, width, _and_ color)\n",
    "* Data are not transformed; no more Z-scores or dummy variables.\n",
    "* Processing time is **_much_** longer.\n",
    "* We now have different layer types. Besides dense layers, we now have convolution layers, and max-pooling layers.\n",
    "* Data will no longer arrive as tabular data stored in CSV files, but as hundred or even thousands of **_images_**. We will take advantage of TensorFlow utilities for \"flowing\" images from folders directly to the input for a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf_otSJdmp8k"
   },
   "source": [
    "## Common Computer Vision Data Sets\n",
    "\n",
    "There are many data sets for computer vision. Two of the most popular classic datasets are the MNIST digits data set and the CIFAR image data sets. We will not use either of these datasets in this lesson, but it is important to be familiar with them since, neural network texts often refer to them.\n",
    "\n",
    "The [MNIST Digits Data Set](http://yann.lecun.com/exdb/mnist/) is very popular in the neural network research community. You can see a sample of it in Figure 6.MNIST.\n",
    "\n",
    "**Figure 6.MNIST: MNIST Data Set**\n",
    "![MNIST Data Set](https://biologicslab.co/BIO1173/images/class_8_mnist.png \"MNIST Data Set\")\n",
    "\n",
    "The MNIST Digit Data Set is a large database of handwritten digits that is commonly used for training various image processing systems. It was created by Yan LeCun, Corinna Cortes, and Christopher Burges as a benchmark for evaluating machine learning algorithms in the field of computer vision. The dataset was first released in 1998 and consists of 60,000 training images and 10,000 testing images of handwritten digits from 0 to 9.\n",
    "\n",
    "The MNIST dataset has been widely used in the research community to develop and test classification algorithms, particularly in the field of deep learning. It has become a standard benchmark for evaluating the performance of machine learning models on image recognition tasks. Despite its simplicity, the MNIST dataset remains popular due to its ease of use and ability to quickly assess the effectiveness of new algorithms.\n",
    "\n",
    "Over the years, the MNIST dataset has been used in numerous research studies and competitions, leading to the development of more advanced techniques in computer vision. It continues to be a valuable resource for researchers and practitioners in the field of machine learning.\n",
    "\n",
    "[MedMINST Data Sets](https://medmnist.com/) are a collection of 18 standardized biomedical datasets produced by a consortium of researchers at Harvard University and colaborators in Germany and China. The image sets cover a variety medical tissues and cell types including Chest X-Rays, Colon Pathology, Breast Ultrasound, Blood Cytology and Abdominal CT scans. The `RetinaMINST` dataset has 1,600 fundus camera samples (1,080 training, 120 validation, 400 test).\n",
    "\n",
    "**Figure 6.MedMNIST: RetinaMNIST Data Set**\n",
    "![RetinaMNIST](https://biologicslab.co/BIO1173/images/class_06/RetinaMNIST.jpg \"RetinaMNIST\")\n",
    "\n",
    "The [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) datasets are also frequently used by the neural network research community.\n",
    "\n",
    "**Figure 6.CIFAR: CIFAR Data Set**\n",
    "![CIFAR Data Set](https://biologicslab.co/BIO1173/images/class_8_cifar.png \"CIFAR Data Set\")\n",
    "\n",
    "The CIFAR-10 data set contains low-rez images that are divided into 10 classes. The CIFAR-100 data set contains 100 classes in a hierarchy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf_otSJdmp8k"
   },
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "The convolutional neural network (CNN) is a neural network technology that has profoundly impacted the area of computer vision (CV). Fukushima  (1980) [[Cite:fukushima1980neocognitron]](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf) introduced the original concept of a convolutional neural network, and   LeCun, Bottou, Bengio & Haffner (1998) [[Cite:lecun1995convolutional]](http://yann.lecun.com/exdb/publis/pdf/lecun-bengio-95a.pdf) greatly improved this work. From this research, Yan LeCun introduced the famous LeNet-5 neural network architecture. This chapter follows the **_LeNet-5 style_** of convolutional neural network. Although computer vision primarily uses CNNs, this technology has some applications outside of the field. You need to realize that if you want to utilize CNNs on non-visual data, you must find a way to encode your data to mimic the properties of visual data.  \n",
    "\n",
    "The order of the input array elements is _crucial_ to the training. In contrast, most neural networks that are not CNNs, treat their input data as a long vector of values. The order in which you arrange the incoming features in this vector is irrelevant. Importantly, you **can't** change the order of the data in these vectors for these types of neural networks once your network has been trained. \n",
    "\n",
    "On the other hand, the CNN network arranges the inputs into a **_grid_**. This arrangement works well with images because the pixels in closer proximity to each other are important to each other. The order of pixels in an image is significant. The human body is a relevant example of this type of order. For the design of the face, we are accustomed to eyes being near to each other. \n",
    "\n",
    "This advance in CNNs is due to years of research on biological eyes. In other words, CNNs utilize overlapping fields of input to simulate features of biological eyes. Until this breakthrough, AI had been unable to reproduce the capabilities of biological vision.\n",
    "\n",
    "Scale, rotation, and noise have presented challenges for AI computer vision research. You can observe the complexity of biological eyes in the example that follows. \n",
    "\n",
    "A friend raises a sheet of paper with a large number written on it. As your friend moves nearer to you, the number is still identifiable. In the same way, you can still identify the number when your friend rotates the paper. Lastly, your friend creates noise by drawing lines on the page, but you can still identify the number. \n",
    "\n",
    "As you can see, these examples demonstrate the high function of the biological eye and allow you to understand better the research breakthrough of CNNs. That is, this neural network can process scale, rotation, and noise in the field of computer vision. You can see this network structure in Figure 6.LENET.\n",
    "\n",
    "**Figure 6.LENET: A LeNET-5 Network (LeCun, 1998)**\n",
    "![A LeNET-5 Network](https://biologicslab.co/BIO1173/images/class_8_lenet5.png \"A LeNET-5 Network\")\n",
    "\n",
    "So far, we have only seen one layer type (dense layers). By the end of this course you will also know about:\n",
    "  \n",
    "* **Convolution Layers** - Used to scan across images. \n",
    "* **Max Pooling Layers** - Used to downsample images. \n",
    "* **Dropout Layers** - Used to add regularization. \n",
    "* **LSTM and Transformer Layers** - Used for time series data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf_otSJdmp8k"
   },
   "source": [
    "## Convolution Layers\n",
    "\n",
    "The first layer that we will examine is the convolutional layer. We will begin by looking at the hyper-parameters that you must specify for a convolutional layer in most neural network frameworks that support the CNN:\n",
    "\n",
    "* Number of filters\n",
    "* Filter Size\n",
    "* Stride\n",
    "* Padding\n",
    "* Activation Function/Non-Linearity\n",
    "\n",
    "The primary purpose of a convolutional layer is to detect features such as edges, lines, blobs of color, and other visual elements. The filters can detect these features. The more filters we give to a convolutional layer, the more features it can see.\n",
    "\n",
    "A filter is a square-shaped object that scans over the image. A grid can represent the individual pixels of a grid. You can think of the convolutional layer as a smaller grid that sweeps left to right over each image row. There is also a hyperparameter that specifies both the width and height of the square-shaped filter. The following figure shows this configuration in which you see the six convolutional filters sweeping over the image grid:\n",
    "\n",
    "A convolutional layer has weights between it and the previous layer or image grid. Each pixel on each convolutional layer is a weight. Therefore, the number of weights between a convolutional layer and its predecessor layer or image field is the following:\n",
    "\n",
    "```\n",
    "[FilterSize] * [FilterSize] * [# of Filters]\n",
    "```\n",
    "\n",
    "For example, if the filter size were 5 (5x5) for 10 filters, there would be 250 weights.\n",
    "\n",
    "You need to understand how the convolutional filters sweep across the previous layer's output or image grid. Figure 6.CNN illustrates the sweep:\n",
    "\n",
    "**Figure 6.CNN: Convolutional Neural Network**\n",
    "![Convolutional Neural Network](https://biologicslab.co/BIO1173/images/class_8_cnn_grid.png \"Convolutional Neural Network\")\n",
    "\n",
    "The above figure shows a convolutional filter with 4 and a padding size of 1. The **_padding size_** is responsible for the border of zeros in the area that the filter sweeps. Even though the image is 8x7, the extra padding provides a virtual image size of 9x8 for the filter to sweep across. The **_stride_** specifies the number of positions the convolutional filters will stop. The convolutional filters move to the right, advancing by the number of cells specified in the stride. Once you reach the far right, the convolutional filter moves back to the far left; then, it moves down by the stride amount and continues to the right again.\n",
    "\n",
    "Some constraints exist concerning the size of the stride. The stride cannot be `0`. The convolutional filter would never move if you set the stride to `0`. Furthermore, neither the stride nor the convolutional filter size can be larger than the previous grid. There are additional constraints on the stride (*s*), padding (*p*), and the filter width (*f*) for an image of width (*w*). Specifically, the convolutional filter must be able to start at the far left or top border, move a certain number of strides, and land on the far right or bottom border. The following equation shows the number of steps a convolutional operator\n",
    "must take to cross the image:\n",
    "\n",
    "$$ steps = \\frac{w - f + 2p}{s}+1 $$\n",
    "\n",
    "The number of steps must be an integer. In other words, it cannot have decimal places. The purpose of the padding (*p*) is to be adjusted to make this equation become an integer value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf_otSJdmp8k"
   },
   "source": [
    "## Max Pooling Layers\n",
    "\n",
    "Max-pool layers downsample a 3D box to a new one with smaller dimensions. Typically, you can always place a max-pool layer immediately following the convolutional layer. The LENET shows the max-pool layer immediately after layers C1 and C3. These max-pool layers progressively decrease the size of the dimensions of the 3D boxes passing through them. This technique can avoid overfitting (Krizhevsky, Sutskever & Hinton, 2012).\n",
    "\n",
    "A pooling layer has the following hyper-parameters:\n",
    "\n",
    "* Spatial Extent (*f*)\n",
    "* Stride (*s*)\n",
    "\n",
    "Unlike convolutional layers, max-pool layers do not use padding. Additionally, max-pool layers have no weights, so training does not affect them. These layers downsample their 3D box input. The 3D box output by a max-pool layer will have a width equal to this equation:\n",
    "\n",
    "$$ w_2 = \\frac{w_1 - f}{s} + 1 $$\n",
    "\n",
    "The height of the 3D box produced by the max-pool layer is calculated similarly with this equation:\n",
    "\n",
    "$$ h_2 = \\frac{h_1 - f}{s} + 1 $$\n",
    "\n",
    "The depth of the 3D box produced by the max-pool layer is equal to the depth the 3D box received as input. The most common setting for the hyper-parameters of a max-pool layer is f=2 and s=2. The spatial extent (f) specifies that boxes of 2x2 will be scaled down to single pixels. Of these four pixels, the pixel with the maximum value will represent the 2x2 pixel in the new grid. Because squares of size 4 are replaced with size 1, 75% of the pixel information is lost. The following figure shows this transformation as a 6x6 grid becomes a 3x3:\n",
    "\n",
    "**Figure 6.MAXPOOL: Max Pooling Layer**\n",
    "![Max Pooling Layer](https://biologicslab.co/BIO1173/images/class_8_conv_maxpool.png \"Max Pooling Layer\")\n",
    "\n",
    "Of course, the above diagram shows each pixel as a single number. A grayscale image would have this characteristic. We usually take the average of the three numbers for an RGB image to determine which pixel has the maximum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "## Supervised _vs_ Unsupervised Machine Learning\n",
    "\n",
    "In **_supervised machine learning_**, the algorithm is trained on a _labeled_ dataset, where each training example is paired with the correct output. The goal is to learn a mapping from input features to the corresponding output labels. During training, the algorithm adjusts its parameters to minimize the difference between the predicted output and the true label. Once the model is trained, it can make predictions on new, unseen data by applying the learned mapping. Common supervised learning tasks include classification and regression.\n",
    "\n",
    "On the other hand, **_unsupervised machine learning_** involves training the algorithm on an _unlabeled_ dataset, where the algorithm must find patterns or relationships in the data without explicit guidance. The goal of unsupervised learning is to discover hidden structures or clusters in the data. This type of learning is often used for tasks such as clustering, anomaly detection, and dimensionality reduction. Unlike supervised learning, there are no explicit output labels to guide the learning process in unsupervised learning.\n",
    "\n",
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf_otSJdmp8k"
   },
   "source": [
    "## **Regression Convolutional Neural Networks**\n",
    "\n",
    "We will now look at two examples, one for regression and another for classification. For **_supervised_** computer vision, your dataset will need some labels. For classification, this label usually specifies _what_ the image is a picture of, e.g., dog, cat, carcinoma, etc. For regression, this \"label\" is some _numeric_ quantity the image should produce, such as a count, e.g. how many white blood cells. We will look at two different means of providing this label.\n",
    "\n",
    "The first example will show how to handle regression with convolution neural networks. We will provide an image and expect the neural network to count items in that image. We will use a [dataset](https://www.kaggle.com/jeffheaton/count-the-paperclips) created by [Jeff Heaton](https://www.heatonresearch.com/) that contains a random number of paperclips. \n",
    "\n",
    "Here are three sample images from the 25,000 images in the paperclips dataset:\n",
    "\n",
    "![Paperclips Sample](https://biologicslab.co/BIO1173/images/class_06/class_06_2_paperclips.png \"Paperclips Sample\")\n",
    "\n",
    "Our goal will be to create a convolutional neural network (CNN) that can _count_ the number of paperclips in an image. To put in a more ecological or biomedical context, a similar neural network could also be trained to count the number of giant Saguaro cacti (_Carnegiea gigantea_) in an image of the Sonoran Desert, or the number of leucocytes in a blood smear from a patient with symptoms of AML (Acute myeloid leukemia).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup ENVIRONMENTAL VARIABLES\n",
    "\n",
    "The code in the cell below creates the ENVIRONMENTAL VARIABLES that are needed to download a zip file from the course HTTPS server, save it to your `/temp` folder and then extract (unzip) it into a new folder called `/clips`. As above, different ENVIRONMENTAL VARIABLES are needed for the different Operating Systems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GpfadrdQcVg8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOAD_SOURCE= https://biologicslab.co/BIO1173/data/paperclips.zip\n",
      "DOWNLOAD_FILE= paperclips.zip\n",
      "PATH= C:\\Users\\David\\BIO1173\\Class_06_2\n",
      "EXTRACT_FOLDER_IN= C:\\Users\\David\\BIO1173\\temp\\\n",
      "EXTRACT_FOLDER_OUT= C:\\Users\\David\\BIO1173\\temp\\clips\\\n",
      "Note: Using WINDOWS\n",
      "SOURCE= C:\\Users\\David\\BIO1173\\temp\\paperclips.zip\n",
      "DOWNLOAD_FILE= paperclips.zip\n"
     ]
    }
   ],
   "source": [
    "# Set ENVIRONMENTAL VARIABLES\n",
    "\n",
    "URL = \"https://biologicslab.co/BIO1173/data/\"\n",
    "DOWNLOAD_SOURCE = URL+\"paperclips.zip\"\n",
    "DOWNLOAD_FILE = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfind('/')+1:]\n",
    "print(\"DOWNLOAD_SOURCE=\",DOWNLOAD_SOURCE)\n",
    "print(\"DOWNLOAD_FILE=\",DOWNLOAD_FILE)\n",
    "\n",
    "if COLAB:\n",
    "    PATH = \"/content\"\n",
    "    EXTRACT_TARGET = os.path.join(PATH,\"clips\")\n",
    "    SOURCE = os.path.join(EXTRACT_TARGET, \"paperclips\")\n",
    "    print(\"Note: You should NOT run this Lesson on COLAB.\")\n",
    "elif WINDOWS:\n",
    "    PATH=LESSON_DIRECTORY\n",
    "    print(\"PATH=\",PATH)\n",
    "    EXTRACT_FOLDER_IN=BASE_DIR+\"\\\\temp\\\\\"\n",
    "    print(\"EXTRACT_FOLDER_IN=\",EXTRACT_FOLDER_IN)\n",
    "    EXTRACT_FOLDER_OUT=EXTRACT_FOLDER_IN+\"clips\\\\\"\n",
    "    print(\"EXTRACT_FOLDER_OUT=\",EXTRACT_FOLDER_OUT)\n",
    "    SOURCE = os.path.join(EXTRACT_FOLDER_IN, \"paperclips.zip\")\n",
    "    print(\"Note: Using WINDOWS\")\n",
    "    print(\"SOURCE=\",SOURCE)\n",
    "    print(\"DOWNLOAD_FILE=\",DOWNLOAD_FILE)\n",
    "else:\n",
    "    PATH=LESSON_DIRECTORY\n",
    "    print(\"PATH=\",PATH)\n",
    "    EXTRACT_FOLDER_IN=BASE_DIR+\"/temp/\"\n",
    "    print(\"EXTRACT_FOLDER_IN=\",EXTRACT_FOLDER_IN)\n",
    "    EXTRACT_FOLDER_OUT=EXTRACT_FOLDER_IN+\"clips/\"\n",
    "    print(\"EXTRACT_FOLDER_OUT=\",EXTRACT_FOLDER_OUT)\n",
    "    SOURCE = os.path.join(EXTRACT_FOLDER_IN, \"paperclips.zip\")\n",
    "    print(\"Note: Using OTHER (MacOS?)\")\n",
    "    print(\"SOURCE=\",SOURCE)\n",
    "    print(\"DOWNLOAD_FILE=\",DOWNLOAD_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the output from the cell when run on a Windows machine:\n",
    "~~~text\n",
    "DOWNLOAD_SOURCE= https://biologicslab.co/BIO1173/data/paperclips.zip\n",
    "DOWNLOAD_FILE= paperclips.zip\n",
    "PATH= C:\\Users\\David\\BIO1173\\Class_06_2\n",
    "EXTRACT_FOLDER_IN= C:\\Users\\David\\BIO1173\\temp\\\n",
    "EXTRACT_FOLDER_OUT= C:\\Users\\David\\BIO1173\\temp\\clips\\\n",
    "Note: Using WINDOWS\n",
    "SOURCE= C:\\Users\\David\\BIO1173\\temp\\paperclips.zip\n",
    "DOWNLOAD_FILE= paperclips.zip\n",
    "~~~\n",
    "It has been printed out in case there is a problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivCHAirHpNyT"
   },
   "source": [
    "### Download and Extract the image data\n",
    "\n",
    "Now that the ENVIRONMENTAL VARIABLES have been defined, we can go ahead and download the datafile. The code in the cell below uses a program called `wget()` to download the datafile `paperclips.zip` from the course HTTPS server, `https://biologicslab.co/BIO1173/data`. The code chunk for downloading the zip file on a WINDOWS machine is:\n",
    "~~~text\n",
    "!python -m wget {DOWNLOAD_SOURCE} -o {SOURCE}\n",
    "~~~\n",
    "The code places the zip file in the `/temp` folder and creates a new folder inside of `/temp` called `/clips`. \n",
    "\n",
    "In the next step, a program called `patoolib()` extracts the zip file:\n",
    "~~~text\n",
    "patoolib.extract_archive(SOURCE,outdir=EXTRACT_FOLDER_OUT)\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CExT2Z6gpAhz",
    "outputId": "44069f67-c041-45da-a4ab-67f5c135c1df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: 'paperclips.zip' already downloaded. C:\\Users\\David\\BIO1173\\temp\\paperclips.zip\n",
      "patool: Extracting C:\\Users\\David\\BIO1173\\temp\\paperclips.zip ...\n",
      "patool: running \"C:\\Program Files\\7-Zip\\7z.EXE\" x -oC:\\Users\\David\\BIO1173\\temp\\clips\\ -- C:\\Users\\David\\BIO1173\\temp\\paperclips.zip\n",
      "patool:     with creationflags=134217728\n",
      "Note: File already extracted\n"
     ]
    }
   ],
   "source": [
    "# Download and extract the image data\n",
    "\n",
    "if COLAB:\n",
    "    !wget -O {os.path.join(PATH,DOWNLOAD_NAME)} {DOWNLOAD_SOURCE}\n",
    "    !mkdir -p {SOURCE}\n",
    "    !mkdir -p {TARGET}\n",
    "    !mkdir -p {EXTRACT_TARGET}\n",
    "    !unzip -o -j -d {SOURCE} {os.path.join(PATH, DOWNLOAD_NAME)} >/dev/null\n",
    "elif WINDOWS:\n",
    "    import patoolib\n",
    "    import wget\n",
    "    DATA_FOLDER=EXTRACT_FOLDER_OUT+\"\\\\paperclips\\\\\"\n",
    "    if os.path.isfile(SOURCE):\n",
    "        print(\"Note: 'paperclips.zip' already downloaded.\", SOURCE)\n",
    "    else:\n",
    "        !python -m wget {DOWNLOAD_SOURCE} -o {SOURCE}\n",
    "    try:\n",
    "        patoolib.extract_archive(SOURCE,outdir=EXTRACT_FOLDER_OUT)\n",
    "    except:\n",
    "        print(\"Note: File already extracted\")\n",
    "else: # MacOS\n",
    "    import patoolib\n",
    "    import wget\n",
    "    DATA_FOLDER=EXTRACT_FOLDER_OUT+\"/paperclips/\"\n",
    "    if os.path.isfile(SOURCE):\n",
    "        print(\"Note: 'paperclips.zip' already downloaded.\", SOURCE)\n",
    "    else:\n",
    "        !wget -v {DOWNLOAD_SOURCE} --output-document={SOURCE}\n",
    "        try:\n",
    "            patoolib.extract_archive(SOURCE,outdir=EXTRACT_FOLDER_OUT)\n",
    "        except:\n",
    "            print(\"Note: File already extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my Windows machine, I recieved the following output when I first ran the code cell above:\n",
    "~~~text\n",
    "Saved under C:\\Users\\David\\BIO1173\\temp\\paperclips.zip\n",
    "patool: Extracting C:\\Users\\David\\BIO1173\\temp\\paperclips.zip ...\n",
    "patool: ... creating output directory `C:\\Users\\David\\BIO1173\\temp\\clips\\'.\n",
    "patool: running \"C:\\Program Files\\7-Zip\\7z.EXE\" x -oC:\\Users\\David\\BIO1173\\temp\\clips\\ -- C:\\Users\\David\\BIO1173\\temp\\paperclips.zip\n",
    "patool:     with creationflags=134217728\n",
    "patool: ... C:\\Users\\David\\BIO1173\\temp\\paperclips.zip extracted to `C:\\Users\\David\\BIO1173\\temp\\clips\\'.\n",
    "~~~\n",
    "\n",
    "If you try to run the code cell above more than once, it _should_ detect that you have already run it and not try to run it a second time:\n",
    "~~~text\n",
    "Note: 'paperclips.zip' already downloaded. C:\\Users\\David\\BIO1173\\temp\\paperclips.zip\n",
    "patool: Extracting C:\\Users\\David\\BIO1173\\temp\\paperclips.zip ...\n",
    "patool: running \"C:\\Program Files\\7-Zip\\7z.EXE\" x -oC:\\Users\\David\\BIO1173\\temp\\clips\\ -- C:\\Users\\David\\BIO1173\\temp\\paperclips.zip\n",
    "patool:     with creationflags=134217728\n",
    "Note: File already extracted\n",
    "~~~\n",
    "\n",
    "However, your mileage may vary..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4LWl8TzzI8a"
   },
   "source": [
    "### Supervised Machine Learning\n",
    "\n",
    "As mentioned previously, image analysis with a CNN is an example of _supervised_ learning. In this type of learning, the CNN always has to know the **_correct answer_** to the problem it is trying to solve. Why? How else would the CNN know whether the predictions it makes at end of each epoch are correct or incorrect? After all, if the CNN's predictions are 100% accurate, it means its synaptic weights are perfect and no more training is necessay. On the other hand, if the predictions are less than perfect, the CNNs still needs adjust its connection weights to improve its accuracy.  \n",
    "\n",
    "For this particular example, our CNN will need to know the **_actual_** number of paperclips shown in each image in the training set. These numbers, i.e. the number of paperclips, are called the **_labels_**. Image datasets for CNNs always come with labels and the Paperclip image set is not exception. When the `paperclips.zip` file was extracted, it automatically created two folders inside of the `/clips` folder. One folder, called `/paperclips`, contains 25,000 images of paperclips (see picture above). The other folder is called `/_MACOSX`. (Jeff Heaton apparently created his `paperclips.zip` file on an Apple computer). Inside the `/_MACOSX` folder is another folder called `/paperclips` and finally, inside this folder, are two CSV files called `_test.csv` and `_train.csv` that contain the labels (number of paperclips). \n",
    "\n",
    "The labels are contained in a CSV file named **train.csv** for regression. This file has just two labels, **id** and **clip_count**. The ID specifies the filename; for example, row id 1 corresponds to the file **clips-1.jpg**. \n",
    "\n",
    "The following code loads the labels for the training set and creates a new column, named **filename**, that contains the filename of each image, based on the **id** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "w4W4LeGSqYya",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the labels for the training set\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if COLAB:\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(SOURCE,\"train.csv\"), \n",
    "        na_values=['NA', '?'])\n",
    "    df['filename']=\"clips-\"+df[\"id\"].astype(str)+\".jpg\"\n",
    "elif WINDOWS:\n",
    "    df = pd.read_csv(\n",
    "    os.path.join(DATA_FOLDER,\"train.csv\"), \n",
    "    na_values=['NA', '?'])\n",
    "    df['filename']=\"clips-\"+df[\"id\"].astype(str)+\".jpg\"\n",
    "else:\n",
    "    df = pd.read_csv(\n",
    "    os.path.join(DATA_FOLDER,\"train.csv\"), \n",
    "    na_values=['NA', '?'])\n",
    "    df['filename']=\"clips-\"+df[\"id\"].astype(str)+\".jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGxjITNtz0sC"
   },
   "source": [
    "This results in the following dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "rJpaMpmfswIp",
    "outputId": "c6462003-f016-4f0c-b8a8-81f276b9b3f4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clip_count</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30001</td>\n",
       "      <td>11</td>\n",
       "      <td>clips-30001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30002</td>\n",
       "      <td>2</td>\n",
       "      <td>clips-30002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30003</td>\n",
       "      <td>26</td>\n",
       "      <td>clips-30003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30004</td>\n",
       "      <td>41</td>\n",
       "      <td>clips-30004.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30005</td>\n",
       "      <td>49</td>\n",
       "      <td>clips-30005.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>49996</td>\n",
       "      <td>35</td>\n",
       "      <td>clips-49996.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>49997</td>\n",
       "      <td>54</td>\n",
       "      <td>clips-49997.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>49998</td>\n",
       "      <td>72</td>\n",
       "      <td>clips-49998.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>49999</td>\n",
       "      <td>24</td>\n",
       "      <td>clips-49999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>50000</td>\n",
       "      <td>35</td>\n",
       "      <td>clips-50000.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  clip_count         filename\n",
       "0      30001          11  clips-30001.jpg\n",
       "1      30002           2  clips-30002.jpg\n",
       "2      30003          26  clips-30003.jpg\n",
       "3      30004          41  clips-30004.jpg\n",
       "4      30005          49  clips-30005.jpg\n",
       "...      ...         ...              ...\n",
       "19995  49996          35  clips-49996.jpg\n",
       "19996  49997          54  clips-49997.jpg\n",
       "19997  49998          72  clips-49998.jpg\n",
       "19998  49999          24  clips-49999.jpg\n",
       "19999  50000          35  clips-50000.jpg\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following table:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_df.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nN1AsQeysmGd"
   },
   "source": [
    "### Create training and validation sets\n",
    "\n",
    "The next step is to separate the data (paperclip images) into a **_training_** set and a **_validation_** set. We will use the validation set for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CI4_qNaSqp31",
    "outputId": "abe450db-ce00-44ac-cbbd-166912687df4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 18000\n",
      "Validate size: 2000\n"
     ]
    }
   ],
   "source": [
    "# Split images into training and validation sets\n",
    "\n",
    "TRAIN_PCT = 0.9\n",
    "TRAIN_CUT = int(len(df) * TRAIN_PCT)\n",
    "\n",
    "df_train = df[0:TRAIN_CUT]\n",
    "df_validate = df[TRAIN_CUT:]\n",
    "\n",
    "print(f\"Training size: {len(df_train)}\")\n",
    "print(f\"Validate size: {len(df_validate)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "~~~text\n",
    "Training size: 18000\n",
    "Validate size: 2000\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNoqk5uIz6FW"
   },
   "source": [
    "### Create **_ImageDataGenerator_** objects\n",
    "\n",
    "CNNs require a large amount of diverse data to learn features that generalize well to new, unseen images. By training on a large dataset, the model can learn a wide range of patterns and variations in the data, leading to better generalization performance. Large datasets provide enough training examples for the model to adjust these parameters effectively and learn complex patterns in the images.\n",
    "\n",
    "One clever way to increase the size of an image dataset, **_without_** downloading additional image files, is to use an **_ImageDataGenerator object_**. An ImageDataGenerator is basically a computer algorithmn that creates additional training data by **_manipulating_** the source material. For example, the generator below flips the paperclip images both vertically and horizontally. After all, you could recognize a picture of your grandmother, if it was flipped upside down, couldn't you?\n",
    "\n",
    "Keras will train the neuron network both on the original images and the flipped images. This augmentation increases the size of the training data considerably. Module 6.4 goes deeper into the transformations you can perform. For example, you can also specify a target size to resize the images automatically.\n",
    "\n",
    "The function **flow_from_dataframe** loads the labels from a Pandas dataframe connected to our **train.csv** file. When we demonstrate classification, we will use the **flow_from_directory**; which loads the labels from the directory structure rather than a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZzeAHdfsy0O",
    "outputId": "c6741a3f-bf89-4e9e-a730-6559134fcc98",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18000 validated image filenames.\n",
      "Found 2000 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "  rescale = 1./255,\n",
    "  horizontal_flip=True,\n",
    "  vertical_flip=True,\n",
    "  fill_mode='nearest')\n",
    "\n",
    "train_generator = training_datagen.flow_from_dataframe(\n",
    "        dataframe=df_train,\n",
    "        directory=DATA_FOLDER,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"clip_count\",\n",
    "        target_size=(256, 256),\n",
    "        batch_size=32,\n",
    "        class_mode='other')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "val_generator = validation_datagen.flow_from_dataframe(\n",
    "        dataframe=df_validate,\n",
    "        directory=DATA_FOLDER,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"clip_count\",\n",
    "        target_size=(256, 256),\n",
    "        class_mode='other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "~~~text\n",
    "Found 18000 validated image filenames.\n",
    "Found 2000 validated image filenames.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GVeYo6p2sdG"
   },
   "source": [
    "We can now train the neural network. The code to build and train the neural network is not that different than in the previous modules. We will use the Keras Sequential class to provide layers to the neural network. We now have several new layer types that we did not previously see.\n",
    "\n",
    "* **Conv2D** - The convolution layers.\n",
    "* **MaxPooling2D** - The max-pooling layers.\n",
    "* **Flatten** - Flatten the 2D (and higher) tensors to allow a Dense layer to process.\n",
    "* **Dense** - Dense layers, the same as demonstrated previously. Dense layers often form the final output layers of the neural network.\n",
    "\n",
    "The training code is very similar to previously. This code is for regression, so a final linear activation is used, along with mean_squared_error for the loss function. The generator provides both the *x* and *y* matrixes we previously supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBHlqCIgtWSq",
    "outputId": "89e48c26-3776-41fa-bff0-bb2b6a39d5d8",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 254, 254, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 127, 127, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 125, 125, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 246016)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               125960704 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 125,999,937\n",
      "Trainable params: 125,999,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Starting training for 15 epochs...\n",
      "Epoch 1/15\n",
      "563/563 [==============================] - 31s 54ms/step - loss: 186.3797 - val_loss: 14.6039\n",
      "Epoch 2/15\n",
      "563/563 [==============================] - 30s 53ms/step - loss: 21.1749 - val_loss: 14.2476\n",
      "Epoch 3/15\n",
      "563/563 [==============================] - 30s 53ms/step - loss: 17.2136 - val_loss: 11.5300\n",
      "Epoch 4/15\n",
      "563/563 [==============================] - 31s 54ms/step - loss: 15.1813 - val_loss: 15.1526\n",
      "Epoch 5/15\n",
      "563/563 [==============================] - 31s 55ms/step - loss: 16.5928 - val_loss: 10.0593\n",
      "Epoch 6/15\n",
      "563/563 [==============================] - 31s 55ms/step - loss: 13.2247 - val_loss: 13.4466\n",
      "Epoch 7/15\n",
      "563/563 [==============================] - 30s 54ms/step - loss: 10.7787 - val_loss: 15.3780\n",
      "Epoch 8/15\n",
      "563/563 [==============================] - 30s 54ms/step - loss: 8.7031 - val_loss: 12.1841\n",
      "Epoch 9/15\n",
      "563/563 [==============================] - 30s 54ms/step - loss: 8.1496 - val_loss: 11.3064\n",
      "Epoch 10/15\n",
      "563/563 [==============================] - 31s 54ms/step - loss: 5.7299 - val_loss: 4.8975\n",
      "Epoch 11/15\n",
      "563/563 [==============================] - 30s 53ms/step - loss: 184.8747 - val_loss: 16.9398\n",
      "Epoch 12/15\n",
      "563/563 [==============================] - 30s 54ms/step - loss: 19.1524 - val_loss: 29.6939\n",
      "Epoch 13/15\n",
      "563/563 [==============================] - 30s 54ms/step - loss: 18.2561 - val_loss: 15.4656\n",
      "Epoch 14/15\n",
      "563/563 [==============================] - 31s 54ms/step - loss: 17.1838 - val_loss: 31.2027\n",
      "Epoch 15/15\n",
      "563/563 [==============================] - ETA: 0s - loss: 13.0535Restoring model weights from the end of the best epoch: 10.\n",
      "563/563 [==============================] - 31s 54ms/step - loss: 13.0535 - val_loss: 10.7387\n",
      "Epoch 15: early stopping\n",
      "Elapsed time: 0:07:38.04\n"
     ]
    }
   ],
   "source": [
    "# Create ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "\n",
    "# Set variables\n",
    "EPOCHS=15  # Use 25 if possible\n",
    "MODEL_STEPS= 250\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 150x150 \n",
    "    # with 3 bytes color.\n",
    "    \n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', \n",
    "        input_shape=(256, 256, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Set model steps\n",
    "epoch_steps = MODEL_STEPS # needed for 2.2\n",
    "validation_steps = len(df_validate)\n",
    "model.compile(loss = 'mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "        patience=5, verbose=1, mode='auto',\n",
    "        restore_best_weights=True)\n",
    "\n",
    "# Record start\n",
    "start_time = time.time()\n",
    "\n",
    "# Train model\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "history = model.fit(train_generator,  \n",
    "  verbose = 1, \n",
    "  validation_data=val_generator, callbacks=[monitor], epochs=EPOCHS)\n",
    "\n",
    "# Print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will run very slowly, even if you use a GPU. The code takes approximately 13 minutes to run 25 epochs with a GPU. To speed up this lesson, the number of epochs has been reduced to only 15. On my Windows machine, it took between 7-8 minutes to run 15 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNiELOX53PtU"
   },
   "source": [
    "\n",
    "\n",
    "## Score Regression Image Data\n",
    "\n",
    "Scoring/predicting from a generator is a bit different than training. We do not want augmented images, and we do not wish to have the dataset shuffled. For scoring, we want a prediction for each input. We construct the generator as follows:\n",
    "\n",
    "* shuffle=False\n",
    "* batch_size=1\n",
    "* class_mode=None\n",
    "\n",
    "We use a **batch_size** of 1 to guarantee that we do not run out of GPU memory if our prediction set is large. You can increase this value for better performance. The **class_mode** is None because there is no *y*, or label. After all, we are predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFXvtr-NtlQt",
    "outputId": "01c75325-c863-4fdd-b6a2-22f9b397ec69",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# Score Regressuin Image Data\n",
    "\n",
    "# Set variables\n",
    "BATCH_SIZE=1\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    os.path.join(DATA_FOLDER,\"test.csv\"), \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "df_test['filename']=\"clips-\"+df_test[\"id\"].astype(str)+\".jpg\"\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_generator = validation_datagen.flow_from_dataframe(\n",
    "        dataframe=df_test,\n",
    "        directory=DATA_FOLDER,\n",
    "        x_col=\"filename\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        target_size=(256, 256),\n",
    "        class_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "~~~text\n",
    "Found 5000 validated image filenames.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOuNIlpLAF5A"
   },
   "source": [
    "We need to reset the generator to ensure we are always at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gS0CjJ4bt8jQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred = model.predict(test_generator,steps=len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "~~~text\n",
    "5000/5000 [==============================] - 26s 5ms/step\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate a CSV file to hold the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4n-t8k5bt_nG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submit = pd.DataFrame({'id':df_test['id'],'clip_count':pred.flatten()})\n",
    "df_submit.to_csv(os.path.join(PATH,\"submit.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following table:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_df2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sicFJd8u5c3v"
   },
   "source": [
    "## **Classification Neural Networks**\n",
    "\n",
    "In this example, we will build a convolutional neural network (CNN) to analyze images of 3 different species of Iris flowers:\n",
    "* iris-setosa\n",
    "* iris-versicolour\n",
    "* iris-virginica\n",
    "\n",
    "In other words, this is an example of a **_classification_** CNN. \n",
    "\n",
    "We have use the famous Iris flower dataset before.  However, when we use the Iris flower dataset previously, we used **_numeric measurements_** of the flower's sepals' and petals' lengths and widths, that were presented in tabular form (i.e. a DataFrame) in a CSV file. \n",
    "\n",
    "In this lesson we will use hundreds of photographic images taken of Iris flowers from the three different species. Here is random example showing the the type of images in the dataset from each species: \n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_06/iris_species.png)\n",
    "\n",
    "As above, we will setup an **_ImageDataGenerator_** to grab the images from a file location and feed them into our CNN along with their labels (species name).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environmental Variables\n",
    "\n",
    "The code in the section below sets ups ENVIRONMENTAL VARIABLES. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "GpfadrdQcVg8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOAD_SOURCE= https://biologicslab.co/BIO1173/data//iris-image.zip\n",
      "DOWNLOAD_FILE= iris-image.zip\n",
      "PATH= C:\\Users\\David\\BIO1173\\Class_06_2\n",
      "EXTRACT_FOLDER_IN= C:\\Users\\David\\BIO1173\\temp\\\n",
      "EXTRACT_FOLDER_OUT= C:\\Users\\David\\BIO1173\\temp\\iris-images\\\n",
      "Note: Using WINDOWS\n",
      "SOURCE= C:\\Users\\David\\BIO1173\\temp\\iris-images.zip\n",
      "DOWNLOAD_FILE= iris-image.zip\n"
     ]
    }
   ],
   "source": [
    "# Setuo Environmental Variables\n",
    "import os\n",
    "PATH=True\n",
    "\n",
    "URL = \"https://biologicslab.co/BIO1173/data/\"\n",
    "DOWNLOAD_SOURCE = URL+\"/iris-image.zip\"\n",
    "DOWNLOAD_FILE = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfind('/')+1:]\n",
    "print(\"DOWNLOAD_SOURCE=\",DOWNLOAD_SOURCE)\n",
    "print(\"DOWNLOAD_FILE=\",DOWNLOAD_FILE)\n",
    "\n",
    "if COLAB:\n",
    "    PATH = \"/content\"\n",
    "    EXTRACT_TARGET = os.path.join(PATH,\"iris-images\")\n",
    "    SOURCE = os.path.join(EXTRACT_TARGET, \"iris-images\")\n",
    "    print(\"Note: Using COLAB\")\n",
    "elif WINDOWS:\n",
    "    PATH=LESSON_DIRECTORY\n",
    "    print(\"PATH=\",PATH)\n",
    "    EXTRACT_FOLDER_IN=BASE_DIR+\"\\\\temp\\\\\"\n",
    "    print(\"EXTRACT_FOLDER_IN=\",EXTRACT_FOLDER_IN)\n",
    "    EXTRACT_FOLDER_OUT=EXTRACT_FOLDER_IN+\"iris-images\\\\\"\n",
    "    print(\"EXTRACT_FOLDER_OUT=\",EXTRACT_FOLDER_OUT)\n",
    "    SOURCE = os.path.join(EXTRACT_FOLDER_IN, \"iris-images.zip\")\n",
    "    print(\"Note: Using WINDOWS\")\n",
    "    print(\"SOURCE=\",SOURCE)\n",
    "    print(\"DOWNLOAD_FILE=\",DOWNLOAD_FILE)\n",
    "else:\n",
    "    PATH=LESSON_DIRECTORY\n",
    "    print(\"PATH=\",PATH)\n",
    "    EXTRACT_FOLDER_IN=BASE_DIR+\"/temp/\"\n",
    "    print(\"EXTRACT_FOLDER_IN=\",EXTRACT_FOLDER_IN)\n",
    "    EXTRACT_FOLDER_OUT=EXTRACT_FOLDER_IN+\"iris-images/\"\n",
    "    print(\"EXTRACT_FOLDER_OUT=\",EXTRACT_FOLDER_OUT)\n",
    "    SOURCE = os.path.join(EXTRACT_FOLDER_IN, \"iris-images.zip\")\n",
    "    print(\"Note: Using OTHER (MacOS?)\")\n",
    "    print(\"SOURCE=\",SOURCE)\n",
    "    print(\"DOWNLOAD_FILE=\",DOWNLOAD_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hset2s3P9MFV"
   },
   "source": [
    "## Download and Extract Images\n",
    "\n",
    "Just as before, we will download a zip file containing the Iris images and then extract (unzip) the image data into the appropiate folders using the ENVIRONMENTAL VARIABLES defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CExT2Z6gpAhz",
    "outputId": "44069f67-c041-45da-a4ab-67f5c135c1df",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under C:\\Users\\David\\BIO1173\\temp\\iris-images.zip\n",
      "In WINDOWS\n",
      "patool: Extracting C:\\Users\\David\\BIO1173\\temp\\iris-images.zip ...\n",
      "patool: ... creating output directory `C:\\Users\\David\\BIO1173\\temp\\iris-images\\'.\n",
      "patool: running \"C:\\Program Files\\7-Zip\\7z.EXE\" x -oC:\\Users\\David\\BIO1173\\temp\\iris-images\\ -- C:\\Users\\David\\BIO1173\\temp\\iris-images.zip\n",
      "patool:     with creationflags=134217728\n",
      "patool: ... C:\\Users\\David\\BIO1173\\temp\\iris-images.zip extracted to `C:\\Users\\David\\BIO1173\\temp\\iris-images\\'.\n"
     ]
    }
   ],
   "source": [
    "# Download and extract the image data\n",
    "\n",
    "if COLAB:\n",
    "    print(\"PATH=\",PATH, \"DOWNLOAD_FILE=\",DOWNLOAD_FILE, \"DOWNLOAD_SOURCE=\",DOWNLOAD_SOURCE)\n",
    "    !wget -O {os.path.join(PATH,DOWNLOAD_FILE)} {DOWNLOAD_SOURCE}\n",
    "    !mkdir -p {SOURCE}\n",
    "    !mkdir -p {TARGET}\n",
    "    !mkdir -p {EXTRACT_TARGET}\n",
    "    !unzip -o -j -d {SOURCE} {os.path.join(PATH, DOWNLOAD_NAME)} >/dev/null\n",
    "\n",
    "elif WINDOWS:\n",
    "    import patoolib\n",
    "    import wget\n",
    "    if os.path.isfile(SOURCE):\n",
    "        print(\"SOURCE already exists.\", SOURCE)\n",
    "    else:\n",
    "        !python -m wget {DOWNLOAD_SOURCE} -o {SOURCE}\n",
    "        print(\"In WINDOWS\")\n",
    "    try:\n",
    "        patoolib.extract_archive(SOURCE,outdir=EXTRACT_FOLDER_OUT)\n",
    "    except:\n",
    "        print(\"Note: File already extracted\")\n",
    "        \n",
    "    DATA_FOLDER=EXTRACT_FOLDER_OUT          # +\"\\\\iris\\\\\"\n",
    "\n",
    "else:\n",
    "    import patoolib\n",
    "    import wget\n",
    "    DATA_FOLDER=EXTRACT_FOLDER_OUT          #+\"/iris/\"\n",
    "    if os.path.isfile(SOURCE):\n",
    "        print(\"SOURCE already exists.\", SOURCE)\n",
    "    else:\n",
    "        print(\"EXTRACT_FOLDER_IN=\",EXTRACT_FOLDER_IN,\"DOWNLOAD_SOURCE=\",DOWNLOAD_SOURCE)\n",
    "        !wget -v {DOWNLOAD_SOURCE} --output-document={SOURCE}\n",
    "        try:\n",
    "            patoolib.extract_archive(SOURCE,outdir=EXTRACT_FOLDER_OUT)\n",
    "        except:\n",
    "            print(\"Note: File already extracted\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see something similar to the following output:\n",
    "~~~text\n",
    "Saved under C:\\Users\\David\\BIO1173\\temp\\iris-images.zip\r\n",
    "In WINDOWS\r\n",
    "patool: Extracting C:\\Users\\David\\BIO1173\\temp\\iris-images.zip ...\r\n",
    "patool: ... creating output directory `C:\\Users\\David\\BIO1173\\temp\\iris-images\\'.\r\n",
    "patool: running \"C:\\Program Files\\7-Zip\\7z.EXE\" x -oC:\\Users\\David\\BIO1173\\temp\\iris-images\\ -- C:\\Users\\David\\BIO1173\\temp\\iris-images.zip\r\n",
    "patool:     with creationflags=134217728\r\n",
    "patool: ... C:\\Users\\David\\BIO1173\\temp\\iris-images.zip extracted to `C:\\Users\\David\\BIO1173\\temp\\iris-images\\'.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYDFKA8i9KMP"
   },
   "source": [
    "### Create **ImageDataGenerator** with `flow_from_directory`\n",
    "\n",
    "In the cell below, we set up the generator as before.  However, this time we use `flow_from_directory` to get the labels from the directory structure using this code chunk:\n",
    "~~~text\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    directory=DATA_FOLDER, target_size=(256, 256), \n",
    "    class_mode='categorical', batch_size=32, shuffle=True)\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7EgpVqAdGvI",
    "outputId": "7c9cc17b-5425-4378-e0ca-561a68fbb9ce",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 421 images belonging to 3 classes.\n",
      "Found 421 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create ImageDataGenerator with flow from directory\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "  rescale = 1./255,\n",
    "  horizontal_flip=True,\n",
    "  vertical_flip=True,\n",
    "  width_shift_range=[-200,200],\n",
    "  rotation_range=360,\n",
    "\n",
    "  fill_mode='nearest')\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "    directory=DATA_FOLDER, target_size=(256, 256), \n",
    "    class_mode='categorical', batch_size=32, shuffle=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    directory=DATA_FOLDER, target_size=(256, 256), \n",
    "    class_mode='categorical', batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "~~~text\n",
    "Found 421 images belonging to 3 classes.\n",
    "Found 421 images belonging to 3 classes.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFesQtNZBZP0"
   },
   "source": [
    "### Construct, compile and train CNN\n",
    "\n",
    "The CNN neural network for image classification is similar to the one constructed above for regression. One important difference, as you should expect, is that instead of the output layer containing only one neuron for regression, our classification CNN needs a separate output neuron for each output class. To figure out the number of output classess, we use the following code chunk to create a variable called `class_count`:\n",
    "~~~text\n",
    "# Compute the number of classes for output layer\n",
    "class_count = len(train_generator.class_indices)\n",
    "~~~\n",
    "We then use the variable `class_count` to add the correct number of neurons to the output layer:\n",
    "~~~text\n",
    "# Output layer has 3 neurons, one for each class\n",
    "tf.keras.layers.Dense(class_count, activation='softmax')\n",
    "~~~\n",
    "\n",
    "The code below sets the variable `EPOCHS=50`. If your computer is really slow, you could reduce this value to 25. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBnwiM-XflQc",
    "outputId": "3279a8b4-3828-4ad3-8c03-6361cb10a975",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_14 (Conv2D)          (None, 254, 254, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 127, 127, 16)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 125, 125, 32)      4640      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 125, 125, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 62, 62, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 60, 60, 64)        18496     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 60, 60, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 30, 30, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 14, 14, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               1180160   \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,279,139\n",
      "Trainable params: 1,279,139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Starting training for 50 epochs...\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 4s 345ms/step - loss: 1.0102\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 4s 402ms/step - loss: 0.9599\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 4s 361ms/step - loss: 0.9155\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 4s 367ms/step - loss: 0.9209\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 4s 363ms/step - loss: 0.8915\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 4s 371ms/step - loss: 0.9875\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 4s 357ms/step - loss: 0.9178\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 4s 393ms/step - loss: 0.8968\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 4s 394ms/step - loss: 0.8482\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 4s 366ms/step - loss: 0.8567\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 4s 394ms/step - loss: 0.8642\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 4s 399ms/step - loss: 0.8679\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 4s 401ms/step - loss: 0.8638\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 4s 403ms/step - loss: 0.8637\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 4s 403ms/step - loss: 0.8686\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 4s 402ms/step - loss: 0.8919\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 4s 394ms/step - loss: 0.8518\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 4s 373ms/step - loss: 0.9006\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 4s 394ms/step - loss: 0.8683\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 4s 369ms/step - loss: 0.8901\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 4s 405ms/step - loss: 0.8785\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 4s 373ms/step - loss: 0.8986\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 4s 399ms/step - loss: 0.8397\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 4s 398ms/step - loss: 0.8506\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 4s 373ms/step - loss: 0.8796\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 4s 393ms/step - loss: 0.8748\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 4s 355ms/step - loss: 0.8491\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 4s 371ms/step - loss: 0.8430\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 4s 360ms/step - loss: 0.9046\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 4s 362ms/step - loss: 0.8383\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 4s 361ms/step - loss: 0.8887\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 4s 414ms/step - loss: 0.8873\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 4s 403ms/step - loss: 0.8485\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 4s 361ms/step - loss: 0.8560\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 4s 413ms/step - loss: 0.8581\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 4s 402ms/step - loss: 0.8453\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 4s 363ms/step - loss: 0.8548\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 4s 415ms/step - loss: 0.8253\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.8721\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.8342\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 4s 360ms/step - loss: 0.8582\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 4s 413ms/step - loss: 0.8457\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 4s 407ms/step - loss: 0.8533\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 4s 404ms/step - loss: 0.8321\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 4s 367ms/step - loss: 0.8876\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 4s 354ms/step - loss: 0.9087\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 4s 352ms/step - loss: 0.8587\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 4s 368ms/step - loss: 0.8580\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 4s 369ms/step - loss: 0.8764\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 4s 364ms/step - loss: 0.8479\n",
      "Elapsed time: 0:03:32.22\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "\n",
    "# Set variables\n",
    "EPOCHS=50  # Use 50 if possible\n",
    "STEPS_PER_EPOCH =10\n",
    "\n",
    "# Compute the number of classes for output layer\n",
    "class_count = len(train_generator.class_indices)\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    # Note the input shape is the desired size of the image \n",
    "    # 300x300 with 3 bytes color\n",
    "    \n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', \n",
    "        input_shape=(256, 256, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # The fifth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    \n",
    "    # Output layer has 3 neurons, one for each class\n",
    "    tf.keras.layers.Dense(class_count, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train model\n",
    "print(f\"Starting training for {EPOCHS} epochs...\") \n",
    "model.fit(train_generator, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    verbose = 1)\n",
    "\n",
    "# Print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my Windows machine, 50 epochs required between 2-4 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmjjtfuv4R9-"
   },
   "source": [
    "### Compute Accuracy Score\n",
    "\n",
    "As before, we use **_RMSE_** to compute the accuracy of a regression analysis, and the **_Accuracy Score_** to compute the accuracy of a classification analysis. \n",
    "\n",
    "The code in the cell below, computes the Accuracy Score for our classification CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BUvoMBK5uYKs",
    "outputId": "f8091a91-f841-45a9-af9a-b531b0741899",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 48ms/step\n",
      "Accuracy: 0.6389548693586699\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "validation_generator.reset()\n",
    "pred = model.predict(validation_generator)\n",
    "\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = validation_generator.classes\n",
    "\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:\n",
    "~~~text\n",
    "14/14 [==============================] - 1s 53ms/step\n",
    "Accuracy: 0.6389548693586699\n",
    "~~~\n",
    "Whereas we previously built a deep neural network able predict the Iris species with near 100% accuracy using the sepal and petal measurements, predicting a flower's species from its image is more difficult. Our CNN was only able to obtain ~64% accuracy rate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFC6ukZ9cTbp"
   },
   "source": [
    "\n",
    "# Other Resources\n",
    "\n",
    "* [Imagenet:Large Scale Visual Recognition Challenge 2014](http://image-net.org/challenges/LSVRC/2014/index)\n",
    "* [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/) - PhD student/instructor at Stanford.\n",
    "* [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/) - Stanford course on computer vision/CNN's.\n",
    "* [CS231n - GitHub](http://cs231n.github.io/)\n",
    "* [ConvNetJS](http://cs.stanford.edu/people/karpathy/convnetjs/) - JavaScript library for deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXHwP_hXwKwZ"
   },
   "source": [
    "Now we can zip the preprocessed files and store them somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
