{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_02_1_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx9UW2CBjYaf"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmMxL6uHjYag"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVzCrgXSjYag"
      },
      "source": [
        "**Module 2: Neural Networks with PyTorch**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 2 Material\n",
        "\n",
        "* **Part 2.1: Introduction to Neural Networks with PyTorch**\n",
        "* Part 2.2: Encoding Feature Vectors\n",
        "* Part 2.3: Controlling Overfitting\n",
        "* Part 2.4: Saving and Loading a PyTorch Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2__7GCYcjYag"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to ```/content/drive```  and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75dde24b-cb4d-4f96-dcde-e2dc27781f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: Using Google CoLab\n",
            "david.senseman@gmail.com\n"
          ]
        }
      ],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "Ec2zE7yUokuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Custom Function\n",
        "\n",
        "The cell below creates a custom function called `hms_string()`. This function is needed to record the time required to train your neural network model.\n",
        "\n",
        "If you fail to run this cell now, you will receive one (or more) error message(s) later in this lesson."
      ],
      "metadata": {
        "id": "_tikfGIOwr9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom function\n",
        "\n",
        "# Simple function to print out elapsed time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "oHXmx6Yzwvgl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to Deep Learning and Neural Networks**\n",
        "\n",
        "Run the next cell to see short introduction to Deep Learning and Neural Network. This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "8_xO72J7UBqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"6M5VXKLf4D4\"\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\">\n",
        "</iframe>\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "Epc3ftJ9UWUT",
        "outputId": "05190b7e-8481-488c-e9b8-aac8ae303729"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"560\" height=\"315\"\n",
              "  src=\"https://www.youtube.com/embed/6M5VXKLf4D4\"\n",
              "  title=\"YouTube video player\"\n",
              "  frameborder=\"0\"\n",
              "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
              "  allowfullscreen\n",
              "  referrerpolicy=\"strict-origin-when-cross-origin\">\n",
              "</iframe>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9zp9WOrjYag"
      },
      "source": [
        "# **Deep Learning and Neural Network Introduction**\n",
        "\n",
        "**Neural networks** were one of the first machine learning models. Their popularity has fallen twice and is now on its third rise. **Deep learning** implies the use of neural networks. The \"deep\" in deep learning refers to a neural network with many hidden layers. Because neural networks have been around for so long, they have quite a bit of baggage. Researchers have created many different training algorithms, activation/transfer functions, and structures. This course is only concerned with the latest, most current state-of-the-art techniques for deep neural networks. We will not spend much time discussing the history of neural networks.\n",
        "\n",
        "Neural networks accept input and produce output. The input to a neural network is called the **_feature vector_**. The size of this vector is always a fixed length. Changing the size of the feature vector usually means recreating the entire neural network. Though the feature vector is called a \"vector,\" this is not always the case. A vector implies a 1D array. Later we will learn about **Convolutional Neural Networks (CNNs)**, which can allow the input size to change without retraining the neural network. Historically the input to a neural network was always 1D. However, with modern neural networks, you might see input data, such as:\n",
        "\n",
        "* **1D vector** - Classic input to a neural network, similar to rows in a spreadsheet. Common in predictive modeling.\n",
        "* **2D Matrix** - Grayscale image input to a CNN.\n",
        "* **3D Matrix** - Color image input to a CNN.\n",
        "* **nD Matrix** - Higher-order input to a CNN.\n",
        "\n",
        "Before CNNs, programs either encoded images to an intermediate form or sent the image input to a neural network by merely squashing the image matrix into a long array by placing the image's rows side-by-side. CNNs are different as the matrix passes through the neural network layers.\n",
        "\n",
        "Initially, this lesson will focus on 1D input to neural networks. However, later lessons will focus more heavily on higher dimension input.\n",
        "\n",
        "The term **dimension** can be confusing in neural networks. In the sense of a 1D input vector, dimension refers to how many elements are in that 1D array. For example, a neural network with ten input neurons has ten dimensions. However, now that we have CNNs, the input has dimensions. The input to the neural network will *usually* have 1, 2, or 3 dimensions. Four or more dimensions are unusual. You might have a 2D input to a neural network with 64x64 pixels. This configuration would result in 4,096 input neurons. This network is either 2D or 4,096D, depending on which dimensions you reference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzD89SWKjYah"
      },
      "source": [
        "## **Neurons and Layers**\n",
        "\n",
        "Most neural network structures use some type of neuron. Many different neural networks exist, and programmers introduce experimental neural network structures. Consequently, it is not possible to cover every neural network architecture. However, there are some commonalities among neural network implementations. A neural network algorithm would typically be composed of individual, interconnected units, even though these units may or may not be called neurons. The name for a neural network processing unit varies among the literature sources. It could be called a node, neuron, or unit.\n",
        "\n",
        "The diagram below shows the abstract structure of a single artificial neuron (ANN).\n",
        "\n",
        "**An Artificial Neuron**\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image00A.png \"An Artifical\")\n",
        "\n",
        "The artificial neuron receives input from one or more sources that may be other neurons or data fed into the network from a computer program. This input is usually floating-point or binary. Often binary input is encoded to floating-point by representing true or false as 1 or 0. Sometimes the program also depicts the binary information using a bipolar system with true as one and false as -1.\n",
        "\n",
        "An artificial neuron multiplies each of these inputs by a **_weight_**. Then it adds these multiplications and passes this sum to an **_activation function_**. Some neural networks do not use an activation function. The following equation summarizes the calculated output of a neuron:\n",
        "\n",
        "$$ f(x,w) = \\phi(\\sum_i(\\theta_i \\cdot x_i)) $$\n",
        "\n",
        "In the above equation, the variables $x$ and $\\theta$ represent the input and weights of the neuron. The variable $i$ corresponds to the number of weights and inputs. You must always have the same number of weights as inputs. The neural network multiplies each weight by its respective input and feeds the products of these multiplications into an activation function, denoted by the Greek letter $\\phi$ (phi). This process results in a single output from the neuron.  \n",
        "\n",
        "The above neuron has two inputs plus the bias as a third. This neuron might accept the following input feature vector:\n",
        "\n",
        "$$ [1,2] $$\n",
        "\n",
        "Because a bias neuron is present, the program should append the value of one as follows:\n",
        "\n",
        "$$ [1,2,1] $$\n",
        "\n",
        "The weights for a 3-input layer (2 real inputs + bias) will always have additional weight for the bias. A weight vector might be:\n",
        "\n",
        "$$ [ 0.1, 0.2, 0.3] $$\n",
        "\n",
        "To calculate the summation, perform the following:\n",
        "\n",
        "$$ 0.1*1 + 0.2*2 + 0.3*1 = 0.8 $$\n",
        "\n",
        "The program passes a value of 0.8 to the $\\phi$ (phi) function, representing the activation function.\n",
        "\n",
        "The above figure shows the structure with just one building block. You can chain together many artificial neurons to build an artificial neural network (ANN). Think of the artificial neurons as **_building blocks_** for which the input and output circles are the connectors. The figure below shows an artificial neural network composed of three neurons:\n",
        "\n",
        "**Three Neuron Neural Network**\n",
        "![Three Neuron Neural Network](https://biologicslab.co/BIO1173/images/ann-simple.png \"Three Neuron Neural Network\")\n",
        "\n",
        "The above diagram shows three interconnected neurons. This representation is essentially this figure, minus a few inputs, repeated three times and then connected. It also has a total of four inputs and a single output. The output of neurons **N1** and **N2** feed **N3** to produce the output **O**.  To calculate the output for this network, we perform the previous equation three times. The first two times calculate **N1** and **N2**, and the third calculation uses the output of **N1** and **N2** to calculate **N3**.\n",
        "   \n",
        "Neural network diagrams do not typically show the detail seen in the previous figure. We can omit the activation functions and intermediate outputs to simplify the chart, resulting in  \n",
        "\n",
        "**Three Neuron Neural Network**\n",
        "![Three Neuron Neural Network](https://biologicslab.co/BIO1173/images/typical-ann.png \"Three Neuron Neural Network\")\n",
        "\n",
        "Looking at the previous figure, you can see two additional components of neural networks. First, consider the graph represents the inputs and outputs as abstract dotted line circles. The input and output could be parts of a more extensive neural network. However, the input and output are often a particular type of neuron that accepts data from the computer program using the neural network. The output neurons return a result to the program. This type of neuron is called an input neuron. We will discuss these neurons in the next section. This figure shows the neurons arranged in layers. The input neurons are the first layer, the **N1** and **N2** neurons create the second layer, the third layer contains **N3**, and the fourth layer has **O**.  Most neural networks arrange neurons into layers.\n",
        "\n",
        "The neurons that form a layer share several characteristics. First, every neuron in a layer has the same activation function. However, the activation functions employed by each layer may be different. Each of the layers fully connects to the next layer. In other words, every neuron in one layer has a connection to neurons in the previous layer. The former figure is not fully connected. Several layers are missing connections. For example, **I1** and **N2** do not connect. The next neural network shown below is fully connected and has an additional layer.\n",
        "\n",
        "**Fully Connected Neural Network Diagram**\n",
        "![Fully Connected Neural Network Diagram](https://biologicslab.co/BIO1173/images/ann-dense.png \"Fully Connected Neural Network Diagram\")\n",
        "\n",
        "In this figure, you see a fully connected, multilayered neural network. Networks such as this one will always have an input and output layer. The hidden layer structure determines the name of the network architecture. The network in this figure is a two-hidden-layer network. Most networks will have between zero and two hidden layers. Without implementing deep learning strategies, networks with more than two hidden layers are rare.    \n",
        "\n",
        "You might also notice that the arrows always point downward or forward from the input to the output. Later in this course, we will see recurrent neural networks that form inverted loops among the neurons. This type of neural network is called a feedforward neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNPZ4tMcjYah"
      },
      "source": [
        "# **Types of Neurons**\n",
        "\n",
        "In the last section, we briefly introduced the idea that different types of neurons exist. Not every neural network will use every kind of neuron. It is also possible for a single neuron to fill the role of several different neuron types. Now we will explain all the neuron types described in the course.\n",
        "\n",
        "There are usually four types of neurons in a neural network:\n",
        "\n",
        "* **Input Neurons** - We map each input neuron to one element in the feature vector.\n",
        "* **Hidden Neurons** - Hidden neurons allow the neural network to be abstract and process the input into the output.\n",
        "* **Output Neurons** - Each output neuron calculates one part of the output.\n",
        "* **Bias Neurons** - Work similar to the y-intercept of a linear equation.  \n",
        "\n",
        "We place each neuron into a layer:\n",
        "\n",
        "* **Input Layer** - The input layer accepts feature vectors from the dataset. Input layers usually have a bias neuron.\n",
        "* **Output Layer** - The output from the neural network. The output layer does not have a bias neuron.\n",
        "* **Hidden Layers** - Layers between the input and output layers. Each hidden layer will usually have a bias neuron.\n",
        "\n",
        "\n",
        "### **Input and Output Neurons**\n",
        "\n",
        "Nearly every neural network has input and output neurons. The input neurons accept data from the program for the network. The output neuron provides processed data from the network back to the program. The program will group these input and output neurons into separate layers called the input and output layers. The program normally represents the input to a neural network as an array or vector. The number of elements contained in the vector must equal the number of input neurons. For example, a neural network with three input neurons might accept the following input vector:\n",
        "\n",
        "$$ [0.5, 0.75, 0.2] $$\n",
        "\n",
        "Neural networks typically accept floating-point vectors as their input. To be consistent, we will represent the output of a single output neuron network as a single-element vector. Likewise, neural networks will output a vector with a length equal to the number of output neurons. The output will often be a single value from a single output neuron.\n",
        "\n",
        "## **Hidden Neurons**\n",
        "\n",
        "Hidden neurons have two essential characteristics. First, hidden neurons only receive input from other neurons, such as input or other hidden neurons. Second, hidden neurons only output to other neurons, such as output or other hidden neurons. Hidden neurons help the neural network understand the input and form the output. Programmers often group hidden neurons into fully connected hidden layers. However, these hidden layers do not directly process the incoming data or the eventual output.\n",
        "\n",
        "A common question for programmers concerns the number of hidden neurons in a network. Since the answer to this question is complex, more than one section of the course will include a relevant discussion of the number of hidden neurons. Before deep learning, researchers generally suggested that anything more than a single hidden layer is excessive. [[Cite:hornik1989multilayer]](https://www.sciencedirect.com/science/article/abs/pii/0893608089900208) Researchers have proven that a single-hidden-layer neural network can function as a universal approximator. In other words, this network should be able to learn to produce (or approximate) any output from any input as long as it has enough hidden neurons in a single layer.\n",
        "\n",
        "Training refers to the process that determines good weight values. Before the advent of deep learning, researchers feared additional layers would lengthen training time or encourage overfitting. Both concerns are true; however, increased hardware speeds and clever techniques can mitigate these concerns. Before researchers introduced deep learning techniques, we did not have an efficient way to train a deep network, which is a neural network with many hidden layers. Although a single-hidden-layer neural network can theoretically learn anything, deep learning facilitates a more complex representation of patterns in the data.  \n",
        "\n",
        "## **Bias Neurons**\n",
        "\n",
        "Programmers add bias neurons to neural networks to help them learn patterns. Bias neurons function like an input neuron that always produces a value of 1. Because the bias neurons have a constant output of 1, they are not connected to the previous layer. The value of 1, called the bias activation, can be set to values other than 1. However, 1 is the most common bias activation. Not all neural networks have bias neurons. The figure below shows a single-hidden-layer neural network with bias neurons:\n",
        "\n",
        "**Neural Network with Bias Neurons**\n",
        "![Neural Network with Bias Neurons](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image00B.png \"Neural Network with Bias Neurons\")\n",
        "\n",
        "The above network contains three bias neurons. Except for the output layer, every level includes a single bias neuron. Bias neurons allow the program to shift the output of an activation function. We will see precisely how this shifting occurs later in the module when discussing activation functions.  \n",
        "\n",
        "## **Other Neuron Types**\n",
        "\n",
        "The individual units that comprise a neural network are not always called neurons. Researchers will sometimes refer to these neurons as nodes, units, or summations. You will almost always construct neural networks of weighted connections between these units.\n",
        "\n",
        "## **Why are Bias Neurons Needed?**\n",
        "\n",
        "The activation functions from the previous section specify the output of a single neuron. Together, the weight and bias of a neuron shape the output of the activation to produce the desired output. To see how this process occurs, consider the following equation. It represents a single-input sigmoid activation neural network.\n",
        "\n",
        "$$ f(x,w,b) = \\frac{1}{1 + e^{-(wx+b)}} $$\n",
        "\n",
        "The $x$ variable represents the single input to the neural network. The $w$ and $b$ variables specify the weight and bias of the neural network. The above equation combines the weighted sum of the inputs and the sigmoid activation function. For this section, we will consider the sigmoid function because it demonstrates a bias neuron's effect.\n",
        "\n",
        "The weights of the neuron allow you to adjust the slope or shape of the activation function. The next figure shows the effect on the output of the sigmoid activation function if the weight is varied:\n",
        "\n",
        "**Neuron Weight Shifting**\n",
        "![Adjusting Weight](https://biologicslab.co/BIO1173/images/class_2_bias_weight.png \"Neuron Weight Shifting\")\n",
        "\n",
        "The above diagram shows several sigmoid curves using the following parameters:\n",
        "\n",
        "$$ f(x,0.5,0.0) $$\n",
        "$$ f(x,1.0,0.0) $$\n",
        "$$ f(x,1.5,0.0) $$\n",
        "$$ f(x,2.0,0.0) $$\n",
        "\n",
        "We did not use bias to produce the curves, which is evident in the third parameter of 0 in each case. Using four weight values yields four different sigmoid curves in the above figure. No matter the weight, we always get the same value of 0.5 when *x* is 0 because all curves hit the same point when x is 0. We might need the neural network to produce other values when the input is near 0.5.  \n",
        "\n",
        "Bias does shift the sigmoid curve, which allows values other than 0.5 when *x* is near 0. The next image shows the effect of using a weight of 1.0 with several different biases:\n",
        "\n",
        "**Neuron Bias Shifting**\n",
        "![Adjusting Bias](https://biologicslab.co/BIO1173/images/class_2_bias_value.png \"Neuron Bias Shifting\")\n",
        "\n",
        "The above diagram shows several sigmoid curves with the following parameters:\n",
        "\n",
        "$$ f(x,1.0,1.0) $$\n",
        "$$ f(x,1.0,0.5) $$\n",
        "$$ f(x,1.0,1.5) $$\n",
        "$$ f(x,1.0,2.0) $$\n",
        "\n",
        "We used a weight of 1.0 for these curves in all cases. When we utilized several different biases, sigmoid curves shifted to the left or right. Because all the curves merge at the top right or bottom left, it is not a complete shift.\n",
        "\n",
        "When we put bias and weights together, they produced a curve that created the necessary output. The above curves are the output from only one neuron. In a complete network, the output from many different neurons will combine to produce intricate output patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRVtvb_CjYah"
      },
      "source": [
        "# **Modern Activation Functions**\n",
        "\n",
        "**_Activation functions_**, also known as transfer functions, are used to calculate the _output_ of each layer of a neural network. Historically neural networks have used a hyperbolic tangent, sigmoid/logistic, or linear activation function. However, modern deep neural networks primarily make use of the following activation functions:\n",
        "\n",
        "* **Rectified Linear Unit (ReLU)** - Used for the output of hidden layers. [[Cite:glorot2011deep]](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)\n",
        "* **Softmax** - Used for the output of classification neural networks.\n",
        "* **Linear** - Used for the output of regression neural networks (or 2-class classification).\n",
        "\n",
        "### **Linear Activation Function**\n",
        "The most basic activation function is the linear function because it does not change the neuron output. The following equation 1.2 shows how the program typically implements a linear activation function:\n",
        "\n",
        "$$ \\phi(x) = x $$\n",
        "\n",
        "As you can observe, this activation function simply returns the value that the neuron inputs passed to it.  The next figure shows the graph for a linear activation function:\n",
        "\n",
        "**Linear Activation Function**\n",
        "![Linear Activation Function](https://biologicslab.co/BIO1173/images/graphs-linear.png \"Linear Activation Function\")\n",
        "\n",
        "\n",
        "Regression neural networks, which learn to provide numeric values, will usually use a linear activation function on their output layer. Classification neural networks, which determine an appropriate class for their input, will often utilize a softmax activation function for their output layer.\n",
        "\n",
        "### **Rectified Linear Units (ReLU)**\n",
        "\n",
        "Since its introduction, researchers have rapidly adopted the **_Rectified Linear Unit (ReLU)_**. [[Cite:nair2010rectified]](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf) Before the ReLU activation function, the programmers generally regarded the hyperbolic tangent as the activation function of choice. Most current research now recommends the ReLU due to superior training results. As a result, most neural networks should utilize the ReLU on hidden layers and either softmax or linear on the output layer. The following equation shows the straightforward ReLU function:\n",
        "\n",
        "$$ \\phi(x) = \\max(0, x) $$\n",
        "\n",
        "The next figure shows the graph of the ReLU activation function:\n",
        "\n",
        "**Rectified Linear Units (ReLU)**\n",
        "![Rectified Linear Units (ReLU)](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image00C.png \"Rectified Linear Units (ReLU)\")\n",
        "\n",
        "Most current research states that the hidden layers of your neural network should use the ReLU activation.\n",
        "\n",
        "### **Softmax Activation Function**\n",
        "\n",
        "The final activation function that we will examine is the **_softmax_** activation function. Along with the linear activation function, you can usually find the softmax function in the output layer of a neural network. Classification neural networks typically employ the softmax function. The neuron with the highest value claims the input as a member of its class. Because it is a preferable method, the softmax activation function forces the neural network's output to represent the probability that the input falls into each of the classes. The neuron's outputs are numeric values without the softmax, with the highest indicating the winning class.\n",
        "\n",
        "To see how the program uses the softmax activation function, we will look at a typical neural network classification problem. The iris data set contains four measurements for 150 different iris flowers. Each of these flowers belongs to one of three species of iris.\n",
        "\n",
        "![Iris Flower Species](https://biologicslab.co/BIO1173/images/class_06/iris_species.png \"Iris Flower Species\")\n",
        "\n",
        "**Iris Flower Species**\n",
        "\n",
        "When you provide the measurements of a flower, the softmax function allows the neural network to give you the probability that these measurements belong to each of the three species. For example, the neural network might tell you that there is an 80% chance that the iris is setosa, a 15% probability that it is virginica, and only a 5% probability of versicolor. Because these are probabilities, they must add up to 100%. There could not be an 80% probability of setosa, a 75% probability of virginica, and a 20% probability of versicolorâ€”this type of result would be nonsensical.\n",
        "\n",
        "To classify input data into one of three iris species, you will need one output neuron for each species. The output neurons do not inherently specify the probability of each of the three species. Therefore, it is desirable to provide probabilities that sum to 100%. The neural network will tell you the likelihood of a flower being each of the three species. To get the probability, use the softmax function in the following equation:\n",
        "\n",
        "$$ \\phi_i(x) = \\frac{exp(x_i)}{\\sum_{j}^{ }exp(x_j)} $$\n",
        "\n",
        "In the above equation, $i$ represents the index of the output neuron ($\\phi$) that the program is calculating, and $j$ represents the indexes of all neurons in the group/level. The variable $x$ designates the array of output neurons. It's important to note that the program calculates the softmax activation differently than the other activation functions in this module. When softmax is the activation function, the output of a single neuron is dependent on the other output neurons.\n",
        "\n",
        "To see the softmax function in operation, refer to this [Softmax example website](http://www.heatonresearch.com/aifh/vol3/softmax.html).\n",
        "\n",
        "Consider a trained neural network that classifies data into three categories: the three iris species. In this case, you would use one output neuron for each of the target classes. Consider if the neural network were to output the following:   \n",
        "\n",
        "* **Neuron 1**: setosa: 0.9\n",
        "* **Neuron 2**: versicolour: 0.2\n",
        "* **Neuron 3**: virginica: 0.4\n",
        "\n",
        "The above output shows that the neural network considers the data to represent a setosa iris. However, these numbers are not probabilities. The 0.9 value does not represent a 90% likelihood of the data representing a setosa. These values sum to 1.5. For the program to treat them as probabilities, they must sum to 1.0. The output vector for this neural network is the following:\n",
        "\n",
        "$$ [0.9,0.2,0.4] $$\n",
        "\n",
        "If you provide this vector to the softmax function it will return the following vector:\n",
        "\n",
        "$$ [0.47548495534876745 , 0.2361188410001125 , 0.28839620365112] $$\n",
        "\n",
        "The above three values do sum to 1.0 and can be treated as probabilities.  The likelihood of the data representing a setosa iris is 48% because the first value in the vector rounds to 0.48 (48%).  You can calculate this value in the following manner:\n",
        "\n",
        "$$ sum=\\exp(0.9)+\\exp(0.2)+\\exp(0.4)=5.17283056695839 $$\n",
        "$$ j_0= \\exp(0.9)/sum = 0.47548495534876745 $$\n",
        "$$ j_1= \\exp(0.2)/sum = 0.2361188410001125 $$\n",
        "$$ j_2= \\exp(0.4)/sum = 0.28839620365112 $$\n",
        "\n",
        "### **Step Activation Function**\n",
        "\n",
        "The step or threshold activation function is another simple activation function. Neural networks were initially called perceptrons. McCulloch & Pitts (1943) introduced the original perceptron and used a step activation function like the following equation:[[Cite:mcculloch1943logical]](https://link.springer.com/article/10.1007/BF02478259) The step activation is 1 if x>=0.5, and 0 otherwise.\n",
        "  \n",
        "This equation outputs a value of 1.0 for incoming values of 0.5 or higher and 0 for all other values. Step functions, also known as threshold functions, only return 1 (true) for values above the specified threshold, as seen in the next figure.\n",
        "\n",
        "**Step Activation Function**\n",
        "![Step Activation Function](https://biologicslab.co/BIO1173/images/graphs-step.png \"Step Activation Function\")\n",
        "\n",
        "### **Sigmoid Activation Function**\n",
        "\n",
        "The sigmoid or logistic activation function is a common choice for feedforward neural networks that need to output only positive numbers. Despite its widespread use, the hyperbolic tangent or the rectified linear unit (ReLU) activation function is usually a more suitable choice. We introduce the ReLU activation function later in this module. The following equation shows the sigmoid activation function:\n",
        "\n",
        "$$ \\phi(x) = \\frac{1}{1 + e^{-x}} $$\n",
        "\n",
        "Use the sigmoid function to ensure that values stay within a relatively small range, as seen in Figure 3.SIGMOID:\n",
        "\n",
        "**Sigmoid Activation Function**\n",
        "![Sigmoid Activation Function](https://biologicslab.co/BIO1173/images/graphs-sigmoid.png \"Sigmoid Activation Function\")\n",
        "\n",
        "As you can see from the above graph, we can force values to a range. Here, the function compressed values above or below 0 to the approximate range between 0 and 1.\n",
        "\n",
        "### **Hyperbolic Tangent Activation Function**\n",
        "\n",
        "The hyperbolic tangent function is also a prevalent activation function for neural networks that must output values between -1 and 1. This activation function is simply the hyperbolic tangent (tanh) function, as shown in the following equation:\n",
        "\n",
        "$$ \\phi(x) = \\tanh(x) $$\n",
        "\n",
        "The graph of the hyperbolic tangent function has a similar shape to the sigmoid activation function, as seen below.\n",
        "\n",
        "**Hyperbolic Tangent Activation Function**\n",
        "![Hyperbolic Tangent Activation Function](https://biologicslab.co/BIO1173/images/graphs-tanh.png \"Hyperbolic Tangent Activation Function\")\n",
        "\n",
        "The hyperbolic tangent function has several advantages over the sigmoid activation function.  \n",
        "\n",
        "### **Why ReLU?**\n",
        "\n",
        "Why is the ReLU activation function so popular? One of the critical improvements to neural networks makes deep learning work. [[Cite:nair2010rectified]](https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf)\n",
        "\n",
        "Before deep learning, the sigmoid activation function was prevalent.  We covered the sigmoid activation function earlier in this module. Frameworks like Keras often train neural networks with gradient descent. For the neural network to use gradient descent, it is necessary to take the derivative of the activation function. The program must derive partial derivatives of each of the weights for the error function. The next figure shows a derivative, the instantaneous rate of change.\n",
        "\n",
        "**Derivative**\n",
        "![Derivative](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image00D.png \"Derivative\")\n",
        "\n",
        "The derivative of the sigmoid function is given here:\n",
        "\n",
        "$$ \\phi'(x)=\\phi(x)(1-\\phi(x)) $$\n",
        "\n",
        "Textbooks often give this derivative in other forms. We use the above form for computational efficiency. To see how we determined this derivative, [refer to the following article](http://www.heatonresearch.com/aifh/vol3/deriv_sigmoid.html).\n",
        "\n",
        "We present the graph of the sigmoid derivative in the following image.\n",
        "\n",
        "**Sigmoid Derivative**\n",
        "![Sigmoid Derivative](https://biologicslab.co/BIO1173/images/class_2_deriv_sigmoid.png \"Sigmoid Derivative\")\n",
        "\n",
        "The derivative quickly saturates to zero as $x$ moves from zero.  This is not a problem for the derivative of the ReLU, which is given here:\n",
        "\n",
        "$$ \\phi'(x) = \\begin{cases} 1 & x > 0 \\\\ 0 & x \\leq 0 \\end{cases} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using PyTorch to Build Neural Network Models**\n",
        "\n",
        "From this introduction to the basic mathematical concepts that underlie deep learning with neural networks, some of you might now have the impression that building and using neural networks is an impossibly hard task. In the next section, we build our first artificial neural network so you can judge for yourself how hard this task is.\n",
        "\n",
        "**PyTorch** provides a submodule called `torch.nn` that makes building neural networks much easier. This module contains pre-built layers that handle all the complex math for you.\n",
        "\n",
        "For example, the following code uses **PyTorch** to add one layer to a neural network:\n",
        "\n",
        "`nn.Linear(in_features=x_0.shape[1], out_features=25)`\n",
        "\n",
        "When you run this line of code, this apparently simple function executes a large number of hidden commands \"behind the scenes\".\n",
        "\n",
        "For example, here is the code PyTorch uses to define a simple Linear layer (similar to what `nn.Linear` does):"
      ],
      "metadata": {
        "id": "j3DzxUJOzHu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```text\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        # Create the state of the layer (weights and bias)\n",
        "        # nn.Parameter tells PyTorch these are trainable weights\n",
        "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.randn(out_features))\n",
        "\n",
        "    # Defines the computation\n",
        "    def forward(self, input):\n",
        "        # Matrix multiplication + bias\n",
        "        return torch.matmul(input, self.weight) + self.bias\n",
        "\n",
        "# Instantiates the layer\n",
        "linear_layer = SimpleLinear(in_features=2, out_features=4)\n",
        "\n",
        "# This calls the forward method\n",
        "input_data = torch.ones(2, 2)\n",
        "y = linear_layer(input_data)\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "hULOucGTzSiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rather than asking you to create each individual layer in a network with hundreds of lines of Python code (defining weights, biases, and matrix math manually), you will be asked to only use a handful of **PyTorch** `nn` commands.\n",
        "\n",
        "Unless you are researching entirely new structures of deep neural networks, it is unlikely that you need to program the raw matrix multiplication directly. For this class, we will use the pre-built layers in `torch.nn`."
      ],
      "metadata": {
        "id": "V8frrzlHzfeh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The `MNIST` Dataset**\n",
        "\n",
        "Our first neural network will solve a basic, but important problem in image recognition: specifically, how to teach a machine to accurately \"read\" hand-drawn numbers (zip code) on an envelope. As you might imagine, this was an important problem for the US Postal Service (USPS). Having a post office clerk manually read, and sort every letter was incredibly slow and labor intensive. As you will see, a rather simple neural network, hooked up to a camera can easily solve this problem.\n",
        "\n",
        "The **`MNIST (Modified National Institute of Standards and Technology)`** dataset is a collection of 70,000 28x28 grayscale images of handwritten digits (0-9). It consists of 60,000 training images and 10,000 test images. Created by Yann LeCun, Corinna Cortes, and Christopher Burges in the late 1990s, it has been instrumental in advancing machine learning, particularly neural networks.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image14A.png)\n",
        "\n",
        "**`MNIST Data Set`**\n",
        "\n",
        "### Historical Importance\n",
        "\n",
        "1. **Standard Benchmark**: MNIST became the gold standard benchmark for evaluating and comparing the performance of various machine learning algorithms, providing a common ground for researchers.\n",
        "\n",
        "2. **Accessible and Manageable**: The dataset is relatively small and easy to process, making it accessible for both beginners and seasoned researchers. Its simplicity enabled quick experimentation and iteration.\n",
        "\n",
        "3. **Catalyst for Innovation**: Solving the MNIST problem spurred significant advancements in neural network architectures and techniques. Many foundational methods and innovations in deep learning were first validated using MNIST.\n",
        "\n",
        "### Modern Impact\n",
        "\n",
        "1. **Training Ground**: MNIST continues to serve as an educational tool and a starting point for newcomers to machine learning and deep learning.\n",
        "2. **Benchmarking**: Despite the emergence of more challenging datasets, MNIST remains a useful benchmark for testing new ideas and architectures.\n",
        "3. **Legacy**: The success stories and breakthroughs achieved on MNIST have inspired countless advancements in AI, establishing the dataset as a cornerstone in the history of machine learning.\n",
        "\n",
        "In summary, the `MNIST` dataset's historical importance lies in its role as a catalyst for innovation, a standard benchmark for evaluation, and a training ground for both models and researchers. It has significantly contributed to the growth and success of neural networks and deep learning.\n"
      ],
      "metadata": {
        "id": "UhbmilOZjYMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Construct, Compile and Train a Neural Network\n",
        "\n",
        "The code in the cell below shows the Python code needed to construct and train a neural network to analyze the MNIST dataset using the **PyTorch** library.\n",
        "\n",
        "In this example, the data is managed by a `DataLoader`. Inside the training loop, the variables are inputs and labels. Each data point in inputs is one image of a hand-drawn digit (see above), while the corresponding labels value is the actual digit value (i.e. correct number) of the hand-drawn digit.\n",
        "\n",
        "The job of the neural network `eg_model` is to learn which image corresponds to one of the 10 possible digits (i.e. 0 to 9). The neural network model in the example is called `eg_model`. The `eg_` prefix is used to indicate that this is the **EXAMPLE model** since the letters `e.g.` are used for example.\n",
        "\n",
        "Exactly how this and other neural networks classify images will be covered in later lessons."
      ],
      "metadata": {
        "id": "R7h2Y1sjzspI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Construct and Train a Neural Network with PyTorch\n",
        "\n",
        "# Import packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import time\n",
        "\n",
        "# Run code on cpu\n",
        "device = torch.device(\"cpu\")\n",
        "print(f\"Forcing usage of: {device}\")\n",
        "\n",
        "# 1. Define constants\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "LEARN_RATE = 0.001\n",
        "VAL_RATIO = 0.1\n",
        "\n",
        "# 2. Prepare Data (Transforms and Loading)\n",
        "# Convert images to tensors and normalize them\n",
        "eg_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Mean and Std Dev for MNIST\n",
        "])\n",
        "\n",
        "# Load full training data\n",
        "eg_full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=eg_transform)\n",
        "eg_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=eg_transform)\n",
        "\n",
        "# Split training into Train and Validation\n",
        "eg_val_size = int(len(eg_full_train_dataset) * VAL_RATIO)\n",
        "eg_train_size = len(eg_full_train_dataset) - eg_val_size\n",
        "eg_train_dataset, eg_val_dataset = random_split(eg_full_train_dataset, [eg_train_size, eg_val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "eg_train_loader = DataLoader(eg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "eg_val_loader = DataLoader(eg_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "eg_test_loader = DataLoader(eg_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 3. Build Neural Network Model\n",
        "class MnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MnistModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 10)\n",
        "            # Note: No Softmax here. CrossEntropyLoss handles it internally in PyTorch.\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "# Initialize Model and move to GPU if available\n",
        "eg_model = MnistModel().to(device)\n",
        "\n",
        "# 4. Define Loss and Optimizer\n",
        "eg_criterion = nn.CrossEntropyLoss()\n",
        "eg_optimizer = optim.Adam(eg_model.parameters(), lr=LEARN_RATE)\n",
        "\n",
        "# 5. Training Loop\n",
        "print(f\"----- Starting training for {EPOCHS} epochs -----\")\n",
        "start_time = time.time()\n",
        "\n",
        "eg_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    eg_model.train() # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in eg_train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        eg_optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = eg_model(inputs)\n",
        "        loss = eg_criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        eg_optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / eg_train_size\n",
        "    epoch_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    eg_model.eval() # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in eg_val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = eg_model(inputs)\n",
        "            loss = eg_criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    eg_val_epoch_loss = val_loss / eg_val_size\n",
        "    eg_val_epoch_acc = val_correct / val_total\n",
        "\n",
        "    eg_history['accuracy'].append(epoch_acc)\n",
        "    eg_history['val_accuracy'].append(eg_val_epoch_acc)\n",
        "    eg_history['loss'].append(epoch_loss)\n",
        "    eg_history['val_loss'].append(eg_val_epoch_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "          f\"loss: {epoch_loss:.4f} - accuracy: {epoch_acc:.4f} - \"\n",
        "          f\"val_loss: {eg_val_epoch_loss:.4f} - val_accuracy: {eg_val_epoch_acc:.4f}\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"\\nTraining complete.\")\n",
        "print(f\"Best validation accuracy: {max(eg_history['val_accuracy']):.4f}\")\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-JgZ1w6zrMo",
        "outputId": "a8b41649-1912-4cf3-fe47-b570e80a5758"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forcing usage of: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 61.4MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 1.60MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 14.4MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 8.47MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Starting training for 20 epochs -----\n",
            "Epoch 1/20 - loss: 0.3279 - accuracy: 0.9027 - val_loss: 0.1619 - val_accuracy: 0.9528\n",
            "Epoch 2/20 - loss: 0.2121 - accuracy: 0.9353 - val_loss: 0.1489 - val_accuracy: 0.9567\n",
            "Epoch 3/20 - loss: 0.1864 - accuracy: 0.9433 - val_loss: 0.1251 - val_accuracy: 0.9658\n",
            "Epoch 4/20 - loss: 0.1614 - accuracy: 0.9493 - val_loss: 0.0995 - val_accuracy: 0.9702\n",
            "Epoch 5/20 - loss: 0.1545 - accuracy: 0.9518 - val_loss: 0.1027 - val_accuracy: 0.9702\n",
            "Epoch 6/20 - loss: 0.1424 - accuracy: 0.9561 - val_loss: 0.0950 - val_accuracy: 0.9717\n",
            "Epoch 7/20 - loss: 0.1350 - accuracy: 0.9578 - val_loss: 0.0924 - val_accuracy: 0.9725\n",
            "Epoch 8/20 - loss: 0.1302 - accuracy: 0.9585 - val_loss: 0.0924 - val_accuracy: 0.9725\n",
            "Epoch 9/20 - loss: 0.1201 - accuracy: 0.9625 - val_loss: 0.1076 - val_accuracy: 0.9740\n",
            "Epoch 10/20 - loss: 0.1186 - accuracy: 0.9629 - val_loss: 0.1050 - val_accuracy: 0.9738\n",
            "Epoch 11/20 - loss: 0.1144 - accuracy: 0.9641 - val_loss: 0.0809 - val_accuracy: 0.9752\n",
            "Epoch 12/20 - loss: 0.1071 - accuracy: 0.9661 - val_loss: 0.0984 - val_accuracy: 0.9757\n",
            "Epoch 13/20 - loss: 0.1059 - accuracy: 0.9662 - val_loss: 0.0973 - val_accuracy: 0.9738\n",
            "Epoch 14/20 - loss: 0.1032 - accuracy: 0.9675 - val_loss: 0.0783 - val_accuracy: 0.9783\n",
            "Epoch 15/20 - loss: 0.0997 - accuracy: 0.9683 - val_loss: 0.0811 - val_accuracy: 0.9755\n",
            "Epoch 16/20 - loss: 0.1000 - accuracy: 0.9680 - val_loss: 0.0917 - val_accuracy: 0.9770\n",
            "Epoch 17/20 - loss: 0.0976 - accuracy: 0.9689 - val_loss: 0.0728 - val_accuracy: 0.9767\n",
            "Epoch 18/20 - loss: 0.0969 - accuracy: 0.9692 - val_loss: 0.0746 - val_accuracy: 0.9790\n",
            "Epoch 19/20 - loss: 0.0932 - accuracy: 0.9700 - val_loss: 0.0706 - val_accuracy: 0.9798\n",
            "Epoch 20/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 0.0803 - val_accuracy: 0.9765\n",
            "\n",
            "Training complete.\n",
            "Best validation accuracy: 0.9798\n",
            "Elapsed time: 0:08:39.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image01D.png)"
      ],
      "metadata": {
        "id": "FDAHYpVm44zk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2A: Create Custom Accuracy Function\n",
        "\n",
        "Once a neural network has been trained, it is customary to measure how \"accurate\" it was in making its predictions. There are different ways to measure accuracy.\n",
        "\n",
        "The code in the cell below creates a custom function called `compute_accuracy()` that provides some additional flexibility compared to other methods:\n",
        "\n",
        "* Evaluate on a different dataset that isnâ€™t the one you pass to fit().\n",
        "* Compute additional metrics that Keras doesnâ€™t expose by default.\n",
        "* Log a metric that uses all predictions at once (e.g. a macroâ€‘averaged score)."
      ],
      "metadata": {
        "id": "QORV4DZuqr9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2A: Create custom accuracy function\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def compute_accuracy(model, data_loader):\n",
        "    \"\"\"\n",
        "    Compute accuracy on a PyTorch DataLoader.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "     # Run code on cpu\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"Forcing usage of: {device}\")\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass (PyTorch doesn't use .predict, just calls the object)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Get predictions (argmax of logits)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "kqokNOy2RDiw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should **not** see any output."
      ],
      "metadata": {
        "id": "iSIL5jbleN2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2B: Compute Accuracy\n",
        "\n",
        "The code in the cell below uses the custom `compute_accuracy()` function to evaluate the accuracy of the `eg_model`."
      ],
      "metadata": {
        "id": "pm-fvJtfTEaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2B: Compute accuracy\n",
        "\n",
        "# Compute accuracy\n",
        "eg_test_acc_manual = compute_accuracy(eg_model, eg_test_loader)\n",
        "\n",
        "# Print results\n",
        "print(f\"eg_model accuracy: {eg_test_acc_manual:.4f}\")\n"
      ],
      "metadata": {
        "id": "kIAXPK5aQpSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b5b4886-c29e-4226-acd7-83dbefa6a98f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forcing usage of: cpu\n",
            "eg_model accuracy: 0.9803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image02D.png)\n",
        "\n",
        "An accuracy of `~98%` is clearly very good."
      ],
      "metadata": {
        "id": "Bs-OegH8b-ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Construct, Compile and Train a MNIST Neural Network**\n",
        "\n",
        "In the cell below write the **PyTorch** code needed to construct, compile and train a neural network to analyze the `Fashion MNIST` dataset.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image15A.jpg)\n",
        "\n",
        "**`Fashion MNIST` Data Set**\n",
        "\n",
        "Instead of analyzing hand-written digits, your neural network `ex_model` will analyze pictures of clothing and classify each image as belonging to one the following categories:\n",
        "\n",
        "* T-shirt/top\n",
        "* Trouser\n",
        "* Pullover\n",
        "* Dress\n",
        "* Coat\n",
        "* Sandal\n",
        "* Shirt\n",
        "* Sneaker\n",
        "* Bag\n",
        "* Ankle boot\n",
        "\n",
        "#### **Code Hints:**\n",
        "\n",
        "You can reuse the same Python code shown in Example 1 after making the following changes:\n",
        "\n",
        "**Change the prefix**\n",
        "\n",
        "Change every instance of the prefix **`eg_`** to **`ex_`**.\n",
        "\n",
        "**Change the transformer**\n",
        "\n",
        "Since the Fashion MNIST images are different you need to change this code:\n",
        "```type\n",
        "# 2. Prepare Data (Transforms and Loading)\n",
        "# Convert images to tensors and normalize them\n",
        "eg_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Mean and Std Dev for MNIST\n",
        "])\n",
        "```\n",
        "to read as\n",
        "```type\n",
        "# 2. Prepare Data (Transforms and Loading)\n",
        "# Using standard mean/std for FashionMNIST normalization\n",
        "ex_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "```\n",
        "this will create the correct transformation of the images in the MNIST Fashion dataset.\n",
        "\n",
        "**Change the data**\n",
        "\n",
        "To load the correct image data you need to change this code:\n",
        "```type\n",
        "# Load full training data\n",
        "eg_full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=eg_transform)\n",
        "eg_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=eg_transform)\n",
        "```\n",
        "to read as\n",
        "```\n",
        "# Load Fashion MNIST data\n",
        "ex_full_train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=ex_transform)\n",
        "ex_test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=ex_transform)\n",
        "```\n",
        "This will read the MNIST Fashion dataset instead of the MNIST Digits.\n",
        "\n"
      ],
      "metadata": {
        "id": "hzkLt8NcIcs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n",
        "# import packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import time\n",
        "\n",
        "# Run code on cpu\n",
        "device = torch.device(\"cpu\")\n",
        "print(f\"Forcing usage of: {device}\")\n",
        "\n",
        "# 1. Define constants\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "LEARN_RATE = 0.001\n",
        "VAL_RATIO = 0.1\n",
        "\n",
        "# 2. Prepare Data (Transforms and Loading)\n",
        "# Using standard mean/std for FashionMNIST normalization\n",
        "ex_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load Fashion MNIST data\n",
        "ex_full_train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=ex_transform)\n",
        "ex_test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=ex_transform)\n",
        "\n",
        "# Split training into Train and Validation\n",
        "ex_val_size = int(len(ex_full_train_dataset) * VAL_RATIO)\n",
        "ex_train_size = len(ex_full_train_dataset) - ex_val_size\n",
        "ex_train_dataset, ex_val_dataset = random_split(ex_full_train_dataset, [ex_train_size, ex_val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "ex_train_loader = DataLoader(ex_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "ex_val_loader = DataLoader(ex_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "ex_test_loader = DataLoader(ex_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 3. Build Neural Network Model (ex_model)\n",
        "class FashionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "# Initialize Model and move to GPU if available\n",
        "ex_model = FashionModel().to(device)\n",
        "\n",
        "# 4. Define Loss and Optimizer\n",
        "ex_criterion = nn.CrossEntropyLoss()\n",
        "ex_optimizer = optim.Adam(ex_model.parameters(), lr=LEARN_RATE)\n",
        "\n",
        "# 5. Training Loop\n",
        "print(f\"----- Starting training for {EPOCHS} epochs -----\")\n",
        "start_time = time.time()\n",
        "\n",
        "ex_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    ex_model.train() # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in ex_train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        ex_optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = ex_model(inputs)\n",
        "        loss = ex_criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        ex_optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    ex_epoch_loss = running_loss / ex_train_size\n",
        "    ex_epoch_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    ex_model.eval() # Set model to evaluation mode\n",
        "    ex_val_loss = 0.0\n",
        "    ex_val_correct = 0\n",
        "    ex_val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in ex_val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = ex_model(inputs)\n",
        "            loss = ex_criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    ex_val_epoch_loss = val_loss / ex_val_size\n",
        "    ex_val_epoch_acc = val_correct / val_total\n",
        "\n",
        "    ex_history['accuracy'].append(epoch_acc)\n",
        "    ex_history['val_accuracy'].append(ex_val_epoch_acc)\n",
        "    ex_history['loss'].append(epoch_loss)\n",
        "    ex_history['val_loss'].append(ex_val_epoch_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "          f\"loss: {epoch_loss:.4f} - accuracy: {epoch_acc:.4f} - \"\n",
        "          f\"val_loss: {ex_val_epoch_loss:.4f} - val_accuracy: {ex_val_epoch_acc:.4f}\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"\\nTraining complete.\")\n",
        "print(f\"Best validation accuracy: {max(ex_history['val_accuracy']):.4f}\")\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXBJqBYV62ga",
        "outputId": "0333e91f-a7fe-4b3c-9a4c-516b75db773a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forcing usage of: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.4M/26.4M [00:01<00:00, 19.1MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5k/29.5k [00:00<00:00, 307kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.42M/4.42M [00:00<00:00, 5.64MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.15k/5.15k [00:00<00:00, 19.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Starting training for 20 epochs -----\n",
            "Epoch 1/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 0.5158 - val_accuracy: 0.9127\n",
            "Epoch 2/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 1.0054 - val_accuracy: 0.8937\n",
            "Epoch 3/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 1.5901 - val_accuracy: 0.8854\n",
            "Epoch 4/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 2.0817 - val_accuracy: 0.8834\n",
            "Epoch 5/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 2.6198 - val_accuracy: 0.8804\n",
            "Epoch 6/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 3.0770 - val_accuracy: 0.8787\n",
            "Epoch 7/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 3.5997 - val_accuracy: 0.8780\n",
            "Epoch 8/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 4.0139 - val_accuracy: 0.8779\n",
            "Epoch 9/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 4.4279 - val_accuracy: 0.8784\n",
            "Epoch 10/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 4.7879 - val_accuracy: 0.8791\n",
            "Epoch 11/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 5.1339 - val_accuracy: 0.8800\n",
            "Epoch 12/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 5.4739 - val_accuracy: 0.8806\n",
            "Epoch 13/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 5.8482 - val_accuracy: 0.8809\n",
            "Epoch 14/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 6.3191 - val_accuracy: 0.8814\n",
            "Epoch 15/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 6.7907 - val_accuracy: 0.8817\n",
            "Epoch 16/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 7.1942 - val_accuracy: 0.8821\n",
            "Epoch 17/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 7.5643 - val_accuracy: 0.8825\n",
            "Epoch 18/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 8.0216 - val_accuracy: 0.8831\n",
            "Epoch 19/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 8.3331 - val_accuracy: 0.8839\n",
            "Epoch 20/20 - loss: 0.0911 - accuracy: 0.9715 - val_loss: 8.7405 - val_accuracy: 0.8841\n",
            "\n",
            "Training complete.\n",
            "Best validation accuracy: 0.9127\n",
            "Elapsed time: 0:07:58.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image03D.png)"
      ],
      "metadata": {
        "id": "HQ0mHaYhEPhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2A: Create Custom Accuracy Function**\n",
        "\n",
        "In the cell below write the code to create a custom `compute_accuracy()` function.\n",
        "\n",
        "#### **Code Hints:**\n",
        "\n",
        "You can reuse the code in Example 2A without any modification."
      ],
      "metadata": {
        "id": "EBuPcC1Hed40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2A here\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def compute_accuracy(model, data_loader):\n",
        "    \"\"\"\n",
        "    Compute accuracy on a PyTorch DataLoader.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "     # Run code on cpu\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"Forcing usage of: {device}\")\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass (PyTorch doesn't use .predict, just calls the object)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Get predictions (argmax of logits)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "xFmJi_xaed41"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should **not** see any output."
      ],
      "metadata": {
        "id": "dlTvCqYaed41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2B: Compute Accuracy**\n",
        "\n",
        "In the cell below use the custom `compute_accuracy()` function to evaluate the accuracy of your `ex_model`.\n",
        "\n",
        "#### **Code Hints:**\n",
        "\n",
        "You can reuse the same Python code shown in Example 2B after making the following changes:\n",
        "\n",
        "**Change the prefix:**\n",
        "\n",
        "Change every instance of the prefix **`eg_`** to **`ex_`** in your code cell.\n",
        "\n",
        "**Change the print statement**\n",
        "\n",
        "Make sure to change print statement to say `ex_model accuracy`."
      ],
      "metadata": {
        "id": "Y7HKCvLHed41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2B here\n",
        "\n",
        "# Compute accuracy\n",
        "ex_test_acc_manual = compute_accuracy(ex_model, ex_test_loader)\n",
        "\n",
        "# Print results\n",
        "print(f\"ex_model accuracy: {ex_test_acc_manual:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac3e9bd-25ad-41f7-cb8d-b3580980fd5b",
        "id": "KeOPxxPyed41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forcing usage of: cpu\n",
            "ex_model accuracy: 0.8680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image08D.png)\n",
        "\n",
        "Your `ex_model's` accuracy of `~85%` is pretty good, but not as good as the accuracy of the `eg_model`. It's quite possible that if you trained your `ex_model` for more than `20` epochs your model would have been able to better recognize the different fashion items."
      ],
      "metadata": {
        "id": "DoznVi8qed41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Visual Example of the Model's Accuracy\n",
        "\n",
        "The output from Example 2 was just a \"number\" so it's a little hard to see how well the model is working. If you run the next code cell several times, you will get a better idea of the model's ability.\n",
        "\n",
        "The code in the cell below picks a random image of a hand-drawn digit from the `mnist` dataset and displays it as a picture. It then uses the trained `eg_model` to examine the image and predict what digit it is.\n",
        "\n",
        "If you re-run this code cell several times you will see how `eg_model` does with different hand-drawn digits.\n"
      ],
      "metadata": {
        "id": "uCgMUwTWsD_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Visual example of the model's accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Pick a random image from the test_dataset\n",
        "index = random.randint(0, len(eg_test_dataset) - 1)\n",
        "image_tensor, label = eg_test_dataset[index]\n",
        "\n",
        "# Show the image\n",
        "plt.imshow(image_tensor.squeeze(), cmap='gray')\n",
        "plt.title(f'Original Image at Index {index}')\n",
        "plt.show()\n",
        "\n",
        "# Predict the item\n",
        "eg_model.eval() # Set model to evaluation mode\n",
        "\n",
        "# Check where the model is currently living to be safe\n",
        "device = next(eg_model.parameters()).device\n",
        "input_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "# Run model to get prediction\n",
        "with torch.no_grad():\n",
        "    predicted_probabilities = eg_model(input_tensor)\n",
        "    predicted_item = predicted_probabilities.argmax(dim=1).item()\n",
        "\n",
        "# Print the prediction\n",
        "print(f'The model predicts this item is: {predicted_item}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "fnQJOlODC6vv",
        "outputId": "7d926845-3ab4-4be4-fd5d-8519a12967b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model predicts this item is: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image05D.png)"
      ],
      "metadata": {
        "id": "7Ze3qk8Njd1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you run the above cell several times to see if the model makes any mistakes. The accuracy of our neural network was very good, but it wasn't perfect. We might expect an error once in 10 tries."
      ],
      "metadata": {
        "id": "H5Q8H0kluFkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Visual Example of the Model's Accuracy**\n",
        "\n",
        "\n",
        "In the cell below, write the Python code to provide a visual example of the accuracy of your `ex_model`.\n",
        "\n",
        "\n",
        "#### **Code Hints:**\n",
        "\n",
        "You can reuse the same Python code shown in Example 3 after making the following changes:\n",
        "\n",
        "**Change the prefix:**\n",
        "\n",
        "Change every instance of the prefix **`eg_`** to **`ex_`** in your code cell.\n",
        "\n",
        "**Map numerical classes**\n",
        "\n",
        "Add this code chunk to the beginning of your code cell, right after your import statements.\n",
        "```text\n",
        "# Map numeric class â†’ humanâ€‘readable name\n",
        "CLASS_NAMES = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\",\n",
        "}\n",
        "```\n",
        "You will use the variable `CLASS_NAMES` to identify the names of a clothing items from their index number.\n",
        "\n",
        "**Change prediction code**\n",
        "\n",
        "To get the correct prediction, you will need to change this code snippet:\n",
        "```text\n",
        "# Run model to get prediction\n",
        "with torch.no_grad():\n",
        "    predicted_probabilities = eg_model(input_tensor)\n",
        "    predicted_item = predicted_probabilities.argmax(dim=1).item()\n",
        "```\n",
        "to read as\n",
        "```text\n",
        "# Run model to get prediction\n",
        "with torch.no_grad():\n",
        "    logits = ex_model(input_tensor)\n",
        "    predicted_class_idx = logits.argmax(dim=1).item()\n",
        "```\n",
        "\n",
        "**Change print statement**\n",
        "\n",
        "To get the correct print output, you will need to change this line of code:\n",
        "```text\n",
        "# Print the prediction\n",
        "print(f'The model predicts this item is: {predicted_item}')\n",
        "```\n",
        "to read as\n",
        "```text\n",
        "# Print prediction\n",
        "predicted_name = CLASS_NAMES[predicted_class_idx]\n",
        "print(f'The model predicts this object is: {predicted_class_idx} ({predicted_name})')\n",
        "```"
      ],
      "metadata": {
        "id": "fQbqKDu3rTtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Map numeric class -> human-readable name\n",
        "CLASS_NAMES = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\",\n",
        "}\n",
        "\n",
        "# Pick a random image from the test set\n",
        "index = random.randint(0, len(ex_test_dataset) - 1)\n",
        "image_tensor, label = ex_test_dataset[index]\n",
        "\n",
        "# Show the image\n",
        "plt.imshow(image_tensor.squeeze(), cmap='gray')\n",
        "plt.title(f'Original Image at Index {index}')\n",
        "plt.show()\n",
        "\n",
        "# Predict the item\n",
        "ex_model.eval() # Set to evaluation mode\n",
        "\n",
        "# Check where the model is currently living to be safe\n",
        "device = next(ex_model.parameters()).device\n",
        "input_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "# Run model to get prediction\n",
        "with torch.no_grad():\n",
        "    logits = ex_model(input_tensor)\n",
        "    predicted_class_idx = logits.argmax(dim=1).item()\n",
        "\n",
        "# Print prediction\n",
        "predicted_name = CLASS_NAMES[predicted_class_idx]\n",
        "print(f'The model predicts this object is: {predicted_class_idx} ({predicted_name})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "6OinR3q-GUlN",
        "outputId": "bdd904c8-5eb9-4d62-b39f-8fc4cc17a8d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model predicts this object is: 5 (Sandal)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image06D.png)"
      ],
      "metadata": {
        "id": "g4ZMNIjaoKg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, make sure you run the above cell several times to see if the model makes any mistakes. The accuracy of your neural network (`ex_model`) wasn't quite as good so we might expect to see errors somewhat more often."
      ],
      "metadata": {
        "id": "SRk34m1arTtO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sgjoLrIjYah"
      },
      "source": [
        "# **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, select **File --> Print.. --> Print to Microsoft PDF** to generate a PDF of your Colab notebook if your computer is running MS Windows. If you have a MAC, then select **File --> Print.. --> Save to PDF** to generate your PDF.\n",
        "\n",
        "Name your PDF `Class_02_1.lastname.pdf` where _lastname_ is your last name, and upload your file to Canvas for grading. Make sure you are turning in a _Copy_ of `Lesson_02_1` that is stored on your GDrive, and not the original Colab notebook, if you want your lesson graded."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "\n",
        "### **FORTRAN**\n",
        "\n",
        "**Fortran** (/ËˆfÉ”ËrtrÃ¦n/; formerly FORTRAN) is a third generation, compiled, imperative programming language that is especially suited to numeric computation and scientific computing.\n",
        "\n",
        "Fortran was originally developed by IBM. It first compiled correctly in 1958. Fortran computer programs have been written to support scientific and engineering applications, such as numerical weather prediction, finite element analysis, computational fluid dynamics, plasma physics, geophysics, computational physics, crystallography and computational chemistry. It is a popular language for high-performance computing and is used for programs that benchmark and rank the world's fastest supercomputers.\n",
        "\n",
        "The IBM Blue Gene/P supercomputer installation in 2007 at the Argonne Leadership Angela Yang Computing Facility located in the Argonne National Laboratory, in Lemont, Illinois, US.\n",
        "\n",
        "Fortran has evolved through numerous versions and dialects. In 1966, the American National Standards Institute (ANSI) developed a standard for Fortran to limit proliferation of compilers using slightly different syntax. Successive versions have added support for a character data type (Fortran 77), structured programming, array programming, modular programming, generic programming (Fortran 90), parallel computing (Fortran 95), object-oriented programming (Fortran 2003), and concurrent programming (Fortran 2008).\n",
        "\n",
        "## **FORTRAN and COBOL genealogy tree**\n",
        "\n",
        "Since April 2024, Fortran has ranked among the top ten languages in the TIOBE index, a measure of the popularity of programming languages.\n",
        "\n",
        "### **Naming**\n",
        "\n",
        "The first manual for FORTRAN describes it as a Formula Translating System, and printed the name with small caps, Fortran. Other sources suggest the name stands for Formula Translator, or Formula Translation.\n",
        "\n",
        "Early IBM computers did not support lowercase letters, and the names of versions of the language through FORTRAN 77 were usually spelled in all-uppercase. FORTRAN 77 was the last version in which the Fortran character set included only uppercase letters.\n",
        "\n",
        "The official language standards for Fortran have referred to the language as \"Fortran\" with initial caps since Fortran 90.\n",
        "\n",
        "### **Origins**\n",
        "\n",
        "In late 1953, John W. Backus submitted a proposal to his superiors at IBM to develop a more practical alternative to assembly language for programming their IBM 704 mainframe computer.Backus' historic FORTRAN team consisted of programmers Richard Goldberg, Sheldon F. Best, Harlan Herrick, Peter Sheridan, Roy Nutt, Robert Nelson, Irving Ziller, Harold Stern, Lois Haibt, and David Sayre. Its concepts included easier entry of equations into a computer, an idea developed by J. Halcombe Laning and demonstrated in the Laning and Zierler system of 1952.\n",
        "\n",
        "A draft specification for The IBM Mathematical Formula Translating System was completed by November 1954. The first manual for FORTRAN appeared in October 1956,with the first FORTRAN compiler delivered in April 1957. Fortran produced efficient enough code for assembly language programmers to accept a high-level programming language replacement.\n",
        "\n",
        "John Backus said during a 1979 interview with Think, the IBM employee magazine, \"Much of my work has come from being lazy. I didn't like writing programs, and so, when I was working on the IBM 701, writing programs for computing missile trajectories, I started work on a programming system to make it easier to write programs.\"\n",
        "\n",
        "The language was widely adopted by scientists for writing numerically intensive programs, which encouraged compiler writers to produce compilers that could generate faster and more efficient code. The inclusion of a complex number data type in the language made Fortran especially suited to technical applications such as electrical engineering.\n",
        "\n",
        "By 1960, versions of FORTRAN were available for the IBM 709, 650, 1620, and 7090 computers. Significantly, the increasing popularity of FORTRAN spurred competing computer manufacturers to provide FORTRAN compilers for their machines, so that by 1963 over 40 FORTRAN compilers existed.\n",
        "\n",
        "FORTRAN was provided for the IBM 1401 computer by an innovative 63-phase compiler that ran entirely in its core memory of only 8000 (six-bit) characters. The compiler could be run from tape, or from a 2200-card deck; it used no further tape or disk storage. It kept the program in memory and loaded overlays that gradually transformed it, in place, into executable form, as described by Haines.[21] This article was reprinted, edited, in both editions of Anatomy of a Compiler[22] and in the IBM manual \"Fortran Specifications and Operating Procedures, IBM 1401\".[23] The executable form was not entirely machine language; rather, floating-point arithmetic, sub-scripting, input/output, and function references were interpreted, preceding UCSD Pascal P-code by two decades. GOTRAN, a simplified, interpreted version of FORTRAN I (with only 12 statements not 32) for \"load and go\" operation was available (at least for the early IBM 1620 computer).[24] Modern Fortran, and almost all later versions, are fully compiled, as done for other high-performance languages.\n",
        "\n",
        "The development of Fortran paralleled the early evolution of compiler technology, and many advances in the theory and design of compilers were specifically motivated by the need to generate efficient code for Fortran programs.\n",
        "\n",
        "## **Fortran â€“ A Comprehensive Overview**\n",
        "\n",
        "Fortran (short for **FORmula TRANslating System**) is one of the oldest highâ€‘level programming languages, designed in the 1950s to simplify scientific and engineering calculations. Created by IBMâ€™s John Backus and his team, the first official version (FORTRANâ€¯66) appeared in 1966, providing a syntax that allowed programmers to express mathematical formulas in a form close to textbook notation. Over the decades, Fortran has evolved through several revisionsâ€”FORTRANâ€¯77, FORTRANâ€¯90/95, FORTRANâ€¯2003, FORTRANâ€¯2008, and the most recent FORTRANâ€¯2018â€”each adding features that reflect the changing needs of scientific computing while preserving backward compatibility.\n",
        "\n",
        "### **Historical Context and Evolution**#\n",
        "\n",
        "- **FORTRANâ€¯66**: The inaugural standard focused on array processing, loops, and conditional statements. It introduced the `DO` loop and `IF` statements, making it easier to write numerical routines.\n",
        "- **FORTRANâ€¯77**: Added structured programming constructs, implicit typing rules, and subroutines/functions with explicit interfaces. It introduced format statements for I/O and the concept of â€œcommon blocksâ€ for shared data.\n",
        "- **FORTRANâ€¯90/95**: Marked a paradigm shift toward modern programming practices. These revisions introduced modules, allocatable arrays, pointer semantics, and array operations. They also dropped implicit typing, making the language more explicit and safer.\n",
        "- **FORTRANâ€¯2003**: Integrated objectâ€‘oriented programming (OOP) features such as type extension, type-bound procedures, and polymorphism. It also added interoperability with C via the `ISO_C_BINDING` module, facilitating mixedâ€‘language projects.\n",
        "- **FORTRANâ€¯2008**: Brought additional parallel computing constructs, including coarrays for distributed memory parallelism, and improved the languageâ€™s support for modern hardware.\n",
        "- **FORTRANâ€¯2018**: Finalizes the standard with enhancements to parallelism, interoperability, and language pragmas, making Fortran a viable choice for highâ€‘performance computing (HPC) on contemporary supercomputers.\n",
        "\n",
        "### **Core Language Features**\n",
        "\n",
        "1. **Declarative Style**: Fortran emphasizes mathematical expressions. Variables are declared with explicit types (`INTEGER`, `REAL`, `DOUBLE PRECISION`, `COMPLEX`, `LOGICAL`, `CHARACTER`) and kinds.\n",
        "2. **Array Operations**: Native support for wholeâ€‘array operations (`A = B + C`) and array slicing (`A(1:10, :)`) reduces boilerplate code and improves performance.\n",
        "3. **Modules and Encapsulation**: Modules encapsulate data and procedures, enabling namespace control and reusable libraries.\n",
        "4. **Implicit Typing (Optional)**: By default, variables starting with letters Iâ€“N are integers, others real. This can be overridden with `IMPLICIT NONE` to enforce explicit typing.\n",
        "5. **Structured Control Flow**: `IF`, `SELECT CASE`, `DO` loops, `WHERE` for elementâ€‘wise selection, and `FORALL` for array assignments.\n",
        "6. **I/O Flexibility**: Formatted and unformatted I/O through `READ`/`WRITE` statements with `FORMAT` specifiers, allowing both humanâ€‘readable and binary data handling.\n",
        "7. **Pointer and Allocatable Variables**: Enable dynamic memory allocation and flexible data structures.\n",
        "8. **Coarrays**: Introduced in Fortranâ€¯2008, coarrays provide a languageâ€‘level abstraction for parallelism, allowing the programmer to express data distribution and synchronization with minimal boilerplate.\n",
        "\n",
        "### **Typical Use Cases**\n",
        "\n",
        "- **Scientific Simulations**: Climate modeling, computational fluid dynamics (CFD), astrophysics, and molecular dynamics.\n",
        "- **Numerical Libraries**: BLAS, LAPACK, FFTW (Fortran bindings), and PETSc (Fortran interfaces) rely on Fortran for performanceâ€‘critical routines.\n",
        "- **Legacy Codebases**: Many national laboratories and universities maintain massive Fortran code repositories; maintaining and extending them requires modern Fortran expertise.\n",
        "- **Highâ€‘Performance Computing**: Modern compilers (Intel, GNU, Fujitsu) generate highly optimized machine code, and features like coarrays map efficiently onto MPI/OpenMP backends.\n",
        "- **Interoperability Projects**: Using `ISO_C_BINDING`, Fortran can call C libraries and vice versa, facilitating integration with Python (via f2py) or other languages.\n",
        "\n",
        "### **Interoperability and Ecosystem**\n",
        "\n",
        "- **C Interoperability**: `ISO_C_BINDING` module exposes C types (`C_INT`, `C_DOUBLE`) and allows Fortran procedures to be declared `BIND(C)` so they can be called from C. This has become a cornerstone of hybrid language projects.\n",
        "- **Python Bindings**: Tools like `f2py` (part of NumPy) let Python import Fortran routines as modules, bridging the gap between scientific Python and legacy Fortran performance.\n",
        "- **MPI and OpenMP**: Fortran compilers support MPI and OpenMP pragmas, making it straightforward to write parallel codes that run on clusters and multiâ€‘core processors.\n",
        "- **Unit Testing Frameworks**: Projects such as `fassert` and `UnitTestFortran` provide a testing infrastructure, encouraging modern software practices.\n",
        "\n",
        "### **Example Code**\n",
        "\n",
        "Below is a concise example illustrating key features: module usage, array operations, and a simple ODE solver.\n",
        "\n",
        "```fortran\n",
        "module ode_solver\n",
        "  implicit none\n",
        "  private\n",
        "  public :: rk4\n",
        "\n",
        "contains\n",
        "\n",
        "  !> Rungeâ€“Kutta 4th order integrator for y' = f(t, y)\n",
        "  subroutine rk4(f, t0, y0, t1, dt, y)\n",
        "    interface\n",
        "      subroutine f(t, y, dydt)\n",
        "        import\n",
        "        real, intent(in)  :: t\n",
        "        real, intent(in)  :: y(:)\n",
        "        real, intent(out) :: dydt(:)\n",
        "      end subroutine f\n",
        "    end interface\n",
        "\n",
        "    real, intent(in)  :: t0, t1, dt\n",
        "    real, intent(in)  :: y0(:)\n",
        "    real, intent(out) :: y(:)\n",
        "\n",
        "    real :: t, k1(:), k2(:), k3(:), k4(:)\n",
        "    integer :: n, i, steps\n",
        "\n",
        "    n = size(y0)\n",
        "    allocate(k1(n), k2(n), k3(n), k4(n))\n",
        "\n",
        "    y = y0\n",
        "    t = t0\n",
        "    steps = int((t1 - t0)/dt)\n",
        "\n",
        "    do i = 1, steps\n",
        "       call f(t, y, k1)\n",
        "       call f(t + 0.5*dt, y + 0.5*dt*k1, k2)\n",
        "       call f(t + 0.5*dt, y + 0.5*dt*k2, k3)\n",
        "       call f(t + dt, y + dt*k3, k4)\n",
        "       y = y + dt/6.0 * (k1 + 2.0*k2 + 2.0*k3 + k4)\n",
        "       t = t + dt\n",
        "    end do\n",
        "\n",
        "    deallocate(k1, k2, k3, k4)\n",
        "  end subroutine rk4\n",
        "\n",
        "end module ode_solver\n",
        "```\n"
      ],
      "metadata": {
        "id": "W07sTeBVeDQh"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}