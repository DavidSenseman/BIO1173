{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_03_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- BIO1173_CLASS_03_3 -->"
      ],
      "metadata": {
        "id": "23YvkIaasNHI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYZVwSpdbE3Y"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExN-OzpYbE3Y"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4imk1kbE3Y"
      },
      "source": [
        "##### **Module 3: Convolutional Neural Networks (CNN's)**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 3 Material\n",
        "\n",
        "* Part 3.1: Using Convolutional Neural Networks\n",
        "* Part 3.2: Using Pre-Trained Neural Networks with Keras\n",
        "* **Part 3.3: Facial Recognition and Analysis**\n",
        "* Part 3.4: Introduction to GAN's for Image and Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change your Runtime Now\n",
        "\n",
        "For this lesson you should pick the A100 GPU hardware accelerator."
      ],
      "metadata": {
        "id": "Ult76BB_wSzg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_-lPkxLbE3Z"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# @title You MUST Run this Cell First\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    !curl ipinfo.io\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see something _similar_ to the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "```text\n",
        "Mounted at /content/drive\n",
        "Note: Using Google CoLab\n",
        "{\n",
        "  \"ip\": \"34.87.87.254\",\n",
        "  \"hostname\": \"254.87.87.34.bc.googleusercontent.com\",\n",
        "  \"city\": \"Singapore\",\n",
        "  \"region\": \"Singapore\",\n",
        "  \"country\": \"SG\",\n",
        "  \"loc\": \"1.2897,103.8501\",\n",
        "  \"org\": \"AS396982 Google LLC\",\n",
        "  \"postal\": \"018989\",\n",
        "  \"timezone\": \"Asia/Singapore\",\n",
        "  \"readme\": \"https://ipinfo.io/missingauth\"\n",
        "}studentbio1173@gmail.com\n",
        "```\n",
        "\n",
        "If your GMAIL address does not appear **Electronic Submission** will not accept your Colab notebook for grading."
      ],
      "metadata": {
        "id": "xG3_sXTDfyjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerated Run-time Check\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be running a Graphics Processing Unit (GPU)."
      ],
      "metadata": {
        "id": "LKhQzBV1wu2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Accelerated Run-time Check\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check for GPU\n",
        "def check_colab_gpu():\n",
        "    print(\"=== Colab GPU Check ===\")\n",
        "\n",
        "    # Check PyTorch\n",
        "    pt_gpu = torch.cuda.is_available()\n",
        "    print(f\"PyTorch GPU available: {pt_gpu}\")\n",
        "\n",
        "    if pt_gpu:\n",
        "        print(f\"PyTorch device count: {torch.cuda.device_count()}\")\n",
        "        print(f\"PyTorch current device: {torch.cuda.current_device()}\")\n",
        "        print(f\"PyTorch device name: {torch.cuda.get_device_name()}\")\n",
        "        print(\"You are good to go!\")\n",
        "\n",
        "    else:\n",
        "        print(\"No compatible device found\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now and start over!\")\n",
        "\n",
        "check_colab_gpu()"
      ],
      "metadata": {
        "id": "8kty-X7j9tDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you current `Runtime` is correct you should see the following output\n",
        "\n",
        "```text\n",
        "=== Colab GPU Check ===\n",
        "PyTorch GPU available: True\n",
        "PyTorch device count: 1\n",
        "PyTorch current device: 0\n",
        "PyTorch device name: NVIDIA A100-SXM4-80GB\n",
        "You are good to go!\n",
        "```\n",
        "\n",
        "However, if you received a warning message, you need to change your `Runtime` now before you continue.\n"
      ],
      "metadata": {
        "id": "HIAs3kcq-WSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Facial Recognition and Analysis**\n",
        "\n",
        "\n",
        "The history of facial recognition using cascaded convolutional networks (CNNs) is quite fascinating and has evolved significantly over the years. Here's a brief overview:\n",
        "\n",
        "**Early Developments**\n",
        "* **Viola-Jones Algorithm (2001):** The Viola-Jones algorithm was one of the earliest and most influential methods for real-time face detection. It used Haar-like features and a cascade of classifiers trained with AdaBoost to detect faces quickly and accurately.\n",
        "\n",
        "**Introduction of CNNs**\n",
        "* **Convolutional Neural Networks (CNNs):** In the early 2010s, the introduction of CNNs revolutionized facial recognition technology. CNNs could learn complex features directly from data, making them more robust to variations in pose, expression, and lighting.\n",
        "\n",
        "**Cascaded CNNs**\n",
        "* **Cascade Architecture:** To improve performance and efficiency, researchers developed cascaded CNN architectures. These architectures use multiple stages of CNNs, where each stage refines the results of the previous one. This approach helps in quickly rejecting non-face regions and focusing on challenging candidates.\n",
        "\n",
        "**MTCNN (2016)**\n",
        "* **Multitask Cascaded Convolutional Networks (MTCNN):** MTCNN is a notable example of a cascaded CNN architecture designed for face detection and alignment. It consists of three stages: PNet (Proposal Network), RNet (Refine Network), and ONet (Output Network)4. MTCNN can detect faces and facial landmarks with high accuracy and efficiency."
      ],
      "metadata": {
        "id": "wCjV86HnxkND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Images for `Class_03_3`\n",
        "\n",
        "The code in the cell below creates a custom function for this lesson called `store_image()` that uses `urllib.request()`.\n",
        "\n",
        "`urllib.request()` is a module in Python's standard library used for opening and reading URLs. It's part of the larger urllib package, which handles URL operations like fetching data across the web.\n",
        "\n",
        "The cell then reads several image files from the course file server that we will use in this lesson."
      ],
      "metadata": {
        "id": "U1bQYcxo_EGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download images\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "# Function to download and store images\n",
        "def store_image(url, local_file_name):\n",
        "  with urllib.request.urlopen(url) as resource:\n",
        "    with open(local_file_name, 'wb') as f:\n",
        "      f.write(resource.read())\n",
        "\n",
        "# Images used in this lesson\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/ChineseAngry.jpg','ChineseAngry.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorSwift1.jpg','Taylor1.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorSwift2.jpg','Taylor2.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorSwift3.jpg','Taylor3.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorDisgust.jpg','TaylorDisgust.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorDisgust2.jpg','TaylorDisgust2.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TravisKelce1.jpg','Travis1.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TravisKelce2.jpg','Travis2.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TravisKelce3.jpg','Travis3.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorTravis.jpg','TaylorTravis.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorGroup.jpg','TaylorGroup.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorEighmy.jpg','TaylorEighmy.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/WomanGorilla.jpg','WomanGorilla.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/ET.jpg','ET.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/SheldonSmile.jpg','SheldonSmile.jpg')"
      ],
      "metadata": {
        "id": "dkj3obC-_JbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The `face_recognition` package**\n",
        "\n",
        "The **face_recognition** package is a simple and easy-to-use facial recognition library for Python. It is built on top of **`dlib`** and **`OpenCV`**, leveraging `dlib's` state-of-the-art face recognition capabilities. Here are some key features and uses of the face_recognition package:\n",
        "\n",
        "#### **Key Features:**\n",
        "* **Face Detection:** It can detect faces in images and videos.\n",
        "* **Face Landmarks:** It can find and manipulate facial features such as eyes, nose, mouth, and chin.\n",
        "* **Face Encoding:** It can generate face encodings, which are numerical representations of faces that can be used for recognition.\n",
        "* **Face Recognition:** It can recognize and compare faces in images.\n",
        "* **Command-Line Tool:** It includes a simple command-line tool for performing face recognition on folders of images.\n",
        "\n",
        "#### **Typical Uses:**\n",
        "* **Photo Organization:** Automatically organizing photos by recognizing and grouping images of the same person.\n",
        "* **Security Systems:** Implementing access control systems that use facial recognition to grant or deny access.\n",
        "* **Social Media:** Identifying and tagging friends in photos.\n",
        "* **Real-Time Applications:** Building real-time face recognition systems for various applications."
      ],
      "metadata": {
        "id": "JvXDEgqs-g4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install face_recognition package\n",
        "\n",
        "!pip install face_recognition"
      ],
      "metadata": {
        "id": "U-bDL1b1_N2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following output\n",
        "\n",
        "```text\n",
        "Collecting face_recognition\n",
        "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
        "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
        "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
        "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.1/100.1 MB 25.4 MB/s eta 0:00:00\n",
        "  Preparing metadata (setup.py) ... done\n",
        "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.12/dist-packages (from face_recognition) (8.3.1)\n",
        "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.12/dist-packages (from face_recognition) (19.24.6)\n",
        "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from face_recognition) (2.0.2)\n",
        "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from face_recognition) (11.3.0)\n",
        "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
        "Building wheels for collected packages: face-recognition-models\n",
        "  Building wheel for face-recognition-models (setup.py) ... done\n",
        "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566166 sha256=6a2474660389e92951ebb448d606615c4417434e1dedc9dffbbcc4dd8af23113\n",
        "  Stored in directory: /root/.cache/pip/wheels/8f/47/c8/f44c5aebb7507f7c8a2c0bd23151d732d0f0bd6884ad4ac635\n",
        "Successfully built face-recognition-models\n",
        "Installing collected packages: face-recognition-models, face_recognition\n",
        "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n",
        "```"
      ],
      "metadata": {
        "id": "UBkVeYIiPZnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Custom Function `face_detector()`**\n",
        "\n",
        "Now that we have installed the `face_recognition()` package, we can use it to create a custom `face_detector()` function.\n",
        "\n",
        "#### **Summary of the Custom Face Detector Code**\n",
        "\n",
        "This program is a custom face detection tool that identifies faces in an input image, draws bounding boxes around them, optionally saves the output image with the boxes, and returns the face bounding box coordinates.\n",
        "\n",
        "#### Key Steps:\n",
        "1. **Load the Image:**\n",
        "   - The image is loaded from the specified file path (`image_path`) using OpenCV and converted to RGB format.\n",
        "\n",
        "2. **Resize the Image:**\n",
        "   - The image is resized to specified dimensions (`resize_dim`, default is 640x480 pixels) for processing.\n",
        "\n",
        "3. **Detect Faces:**\n",
        "   - The `face_recognition.face_locations` method is used to detect faces in the image, returning bounding box coordinates for each face in the format `(top, right, bottom, left)`.\n",
        "\n",
        "4. **Draw Bounding Boxes:**\n",
        "   - Detected faces are outlined with red bounding boxes drawn using OpenCV's `cv2.rectangle`.\n",
        "\n",
        "5. **Display the Image:**\n",
        "   - The processed image, with bounding boxes, is displayed using Matplotlib.\n",
        "\n",
        "6. **Optional Save:**\n",
        "   - If `save_output=True`, the image with bounding boxes is saved to a specified file path (`output_path`) or a default file named \"output_image.jpg\".\n",
        "\n",
        "7. **Output Face Details:**\n",
        "   - The program prints the number of faces detected and the coordinates of each face.\n",
        "   - It returns a list of bounding boxes for all detected faces.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z2qGkCiWx5Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Custom Face Detector\n",
        "\n",
        "The code in the cell below creates a function called `face_detector()` that we will use in this lesson."
      ],
      "metadata": {
        "id": "O85_HjU0BOJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Custom Face Detector\n",
        "\n",
        "import face_recognition\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def face_detector(image_path, resize_dim=(640, 480), save_output=False):\n",
        "    # Load image\n",
        "    image = cv2.imread(image_path)\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize image\n",
        "    resized_image = cv2.resize(rgb_image, resize_dim)\n",
        "\n",
        "    # Find faces\n",
        "    face_locations = face_recognition.face_locations(resized_image)\n",
        "\n",
        "    # Draw boxes\n",
        "    for (top, right, bottom, left) in face_locations:\n",
        "        cv2.rectangle(resized_image, (left, top), (right, bottom), (255, 0, 0), 2)\n",
        "\n",
        "    # Show result\n",
        "    plt.imshow(resized_image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Save if needed\n",
        "    if save_output:\n",
        "        output_path = os.path.splitext(image_path)[0] + \"_faces.jpg\"\n",
        "        output_image = cv2.cvtColor(resized_image, cv2.COLOR_RGB2BGR)\n",
        "        cv2.imwrite(output_path, output_image)\n",
        "        print(f\"Saved to {output_path}\")\n",
        "\n",
        "    # Show results\n",
        "    print(f\"Found {len(face_locations)} face(s)\")\n",
        "    return face_locations"
      ],
      "metadata": {
        "id": "kk73PfD4s_nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 1A: Detect a Face\n",
        "\n",
        "Let's start by giving our `face_detector()` an easy image to analyse--a close-up portrait of Taylor Swift."
      ],
      "metadata": {
        "id": "fkuLAEWpx-pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 1: Detect a Face\n",
        "\n",
        "# Define image path\n",
        "IMAGE_PATH = 'Taylor1.jpg'\n",
        "\n",
        "face_detector(IMAGE_PATH)"
      ],
      "metadata": {
        "id": "Np7QHbJp7iq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image01A.png)\n",
        "\n",
        "```text\n",
        "Found 1 face(s)\n",
        "[(99, 419, 420, 98)]\n",
        "```"
      ],
      "metadata": {
        "id": "XktN4s6z6Bnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `face_detector()` function had no trouble seeing Taylor Swift's face and putting a red \"bounding box\" around it. Here are the coordinates for the \"box\":\n",
        "\n",
        "~~~text\n",
        "Face 1: Top:99m Right: 419, Bottom: 420, Left: 99\n",
        "~~~\n",
        "\n",
        "We can now classify any facial image -- just specify the URL of any image you wish to classify."
      ],
      "metadata": {
        "id": "SGIkjihvyl3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 1B: Detect a Face\n",
        "\n",
        "Does our `face_detector()` function work as well with a male face? Let's see how our function works with another person with the same first name `Taylor`, Taylor Eighmy -- The President of UT San Antonio?\n",
        "\n",
        "The code in the cell below uses the function `face_detector()` to analyze an image of Taylor Eighmy (`TaylorEighmy.jpg`)."
      ],
      "metadata": {
        "id": "9Mrt8H0R81h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 1B: Detect Face\n",
        "\n",
        "# Define image path\n",
        "IMAGE_PATH = 'TaylorEighmy.jpg'\n",
        "\n",
        "face_detector(IMAGE_PATH)"
      ],
      "metadata": {
        "id": "ADg2vo8j890E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image02A.png)\n",
        "\n",
        "```text\n",
        "Found 1 face(s)\n",
        "[(98, 366, 284, 180)]\n",
        "```"
      ],
      "metadata": {
        "id": "Ayg-LmKm9FYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `face_detector()` again has no trouble \"seeing\"  a face in the image.\n",
        "\n",
        "Here are the coordinates for the \"box\" the function placed around President Eighmy's face:\n",
        "\n",
        "~~~text\n",
        "[98, 366, 284, 180]\n",
        "~~~\n",
        "\n"
      ],
      "metadata": {
        "id": "AgGFr87sAPid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1A: Detect a Face**\n",
        "\n",
        "So far, we have only used images that contained a portrait of a person. Can our face detector find the face in an image of the whole person?\n",
        "\n",
        "In the cell below, use the function `face_detect()` to analyze an image of Taylor Swift where she is standing outside (`Taylor2.jpg`)."
      ],
      "metadata": {
        "id": "yQn3GXQe9OnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 1A: Detect a Face\n",
        "\n"
      ],
      "metadata": {
        "id": "H1OGthZ09THz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image03A.png)\n",
        "\n",
        "```text\n",
        "Found 0 face(s)\n",
        "[]\n",
        "```"
      ],
      "metadata": {
        "id": "6TkU0X_b9WqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time our `face_detector()` couldn't pick Taylor's face. Perhaps its too small relatve to the image?\n"
      ],
      "metadata": {
        "id": "21FlJT_10MI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1B: Detect a Face**\n",
        "\n",
        "An interesting question is \"How specific is our `face_detector()` function?\" For example, can it tell the difference between a human face and the face of a non-human primate like a baby gorilla?\n",
        "\n",
        "In the cell below, use the function `face_detector()` to analyze an image of a Woman holding a baby gorilla (`WomanGorilla.jpg`)."
      ],
      "metadata": {
        "id": "wP7Yge269bWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #title *Exercise 1B: Detect a Face\n",
        "\n"
      ],
      "metadata": {
        "id": "fTWlNJzA9fti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image04A.png)\n",
        "\n",
        "```text\n",
        "Found 1 face(s)\n",
        "[(93, 365, 316, 142)]\n",
        "```\n"
      ],
      "metadata": {
        "id": "Om3ueI6p9idK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time our `face_detector()` picked-up the woman's face, but not the face of the baby gorilla."
      ],
      "metadata": {
        "id": "F6P8jEg_A4Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1C: Detect a Face**\n",
        "\n",
        "What about a face that is clearly not human, but has some human-like features?\n",
        "\n",
        "In the cell below, use the function `face_detector()` to analyze an image of .**ET**, the Extra-Terrestrial, from the 1982 science fiction film directed by Steven Spielberg (`ET.jpg`)."
      ],
      "metadata": {
        "id": "7NVtEsrt-xd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 1C: Detect a Face\n",
        "\n"
      ],
      "metadata": {
        "id": "RgkJhlPB_Wl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image05A.png)\n",
        "\n",
        "```text\n",
        "Found 0 face(s)\n",
        "[]\n",
        "```"
      ],
      "metadata": {
        "id": "uKVJKqIo_fhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there is no bounding box, and the printout says `Found 0 face(s)`, our `face_detector()` function didn't find any face when \"looking\" at ET's picture. So clearly there are limits to what is detected as a human face."
      ],
      "metadata": {
        "id": "KPjigt0tBJb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1D: Detect Faces**\n",
        "\n",
        "One final question we might want to ask is how good is our `face_detector()` function at identifying multiple faces of a group of people in a \"normal\" picture--a picture that you might take will your cell phone?\n",
        "\n",
        "In the cell below, use `face_detector()` to analyze an image of Taylor Swift, Travis Kelse and a third person in the image `TaylorTravis.jpg`)."
      ],
      "metadata": {
        "id": "OcoPhtQi_kiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 1D: Detect Faces\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IwCfhyUa7l6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image06A.png)\n",
        "\n",
        "```text\n",
        "Found 3 face(s)\n",
        "[(68, 581, 175, 474), (39, 170, 101, 107), (113, 395, 156, 352)]\n",
        "```"
      ],
      "metadata": {
        "id": "TIh2v4jvBX0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though Taylor Swift isn't looking straight into the camera, our `face_detector()` function had no problem \"seeing\" her face along with the face of Travis and another man in the picture."
      ],
      "metadata": {
        "id": "SkBsFOB47wWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VGG16 Model**\n",
        "\n",
        "The **VGG16 model** is a convolutional neural network (CNN) architecture developed by the Visual Geometry Group (VGG) at the University of Oxford. It's widely used for image classification tasks. Here are some key points about VGG16:\n",
        "\n",
        "**Architecture**\n",
        "* **16 Layers:** The model has 16 layers with weights, including 13 convolutional layers and 3 fully connected layers.\n",
        "* **3x3 Filters:** It uses small 3x3 convolution filters throughout the network.\n",
        "* **Max Pooling:** It includes max pooling layers to reduce the spatial dimensions of the feature maps.\n",
        "* **Fully Connected Layers:** The final layers are fully connected, followed by a softmax activation function for classification.\n",
        "\n",
        "**Pre-trained Weights**\n",
        "* **ImageNet Pre-training:** The VGG16 model is often pre-trained on the ImageNet dataset, which contains over a million images across 1,000 categories.\n",
        "* **Transfer Learning:** This pre-trained model can be fine-tuned for specific tasks, making it a popular choice for transfer learning.\n",
        "\n",
        "**Applications***\n",
        "* **Image Classification:** VGG16 is used for classifying images into different categories, such as identifying objects, animals, or plants in images.\n",
        "* **Feature Extraction:** It can be used to extract features from images, which can then be used for other machine learning tasks1.\n"
      ],
      "metadata": {
        "id": "rl_-kxWhbuhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use the VGG16 model, we need to create the 2 following two image functions (1) `load_image()` and (2) `predict_imaage()`."
      ],
      "metadata": {
        "id": "Crdn0B25BysH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Image Functions\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# Load the PyTorch VGG16 model\n",
        "model_VGG16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
        "model_VGG16.eval()  # Set to evaluation mode\n",
        "\n",
        "# Download ImageNet labels for the \"decode_predictions\" equivalent\n",
        "LABELS_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "imagenet_labels = requests.get(LABELS_URL).text.splitlines()\n",
        "\n",
        "# ---\n",
        "\n",
        "# Create Function load_image()\n",
        "def load_image(filename):\n",
        "    # 1. Open and resize\n",
        "    img = Image.open(filename).convert('RGB')\n",
        "\n",
        "    # 2. Define the PyTorch preprocessing pipeline\n",
        "    # This replaces Keras's preprocess_input\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(), # Converts to [0,1] and moves channels to the front (C, H, W)\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # 3. Apply transformations and add batch dimension (1, 3, 224, 224)\n",
        "    img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "    return img_tensor\n",
        "\n",
        "# Create function predict_image()\n",
        "def predict_image(img_tensor):\n",
        "    # Give the image to the AI model without tracking gradients (saves memory)\n",
        "    with torch.no_grad():\n",
        "        output = model_VGG16(img_tensor)\n",
        "\n",
        "    # Convert output to probabilities using Softmax\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "\n",
        "    # Get the top 5 results\n",
        "    top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "\n",
        "    # Format results to match the Keras (label, percentage) style\n",
        "    results = []\n",
        "    for i in range(top5_prob.size(0)):\n",
        "        label = imagenet_labels[top5_catid[i]]\n",
        "        percentage = top5_prob[i].item() # Convert tensor to float\n",
        "        results.append((None, label, percentage))\n",
        "\n",
        "    return results\n",
        "\n",
        "print(f\"✅ The functions load_image() and predict_image() have been created.\")"
      ],
      "metadata": {
        "id": "-xATAU7fE0Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "```text\n",
        "✅ The functions load_image() and predict_image() have been created.\n",
        "```"
      ],
      "metadata": {
        "id": "jKtjbsOhGNOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2: Analyze Non-Facial Content\n",
        "\n",
        "The code in the cell below uses our 2 new functions `load_image()` and `predict_image()` to analyze the same picture of Taylor Swift that you used above in **Exercise 1A**. The VGG16 model is _not_ trained to find faces, but to analyze everything else it \"sees\" in the image."
      ],
      "metadata": {
        "id": "PhDXUkydD64K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 2: Analyze Non-Facial Content\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "# 1️⃣ Set image path\n",
        "img_path = \"Taylor2.jpg\"\n",
        "\n",
        "# 2️⃣ Load image\n",
        "processed_img = load_image(img_path)\n",
        "\n",
        "# 3️⃣ Predict image\n",
        "results = predict_image(processed_img)\n",
        "\n",
        "# 4️⃣ Show the photo (using the original file for display)\n",
        "plt.imshow(Image.open(img_path))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# 5️⃣ Show what the AI thinks it is\n",
        "print(\"AI's guesses:\")\n",
        "for pred in results:\n",
        "    # pred[1] is the label, pred[2] is the probability\n",
        "    print(f\"{pred[1]}: {pred[2]*100:.2f}%\")"
      ],
      "metadata": {
        "id": "8-gkKGTRGzTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image07A.png)\n",
        "\n",
        "```text\n",
        "AI's guesses:\n",
        "jean: 82.95%\n",
        "cardigan: 6.81%\n",
        "miniskirt: 3.56%\n",
        "sock: 0.82%\n",
        "stole: 0.72%\n",
        "```"
      ],
      "metadata": {
        "id": "Wq-QSomoEMuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analysis of the output**\n",
        "\n",
        "Let's take a closer look at the output since it gives us some insight into how the **VGG16** neural network extracts features and classifies them.\n",
        "\n",
        "By \"looking\" at the image of Taylor Swift, the **VGG16 model** correctly identifies that image contained a pair of jeans (jean: 82.95%), but since the image did not include a full view of her legs, the model thought there was a small chance that it saw a miniskirt (miniskirt: 3.56%).\n",
        "\n",
        "The AI thought that Taylor might be wearing a `cardigan` sweater but Taylor wasn't really wearing her sweater in a typical manner either.\n",
        "\n",
        "Here is an analysis of the image by a current \"frontier model\": Gemini's Nano Banana 2:\n",
        "\n",
        "---------\n",
        "\n",
        "### **Nano Banana 2: Content Analysis of Taylor Swift**\n",
        "In this photo, Taylor Swift is wearing a classic, relaxed street style look.\n",
        "\n",
        "##### **Outfit Breakdown**\n",
        "* **Top:** A white, off-the-shoulder, long-sleeved ribbed knit sweater.\n",
        "\n",
        "* **Bottoms:** High-waisted, distressed light-wash blue denim jeans.\n",
        "\n",
        "##### **Accessories:**\n",
        "\n",
        "* A gold-tone choker necklace with a delicate drop pendant.\n",
        "\n",
        "* A small black leather handbag with a crossbody strap worn over her shoulder.\n",
        "\n",
        "##### **Makeup:** Her signature red lipstick and winged eyeliner.\n",
        "\n",
        "This ensemble is characteristic of her street style around 2016–2018, blending \"cozy\" elements like oversized knits with edgy accessories like chokers.\n"
      ],
      "metadata": {
        "id": "9CuvngfiEUgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Analyze Non-Facial Content**\n",
        "\n",
        "In the cell below, use the VGG16 model to make predictions about the contents of an image of Kelse Travis in a football uniform (`Travis2.jpg`)."
      ],
      "metadata": {
        "id": "J98LVuPN8ub0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 2: Analyze Non-Facial Content\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "puwDAMo48xbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image08A.png)\n",
        "\n",
        "```text\n",
        "AI's guesses:\n",
        "football helmet: 68.48%\n",
        "basketball: 16.03%\n",
        "rugby ball: 2.77%\n",
        "horizontal bar: 2.11%\n",
        "maillot: 1.68%\n",
        "```\n",
        "\n",
        "Again the output is interesting. The VGG16 model correctly identifies that image contained a football helmet (football_helmet: 68.48%) even though only a small part (the face mask) is actually visible in the image. Somewhat bizarrely, the AI model also thought it \"saw\" a basketball: 16.03% and a rugby ball (2.77%).\n",
        "\n",
        "For comparison, here is the analysis of exaxtly ths same image by Gemini's Nano Banna 2:\n",
        "\n",
        "--------------------------\n",
        "\n",
        "### **Nano Banana 2: Content Analysis of NFL Player (KC Chiefs, #87)**\n",
        "\n",
        "#### **Main Subject:** An adult male is prominently featured in the image, clearly identifiable as an NFL football player in a stadium setting.\n",
        "\n",
        "#### **Face and Physical Appearance:**\n",
        "\n",
        "* **Facial Hair:** The man has a distinct and well-maintained full beard.\n",
        "\n",
        "* **Expression:** He has a determined and serious facial expression.\n",
        "\n",
        "* **Hair:** His hair is short and neat.\n",
        "\n",
        "* **Build:** He possesses a clearly defined and padded build, consistent with a professional football player.\n",
        "\n",
        "#### **Detailed Uniform Breakdown:**\n",
        "\n",
        "###### **Jersey (Upper Body):**\n",
        "\n",
        "* **Color:** The jersey is primarily a vibrant red.\n",
        "\n",
        "* **Numbers:** A large, prominent \"87\" is centered on the chest in white digits with yellow/gold outlines.\n",
        "\n",
        "###### **Logos and Text:**\n",
        "\n",
        "* \"CHIEFS\" text is visible in white and yellow above the central number.\n",
        "\n",
        "* A small, official NFL shield logo is positioned at the base of the V-neck collar.\n",
        "\n",
        "* Arrowhead logos, typical of the team, are visible on the shoulder area.\n",
        "\n",
        "* Small patch-like elements (logos, potentially game-specific patches) are present on the upper right and left chest.\n",
        "\n",
        "###### **Sleeves:**\n",
        "\n",
        "* Short sleeves featuring horizontal white and yellow stripes and potentially other logo details on the padded shoulders. Padded shoulders are clearly visible beneath the jersey.\n",
        "\n",
        "* Logos: An official NFL shield logo patch is visible on the upper left leg area.\n",
        "\n",
        "##### **Analysis Summary:** This image captures a male professional NFL player, factually identified as a member of the Kansas City Chiefs with jersey number 87, celebrating or celebrating or acknowledging in a game setting. The detailed analysis covers all visible components of his uniform, padded gear, personal details, and the background context of a stadium environment.\n"
      ],
      "metadata": {
        "id": "AeqA1Dnk8z90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Face Extraction from Image**\n",
        "\n",
        "**Face extraction** in Convolutional Neural Networks (CNNs) refers to the process of detecting and isolating faces from an image before feeding them into a CNN for further processing, such as recognition or classification. This step is crucial because it ensures that the CNN focuses only on the relevant part of the image (the face) and ignores the background or other irrelevant detail."
      ],
      "metadata": {
        "id": "axjJIaLgEiPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create Function `extract_face_from_image()`**\n",
        "\n",
        "The code in the cell below, creates a function called `extract_face_from_image()`. The function uses the MTCNN neural network to extract facial image(s) from a larger image and then returns the extracted face as an image to the program that called the function."
      ],
      "metadata": {
        "id": "no6ukdjb9E3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Function extract_face_from_image()\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import face_recognition\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def extract_face(image_path, required_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Detects faces, crops them, and prepares them as PyTorch tensors.\n",
        "\n",
        "    Returns:\n",
        "    - face_tensors (list): List of tensors ready for model.predict()\n",
        "    - face_images (list): List of PIL images (useful for plt.imshow())\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"❌ Error: {image_path} not found.\")\n",
        "        return [], []\n",
        "\n",
        "    # 1️⃣ Load image for detection\n",
        "    # face_recognition works best with numpy arrays\n",
        "    image_np = face_recognition.load_image_file(image_path)\n",
        "    face_locations = face_recognition.face_locations(image_np)\n",
        "\n",
        "    if not face_locations:\n",
        "        print(f\"No faces detected in {image_path}\")\n",
        "        return [], []\n",
        "\n",
        "    # 2️⃣ Define the PyTorch preprocessing pipeline\n",
        "    # Matches the normalization used in our load_image() function\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(required_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    face_tensors = []\n",
        "    face_images = []\n",
        "\n",
        "    # 3️⃣ Process each face found\n",
        "    for face in face_locations:\n",
        "        top, right, bottom, left = face\n",
        "\n",
        "        # Crop the face from the original numpy array\n",
        "        face_crop = image_np[top:bottom, left:right]\n",
        "\n",
        "        # Convert to PIL for resizing and transformations\n",
        "        pil_face = Image.fromarray(face_crop)\n",
        "\n",
        "        # Create the tensor and add batch dimension (1, 3, 224, 224)\n",
        "        tensor = preprocess(pil_face).unsqueeze(0)\n",
        "\n",
        "        face_tensors.append(tensor)\n",
        "        face_images.append(pil_face)\n",
        "\n",
        "    print(f\"✅ Extracted {len(face_tensors)} face(s) from {image_path}\")\n",
        "    return face_tensors, face_images\n",
        "\n",
        "print(\"✅ The function extract_face_from_image() is ready for use.\")"
      ],
      "metadata": {
        "id": "jmyHzRb8Pa_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "```text\n",
        "print(\"✅ The function extract_face_from_image() is ready for use.\")\n",
        "```"
      ],
      "metadata": {
        "id": "RRbSKwGsQBZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 3: Extract Face from Image\n",
        "\n",
        "The code in the cell below, uses our function `extract_face_from_image()` to extract Taylor Swift's face from the image of her used above in Example 2.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image07A.png))"
      ],
      "metadata": {
        "id": "ole0y-2y2NCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 3: Extract Face from Image\n",
        "\n",
        "# Define Image path\n",
        "IMAGE_PATH = \"Taylor2.jpg\"\n",
        "\n",
        "# The new function returns TWO lists: tensors and images\n",
        "face_tensors, face_images = extract_face(IMAGE_PATH)\n",
        "\n",
        "# Check if any faces were actually found before trying to display\n",
        "if face_images:\n",
        "    # Display the first face from the extracted face images\n",
        "    plt.imshow(face_images[0])\n",
        "    plt.title(\"Extracted Face\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No faces to display.\")"
      ],
      "metadata": {
        "id": "ZaVfL9UIRCyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image09A.png)"
      ],
      "metadata": {
        "id": "6zuWWX0h975n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `extract_face_from_image()` function had no trouble with this image."
      ],
      "metadata": {
        "id": "IprevHQQ97m_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Extract Face from Image**\n",
        "\n",
        "In the cell below, use our custom function `extract_face_from_image()` to extract Kelse Travis' face from the image of him standing in his football uniform (`Travis2.jpg`) that was used above in **Exercise 2**.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image08A.png)\n"
      ],
      "metadata": {
        "id": "ptyNFt4hD1pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 3: Extract Face from Image\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LyP3XERu-H7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image10A.png)"
      ],
      "metadata": {
        "id": "Qszl3oXr-HZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CNNs to Automatically Blur Faces in Images**\n",
        "\n",
        "Automatically **blurring faces** in images is important for several reasons, especially when it comes to privacy, security, and ethical considerations:\n",
        "\n",
        "#### **Privacy Protection**\n",
        "* **Personal Privacy:** Blurring faces helps protect individuals' privacy by making them less recognizable in images. This is crucial in situations where individuals have not given their consent to be photographed or identified.\n",
        "* **Data Privacy Regulations:** Regulations like the General Data Protection Regulation (GDPR) in the EU emphasize the importance of protecting personal data. Automatically blurring faces ensures compliance with these regulations.\n",
        "\n",
        "#### **Security Concerns**\n",
        "* **Anonymity:** In sensitive contexts, such as protests or political gatherings, blurring faces can protect individuals from potential repercussions or surveillance.\n",
        "* **Witness Protection:** In law enforcement and legal contexts, blurring faces of witnesses and victims can protect their identities and ensure their safety.\n",
        "\n",
        "#### **Ethical Considerations**\n",
        "* **Consent:** It is ethically responsible to blur faces when sharing images of people who haven't explicitly consented to be photographed or identified. This is especially important in public places or when dealing with vulnerable populations, such as children.\n",
        "* **Minimizing Harm:** By blurring faces, content creators and organizations can minimize the potential harm that could come from individuals being identified without their permission.\n",
        "\n",
        "#### **Public Sharing and Media**\n",
        "* **Social Media:** Automatically blurring faces is particularly important for images shared on social media, where privacy settings might not be strict, and images can spread quickly.\n",
        "* **News and Journalism:** In journalism, blurring faces can protect the identities of individuals in sensitive or dangerous situations while still conveying important information.\n",
        "\n",
        "#### **Example Use Cases**\n",
        "* **CCTV Footage:** Automatically blurring faces in CCTV footage can help maintain the privacy of individuals who are not involved in any incidents being monitored.\n",
        "* **Photo Albums:** Photo-sharing platforms can use face blurring to respect the privacy of people in group photos before these images are made public."
      ],
      "metadata": {
        "id": "C5GQcsVED7Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that correct packages have been loaded, we can create our image generators.\n"
      ],
      "metadata": {
        "id": "iGnwqL4GEIWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 4: Blur Faces in an Image\n",
        "\n",
        "Let's see what we can do with the `face_recognition` package. One practical function is to automatically find faces in an image and blur it.\n",
        "\n",
        "The code in the cell below uses the same image you used in **Exercise 1D** above.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/TaylorTravis.jpg)\n",
        "\n"
      ],
      "metadata": {
        "id": "UEQZVkgeBad7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 4: Blur Faces in an Image\n",
        "import face_recognition\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "\n",
        "# 1. Path to group image\n",
        "GROUP_PHOTO = \"TaylorTravis.jpg\"\n",
        "\n",
        "# 2. Check if file exists (Good practice for Colab notebooks)\n",
        "if not os.path.exists(GROUP_PHOTO):\n",
        "    print(f\"❌ Error: {GROUP_PHOTO} not found. Please upload the file.\")\n",
        "else:\n",
        "    # 3. Load the image using face_recognition\n",
        "    # This gives us the image as a NumPy array for easy processing\n",
        "    image = face_recognition.load_image_file(GROUP_PHOTO)\n",
        "\n",
        "    # 4. Find all face locations (using your established logic)\n",
        "    # Returns a list of (top, right, bottom, left) coordinates\n",
        "    face_locations = face_recognition.face_locations(image)\n",
        "\n",
        "    # 5. Convert color for OpenCV (RGB to BGR)\n",
        "    # OpenCV uses Blue-Green-Red order, unlike the rest of Python\n",
        "    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # 6. Apply Blur to each face coordinate\n",
        "    for (top, right, bottom, left) in face_locations:\n",
        "        # Extract the specific face area\n",
        "        face_roi = image_bgr[top:bottom, left:right]\n",
        "\n",
        "        # Apply a Gaussian Blur\n",
        "        # (99, 99) is the kernel size—the higher the number, the more blurry it gets\n",
        "        blurred_face = cv2.GaussianBlur(face_roi, (99, 99), 30)\n",
        "\n",
        "        # Paste the blurred face back into the original image\n",
        "        image_bgr[top:bottom, left:right] = blurred_face\n",
        "\n",
        "    # 7. Display the final result\n",
        "    print(f\"✅ Blurred {len(face_locations)} face(s) in the photo.\")\n",
        "    cv2_imshow(image_bgr)"
      ],
      "metadata": {
        "id": "kL9wAn6BSVJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image11A.png)"
      ],
      "metadata": {
        "id": "BIKmmSjH2cxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code worked as expected."
      ],
      "metadata": {
        "id": "flCkwma-BtHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 4: Blur Faces in an Image**\n",
        "\n",
        "In the cell below, finds faces and blur them in a group image called `TaylorGroup.jpg`.\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorGroup.jpg)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GLXXPfCSExKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 4: Blur Faces in an Image\n",
        "import face_recognition\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "\n",
        "# 1. Path to group image\n",
        "GROUP_PHOTO = \"TaylorGroup.jpg\"\n",
        "\n",
        "# 2. Check if file exists (Good practice for Colab notebooks)\n",
        "if not os.path.exists(GROUP_PHOTO):\n",
        "    print(f\"❌ Error: {GROUP_PHOTO} not found. Please upload the file.\")\n",
        "else:\n",
        "    # 3. Load the image using face_recognition\n",
        "    # This gives us the image as a NumPy array for easy processing\n",
        "    image = face_recognition.load_image_file(GROUP_PHOTO)\n",
        "\n",
        "    # 4. Find all face locations (using your established logic)\n",
        "    # Returns a list of (top, right, bottom, left) coordinates\n",
        "    face_locations = face_recognition.face_locations(image)\n",
        "\n",
        "    # 5. Convert color for OpenCV (RGB to BGR)\n",
        "    # OpenCV uses Blue-Green-Red order, unlike the rest of Python\n",
        "    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # 6. Apply Blur to each face coordinate\n",
        "    for (top, right, bottom, left) in face_locations:\n",
        "        # Extract the specific face area\n",
        "        face_roi = image_bgr[top:bottom, left:right]\n",
        "\n",
        "        # Apply a Gaussian Blur\n",
        "        # (99, 99) is the kernel size—the higher the number, the more blurry it gets\n",
        "        blurred_face = cv2.GaussianBlur(face_roi, (99, 99), 30)\n",
        "\n",
        "        # Paste the blurred face back into the original image\n",
        "        image_bgr[top:bottom, left:right] = blurred_face\n",
        "\n",
        "    # 7. Display the final result\n",
        "    print(f\"✅ Blurred {len(face_locations)} face(s) in the photo.\")\n",
        "    cv2_imshow(image_bgr)\n"
      ],
      "metadata": {
        "id": "308ZVg-YB4Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image12A.png)"
      ],
      "metadata": {
        "id": "kWNLjLh6B6_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Facial Analysis**\n",
        "\n",
        "Besides finding and blurring faces, the `facial_recognition` package can also be used to find facial features by identifying and returning the locations of facial landmarks such as eyes, nose, mouth, and chin.\n",
        "\n",
        "In order to utilize this capability, we need to create a new function called `analyze_facial_attribute()` in the cell below."
      ],
      "metadata": {
        "id": "MAJmO4ZGEeWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create Function `analyze_facial_attributes()`**\n",
        "\n",
        "#### Explanation of the `analyze_facial_attributes` function:\n",
        "\n",
        "1. **Load the Image**: The function uses the `face_recognition` library to load an image from the specified file path.\n",
        "2. **Find Face Locations**: It detects all face locations in the image and stores the coordinates of these faces.\n",
        "3. **Find Facial Features**: The function identifies various facial features (landmarks) like eyes, nose, and mouth for each detected face.\n",
        "4. **Display the Image**: It uses the `PIL` library to open and display the image with `matplotlib`, turning off axis labels for a cleaner view.\n",
        "5. **Plot Facial Features**: For each set of facial features, it plots the points using `matplotlib`, connecting the landmarks with lines to visualize the features.\n",
        "6. **Show Image with Landmarks**: The function displays the image with the overlaid facial landmarks.\n",
        "7. **Return Results**: Finally, it returns the face locations and facial landmarks."
      ],
      "metadata": {
        "id": "Dk-lYSkSCEGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Function analyze_facial_attributes()\n",
        "\n",
        "import face_recognition\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def analyze_facial_attributes(image_path):\n",
        "    \"\"\"\n",
        "    Analyze facial attributes by detecting faces and plotting detailed landmarks.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (face_locations, face_landmarks_list) or ([], []) if no faces found.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Validation: Check if the file actually exists\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"❌ Error: File not found - {image_path}\")\n",
        "        return [], []\n",
        "\n",
        "    try:\n",
        "        # 2. Load the image into a numpy array for detection\n",
        "        image_np = face_recognition.load_image_file(image_path)\n",
        "\n",
        "        # 3. Detect Face Locations and Landmarks\n",
        "        face_locations = face_recognition.face_locations(image_np)\n",
        "        face_landmarks_list = face_recognition.face_landmarks(image_np)\n",
        "\n",
        "        # If no faces are found, exit early\n",
        "        if not face_locations:\n",
        "            print(f\"No faces detected in {image_path}\")\n",
        "            return [], []\n",
        "\n",
        "        # 4. Visualization Setup\n",
        "        img = Image.open(image_path)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Facial Landmark Analysis: {os.path.basename(image_path)}\")\n",
        "\n",
        "        # Define specific colors for biological features\n",
        "        # This helps students distinguish between different anatomical structures\n",
        "        feature_colors = {\n",
        "            'chin': 'red',\n",
        "            'left_eyebrow': 'blue',\n",
        "            'right_eyebrow': 'blue',\n",
        "            'nose_bridge': 'green',\n",
        "            'nose_tip': 'green',\n",
        "            'left_eye': 'purple',\n",
        "            'right_eye': 'purple',\n",
        "            'top_lip': 'orange',\n",
        "            'bottom_lip': 'orange'\n",
        "        }\n",
        "\n",
        "        # 5. Draw Landmarks for each face\n",
        "        for face_landmarks, face_location in zip(face_landmarks_list, face_locations):\n",
        "\n",
        "            # Plot the landmarks (eyes, nose, mouth, etc.)\n",
        "            for feature, points in face_landmarks.items():\n",
        "                if points:\n",
        "                    # Separate x and y coordinates for plotting\n",
        "                    x, y = zip(*points)\n",
        "                    color = feature_colors.get(feature, 'white')\n",
        "\n",
        "                    # Plot individual points (the 'dots')\n",
        "                    plt.scatter(x, y, s=10, color=color, alpha=0.8)\n",
        "\n",
        "                    # Draw the connecting lines (the 'skeleton')\n",
        "                    plt.plot(x, y, linewidth=1.5, color=color, alpha=0.7)\n",
        "\n",
        "            # 6. Draw the Bounding Box (Yellow Box)\n",
        "            top, right, bottom, left = face_location\n",
        "            # Define box coordinates to close the rectangle\n",
        "            box_x = [left, right, right, left, left]\n",
        "            box_y = [top, top, bottom, bottom, top]\n",
        "            plt.plot(box_x, box_y, linewidth=2, color='yellow', linestyle='--')\n",
        "\n",
        "        plt.show()\n",
        "        print(f\"✅ Analysis complete! Found {len(face_locations)} face(s).\")\n",
        "        return face_locations, face_landmarks_list\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing {image_path}: {e}\")\n",
        "        return [], []\n",
        "\n",
        "# Example usage for your notebook:\n",
        "# locations, landmarks = analyze_facial_attributes(\"Taylor2.jpg\")"
      ],
      "metadata": {
        "id": "HcRG53zrVV-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Analyze Facial Attributes\n",
        "\n",
        "The code in the cell below uses our function `analyze_facial_attributes()` to identify and analyze the facial features in an image of Taylor Swift (`Taylor1.jpg`).\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/TaylorSwift1.jpg)"
      ],
      "metadata": {
        "id": "0z6QZGh63MyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 5: Analyze Facial Attributes\n",
        "\n",
        "# 1. Define the Image path\n",
        "# Ensure this file is uploaded to your Colab environment\n",
        "IMAGE_PATH = 'Taylor1.jpg'\n",
        "\n",
        "# 2. Run the detailed facial analysis\n",
        "# This function handles the detection, landmarks, and visualization\n",
        "face_locations, face_landmarks_list = analyze_facial_attributes(IMAGE_PATH)\n",
        "\n",
        "# 3. Accessing specific biological data\n",
        "# Example: Let's see how many coordinates make up the 'chin' for the first face found\n",
        "if face_landmarks_list:\n",
        "    first_face = face_landmarks_list[0]\n",
        "    chin_points = first_face.get('chin', [])\n",
        "    print(f\"✅ The AI mapped the jawline using {len(chin_points)} specific anatomical points.\")\n",
        "else:\n",
        "    print(\"No landmarks were found to analyze.\")"
      ],
      "metadata": {
        "id": "oYeLzO5rVtGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image13A.png)"
      ],
      "metadata": {
        "id": "8aYkVhl2CqIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each colored line resprents a different facial feature (attribute) extacted from the image.\n"
      ],
      "metadata": {
        "id": "1WJK-VsWCtjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Analyze Facial Attributes**\n",
        "\n",
        "In the cell below, use `analyze_facial_attributes()` to analyze the facial features in an image of Travis Kelse (`Travis3.jpg`).\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/TravisKelce3.jpg)"
      ],
      "metadata": {
        "id": "M-ISp-QxRCCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 5: Analyze Facial Attributes\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nbG9UvhiEyYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image14A.png)"
      ],
      "metadata": {
        "id": "Ihs9z8XYKyUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Facial Recognition using `DeepFace`**\n",
        "\n",
        "**DeepFace** is a **deep learning facial recognition system** developed by a research group at [Facebook](https://en.wikipedia.org/wiki/Facebook). It was designed to identify human faces in digital images with high accuracy.\n",
        "\n",
        "Here are some key points about `DeepFace`:\n",
        "\n",
        "* **Architecture:** DeepFace uses a nine-layer neural network with over 120 million connection weights. This complex architecture allows it to achieve impressive accuracy in facial recognition tasks.\n",
        "\n",
        "* **Training Data:** The system was trained on four million images uploaded by Facebook users. This extensive dataset helped the model learn a wide variety of facial features and variations.\n",
        "\n",
        "* **Accuracy:** DeepFace has an accuracy of 97.35% on the Labeled Faces in the Wild (LFW) dataset, which is comparable to human performance. This means it can sometimes outperform humans in recognizing faces.\n",
        "\n",
        "* **Applications:** Initially, DeepFace was used to alert Facebook users when their face appeared in any photo posted on the platform. Users could then choose to remove their face from the photo if they wished.\n",
        "\n",
        "DeepFace represents a significant advancement in facial recognition technology and has influenced many subsequent developments in the field."
      ],
      "metadata": {
        "id": "Nafwa_m3TwoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install `deepface`**\n",
        "\n",
        "Run the next cell to install DeepFace into your current Colab environment.\n",
        "\n",
        "The command `!pip install -U deepface` is used to install or upgrade the DeepFace library in a Python environment.\n",
        "\n",
        "Here’s a breakdown of what the command does:\n",
        "\n",
        "* **!pip:** The exclamation mark (!) indicates that this command should be executed in a Jupyter notebook or similar environment where the ! symbol is used to run shell commands.\n",
        "\n",
        "* **install:** This tells pip (the Python package installer) to install a package.\n",
        "\n",
        "* **-U:** This flag stands for \"upgrade\" and tells pip to upgrade the package to the latest version if it's already installed.\n",
        "\n",
        "* **deepface:** This specifies the name of the package to be installed or upgraded, which in this case is DeepFace.\n",
        "\n",
        "So, running this command will either install the DeepFace library if it’s not already present in your environment or upgrade it to the latest version if it is already installed. To prevent extra output being printed to your notebook, the `pip` install command if followed by `> /dev/null`."
      ],
      "metadata": {
        "id": "lzbm5Mx-DKiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install deepface\n",
        "\n",
        "!pip install -U deepface > /dev/null"
      ],
      "metadata": {
        "id": "In4FTm1QUEmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you shouldn't see any output."
      ],
      "metadata": {
        "id": "oaf25gkPUPSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------\n",
        "\n",
        "\n",
        "## **A Short History of Emotion Detection**\n",
        "\n",
        "The history of emotion detection using Convolutional Neural Networks (CNNs) reflects the broader advancements in both technology and our understanding of human emotions. Here's an overview:\n",
        "\n",
        "#### Early Days of Emotion Recognition\n",
        "- **19th Century**: The formal study of emotions can be traced back to Charles Darwin, who suggested that emotional expressions have evolved and serve social functions. Although his work did not benefit from modern technology, it laid the groundwork for understanding how behaviors related to emotions could be studied scientifically.\n",
        "- **1970s**: Researchers began to employ more systematic methods to study emotional expressions. The emergence of facial coding systems, such as Paul Ekman's Facial Action Coding System (FACS), transformed emotion recognition research. FACS categorized facial movements and expressions, allowing researchers to conduct more precise analysis based on visible emotional cues.\n",
        "\n",
        "#### Rise of Machine Learning and AI\n",
        "- **Late 20th Century**: The turning point for emotion recognition technologies came with the advent of machine learning (ML) and artificial intelligence (AI). By the early 2000s, the availability of more extensive datasets and more powerful computing resources allowed researchers to explore various algorithms for facial recognition and emotion detection.\n",
        "- **Early 2000s**: Researchers began to use machine learning techniques to automate emotion recognition. These early attempts relied on basic computer vision techniques but struggled to accurately interpret the nuances of human expressions.\n",
        "\n",
        "#### Advancements in Deep Learning\n",
        "- **2010s**: The introduction of deep learning and CNNs revolutionized emotion detection. CNNs, with their ability to learn hierarchical representations of data, proved to be highly effective in recognizing and classifying emotions from facial expressions.\n",
        "- **Recent Developments**: Modern CNNs have achieved impressive accuracy in emotion recognition tasks. Researchers have also explored the intrinsic ability of CNNs to represent the affective significance of visual input, suggesting that emotional perception might be an intrinsic property of the visual cortex.\n",
        "\n",
        "#### Current Trends and Applications\n",
        "- **Applications**: Emotion detection by CNNs is now used in various applications, including social media, customer service, healthcare, and security systems.\n",
        "- **Ethical Considerations**: As emotion detection technologies become more widespread, ethical considerations regarding privacy, consent, and the potential misuse of these technologies have come to the forefront.\n",
        "\n",
        "The history of emotion detection by CNNs showcases the rapid evolution of technology and its impact on our ability to understand and interact with human emotions. It's an exciting field that continues to grow and develop, offering new possibilities for enhancing human-computer interaction.\n",
        "\n",
        "-------------------------------------------\n"
      ],
      "metadata": {
        "id": "0IIW6CJqD1GK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Detect Emotion with `DeepFace`**\n",
        "\n",
        "The `DeepFace` system can analyze facial attributes to predict the age, gender, emotion, and race/ethnicity of the person in the image.\n",
        "\n",
        "In the cell below, we create a function called `detect_emotion()` that uses the DeepFace system."
      ],
      "metadata": {
        "id": "3xh_uux3EA2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create function `detect_emotion()`**\n",
        "\n",
        "The code in the cell below creates a custom function called `detect_emotion()`. This code defines a function detect_emotion that takes an image file path as an input, analyzes the image for emotions using DeepFace, and then displays the detected emotions along with the image. Here's a breakdown of what the code does:\n",
        "\n",
        "* **Function Definition:** The function detect_emotion is defined with the parameter image_path, which represents the file path of the image to be analyzed.\n",
        "* **Analyze the Image**: The DeepFace library's analyze function is used to analyze the image for emotions. It takes the image path and a list of actions (in this case, ['emotion']) to perform the emotion analysis. The result is stored in the variable result.\n",
        "* **Print Detected Emotion:** The dominant emotion detected in the image is printed using print(f\"Detected emotion: {result[0]['dominant_emotion']}\"). It also prints a detailed emotion analysis showing the probabilities of different emotions in the image.\n",
        "* **Display the Image:** The image is opened using the Image.open function from the PIL (Python Imaging Library). It is then displayed using plt.imshow(img) from the matplotlib library, with the axis turned off using plt.axis('off') to avoid showing axis labels.\n",
        "* **Return Result:** The function returns the result variable, which contains the detailed emotion analysis.\n",
        "\n",
        "Overall, this function analyzes the emotions in the given image and provides a visual representation along with detailed emotion probabilities."
      ],
      "metadata": {
        "id": "HE7IWT8rEEBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create function detect_emotion()\n",
        "\n",
        "from deepface import DeepFace\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def detect_emotion(image_path):\n",
        "    \"\"\"\n",
        "    Detects emotions using DeepFace and provides a detailed visual breakdown.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of result dictionaries for each face found.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"❌ Error: File not found - {image_path}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # 1. Run the DeepFace analysis\n",
        "        # We enforce detector_backend='opencv' for consistency with our other functions\n",
        "        results = DeepFace.analyze(img_path=image_path,\n",
        "                                 actions=['emotion'],\n",
        "                                 enforce_detection=False,\n",
        "                                 detector_backend='opencv')\n",
        "\n",
        "        # 2. Setup the Visualization\n",
        "        img = Image.open(image_path)\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "        # Plot the Original Image\n",
        "        ax1.imshow(img)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title(f\"Original: {os.path.basename(image_path)}\")\n",
        "\n",
        "        # 3. Process the results for the first face detected\n",
        "        # results is a list; we'll focus on the primary subject (index 0)\n",
        "        dominant_emotion = results[0]['dominant_emotion']\n",
        "        emotion_scores = results[0]['emotion']\n",
        "\n",
        "        # 4. Create a Bar Chart for the Emotion Distribution\n",
        "        # This helps students see the 'confusion' or 'blend' of emotions\n",
        "        emotions = list(emotion_scores.keys())\n",
        "        scores = list(emotion_scores.values())\n",
        "\n",
        "        ax2.barh(emotions, scores, color='skyblue')\n",
        "        ax2.set_xlabel('Confidence (%)')\n",
        "        ax2.set_title(f\"Dominant Emotion: {dominant_emotion.upper()}\")\n",
        "        ax2.set_xlim(0, 100)\n",
        "\n",
        "        # 5. Overlay the dominant emotion on the image\n",
        "        ax1.text(0.5, 0.05, f\"Emotion: {dominant_emotion}\",\n",
        "                 transform=ax1.transAxes, ha='center', fontsize=12,\n",
        "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print text summary for the notebook console\n",
        "        print(f\"✅ Detection Complete. Primary Subject: {dominant_emotion}\")\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error during emotion analysis: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "# analysis_data = detect_emotion(\"Taylor1.jpg\")"
      ],
      "metadata": {
        "id": "-a05b0EpW6f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "```\n",
        "26-03-01 21:44:36 - Directory /root/.deepface has been created\n",
        "26-03-01 21:44:36 - Directory /root/.deepface/weights has been created\n",
        "```"
      ],
      "metadata": {
        "id": "YcO8Xy93XIb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Detect Emotion\n",
        "\n",
        "Let's see how well our `detect_emotion()` function works.\n",
        "\n",
        "The code in the cell below used the `detect_emotion()` function to analyze an image of Taylor Swift (`Taylor1.jpg`).\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorSwift1.jpg)\n",
        "\n",
        "Before you run the code, what emotion(s) do you think Taylor was feeling when this photograph was taken?"
      ],
      "metadata": {
        "id": "2ne3-VvmUS5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 6: Detect Emotion Analysis\n",
        "\n",
        "# 1. Define the Image path\n",
        "# Make sure 'Taylor1.jpg' is available in your Colab file list\n",
        "IMAGE_PATH = 'Taylor1.jpg'\n",
        "\n",
        "# 2. Run the emotion detection analysis\n",
        "# This now generates both the image overlay and the probability bar chart\n",
        "emotion_results = detect_emotion(IMAGE_PATH)\n",
        "\n",
        "# 3. Accessing the raw numerical data for further analysis\n",
        "if emotion_results:\n",
        "    # Access the primary face results (index 0)\n",
        "    primary_emotions = emotion_results[0]['emotion']\n",
        "\n",
        "    # Example: Print the specific confidence score for 'happy'\n",
        "    happy_score = primary_emotions.get('happy', 0)\n",
        "    print(f\"📊 Statistical Confidence for 'Happy': {happy_score:.2f}%\")\n",
        "else:\n",
        "    print(\"No emotional data could be extracted.\")"
      ],
      "metadata": {
        "id": "lgkpxPqFXdGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image15A.png)"
      ],
      "metadata": {
        "id": "-5D5nUgRUjI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is what our `detect_emotion()` function predicted:\n",
        "\n",
        "#### **Detected Emotion:**\n",
        "- **Neutral**: The dominant emotion detected in the image is \"neutral,\" which means the person's facial expression does not strongly convey any specific emotion.\n"
      ],
      "metadata": {
        "id": "SLAJOyibEzz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6A: Detect Emotion**\n",
        "\n",
        "In Example 6, our `detect_emotion()` function really couldn't figure out what emotion Taylor Swift was feeling in the image.\n",
        "\n",
        "What if you try to analyze an image of Taylor Swift where she appears to be `angry`?\n",
        "\n",
        "In the cell below use the `detect_emotion()` function to analyze an image of Taylor Swift (`TaylorDisgust2.jpg`).\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorDisgust.jpg)"
      ],
      "metadata": {
        "id": "HRGWXOK7UpkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 6A: Detect Emotion Analysis\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9rXPBp4SE_LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image16A.png)"
      ],
      "metadata": {
        "id": "V40RBvaRU8Oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, our `detect_emotion()` function failed to detect any emotion in the image of Taylor Swift.\n",
        "\n",
        "Our `detect_emotion()` function again concluded that Taylor Swift's emotion was `neutral`, which means the person's facial expression does not strongly convey any specific emotion."
      ],
      "metadata": {
        "id": "ELj3-Zk_FLjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6B: Detect Emotion**\n",
        "\n",
        "Maybe there is something \"unusual\" about Taylor Swift's expression? After all,Taylor Swift has been performing for over two decades. She began her career in 2006 with the release of her self-titled debut album. Since then, she has released numerous albums, won multiple awards, and captivated audiences worldwide with her remarkable talent as a singer-songwriter.\n",
        "\n",
        "Let's use an image that clear shows a person with strong emotions. Specifically, let's use an image of angry Chinese woman that was generated by AI.\n",
        "\n",
        "In the cell below, use the `detect_emotion()` function to analyze the image `ChineseAngry.jpg`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/ChineseAngry.jpg)"
      ],
      "metadata": {
        "id": "gmQ7IhcOVC-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 6B: Detect Emotion Analysis\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AqojUZhrFTN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image17A.png)"
      ],
      "metadata": {
        "id": "0BrDYe4XVCX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time our function worked as expected.\n",
        "\n",
        "Unlike the `neutral` emotion for the Taylor Swift images, this time the software predicted almost a 90% probability that the woman in the picture was angry."
      ],
      "metadata": {
        "id": "YmVvKGTrFe3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6C: Detect Emotion**\n",
        "\n",
        "The image used in **Exercise 6B** was AI generated which begs the question of whether our function works as well with a real image?\n",
        "\n",
        "In the cell below use the `detect_emotion()` function to analyze the image `TaylorEighmy.jpg`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorEighmy.jpg)"
      ],
      "metadata": {
        "id": "GkI2vqQ5VVsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 6C: Detect Emotion Analysis\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yr9jWR6cVVXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image18A.png)"
      ],
      "metadata": {
        "id": "lvPgg1R8VUcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our function worked very well, predicting that there was a 99% chance that the President was happy when this picture was taken."
      ],
      "metadata": {
        "id": "qq-l2QibFu7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6D: Detect Emotion**\n",
        "\n",
        "Humans are extremely good at detecting a \"fake smile\". A \"fake smile\" is an expression where a person smiles, but the smile is not genuine or sincere. A genuine smile, known as a **Duchenne smile**, involves the activation of the zygomatic major muscle (which raises the corners of the mouth) and the orbicularis oculi muscle (which causes the eyes to crinkle). This combination creates a natural and authentic smile. Authentic emotional expressions involve involuntary muscle movements that are difficult to consciously replicate. For example, a genuine smile engages the _orbicularis oculi_ muscle around the eyes, creating \"crow's feet,\" which is hard to fake. Audiences are generally adept at detecting insincerity. A performance that lacks genuine emotion can come across as forced or unconvincing, breaking the immersion and reducing the impact of the story.\n",
        "\n",
        "In the TV series, the Big Bang Series, the character Sheldon Cooper uses this insincerity for comic effect when I forces a patently exagerrated \"smile\" in this image.\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/SheldonSmile.jpg)\n",
        "\n",
        "\n",
        "Let's see what happens when your ask our function to analyze this image of Sheldon Cooper (`SheldonSmile.jpg`) giving a very obviously \"faked smile\" from the Big Bang Series?"
      ],
      "metadata": {
        "id": "gpd_P0QaVi85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 6D: Detect Emotion Analysis\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0S1xYQsxVoI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image19A.png)"
      ],
      "metadata": {
        "id": "GS0XZZW-WFk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is pretty funny! Our `detect_emotion()` function thought that there was a nearly a 100% chance that Sheldon was `happy` in this picture. Clearly, our `detect_emotion()` function is unable to spot a \"fake smile\"."
      ],
      "metadata": {
        "id": "O9lvu7nYWIrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FaceNet**\n",
        "\n",
        "**FaceNet** is a facial recognition system developed by researchers at Google, including Florian Schroff, Dmitry Kalenichenko, and James Philbin. It was first presented at the 2015 IEEE Conference on Computer Vision and Pattern Recognition.\n",
        "\n",
        "Here are some key points about FaceNet:\n",
        "\n",
        "* **Deep Convolutional Network:** FaceNet uses a deep convolutional neural network (CNN) to learn a mapping from face images to a 128-dimensional Euclidean space. This means that each face image is represented as a 128-dimensional vector, and the similarity between faces can be measured by the Euclidean distance between these vectors.\n",
        "* **Triplet Loss Function:** The system uses a triplet loss function to train the network. This involves comparing a \"triplet\" of images: an anchor image, a positive image (same person as the anchor), and a negative image (different person)1. The goal is to minimize the distance between the anchor and the positive while maximizing the distance between the anchor and the negative.\n",
        "* **High Accuracy:** FaceNet achieved an accuracy of 99.63% on the Labeled Faces in the Wild (LFW) dataset, which was the highest score at the time. This high accuracy makes it suitable for various applications, including face verification, recognition, and clustering.\n",
        "* **Efficiency:** By directly optimizing the embedding itself rather than using an intermediate bottleneck layer, FaceNet achieves greater representational efficiency. It can perform face recognition tasks using only 128 bytes per face.\n",
        "\n",
        "FaceNet has been influential in the field of facial recognition and has inspired many subsequent developments and implementations."
      ],
      "metadata": {
        "id": "mdRQgUDEGMTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7: Verify Faces\n",
        "\n",
        "Another capability of `DeepFace` is its ability to identify and verify faces in digital images with high precision.\n",
        "\n",
        "The code in the cell below, uses `DeepFace` in combination with `FaceNet` to identify and verify faces using a `know_image` as a reference and an `unknown_image` as the test image.\n",
        "\n",
        "Here is the known image:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorSwift1.jpg)\n",
        "\n",
        "\n",
        "\n",
        "And here is the unknown image:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorDisgust2.jpg)"
      ],
      "metadata": {
        "id": "_GaN23AxGUJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 7: Verify faces\n",
        "\n",
        "from deepface import DeepFace\n",
        "\n",
        "# Image path known person\n",
        "KNOWN_PERSON = 'Taylor1.jpg'\n",
        "\n",
        "# Image path to unknown person\n",
        "UNKNOWN_PERSON = 'TaylorDisgust2.jpg'\n",
        "\n",
        "# Perform face verification using Facenet\n",
        "result = DeepFace.verify(KNOWN_PERSON, UNKNOWN_PERSON, model_name='Facenet')\n",
        "\n",
        "# Print results\n",
        "if result[\"verified\"]:\n",
        "    print(\"Faces Matched\")\n",
        "else:\n",
        "    print(\"Faces Not Matched\")"
      ],
      "metadata": {
        "id": "mfPINOEhGS-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "26-03-01 22:05:05 - 🔗 facenet_weights.h5 will be downloaded from https://github.com/serengil/deepface_models/releases/download/v1.0/facenet_weights.h5 to /root/.deepface/weights/facenet_weights.h5...\n",
        "Downloading...\n",
        "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facenet_weights.h5\n",
        "To: /root/.deepface/weights/facenet_weights.h5\n",
        "100%|██████████| 92.2M/92.2M [00:02<00:00, 38.5MB/s]\n",
        "Faces Matched\n",
        "```\n",
        "The bottom line says `Faces Matched`. So far...so good."
      ],
      "metadata": {
        "id": "mNGFTYQ6Gf64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7A: Verify Faces**\n",
        "\n",
        "In the cell below, write the code to verify that Travis Kelce, shown in this picture:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TravisKelce3.jpg)\n",
        "\n",
        "\n",
        "is also seen in this picture:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorTravis.jpg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the filename `Travis3.jpg` for the KNOWN_PERSON and `TaylorTravis.jpg` for the UNKNOWN_PERSON."
      ],
      "metadata": {
        "id": "kmblqS8RGlmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 7A: Verify faces\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gf5PG_jKGsBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "Faces Matched\n",
        "```\n",
        "That's pretty impressive since Travis Kelce looked pretty different in the two images."
      ],
      "metadata": {
        "id": "ND3r8vDXGwMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7B: Verify Faces**\n",
        "\n",
        "Before we end this lesson, we should make sure that our software can also tell when two faces are **not** a \"match\".\n",
        "\n",
        "In the cell below, write the code to verify that Travis Kelce, shown in this picture:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TravisKelce3.jpg)\n",
        "\n",
        "\n",
        "with the picture of UTSA President, Taylor Eighmy:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorEighmy.jpg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the filename `Travis3.jpg` for the KNOWN_PERSON and `TaylorEighmy.jpg` for the UNKNOWN_PERSON."
      ],
      "metadata": {
        "id": "wyUdlGX-G4du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 7B: Verify faces\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zK1h5o7SG7g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "```text\n",
        "Faces Not Matched\n",
        "```\n",
        "This is the expected result."
      ],
      "metadata": {
        "id": "C0cJTS87HZTF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MhC_-6ebE3l"
      },
      "source": [
        "# **Electronic Submission**\n",
        "\n",
        "When you run the code in the cell below, it will grade your Colab notebook and tell you your pending grade as it currently stands. You will be given the choice to either submit your notebook for final grading or the option to continue your work on one (or more) Exercises. You no longer have the option to upload a PDF of your Colab notebook to Canvas for grading. Grant Access to your Colab Secrets if you are asked to do so.\n",
        "\n",
        "**NOTE:** You grade on this Colab notebook will be based solely on the code in your **Exercises**. Failure to run one (or more) Examples will not affect your grade."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Electronic Submission\n",
        "\n",
        "import urllib.request\n",
        "import ssl\n",
        "import time\n",
        "\n",
        "url = \"https://biologicslab.co/BIO1173/backend_code/validate.py?v=\" + str(time.time())\n",
        "\n",
        "ctx = ssl.create_default_context()\n",
        "ctx.check_hostname = False\n",
        "ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "req = urllib.request.Request(\n",
        "    url,\n",
        "    headers={\n",
        "        \"Cache-Control\": \"no-cache, no-store, must-revalidate\",\n",
        "        \"Pragma\": \"no-cache\",\n",
        "        \"Expires\": \"0\"\n",
        "    }\n",
        ")\n",
        "\n",
        "with urllib.request.urlopen(req, context=ctx) as r:\n",
        "    exec(r.read().decode(\"utf-8\"))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "NSedRW-3sdVI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}