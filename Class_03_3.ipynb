{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_03_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYZVwSpdbE3Y"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExN-OzpYbE3Y"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4imk1kbE3Y"
      },
      "source": [
        "##### **Module 3: Convolutional Neural Networks (CNN's)**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 3 Material\n",
        "\n",
        "* Part 3.1: Using Convolutional Neural Networks\n",
        "* Part 3.2: Using Pre-Trained Neural Networks with PyTorch\n",
        "* **Part 3.3: Facial Recognition and Analysis**\n",
        "* Part 3.4: Introduction to GAN's for Image and Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change your Runtime Now\n",
        "\n",
        "For this lesson you should pick the A100 GPU hardware accelerator."
      ],
      "metadata": {
        "id": "Ult76BB_wSzg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_-lPkxLbE3Z"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4ae867-2e2b-40c8-da49-1a025db97173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: Using Google CoLab\n",
            "david.senseman@gmail.com\n"
          ]
        }
      ],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_01/class_01_6_image01A.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "xG3_sXTDfyjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerated Run-time Check\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be running a Graphics Processing Unit (GPU)."
      ],
      "metadata": {
        "id": "LKhQzBV1wu2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell second\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check for GPU\n",
        "def check_colab_gpu():\n",
        "    print(\"=== Colab GPU Check ===\")\n",
        "\n",
        "    # Check PyTorch\n",
        "    pt_gpu = torch.cuda.is_available()\n",
        "    print(f\"PyTorch GPU available: {pt_gpu}\")\n",
        "\n",
        "    if pt_gpu:\n",
        "        print(f\"PyTorch device count: {torch.cuda.device_count()}\")\n",
        "        print(f\"PyTorch current device: {torch.cuda.current_device()}\")\n",
        "        print(f\"PyTorch device name: {torch.cuda.get_device_name()}\")\n",
        "        print(\"You are good to go!\")\n",
        "\n",
        "    else:\n",
        "        print(\"No compatible device found\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now and start over!\")\n",
        "\n",
        "check_colab_gpu()"
      ],
      "metadata": {
        "id": "8kty-X7j9tDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c618479b-5d29-43f7-f998-cb11ea59d698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Colab GPU Check ===\n",
            "PyTorch GPU available: True\n",
            "PyTorch device count: 1\n",
            "PyTorch current device: 0\n",
            "PyTorch device name: NVIDIA A100-SXM4-80GB\n",
            "You are good to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using a Runtime with a GPU you should see the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image29E.png)"
      ],
      "metadata": {
        "id": "ktWDP_b1KWwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to AI Facial Recognition**\n",
        "\n",
        "Run the next cell to see short introduction to AI Facial Recognition. This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "ObtHt8Azbijp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"9zO9kaH8fH0\"\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\">\n",
        "</iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "Qigt7uxQarPf",
        "outputId": "28c8f21f-f7f1-4a17-f705-fca258ad40d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"560\" height=\"315\"\n",
              "  src=\"https://www.youtube.com/embed/9zO9kaH8fH0\"\n",
              "  title=\"YouTube video player\"\n",
              "  frameborder=\"0\"\n",
              "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
              "  allowfullscreen\n",
              "  referrerpolicy=\"strict-origin-when-cross-origin\">\n",
              "</iframe>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Facial Recognition and Analysis**\n",
        "\n",
        "\n",
        "The history of facial recognition using cascaded convolutional networks (CNNs) is quite fascinating and has evolved significantly over the years. Here's a brief overview:\n",
        "\n",
        "**Early Developments**\n",
        "* **Viola-Jones Algorithm (2001):** The Viola-Jones algorithm was one of the earliest and most influential methods for real-time face detection. It used Haar-like features and a cascade of classifiers trained with AdaBoost to detect faces quickly and accurately.\n",
        "\n",
        "**Introduction of CNNs**\n",
        "* **Convolutional Neural Networks (CNNs):** In the early 2010s, the introduction of CNNs revolutionized facial recognition technology. CNNs could learn complex features directly from data, making them more robust to variations in pose, expression, and lighting.\n",
        "\n",
        "**Cascaded CNNs**\n",
        "* **Cascade Architecture:** To improve performance and efficiency, researchers developed cascaded CNN architectures. These architectures use multiple stages of CNNs, where each stage refines the results of the previous one. This approach helps in quickly rejecting non-face regions and focusing on challenging candidates.\n",
        "\n",
        "**MTCNN (2016)**\n",
        "* **Multitask Cascaded Convolutional Networks (MTCNN):** MTCNN is a notable example of a cascaded CNN architecture designed for face detection and alignment. It consists of three stages: PNet (Proposal Network), RNet (Refine Network), and ONet (Output Network)4. MTCNN can detect faces and facial landmarks with high accuracy and efficiency."
      ],
      "metadata": {
        "id": "wCjV86HnxkND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Images for Class_03_3\n",
        "\n",
        "The code in the cell below creates a custom function for this lesson called `store_image()` that uses `urllib.request()`.\n",
        "\n",
        "`urllib.request()` is a module in Python's standard library used for opening and reading URLs. It's part of the larger urllib package, which handles URL operations like fetching data across the web.\n",
        "\n",
        "The cell then reads several image files from the course file server that we will use in this lesson."
      ],
      "metadata": {
        "id": "U1bQYcxo_EGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download images\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "# Function to download and store images\n",
        "def store_image(url, local_file_name):\n",
        "  with urllib.request.urlopen(url) as resource:\n",
        "    with open(local_file_name, 'wb') as f:\n",
        "      f.write(resource.read())\n",
        "\n",
        "# Images used in this lesson\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/ChineseAngry.jpg','ChineseAngry.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorSwift1.jpg','Taylor1.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorSwift2.jpg','Taylor2.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorSwift3.jpg','Taylor3.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorDisgust.jpg','TaylorDisgust.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorDisgust2.jpg','TaylorDisgust2.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TravisKelce1.jpg','Travis1.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TravisKelce2.jpg','Travis2.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TravisKelce3.jpg','Travis3.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorTravis.jpg','TaylorTravis.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorGroup.jpg','TaylorGroup.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorEighmy.jpg','TaylorEighmy.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/WomanGorilla.jpg','WomanGorilla.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/ET.jpg','ET.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/SheldonSmile.jpg','SheldonSmile.jpg')"
      ],
      "metadata": {
        "id": "dkj3obC-_JbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The `face_recognition` package**\n",
        "\n",
        "The **face_recognition** package is a simple and easy-to-use facial recognition library for Python. It is built on top of **`dlib`** and **`OpenCV`**, leveraging `dlib's` state-of-the-art face recognition capabilities. Here are some key features and uses of the face_recognition package:\n",
        "\n",
        "#### **Key Features:**\n",
        "* **Face Detection:** It can detect faces in images and videos.\n",
        "* **Face Landmarks:** It can find and manipulate facial features such as eyes, nose, mouth, and chin.\n",
        "* **Face Encoding:** It can generate face encodings, which are numerical representations of faces that can be used for recognition.\n",
        "* **Face Recognition:** It can recognize and compare faces in images.\n",
        "* **Command-Line Tool:** It includes a simple command-line tool for performing face recognition on folders of images.\n",
        "\n",
        "#### **Typical Uses:**\n",
        "* **Photo Organization:** Automatically organizing photos by recognizing and grouping images of the same person.\n",
        "* **Security Systems:** Implementing access control systems that use facial recognition to grant or deny access.\n",
        "* **Social Media:** Identifying and tagging friends in photos.\n",
        "* **Real-Time Applications:** Building real-time face recognition systems for various applications."
      ],
      "metadata": {
        "id": "JvXDEgqs-g4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install face_recognition package\n",
        "\n",
        "!pip install -q face_recognition"
      ],
      "metadata": {
        "id": "U-bDL1b1_N2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image02E.png)\n"
      ],
      "metadata": {
        "id": "UBkVeYIiPZnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Custom Function `face_detector()`**\n",
        "\n",
        "Now that we have installed the `face_recognition()` package, we can use it to create a custom `face_detector()` function.\n",
        "\n",
        "#### **Summary of the Custom Face Detector Code**\n",
        "\n",
        "This program is a custom face detection tool that identifies faces in an input image, draws bounding boxes around them, optionally saves the output image with the boxes, and returns the face bounding box coordinates.\n",
        "\n",
        "#### Key Steps:\n",
        "1. **Load the Image:**\n",
        "   - The image is loaded from the specified file path (`image_path`) using OpenCV and converted to RGB format.\n",
        "\n",
        "2. **Resize the Image:**\n",
        "   - The image is resized to specified dimensions (`resize_dim`, default is 640x480 pixels) for processing.\n",
        "\n",
        "3. **Detect Faces:**\n",
        "   - The `face_recognition.face_locations` method is used to detect faces in the image, returning bounding box coordinates for each face in the format `(top, right, bottom, left)`.\n",
        "\n",
        "4. **Draw Bounding Boxes:**\n",
        "   - Detected faces are outlined with red bounding boxes drawn using OpenCV's `cv2.rectangle`.\n",
        "\n",
        "5. **Display the Image:**\n",
        "   - The processed image, with bounding boxes, is displayed using Matplotlib.\n",
        "\n",
        "6. **Optional Save:**\n",
        "   - If `save_output=True`, the image with bounding boxes is saved to a specified file path (`output_path`) or a default file named \"output_image.jpg\".\n",
        "\n",
        "7. **Output Face Details:**\n",
        "   - The program prints the number of faces detected and the coordinates of each face.\n",
        "   - It returns a list of bounding boxes for all detected faces.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z2qGkCiWx5Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Custom Face Detector\n",
        "\n",
        "The code in the cell below creates a function called `face_detector()` that we will use in this lesson."
      ],
      "metadata": {
        "id": "O85_HjU0BOJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom face detector\n",
        "\n",
        "import face_recognition\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def face_detector(image_path, resize_dim=(640, 480), save_output=False):\n",
        "    # Load image\n",
        "    image = cv2.imread(image_path)\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize image\n",
        "    resized_image = cv2.resize(rgb_image, resize_dim)\n",
        "\n",
        "    # Find faces\n",
        "    face_locations = face_recognition.face_locations(resized_image)\n",
        "\n",
        "    # Draw boxes\n",
        "    for (top, right, bottom, left) in face_locations:\n",
        "        cv2.rectangle(resized_image, (left, top), (right, bottom), (255, 0, 0), 2)\n",
        "\n",
        "    # Show result\n",
        "    plt.imshow(resized_image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Save if needed\n",
        "    if save_output:\n",
        "        output_path = os.path.splitext(image_path)[0] + \"_faces.jpg\"\n",
        "        output_image = cv2.cvtColor(resized_image, cv2.COLOR_RGB2BGR)\n",
        "        cv2.imwrite(output_path, output_image)\n",
        "        print(f\"Saved to {output_path}\")\n",
        "\n",
        "    # Show results\n",
        "    print(f\"Found {len(face_locations)} face(s)\")\n",
        "    return face_locations"
      ],
      "metadata": {
        "id": "kk73PfD4s_nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should _not_ see any output."
      ],
      "metadata": {
        "id": "dKaxD-csvsBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1A: Detect Face\n",
        "\n",
        "Let's start by giving our `face_detector()` an easy image to analyse--a close-up portrait of Taylor Swift."
      ],
      "metadata": {
        "id": "fkuLAEWpx-pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Detect and Display Image\n",
        "\n",
        "# Define image path\n",
        "IMAGE_PATH = 'Taylor1.jpg'\n",
        "\n",
        "face_detector(IMAGE_PATH)"
      ],
      "metadata": {
        "id": "Np7QHbJp7iq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image07G.png)"
      ],
      "metadata": {
        "id": "XktN4s6z6Bnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `face_detector()` function had no trouble seeing Taylor Swift's face and putting a red \"bounding box\" around it. Here are the coordinates for the \"box\":\n",
        "\n",
        "~~~text\n",
        "Face 1: Top:99m Right: 419, Bottom: 420, Left: 98\n",
        "~~~\n",
        "\n",
        "We can now classify any facial image -- just specify the URL of any image you wish to classify."
      ],
      "metadata": {
        "id": "SGIkjihvyl3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1B: Detect Face\n",
        "\n",
        "Does our `face_detector()` function work as well with a male face? Let's see how our function works with another person with the same first name `Taylor`, Taylor Eighmy -- The President of UT San Antonio?\n",
        "\n",
        "The code in the cell below uses the function `face_detector()` to analyze an image of Taylor Eighmy (`TaylorEighmy.jpg`)."
      ],
      "metadata": {
        "id": "9Mrt8H0R81h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1B: Detect Face\n",
        "\n",
        "# Define image path\n",
        "IMAGE_PATH = 'TaylorEighmy.jpg'\n",
        "\n",
        "face_detector(IMAGE_PATH)"
      ],
      "metadata": {
        "id": "ADg2vo8j890E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image08G.png)"
      ],
      "metadata": {
        "id": "Ayg-LmKm9FYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `face_detector()` again has no trouble \"seeing\"  a face in the image.\n",
        "\n",
        "Here are the coordinates for the \"box\" the function placed around President Eighmy's face:\n",
        "\n",
        "~~~text\n",
        "[98, 366, 284, 180]\n",
        "~~~\n",
        "\n"
      ],
      "metadata": {
        "id": "AgGFr87sAPid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1A: Detect Face**\n",
        "\n",
        "So far, we have only used images that contained a portrait of a person. Can our face detector find the face in an image of the whole person?\n",
        "\n",
        "In the cell below, use the function `face_detect()` to analyze an image of Taylor Swift where she is standing outside (`Taylor2.jpg`)."
      ],
      "metadata": {
        "id": "yQn3GXQe9OnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1A here\n",
        "\n"
      ],
      "metadata": {
        "id": "H1OGthZ09THz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image09G.png)"
      ],
      "metadata": {
        "id": "6TkU0X_b9WqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time our `face_detector()` couldn't pick Taylor's face. Perhaps its too small relatve to the image?\n"
      ],
      "metadata": {
        "id": "21FlJT_10MI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1B: Detect Face**\n",
        "\n",
        "An interesting question is \"How specific is our `face_detector()` function?\" For example, can it tell the difference between a human face and the face of a non-human primate like a baby gorilla?\n",
        "\n",
        "In the cell below, use the function `face_detector()` to analyze an image of a Woman holding a baby gorilla (`WomanGorilla.jpg`)."
      ],
      "metadata": {
        "id": "wP7Yge269bWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1B here\n",
        "\n"
      ],
      "metadata": {
        "id": "fTWlNJzA9fti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image10G.png)"
      ],
      "metadata": {
        "id": "Om3ueI6p9idK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time our `face_detector()` picked-up the woman's face, but not the face of the baby gorilla."
      ],
      "metadata": {
        "id": "F6P8jEg_A4Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1C: Detect Face**\n",
        "\n",
        "What about a face that is clearly not human, but has some human-like features?\n",
        "\n",
        "In the cell below, use the function `face_detector()` to analyze an image of .**ET**, the Extra-Terrestrial, from the 1982 science fiction film directed by Steven Spielberg (`ET.jpg`)."
      ],
      "metadata": {
        "id": "7NVtEsrt-xd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1C here\n",
        "\n"
      ],
      "metadata": {
        "id": "RgkJhlPB_Wl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image11G.png)"
      ],
      "metadata": {
        "id": "uKVJKqIo_fhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there is no bounding box, and the printout says `Found 0 face(s)`, our `face_detector()` function didn't find any face when \"looking\" at ET's picture. So clearly there are limits to what is detected as a human face."
      ],
      "metadata": {
        "id": "KPjigt0tBJb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1D: Detect Faces**\n",
        "\n",
        "One final question we might want to ask is how good is our `face_detector()` function at identifying multiple faces of a group of people in a \"normal\" picture--a picture that you might take will your cell phone?\n",
        "\n",
        "In the cell below, use `face_detector()` to analyze an image of Taylor Swift, Travis Kelse and a third person in the image `TaylorTravis.jpg`)."
      ],
      "metadata": {
        "id": "OcoPhtQi_kiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1D here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IwCfhyUa7l6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image12G.png)"
      ],
      "metadata": {
        "id": "TIh2v4jvBX0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though Taylor Swift isn't looking straight into the camera, our `face_detector()` function had no problem \"seeing\" her face along with the faces of the other two men in the picture."
      ],
      "metadata": {
        "id": "SkBsFOB47wWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VGG16 Model**\n",
        "\n",
        "The **VGG16 model** is a convolutional neural network (CNN) architecture developed by the Visual Geometry Group (VGG) at the University of Oxford. It's widely used for image classification tasks. Here are some key points about VGG16:\n",
        "\n",
        "**Architecture**\n",
        "* **16 Layers:** The model has 16 layers with weights, including 13 convolutional layers and 3 fully connected layers.\n",
        "* **3x3 Filters:** It uses small 3x3 convolution filters throughout the network.\n",
        "* **Max Pooling:** It includes max pooling layers to reduce the spatial dimensions of the feature maps.\n",
        "* **Fully Connected Layers:** The final layers are fully connected, followed by a softmax activation function for classification.\n",
        "\n",
        "**Pre-trained Weights**\n",
        "* **ImageNet Pre-training:** The VGG16 model is often pre-trained on the ImageNet dataset, which contains over a million images across 1,000 categories.\n",
        "* **Transfer Learning:** This pre-trained model can be fine-tuned for specific tasks, making it a popular choice for transfer learning.\n",
        "\n",
        "**Applications***\n",
        "* **Image Classification:** VGG16 is used for classifying images into different categories, such as identifying objects, animals, or plants in images.\n",
        "* **Feature Extraction:** It can be used to extract features from images, which can then be used for other machine learning tasks1.\n"
      ],
      "metadata": {
        "id": "rl_-kxWhbuhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use the VGG16 model, we need to create the 2 following functions."
      ],
      "metadata": {
        "id": "Crdn0B25BysH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create Function `load_image()`**\n",
        "\n",
        "The `load_image()` function is designed to load an image from a file, preprocess it, and prepare it for input into a neural network model.\n",
        "\n",
        "#### **Explanation of load_image Function:**\n",
        "1. **Load the Image:** The function uses the PIL library to open the image file specified by the filename and ensures that the image is in RGB format, which is essential for consistent processing.\n",
        "2. **Resize the Image:** It resizes the image to 224x224 pixels, which is the input size expected by many neural network models, like VGG16.\n",
        "3. **Convert Image to Numpy Array:** The function converts the image into a NumPy array, a common format used for numerical computations in machine learning.\n",
        "4. **Expand Dimensions:** It adds an extra dimension to the image array to match the expected input shape for the neural network. This extra dimension represents the batch size.\n",
        "5. **Preprocess the Image:** The function applies model-specific preprocessing to the image array. This step might normalize the pixel values to a range suitable for the neural network model.\n",
        "6. **Return the Preprocessed Image:** Finally, the function returns the preprocessed image array, ready to be used as input for the neural network model."
      ],
      "metadata": {
        "id": "v3FMuLDNcExU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create load image function\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def load_image(filename):\n",
        "    # Open the image file\n",
        "    img = Image.open(filename)\n",
        "\n",
        "    # Convert to RGB (red, green, blue colors)\n",
        "    img = img.convert('RGB')\n",
        "\n",
        "    # Make it the right size (224x224 pixels)\n",
        "    img = img.resize((224, 224))\n",
        "\n",
        "    # Turn image into numbers that computer can understand\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # Add one more dimension (needed for the model)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Preprocess the numbers for the AI model\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    return img_array\n",
        "\n",
        "# How to use:\n",
        "# image_data = load_image(\"my_photo.jpg\")\n"
      ],
      "metadata": {
        "id": "edj6IIAWuf57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create Function `predict_image()`**\n",
        "\n",
        "The code in the cell below creates a function called `predict_image()`. Here is a step-by-step explanation of this function:\n",
        "\n",
        "1. **Input Parameter:**\n",
        "* **img_array:** This is the input image array that you want to classify. It should be preprocessed and in the format expected by the VGG16 model.\n",
        "\n",
        "2. **Model Prediction:**\n",
        "* **preds = model_VGG16.predict(img_array):** This line uses the predict method of the model_VGG16 (a pre-trained VGG16 model) to make predictions on the input image array. The predict method returns a list of probabilities for each of the classes in the dataset.\n",
        "\n",
        "3, **Decode Predictions:**\n",
        "\n",
        "* **return decode_predictions(preds, top=5)[0]:** This line decodes the predicted probabilities into human-readable class names and probabilities. The decode_predictions function takes the following parameters:\n",
        "  - **preds:** The list of predicted probabilities returned by the model.\n",
        "  - **top=5:** This parameter specifies that we want the top 5 predictions.\n",
        "\n",
        "* The [0] at the end selects the top 5 predictions for the first image in the input array (assuming img_array could contain multiple images)."
      ],
      "metadata": {
        "id": "aFoIolXeCG92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function predict_image()\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def predict_image(img_array):\n",
        "    # Give the image to the AI model to guess what it is\n",
        "    predictions = model_VGG16.predict(img_array)\n",
        "\n",
        "    # Turn the AI's guesses into readable words\n",
        "    results = decode_predictions(predictions, top=5)[0]\n",
        "\n",
        "    return results\n",
        "\n",
        "# How to use:\n",
        "# results = predict_image(image_data)\n",
        "# print(results)  # Shows the top 5 guesses\n"
      ],
      "metadata": {
        "id": "vJlsRFIYuxzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Analyze Non-Facial Content\n",
        "\n",
        "The code in the cell below uses our 2 new functions `load_image()` and `predict_image()` to analyze the same picture of Taylor Swift that you used above in **Exercise 1A**. The VGG16 model is _not_ trained to find faces, but to analyze everything else it \"sees\" in the image."
      ],
      "metadata": {
        "id": "PhDXUkydD64K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Analyze non-facial content\n",
        "\n",
        "import torch\n",
        "from torchvision.models import vgg16, VGG16_Weights\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. Load the AI model\n",
        "# In PyTorch, we load specific weights (IMAGENET1K_V1 is the standard ImageNet equivalent)\n",
        "weights = VGG16_Weights.IMAGENET1K_V1\n",
        "model = vgg16(weights=weights)\n",
        "\n",
        "# Set the model to evaluation mode (essential for inference)\n",
        "model.eval()\n",
        "\n",
        "# 2. Load and prepare your photo\n",
        "img_path = \"Taylor2.jpg\"\n",
        "try:\n",
        "    img = Image.open(img_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find image at {img_path}. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "# 3. Preprocessing\n",
        "# PyTorch models come with specific transforms attached to their weights.\n",
        "# This automatically handles resizing (224x224), normalization, and tensor conversion.\n",
        "preprocess = weights.transforms()\n",
        "img_tensor = preprocess(img)\n",
        "\n",
        "# Add a batch dimension (PyTorch expects [Batch, Channels, Height, Width])\n",
        "# Equivalent to tf.expand_dims\n",
        "img_batch = img_tensor.unsqueeze(0)\n",
        "\n",
        "# 4. Ask the AI what it sees\n",
        "with torch.no_grad(): # Tells PyTorch not to calculate gradients (faster for inference)\n",
        "    prediction = model(img_batch)\n",
        "\n",
        "# Apply Softmax to convert raw output (logits) into probabilities\n",
        "probabilities = F.softmax(prediction[0], dim=0)\n",
        "\n",
        "# 5. Turn numbers into readable words\n",
        "# Get the top 5 probabilities and their indices\n",
        "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "\n",
        "# PyTorch weights include the class names metadata\n",
        "class_names = weights.meta[\"categories\"]\n",
        "\n",
        "# 6. Show the photo\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# 7. Show what the AI thinks it is\n",
        "print(\"AI's guesses:\")\n",
        "for i in range(5):\n",
        "    label = class_names[top5_catid[i]]\n",
        "    score = top5_prob[i].item()\n",
        "    print(f\"{label}: {score*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "q15P_vnLxUEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image03E.png)"
      ],
      "metadata": {
        "id": "Wq-QSomoEMuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analysis of the output**\n",
        "\n",
        "Let's take a closer look at the output since it gives us some insight into how the **VGG16** neural network extracts features and classifies them.\n",
        "\n",
        "#### **Downloaded Files**\n",
        "\n",
        "First, if this is your first time running the code, you will see that PyTorch downloaded the model weights to a local cache (usually `~/.cache/torch/hub/checkpoints/`). Unlike the previous TensorFlow example, PyTorch handles these files slightly differently:\n",
        "\n",
        "1.  **The Weights File (`.pth`):**\n",
        "    Instead of an `.h5` file, PyTorch downloads a file (often named something like `vgg16-397923af.pth`).\n",
        "    * **vgg16:** Indicates the model architecture.\n",
        "    * **pth:** Stands for a PyTorch checkpoint. This is a serialized pickle file containing the learned parameters (weights and biases) of the network.\n",
        "\n",
        "    > This file allows you to load the pre-trained VGG16 model in PyTorch, leveraging features learned from ImageNet without training from scratch.\n",
        "\n",
        "2.  **Class Labels (Internal Metadata):**\n",
        "    You won't see a separate `imagenet_class_index.json` file download.\n",
        "    * In modern `torchvision`, the class names are bundled directly with the model weights object (`weights.meta[\"categories\"]`).\n",
        "    * This ensures that the preprocessing transforms and the class labels always match the specific version of the weights you are using.\n",
        "\n",
        "#### **Image Analysis**\n",
        "\n",
        "By \"looking\" at the image of Taylor Swift, the **VGG16 model** correctly identifies that the image contained a pair of jeans (jean: ~70%), but since the image did not include a full view of her legs, the model might assign a probability to a miniskirt.\n",
        "\n",
        "The model was unsure if it \"saw\" Taylor wearing a `cardigan` sweater or a sweatshirt. The VGG16 model didn't get either item exactly right, but Taylor wasn't really wearing her sweater in a typical manner either.\n",
        "\n",
        "Finally, Taylor Swift's heavy eye make-up made it appear to the model that there was a small chance of a pair of sunglasses."
      ],
      "metadata": {
        "id": "oiaRMG9byXTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Analyze Non-Facial Content**\n",
        "\n",
        "In the cell below, use the VGG16 model to make predictions about the contents of an image of Kelse Travis in a football uniform (`Travis2.jpg`)."
      ],
      "metadata": {
        "id": "J98LVuPN8ub0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "puwDAMo48xbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image04E.png)"
      ],
      "metadata": {
        "id": "AeqA1Dnk8z90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again the output is interesting. The VGG16 model correctly identifies that image contained a football helmet (`football_helmet: 42.24%`) even though only a small part (the face mask) is actually visible in the image. Somewhat bizarrely, the model thought it \"saw\" a basketball (48.50%)."
      ],
      "metadata": {
        "id": "5hDmcpn184Wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Face Extraction from Image**\n",
        "\n",
        "**Face extraction** in Convolutional Neural Networks (CNNs) refers to the process of detecting and isolating faces from an image before feeding them into a CNN for further processing, such as recognition or classification. This step is crucial because it ensures that the CNN focuses only on the relevant part of the image (the face) and ignores the background or other irrelevant detail."
      ],
      "metadata": {
        "id": "axjJIaLgEiPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create Function `extract_face_from_image()`**\n",
        "\n",
        "The code in the cell below, creates a function called `extract_face_from_image()`. The function uses the MTCNN neural network to extract facial image(s) from a larger image and then returns the extracted face as an image to the program that called the function."
      ],
      "metadata": {
        "id": "no6ukdjb9E3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create extract face function\n",
        "\n",
        "from numpy import asarray\n",
        "from PIL import Image\n",
        "import face_recognition\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def extract_face_from_image(image_path, required_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Detect faces in an image and extract them as resized arrays.\n",
        "\n",
        "    Args:\n",
        "    - image_path (str): Path to the input image.\n",
        "    - required_size (tuple): Desired dimensions for the output face images (width, height).\n",
        "\n",
        "    Returns:\n",
        "    - face_images (list): List of face arrays, resized to the required size.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
        "\n",
        "    try:\n",
        "        # Load the image\n",
        "        image = plt.imread(image_path)\n",
        "\n",
        "        # Detect face locations\n",
        "        face_locations = face_recognition.face_locations(image)\n",
        "\n",
        "        # If no faces found, return empty list\n",
        "        if not face_locations:\n",
        "            print(f\"No faces detected in {image_path}\")\n",
        "            return []\n",
        "\n",
        "        # List to store extracted face arrays\n",
        "        face_images = []\n",
        "\n",
        "        # Loop through detected faces\n",
        "        for i, face in enumerate(face_locations):\n",
        "            # Extract the bounding box for the face\n",
        "            top, right, bottom, left = face\n",
        "\n",
        "            # Extract the face using the bounding box\n",
        "            face_boundary = image[top:bottom, left:right]\n",
        "\n",
        "            # Resize the face to the required dimensions\n",
        "            face_image = Image.fromarray(face_boundary)\n",
        "            face_image = face_image.resize(required_size)\n",
        "            face_array = asarray(face_image)\n",
        "\n",
        "            # Append the resized face array to the list\n",
        "            face_images.append(face_array)\n",
        "\n",
        "        print(f\"Found {len(face_images)} face(s) in {image_path}\")\n",
        "        return face_images\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "LKOivc7O3J7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Extract Face from Image\n",
        "\n",
        "The code in the cell below, uses our function `extract_face_from_image()` to extract Taylor Swift's face from the image of her used above in Example 2.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorSwift2.jpg)"
      ],
      "metadata": {
        "id": "ole0y-2y2NCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Extract face\n",
        "\n",
        "# Define Image path\n",
        "IMAGE_PATH = \"Taylor2.jpg\"\n",
        "\n",
        "extracted_face = extract_face_from_image(IMAGE_PATH)\n",
        "\n",
        "# Display the first face from the extracted faces\n",
        "plt.imshow(extracted_face[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XbUSspWm9Qjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image15G.png)"
      ],
      "metadata": {
        "id": "6zuWWX0h975n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `extract_face_from_image()` function had no trouble with this image."
      ],
      "metadata": {
        "id": "IprevHQQ97m_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Extract Face from Image**\n",
        "\n",
        "In the cell below, use our custom function `extract_face_from_image()` to extract Kelse Travis' face from the image of him standing in his football uniform (`Travis2.jpg`) that was used above in **Exercise 2**.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TravisKelce2.jpg)"
      ],
      "metadata": {
        "id": "ptyNFt4hD1pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n"
      ],
      "metadata": {
        "id": "LyP3XERu-H7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image16G.png)"
      ],
      "metadata": {
        "id": "Qszl3oXr-HZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CNNs to Automatically Blur Faces in Images**\n",
        "\n",
        "Automatically **blurring faces** in images is important for several reasons, especially when it comes to privacy, security, and ethical considerations:\n",
        "\n",
        "#### **Privacy Protection**\n",
        "* **Personal Privacy:** Blurring faces helps protect individuals' privacy by making them less recognizable in images. This is crucial in situations where individuals have not given their consent to be photographed or identified.\n",
        "* **Data Privacy Regulations:** Regulations like the General Data Protection Regulation (GDPR) in the EU emphasize the importance of protecting personal data. Automatically blurring faces ensures compliance with these regulations.\n",
        "\n",
        "#### **Security Concerns**\n",
        "* **Anonymity:** In sensitive contexts, such as protests or political gatherings, blurring faces can protect individuals from potential repercussions or surveillance.\n",
        "* **Witness Protection:** In law enforcement and legal contexts, blurring faces of witnesses and victims can protect their identities and ensure their safety.\n",
        "\n",
        "#### **Ethical Considerations**\n",
        "* **Consent:** It is ethically responsible to blur faces when sharing images of people who haven't explicitly consented to be photographed or identified. This is especially important in public places or when dealing with vulnerable populations, such as children.\n",
        "* **Minimizing Harm:** By blurring faces, content creators and organizations can minimize the potential harm that could come from individuals being identified without their permission.\n",
        "\n",
        "#### **Public Sharing and Media**\n",
        "* **Social Media:** Automatically blurring faces is particularly important for images shared on social media, where privacy settings might not be strict, and images can spread quickly.\n",
        "* **News and Journalism:** In journalism, blurring faces can protect the identities of individuals in sensitive or dangerous situations while still conveying important information.\n",
        "\n",
        "#### **Example Use Cases**\n",
        "* **CCTV Footage:** Automatically blurring faces in CCTV footage can help maintain the privacy of individuals who are not involved in any incidents being monitored.\n",
        "* **Photo Albums:** Photo-sharing platforms can use face blurring to respect the privacy of people in group photos before these images are made public."
      ],
      "metadata": {
        "id": "C5GQcsVED7Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that correct packages have been loaded, we can create our image generators.\n"
      ],
      "metadata": {
        "id": "iGnwqL4GEIWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Blur Faces in an Image\n",
        "\n",
        "Let's see what we can do with the `face_recognition` package. One practical function is to automatically find faces in an image and blur it.\n",
        "\n",
        "The code in the cell below uses the same image you used in **Exercise 1D** above.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorTravis.jpg)\n"
      ],
      "metadata": {
        "id": "UEQZVkgeBad7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Blur Faces\n",
        "\n",
        "import face_recognition\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Path to group image\n",
        "GROUP_PHOTO = \"TaylorTravis.jpg\"\n",
        "\n",
        "# Load the image\n",
        "image = face_recognition.load_image_file(GROUP_PHOTO)\n",
        "\n",
        "# Find all face locations\n",
        "face_locations = face_recognition.face_locations(image)\n",
        "\n",
        "# Blur faces\n",
        "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "for (top, right, bottom, left) in face_locations:\n",
        "    face = image[top:bottom, left:right]\n",
        "    blurred_face = cv2.GaussianBlur(face, (99, 99), 30)\n",
        "    image[top:bottom, left:right] = blurred_face\n",
        "\n",
        "# Display the image with blurred faces\n",
        "cv2_imshow(image)\n",
        "\n"
      ],
      "metadata": {
        "id": "cBFB4mDOELpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image05B.png)"
      ],
      "metadata": {
        "id": "BIKmmSjH2cxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code worked as expected."
      ],
      "metadata": {
        "id": "flCkwma-BtHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Blur Faces**\n",
        "\n",
        "In the cell below, finds faces and blur them in a group image called `TaylorGroup.jpg`.\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorGroup.jpg)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GLXXPfCSExKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "308ZVg-YB4Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image06B.png)"
      ],
      "metadata": {
        "id": "kWNLjLh6B6_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Facial Analysis**\n",
        "\n",
        "Besides finding and blurring faces, the `facial_recognition` package can also be used to find facial features by identifying and returning the locations of facial landmarks such as eyes, nose, mouth, and chin.\n",
        "\n",
        "In order to utilize this capability, we need to create a new function called `analyze_facial_attribute()` in the cell below."
      ],
      "metadata": {
        "id": "MAJmO4ZGEeWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create Function `analyze_facial_attributes()`**\n",
        "\n",
        "#### Explanation of the `analyze_facial_attributes` function:\n",
        "\n",
        "1. **Load the Image**: The function uses the `face_recognition` library to load an image from the specified file path.\n",
        "2. **Find Face Locations**: It detects all face locations in the image and stores the coordinates of these faces.\n",
        "3. **Find Facial Features**: The function identifies various facial features (landmarks) like eyes, nose, and mouth for each detected face.\n",
        "4. **Display the Image**: It uses the `PIL` library to open and display the image with `matplotlib`, turning off axis labels for a cleaner view.\n",
        "5. **Plot Facial Features**: For each set of facial features, it plots the points using `matplotlib`, connecting the landmarks with lines to visualize the features.\n",
        "6. **Show Image with Landmarks**: The function displays the image with the overlaid facial landmarks.\n",
        "7. **Return Results**: Finally, it returns the face locations and facial landmarks."
      ],
      "metadata": {
        "id": "Dk-lYSkSCEGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function analyze_facial_attributes()\n",
        "\n",
        "import face_recognition\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def analyze_facial_attributes(image_path):\n",
        "    \"\"\"\n",
        "    Analyze facial attributes in an image by detecting faces and facial landmarks.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image file\n",
        "\n",
        "    Returns:\n",
        "        tuple: (face_locations, face_landmarks_list) or (None, None) if no faces found\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"Error: File not found - {image_path}\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        # Load the image\n",
        "        image = face_recognition.load_image_file(image_path)\n",
        "\n",
        "        # Find all face locations in the image\n",
        "        face_locations = face_recognition.face_locations(image)\n",
        "\n",
        "        # If no faces found, return empty results\n",
        "        if not face_locations:\n",
        "            print(f\"No faces detected in {image_path}\")\n",
        "            return [], []\n",
        "\n",
        "        # Find all facial features in the image\n",
        "        face_landmarks_list = face_recognition.face_landmarks(image)\n",
        "\n",
        "        # Display the image with face landmarks\n",
        "        img = Image.open(image_path)\n",
        "\n",
        "        # Set figure size\n",
        "        plt.figure(figsize=(7, 5))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Facial Analysis - {os.path.basename(image_path)}\")\n",
        "\n",
        "        # Define colors for different facial features\n",
        "        feature_colors = {\n",
        "            'chin': 'red',\n",
        "            'left_eyebrow': 'blue',\n",
        "            'right_eyebrow': 'blue',\n",
        "            'nose_bridge': 'green',\n",
        "            'nose_tip': 'green',\n",
        "            'left_eye': 'purple',\n",
        "            'right_eye': 'purple',\n",
        "            'top_lip': 'orange',\n",
        "            'bottom_lip': 'orange'\n",
        "        }\n",
        "\n",
        "        # Draw facial landmarks for each detected face\n",
        "        for i, (face_landmarks, face_location) in enumerate(zip(face_landmarks_list, face_locations)):\n",
        "\n",
        "            # Draw each facial feature with its specific color\n",
        "            for feature, points in face_landmarks.items():\n",
        "                if isinstance(points, list) and len(points) > 0:\n",
        "                    # Convert points to tuples and separate x,y coordinates\n",
        "                    points = [tuple(point) for point in points]\n",
        "                    x, y = zip(*points)\n",
        "\n",
        "                    # Use specific color for each feature type\n",
        "                    color = feature_colors.get(feature, 'white')  # Default to white if unknown feature\n",
        "\n",
        "                    # Plot the landmark points\n",
        "                    plt.plot(x, y, marker='o', markersize=1.5, color=color, alpha=0.7, linewidth=1.0)\n",
        "\n",
        "                    # Connect the points to form features (like eyebrows, lips, etc.)\n",
        "                    if len(points) > 1:\n",
        "                        plt.plot(x, y, linewidth=1.0, color=color, alpha=0.8)\n",
        "\n",
        "            # Draw bounding box around each face\n",
        "            top, right, bottom, left = face_location\n",
        "            plt.plot([left, right, right, left, left],\n",
        "                    [top, top, bottom, bottom, top],\n",
        "                    linewidth=2.0, color='yellow', alpha=0.8)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Analysis complete! Found {len(face_locations)} face(s) in {image_path}\")\n",
        "        return face_locations, face_landmarks_list\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "9Or6Vsib7LFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Analyze Facial Attributes\n",
        "\n",
        "The code in the cell below uses our function `analyze_facial_attributes()` to identify and analyze the facial features in an image of Taylor Swift (`Taylor1.jpg`).\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/TaylorSwift1.jpg)"
      ],
      "metadata": {
        "id": "0z6QZGh63MyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Analyze Facial Attributes\n",
        "\n",
        "# Define Image path\n",
        "image_path = 'Taylor1.jpg'\n",
        "\n",
        "# Analyze facial attributes\n",
        "face_locations, face_landmarks_list = analyze_facial_attributes(image_path)"
      ],
      "metadata": {
        "id": "Hiiy2ICEQyTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image17G.png)"
      ],
      "metadata": {
        "id": "8aYkVhl2CqIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each colored line resprents a different facial feature (attribute) extacted from the image.\n"
      ],
      "metadata": {
        "id": "1WJK-VsWCtjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Analyze Facial Attributes**\n",
        "\n",
        "In the cell below, use `analyze_facial_attributes()` to analyze the facial features in an image of Travis Kelse (`Travis3.jpg`).\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/TravisKelce3.jpg)"
      ],
      "metadata": {
        "id": "M-ISp-QxRCCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nbG9UvhiEyYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image05E.png)"
      ],
      "metadata": {
        "id": "Ihs9z8XYKyUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Facial Recognition using `DeepFace`**\n",
        "\n",
        "**DeepFace** is a **deep learning facial recognition system** developed by a research group at [Facebook](https://en.wikipedia.org/wiki/Facebook). It was designed to identify human faces in digital images with high accuracy.\n",
        "\n",
        "Here are some key points about `DeepFace`:\n",
        "\n",
        "* **Architecture:** DeepFace uses a nine-layer neural network with over 120 million connection weights. This complex architecture allows it to achieve impressive accuracy in facial recognition tasks.\n",
        "\n",
        "* **Training Data:** The system was trained on four million images uploaded by Facebook users. This extensive dataset helped the model learn a wide variety of facial features and variations.\n",
        "\n",
        "* **Accuracy:** DeepFace has an accuracy of 97.35% on the Labeled Faces in the Wild (LFW) dataset, which is comparable to human performance. This means it can sometimes outperform humans in recognizing faces.\n",
        "\n",
        "* **Applications:** Initially, DeepFace was used to alert Facebook users when their face appeared in any photo posted on the platform. Users could then choose to remove their face from the photo if they wished.\n",
        "\n",
        "DeepFace represents a significant advancement in facial recognition technology and has influenced many subsequent developments in the field."
      ],
      "metadata": {
        "id": "Nafwa_m3TwoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install `deepface`**\n",
        "\n",
        "Run the next cell to install DeepFace into your current Colab environment.\n",
        "\n",
        "The command `!pip install -U deepface` is used to install or upgrade the DeepFace library in a Python environment.\n",
        "\n",
        "Heres a breakdown of what the command does:\n",
        "\n",
        "* **!pip:** The exclamation mark (!) indicates that this command should be executed in a Jupyter notebook or similar environment where the ! symbol is used to run shell commands.\n",
        "\n",
        "* **install:** This tells pip (the Python package installer) to install a package.\n",
        "\n",
        "* **-U:** This flag stands for \"upgrade\" and tells pip to upgrade the package to the latest version if it's already installed.\n",
        "\n",
        "* **deepface:** This specifies the name of the package to be installed or upgraded, which in this case is DeepFace.\n",
        "\n",
        "So, running this command will either install the DeepFace library if its not already present in your environment or upgrade it to the latest version if it is already installed. To prevent extra output being printed to your notebook, the `pip` install command if followed by `> /dev/null`."
      ],
      "metadata": {
        "id": "lzbm5Mx-DKiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U deepface > /dev/null"
      ],
      "metadata": {
        "id": "In4FTm1QUEmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you shouldn't see any output."
      ],
      "metadata": {
        "id": "oaf25gkPUPSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------\n",
        "\n",
        "\n",
        "## **A Short History of Emotion Detection**\n",
        "\n",
        "The history of emotion detection using Convolutional Neural Networks (CNNs) reflects the broader advancements in both technology and our understanding of human emotions. Here's an overview:\n",
        "\n",
        "#### Early Days of Emotion Recognition\n",
        "- **19th Century**: The formal study of emotions can be traced back to Charles Darwin, who suggested that emotional expressions have evolved and serve social functions. Although his work did not benefit from modern technology, it laid the groundwork for understanding how behaviors related to emotions could be studied scientifically.\n",
        "- **1970s**: Researchers began to employ more systematic methods to study emotional expressions. The emergence of facial coding systems, such as Paul Ekman's Facial Action Coding System (FACS), transformed emotion recognition research. FACS categorized facial movements and expressions, allowing researchers to conduct more precise analysis based on visible emotional cues.\n",
        "\n",
        "#### Rise of Machine Learning and AI\n",
        "- **Late 20th Century**: The turning point for emotion recognition technologies came with the advent of machine learning (ML) and artificial intelligence (AI). By the early 2000s, the availability of more extensive datasets and more powerful computing resources allowed researchers to explore various algorithms for facial recognition and emotion detection.\n",
        "- **Early 2000s**: Researchers began to use machine learning techniques to automate emotion recognition. These early attempts relied on basic computer vision techniques but struggled to accurately interpret the nuances of human expressions.\n",
        "\n",
        "#### Advancements in Deep Learning\n",
        "- **2010s**: The introduction of deep learning and CNNs revolutionized emotion detection. CNNs, with their ability to learn hierarchical representations of data, proved to be highly effective in recognizing and classifying emotions from facial expressions.\n",
        "- **Recent Developments**: Modern CNNs have achieved impressive accuracy in emotion recognition tasks. Researchers have also explored the intrinsic ability of CNNs to represent the affective significance of visual input, suggesting that emotional perception might be an intrinsic property of the visual cortex.\n",
        "\n",
        "#### Current Trends and Applications\n",
        "- **Applications**: Emotion detection by CNNs is now used in various applications, including social media, customer service, healthcare, and security systems.\n",
        "- **Ethical Considerations**: As emotion detection technologies become more widespread, ethical considerations regarding privacy, consent, and the potential misuse of these technologies have come to the forefront.\n",
        "\n",
        "The history of emotion detection by CNNs showcases the rapid evolution of technology and its impact on our ability to understand and interact with human emotions. It's an exciting field that continues to grow and develop, offering new possibilities for enhancing human-computer interaction.\n",
        "\n",
        "-------------------------------------------\n"
      ],
      "metadata": {
        "id": "0IIW6CJqD1GK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Detect Emotion with `DeepFace`**\n",
        "\n",
        "The `DeepFace` system can analyze facial attributes to predict the age, gender, emotion, and race/ethnicity of the person in the image.\n",
        "\n",
        "In the cell below, we create a function called `detect_emotion()` that uses the DeepFace system."
      ],
      "metadata": {
        "id": "3xh_uux3EA2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create function `detect_emotion()`**\n",
        "\n",
        "The code in the cell below creates a custom function called `detect_emotion()`. This code defines a function detect_emotion that takes an image file path as an input, analyzes the image for emotions using DeepFace, and then displays the detected emotions along with the image. Here's a breakdown of what the code does:\n",
        "\n",
        "* **Function Definition:** The function detect_emotion is defined with the parameter image_path, which represents the file path of the image to be analyzed.\n",
        "* **Analyze the Image**: The DeepFace library's analyze function is used to analyze the image for emotions. It takes the image path and a list of actions (in this case, ['emotion']) to perform the emotion analysis. The result is stored in the variable result.\n",
        "* **Print Detected Emotion:** The dominant emotion detected in the image is printed using print(f\"Detected emotion: {result[0]['dominant_emotion']}\"). It also prints a detailed emotion analysis showing the probabilities of different emotions in the image.\n",
        "* **Display the Image:** The image is opened using the Image.open function from the PIL (Python Imaging Library). It is then displayed using plt.imshow(img) from the matplotlib library, with the axis turned off using plt.axis('off') to avoid showing axis labels.\n",
        "* **Return Result:** The function returns the result variable, which contains the detailed emotion analysis.\n",
        "\n",
        "Overall, this function analyzes the emotions in the given image and provides a visual representation along with detailed emotion probabilities."
      ],
      "metadata": {
        "id": "HE7IWT8rEEBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function `detect_emotion()`\n",
        "\n",
        "from deepface import DeepFace\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def detect_emotion(image_path):\n",
        "    \"\"\"\n",
        "    Detect emotions in an image using DeepFace.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "\n",
        "    Returns:\n",
        "        dict: Emotion analysis results\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Analyze the image for emotions\n",
        "        result = DeepFace.analyze(img_path=image_path, actions=['emotion'])\n",
        "\n",
        "        # Extract emotion information\n",
        "        dominant_emotion = result[0]['dominant_emotion']\n",
        "        emotion_scores = result[0]['emotion']\n",
        "\n",
        "        # Print the detected emotion\n",
        "        print(f\"Detected emotion: {dominant_emotion}\")\n",
        "        print(\"Emotion analysis:\")\n",
        "        for emotion, score in emotion_scores.items():\n",
        "            print(f\"  {emotion}: {score:.2f}%\")\n",
        "\n",
        "        # Display the image with emotion information\n",
        "        img = Image.open(image_path)\n",
        "\n",
        "        # Create figure with subplots\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Add emotion text on the image\n",
        "        emotion_text = f\"Detected: {dominant_emotion}\"\n",
        "        ax.text(0.5, 0.02, emotion_text, transform=ax.transAxes,\n",
        "                ha='center', va='bottom', fontsize=14,\n",
        "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing image: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "02LFVudq9hr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image06E.png)"
      ],
      "metadata": {
        "id": "6P8zJhXeEMRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Detect Emotion\n",
        "\n",
        "Let's see how well our `detect_emotion()` function works.\n",
        "\n",
        "The code in the cell below used the `detect_emotion()` function to analyze an image of Taylor Swift (`Taylor1.jpg`).\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorSwift1.jpg)\n",
        "\n",
        "Before you run the code, what emotion(s) do you think Taylor was feeling when this photograph was taken?"
      ],
      "metadata": {
        "id": "2ne3-VvmUS5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Detect Emotion\n",
        "\n",
        "# Define Image path\n",
        "image_path = 'Taylor1.jpg'\n",
        "\n",
        "# Detect emotion\n",
        "emotion_attributes = detect_emotion(image_path)"
      ],
      "metadata": {
        "id": "Qk5U8xPaUfVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image07E.png)"
      ],
      "metadata": {
        "id": "-5D5nUgRUjI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is what our `detect_emotion()` function predicted:\n",
        "\n",
        "~~~text\n",
        "Detected emotion: neutral\n",
        "Emotion analysis:\n",
        "  angry: 0.31%\n",
        "  disgust: 0.00%\n",
        "  fear: 3.60%\n",
        "  happy: 3.15%\n",
        "  sad: 2.88%\n",
        "  surprise: 0.02%\n",
        "  neutral: 90.03%\n",
        "~~~\n",
        "\n",
        "Here's how to interpret this output:\n",
        "\n",
        "#### **Detected Emotion:**\n",
        "- **Neutral**: The dominant emotion detected in the image is \"neutral,\" which means the person's facial expression does not strongly convey any specific emotion.\n",
        "\n",
        "#### **Emotion Analysis:**\n",
        "The analysis includes the probabilities (in percentages) of various emotions detected in the image:\n",
        "```text\n",
        "  angry: 0.31%\n",
        "  disgust: 0.00%\n",
        "  fear: 3.60%\n",
        "  happy: 3.15%\n",
        "  sad: 2.88%\n",
        "  surprise: 0.02%\n",
        "  neutral: 90.03%\n",
        "```\n",
        "\n",
        "The dominant emotion is \"neutral\" because it has the highest probability (90.03%), indicating that the person's expression is mostly neutral with some minor traces of other emotions."
      ],
      "metadata": {
        "id": "SLAJOyibEzz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6A: Detect Emotion**\n",
        "\n",
        "In Example 6, our `detect_emotion()` function really couldn't figure out what emotion Taylor Swift was feeling in the image.\n",
        "\n",
        "What if you try to analyze an image of Taylor Swift where she appears to be `angry`?\n",
        "\n",
        "In the cell below use the `detect_emotion()` function to analyze an image of Taylor Swift (`TaylorDisgust2.jpg`).\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorDisgust.jpg)"
      ],
      "metadata": {
        "id": "HRGWXOK7UpkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6A here\n",
        "\n"
      ],
      "metadata": {
        "id": "9rXPBp4SE_LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image09E.png)"
      ],
      "metadata": {
        "id": "V40RBvaRU8Oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, our `detect_emotion()` function failed to detect any emotion in the image of Taylor Swift.\n",
        "\n",
        "Here is what our `detect_emotion()` function predicted:\n",
        "\n",
        "~~~text\n",
        "Detected emotion: neutral\n",
        "Emotion analysis:\n",
        "  angry: 0.86%\n",
        "  disgust: 0.00%\n",
        "  fear: 6.06%\n",
        "  happy: 0.85%\n",
        "  sad: 13.08%\n",
        "  surprise: 0.79%\n",
        "  neutral: 78.36%\n",
        "~~~\n",
        "\n",
        "Our `detect_emotion()` function again concluded that there was a 78% probability that Taylor Swift's emotion was `neutral`, which means the person's facial expression does not strongly convey any specific emotion."
      ],
      "metadata": {
        "id": "ELj3-Zk_FLjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6B: Detect Emotion**\n",
        "\n",
        "Maybe there is something \"unusual\" about Taylor Swift's expression? After all,Taylor Swift has been performing for over two decades. She began her career in 2006 with the release of her self-titled debut album. Since then, she has released numerous albums, won multiple awards, and captivated audiences worldwide with her remarkable talent as a singer-songwriter.\n",
        "\n",
        "Let's use an image that clear shows a person with strong emotions. Specifically, let's use an image of angry Chinese woman that was generated by AI.\n",
        "\n",
        "In the cell below, use the `detect_emotion()` function to analyze the image `ChineseAngry.jpg`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/ChineseAngry.jpg)"
      ],
      "metadata": {
        "id": "gmQ7IhcOVC-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6B here\n",
        "\n"
      ],
      "metadata": {
        "id": "AqojUZhrFTN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image10E.png)"
      ],
      "metadata": {
        "id": "0BrDYe4XVCX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time our function worked as expected.\n",
        "\n",
        "Here is what our `detect_emotion()` function predicted after \"seeing\" the image:\n",
        "\n",
        "~~~text\n",
        "Detected emotion: angry\n",
        "Emotion analysis:\n",
        "  angry: 88.78%\n",
        "  disgust: 0.02%\n",
        "  fear: 10.39%\n",
        "  happy: 0.02%\n",
        "  sad: 0.40%\n",
        "  surprise: 0.06%\n",
        "  neutral: 0.33%\n",
        "~~~\n",
        "\n",
        "Unlike the `neutral` emotion for the Taylor Swift images, this time the software predicted almost a 90% probability that the woman in the picture was angry."
      ],
      "metadata": {
        "id": "YmVvKGTrFe3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6C: Detect Emotion**\n",
        "\n",
        "The image used in **Exercise 6B** was AI generated which begs the question of whether our function works as well with a real image?\n",
        "\n",
        "In the cell below use the `detect_emotion()` function to analyze the image `TaylorEighmy.jpg`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorEighmy.jpg)"
      ],
      "metadata": {
        "id": "GkI2vqQ5VVsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6C here\n",
        "\n"
      ],
      "metadata": {
        "id": "yr9jWR6cVVXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image11E.png)"
      ],
      "metadata": {
        "id": "lvPgg1R8VUcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is what our `detect_emotion()` function predicted after \"seeing\" the image of President Eighmy:\n",
        "\n",
        "~~~text\n",
        "Detected emotion: happy\n",
        "Emotion analysis:\n",
        "  angry: 0.00%\n",
        "  disgust: 0.00%\n",
        "  fear: 0.00%\n",
        "  happy: 99.35%\n",
        "  sad: 0.00%\n",
        "  surprise: 0.00%\n",
        "  neutral: 0.65%\n",
        "~~~\n",
        "\n",
        "Our function worked very well, predicting that there was a 99% chance that the President was happy when this picture was taken."
      ],
      "metadata": {
        "id": "qq-l2QibFu7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6D: Detect Emotion**\n",
        "\n",
        "Humans are extremely good at detecting a \"fake smile\". A \"fake smile\" is an expression where a person smiles, but the smile is not genuine or sincere. A genuine smile, known as a **Duchenne smile**, involves the activation of the zygomatic major muscle (which raises the corners of the mouth) and the orbicularis oculi muscle (which causes the eyes to crinkle). This combination creates a natural and authentic smile. Authentic emotional expressions involve involuntary muscle movements that are difficult to consciously replicate. For example, a genuine smile engages the _orbicularis oculi_ muscle around the eyes, creating \"crow's feet,\" which is hard to fake. Audiences are generally adept at detecting insincerity. A performance that lacks genuine emotion can come across as forced or unconvincing, breaking the immersion and reducing the impact of the story.\n",
        "\n",
        "In the TV series, the Big Bang Series, the character Sheldon Cooper uses this insincerity for comic effect when I forces a patently exagerrated \"smile\" in this image.\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/SheldonSmile.jpg)\n",
        "\n",
        "\n",
        "Let's see what happens when your ask our function to analyze this image of Sheldon Cooper (`SheldonSmile.jpg`) giving a very obviously \"faked smile\" from the Big Bang Series?"
      ],
      "metadata": {
        "id": "gpd_P0QaVi85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6D here\n",
        "\n"
      ],
      "metadata": {
        "id": "0S1xYQsxVoI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image12E.png)"
      ],
      "metadata": {
        "id": "GS0XZZW-WFk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is what our `detect_emotion()` function predicted after \"seeing\" the image of a \"smiling\" Sheldon Cooper:\n",
        "\n",
        "~~~text\n",
        "Detected emotion: happy\n",
        "Emotion analysis:\n",
        "  angry: 0.00%\n",
        "  disgust: 0.00%\n",
        "  fear: 0.00%\n",
        "  happy: 100.00%\n",
        "  sad: 0.00%\n",
        "  surprise: 0.00%\n",
        "  neutral: 0.00%\n",
        "~~~\n",
        "\n",
        "This is pretty funny! Our `detect_emotion()` function thought that there was a nearly a 100% chance that Sheldon was `happy` in this picture. Clearly, our `detect_emotion()` function is unable to spot a \"fake smile\"."
      ],
      "metadata": {
        "id": "O9lvu7nYWIrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FaceNet**\n",
        "\n",
        "**FaceNet** is a facial recognition system developed by researchers at Google, including Florian Schroff, Dmitry Kalenichenko, and James Philbin. It was first presented at the 2015 IEEE Conference on Computer Vision and Pattern Recognition.\n",
        "\n",
        "Here are some key points about FaceNet:\n",
        "\n",
        "* **Deep Convolutional Network:** FaceNet uses a deep convolutional neural network (CNN) to learn a mapping from face images to a 128-dimensional Euclidean space. This means that each face image is represented as a 128-dimensional vector, and the similarity between faces can be measured by the Euclidean distance between these vectors.\n",
        "* **Triplet Loss Function:** The system uses a triplet loss function to train the network. This involves comparing a \"triplet\" of images: an anchor image, a positive image (same person as the anchor), and a negative image (different person)1. The goal is to minimize the distance between the anchor and the positive while maximizing the distance between the anchor and the negative.\n",
        "* **High Accuracy:** FaceNet achieved an accuracy of 99.63% on the Labeled Faces in the Wild (LFW) dataset, which was the highest score at the time. This high accuracy makes it suitable for various applications, including face verification, recognition, and clustering.\n",
        "* **Efficiency:** By directly optimizing the embedding itself rather than using an intermediate bottleneck layer, FaceNet achieves greater representational efficiency. It can perform face recognition tasks using only 128 bytes per face.\n",
        "\n",
        "FaceNet has been influential in the field of facial recognition and has inspired many subsequent developments and implementations."
      ],
      "metadata": {
        "id": "mdRQgUDEGMTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7: Verify Faces\n",
        "\n",
        "Another capability of `DeepFace` is its ability to identify and verify faces in digital images with high precision.\n",
        "\n",
        "The code in the cell below, uses `DeepFace` in combination with `FaceNet` to identify and verify faces using a `know_image` as a reference and an `unknown_image` as the test image.\n",
        "\n",
        "Here is the known image:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorSwift1.jpg)\n",
        "\n",
        "\n",
        "\n",
        "And here is the unknown image:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorDisgust2.jpg)"
      ],
      "metadata": {
        "id": "_GaN23AxGUJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: Verify faces\n",
        "\n",
        "from deepface import DeepFace\n",
        "\n",
        "# Image path known person\n",
        "KNOWN_PERSON = 'Taylor1.jpg'\n",
        "\n",
        "# Image path to unknown person\n",
        "UNKNOWN_PERSON = 'TaylorDisgust2.jpg'\n",
        "\n",
        "# Perform face verification using Facenet\n",
        "result = DeepFace.verify(KNOWN_PERSON, UNKNOWN_PERSON, model_name='Facenet')\n",
        "\n",
        "# Print results\n",
        "if result[\"verified\"]:\n",
        "    print(\"Faces Matched\")\n",
        "else:\n",
        "    print(\"Faces Not Matched\")"
      ],
      "metadata": {
        "id": "mfPINOEhGS-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image13E.png)"
      ],
      "metadata": {
        "id": "mNGFTYQ6Gf64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7A: Verify Faces**\n",
        "\n",
        "In the cell below, write the code to verify that Travis Kelce, shown in this picture:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TravisKelce3.jpg)\n",
        "\n",
        "\n",
        "is also seen in this picture:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorTravis.jpg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the filename `Travis3.jpg` for the KNOWN_PERSON and `TaylorTravis.jpg` for the UNKNOWN_PERSON."
      ],
      "metadata": {
        "id": "kmblqS8RGlmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 7A here\n",
        "\n"
      ],
      "metadata": {
        "id": "gf5PG_jKGsBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image16B.png)\n",
        "\n",
        "That's pretty impressive since Travis Kelce looked pretty different in the two images."
      ],
      "metadata": {
        "id": "ND3r8vDXGwMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7B: Verify Faces**\n",
        "\n",
        "Before we end this lesson, we should make sure that our software can also tell when two faces are **not** a \"match\".\n",
        "\n",
        "In the cell below, write the code to verify that Travis Kelce, shown in this picture:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TravisKelce3.jpg)\n",
        "\n",
        "\n",
        "with the picture of UTSA President, Taylor Eighmy:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorEighmy.jpg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the filename `Travis3.jpg` for the KNOWN_PERSON and `TaylorEighmy.jpg` for the UNKNOWN_PERSON."
      ],
      "metadata": {
        "id": "wyUdlGX-G4du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 7B here\n",
        "\n"
      ],
      "metadata": {
        "id": "zK1h5o7SG7g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image17B.png)\n",
        "\n",
        "Since Travis Kelce and President Eighmy look pretty different, it's good to see this result."
      ],
      "metadata": {
        "id": "C0cJTS87HZTF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MhC_-6ebE3l"
      },
      "source": [
        "# **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_03_3.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas. Make sure you turn in a PDF of a COPY of the lesson that was saved on your GDroive and not the original Colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **Google Nano Banana Pro**\n",
        "\n",
        "##### **Turn your visions into studio-quality designs with unprecedented control, improved text rendering and enhanced world knowledge.**\n",
        "\n",
        "**How Nano Banana Pro helps you bring any idea or design to life**\n",
        "\n",
        "Nano Banana Pro can help you visualize any idea and design anything  from prototypes, to representing data as infographics, to turning handwritten notes into diagrams.\n",
        "\n",
        "With Nano Banana Pro, now you can:\n",
        "\n",
        "**Generate more accurate, context-rich visuals based on enhanced reasoning, world knowledge and real-time information.**\n",
        "\n",
        "With Gemini 3s advanced reasoning, Nano Banana Pro doesn't just create beautiful images, it also helps you create more helpful content. You can get accurate educational explainers to learn more about a new subject, like context-rich infographics and diagrams based on the content you provide or facts from the real world. Nano Banana Pro can also connect to Google Search's vast knowledge base to help you create a quick snapshot for a recipe or visualize real-time information like weather or sports.\n",
        "\n",
        "###### **Infographic about plants using nano banana pro**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image15E.png)\n",
        "\n",
        "\n",
        "##### **Infographic about making chai made using Nano Banana Pro**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image16E.png)\n",
        "\n",
        "\n",
        "We used Nano Banana Pro to pull in real-time weather via Search grounding to build a pop-art infographic.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image17E.png)\n",
        "\n",
        "Generate better visuals with more accurate, legible text directly in the image in multiple languages\n",
        "\n",
        "Nano Banana Pro is the best model for creating images with correctly rendered and legible text directly in the image, whether youre looking for a short tagline, or a long paragraph. Gemini 3 is great at understanding depth and nuance, which unlocks a world of possibilities with image editing and generation  especially with text. Now you can create more detailed text in mockups or posters with a wider variety of textures, fonts and calligraphy. With Geminis enhanced multilingual reasoning, you can generate text in multiple languages, or localize and translate your content so you can scale internationally and/or share content more easily with friends and family.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image18E.png)\n",
        "\n",
        "The word 'BERLIN' integrated into the architecture of a city block, spanning across multiple buildings. Prompt: View of a cozy street in Berlin on a bright sunny day, stark shadows. the old houses are oddly shaped like letters that spell out \"BERLIN\" Colored in Blue, Red, White and black. The houses still look like houses and the resemblance to letters is subtle.\n",
        "Crash, whoosh, shiver, bang, drip, squeeze, roar, wobble in stylized fonts\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image19E.png)\n",
        "\n",
        "Calligraphy inspired by meaning, showcasing the ability to generate expressive text with a wider variety of textures and fonts. A graphic design featuring the word 'TYPOGRAPHY' with a retro, screen-printed texture. The word \"TYPOGRAPHY\" in bold, stacked blue and magenta sans-serif text with a halftone/distressed effect.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image20E.png)\n",
        "\n",
        "Blending text and texture in a creative way by integrating the phrase into a woodchopping scene. Create an image showing the phrase \"How much wood would a woodchuck chuck if a woodchuck could chuck wood\" made out of wood chucked by a woodchuck.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image21E.png)\n",
        "\n",
        "\n",
        "Create high-fidelity visuals with upgraded creative capabilities\n",
        "\n",
        "Consistency by design: With Nano Banana Pro, you can blend more elements than ever before, using up to 14 images and maintaining the consistency and resemblance of up to 5 people. Whether turning sketches into products or blueprints into photorealistic 3D structures, you can now bridge the gap between concept and creation. Apply your desired visual look and feel to your mockups with ease, ensuring your branding remains seamless and consistent across every touchpoint.\n",
        "\n",
        "### **Fluffy characters watching TV**\n",
        "\n",
        "Maintaining the consistency of up to 14 inputs, including multiple characters, across a complex composition.\n",
        "\n",
        "Prompt: A medium shot of the 14 fluffy characters sitting squeezed together side-by-side on a worn beige fabric sofa and on the floor. They are all facing forwards, watching a vintage, wooden-boxed television set placed on a low wooden table in front of the sofa. The room is dimly lit, with warm light from a window on the left and the glow from the TV illuminating the creatures' faces and fluffy textures. The background is a cozy, slightly cluttered living room with a braided rug, a bookshelf with old books, and rustic kitchen elements in the background. The overall atmosphere is warm, cozy, and amused.\n",
        "combining images of a gown, plants and a chair into one image\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image22E.png)\n",
        "\n",
        "\n",
        "### **Craft lifestyle scenes by combining multiple elements**\n",
        "\n",
        "Prompt: Combine these images into one appropriately arranged cinematic image in 16:9 format and change the dress on the mannequin to the dress in the image\n",
        "generation of a futuristic sunset image\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image25E.png)\n",
        "\n",
        "Create surreal landscapes by combining multiple input elements.\n",
        "\n",
        "Prompt: Combine these images into one appropriately arranged cinematic image in 16:9 format\n",
        "Generated image of a fashion show from multiple elements\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image23E.png)\n",
        "\n",
        "A high-fashion editorial shot set in a desert landscape that maintains the consistency and resemblance of the people from the 6 input photos.\n",
        "\n",
        "Prompt: Put these five people and this dog into a single image, they should fit into a stunning award-winning shot in the style if [sic] a fashion editorial. The identity of all five people and their attire and the dog must stay consistent throughout but they can and should be seen from different angles and distances in [sic] as is most natural and suitable to the scene. Make the colour and lighting look natural on them all, they look like they naturally fit into this fashion show.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image26E.png)\n",
        "\n",
        "Studio-quality creative controls: With Nano Banana Pro's new capabilities we are putting advanced creative controls directly into your hands. Select, refine and transform any part of an image with improved localized editing. Adjust camera angles, change the focus and apply sophisticated color grading, or even transform scene lighting (e.g. changing day to night or creating a bokeh effect). Your creations are ready for any platform, from social media to print, thanks to a range of available aspect ratios and available 2K and 4K resolution\n",
        "\n",
        "### **Changing the aspect ratio of an image using Nano Banana**\n",
        "\n",
        "Change the look and feel of an image for a range of platforms by adapting the aspect ratio.\n",
        "\n",
        "Prompt: change aspect ratio to 1:1 by reducing background. The character, remains exactly locked in its current position\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image24E.png)\n",
        "\n",
        "\n",
        "#### **A fox in the snow in the daytime and at night**\n",
        "\n",
        "Lighting and focus controls applied to transform a scene from day to night.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image27E.png)\n",
        "\n",
        "Prompt: Turn this scene into nighttime\n",
        "Changing the lighting on a man's face to just show the eyes\n",
        "\n",
        "Obscure or enlighten a section of your image with lighting controls to achieve specific dramatic effects.\n",
        "\n",
        "\n",
        "Prompt: Generate an image with an intense chiaroscuro effect. The man should retain his original features and expression. Introduce harsh, directional light, appearing to come from above and slightly to the left, casting deep, defined shadows across the face. Only slivers of light illuminating his eyes and cheekbones, the rest of the face is in deep shadow.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_3_image28E.png)\n",
        "\n",
        "## **Bottom Line**\n",
        "\n",
        "Nano Banana Pro can help you visualize any idea and design anything  from prototypes, to representing data as infographics, to turning handwritten notes into diagrams.\n",
        "\n"
      ],
      "metadata": {
        "id": "mRPIbwiJHhOc"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}