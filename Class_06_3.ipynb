{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_06_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ki7Qaf8aomp"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZuqN8TMaomp"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb7ypOhxaomp"
      },
      "source": [
        "**Module 6: Convolutional Neural Networks (CNN) for Computer Vision**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 6 Material\n",
        "\n",
        "* Part 6.1: Using Convolutional Neural Networks\n",
        "* Part 6.2: Using Pretrained Neural Networks with Keras\n",
        "* **Part 6.3: Facial Recognition and Analysis**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6LD8wUiLJwH"
      },
      "source": [
        "### **---WARNING---WARNING---WARNING---WARNING---**\n",
        "\n",
        "You will **_not_** be able to run all of the Examples and **Exercises** in this lesson unless you change your Runtime type to a GPU or a TPU. You will receive an error if you try to use the CPU. The `L4 GPU` is perfectly fine for this lesson and it is available to Colab users who are using it for free or have a paid subscription. You should change your runtime *now* before you start this lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKQylnEiLDUM"
      },
      "source": [
        "### Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n",
        "  Running the following code will map your GDrive to ```/content/drive```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8qGczM1Kem1"
      },
      "outputs": [],
      "source": [
        "# YOU MUST RUN THIS CODE CELL FIRST\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5v0g41mrY91"
      },
      "source": [
        "# **Part 6.3: Facial Recognition and Analysis**\n",
        "\n",
        "\n",
        "The history of facial recognition using cascaded convolutional networks (CNNs) is quite fascinating and has evolved significantly over the years. Here's a brief overview:\n",
        "\n",
        "**Early Developments**\n",
        "* **Viola-Jones Algorithm (2001):** The Viola-Jones algorithm was one of the earliest and most influential methods for real-time face detection. It used Haar-like features and a cascade of classifiers trained with AdaBoost to detect faces quickly and accurately.\n",
        "\n",
        "**Introduction of CNNs**\n",
        "* **Convolutional Neural Networks (CNNs):** In the early 2010s, the introduction of CNNs revolutionized facial recognition technology. CNNs could learn complex features directly from data, making them more robust to variations in pose, expression, and lighting.\n",
        "\n",
        "**Cascaded CNNs**\n",
        "* **Cascade Architecture:** To improve performance and efficiency, researchers developed cascaded CNN architectures. These architectures use multiple stages of CNNs, where each stage refines the results of the previous one. This approach helps in quickly rejecting non-face regions and focusing on challenging candidates.\n",
        "\n",
        "**MTCNN (2016)**\n",
        "* **Multitask Cascaded Convolutional Networks (MTCNN):** MTCNN is a notable example of a cascaded CNN architecture designed for face detection and alignment. It consists of three stages: PNet (Proposal Network), RNet (Refine Network), and ONet (Output Network)4. MTCNN can detect faces and facial landmarks with high accuracy and efficiency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Pji0hj2zNJ"
      },
      "source": [
        "## **MTCNN (Multitask Cascaded Convolutional Networks)**\n",
        "\n",
        "The **MTCNN (Multitask Cascaded Convolutional Networks)** package is a robust face detection and alignment library implemented for Python (≥ 3.10) and TensorFlow (≥ 2.12). It is designed to detect faces and their landmarks using a multitask cascaded convolutional network. Here are some key features:\n",
        "\n",
        "1. **Face Detection:** MTCNN uses a cascade of three networks (PNet, RNet, and ONet) to detect faces. PNet scans the image and proposes candidate face regions, RNet refines these proposals, and ONet detects facial landmarks and provides final refinement2.\n",
        "2. **Facial Landmark Detection:** It can detect key facial landmarks such as eyes, nose, and mouth.\n",
        "3. **Batch Processing:** Supports processing multiple images at once.\n",
        "4. **Performance:** Optimized for faster execution by avoiding unnecessary operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZk20wGz3ZRJ"
      },
      "source": [
        "### Run the next cell to install MTCNN\n",
        "\n",
        "The software package `mtcnn` is not part of the standard Colab library, so we need to install it ourselves by running the next code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKRh9PRtjnx5"
      },
      "outputs": [],
      "source": [
        "!pip install mtcnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryu7PWIOspEf"
      },
      "source": [
        "If the code is correct, you should see something like the following:\n",
        "\n",
        "~~~text\n",
        "Collecting mtcnn\n",
        "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
        "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (1.4.2)\n",
        "Collecting lz4>=4.3.3 (from mtcnn)\n",
        "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
        "Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
        "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 20.7 MB/s eta 0:00:00\n",
        "Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
        "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 52.5 MB/s eta 0:00:00\n",
        "Installing collected packages: lz4, mtcnn\n",
        "Successfully installed lz4-4.3.3 mtcnn-1.0.0\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV8-gpeA4Voo"
      },
      "source": [
        "### Download Images for Class_06_3\n",
        "\n",
        "The code in the cell below creates a custom function for this lesson called `store_image()` that uses `urllib.request()`.\n",
        "\n",
        "`urllib.request()` is a module in Python's standard library used for opening and reading URLs. It's part of the larger urllib package, which handles URL operations like fetching data across the web.\n",
        "\n",
        "The cell then reads several image files from the course file server that we will use in this lesson.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l27xhL6kOxl5"
      },
      "outputs": [],
      "source": [
        "# Download images\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "# Function to download and store images\n",
        "def store_image(url, local_file_name):\n",
        "  with urllib.request.urlopen(url) as resource:\n",
        "    with open(local_file_name, 'wb') as f:\n",
        "      f.write(resource.read())\n",
        "\n",
        "# Images used in this lesson\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/ChineseAngry.jpg','ChineseAngry.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorSwift1.jpg','Taylor1.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorSwift2.jpg','Taylor2.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorSwift3.jpg','Taylor3.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorDisgust.jpg','TaylorDisgust.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorDisgust2.jpg','TaylorDisgust2.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TravisKelce1.jpg','Travis1.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TravisKelce2.jpg','Travis2.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TravisKelce3.jpg','Travis3.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorTravis.jpg','TaylorTravis.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorGroup.jpg','TaylorGroup.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/TaylorEighmy.jpg','TaylorEighmy.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/WomanGorilla.jpg','WomanGorilla.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/ET.jpg','ET.jpg')\n",
        "store_image('https://biologicslab.co/BIO1173/images/class_06/SheldonSmile.jpg','SheldonSmile.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd7hLQpU62fy"
      },
      "source": [
        "### **Create Custom Function `face_detector()`**\n",
        "\n",
        "The code in the cell below createss a function called `face_detector()` using the MTCNN library to detect faces in images. Here’s a breakdown of what each part does:\n",
        "\n",
        "1. **Imports the necessary libraries:**\n",
        "   - *matplotlib.pyplot* for displaying images.\n",
        "   - *MTCNN* from the mtcnn package for face detection.\n",
        "   - *cv2* from OpenCV for image processing.\n",
        "\n",
        "2. **Initializes the MTCNN face detector.**\n",
        "\n",
        "3. **Defines the detect_and_display function:**\n",
        "\n",
        "  - **Loads the image:** Reads the image from the specified path and converts it from BGR (OpenCV default) to RGB.\n",
        "  -**Resizes the image:** Resizes the image to a fixed size of 640x480 pixels.\n",
        "  -**Detects faces:** Uses the MTCNN detector to find faces in the resized image.\n",
        "4. **Displays the image and draws bounding boxes around detected faces:**\n",
        "  - Displays the image using plt.imshow.\n",
        "  - For each detected face, draws a red rectangle around it using the coordinates from face['box'].\n",
        "5. **Prints face details**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-mzDJTfSJIP"
      },
      "outputs": [],
      "source": [
        "# Create face_detector() function\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "import cv2\n",
        "\n",
        "# Initialize the MTCNN face detector\n",
        "detector = MTCNN()\n",
        "\n",
        "def face_detector(image_path):\n",
        "    # Load the image\n",
        "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize the image to a fixed size (e.g., 640x480)\n",
        "    resized_image = cv2.resize(image, (640, 480))\n",
        "\n",
        "    # Detect faces in the resized image\n",
        "    faces = detector.detect_faces(resized_image)\n",
        "\n",
        "    # Display the image and draw bounding boxes for detected faces\n",
        "    plt.imshow(resized_image)\n",
        "    for face in faces:\n",
        "        x, y, width, height = face['box']\n",
        "        plt.gca().add_patch(plt.Rectangle((x, y), width, height, edgecolor='red', facecolor='none'))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Print face details\n",
        "    for face in faces:\n",
        "        print(face)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q1ldN2R8ooV"
      },
      "source": [
        "### Example 1A: Detect Face\n",
        "\n",
        "Let's start by giving our `face_detector()` an easy image to analyse--a close-up portrait of Taylor Swift."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IWA_4WGT0ld"
      },
      "outputs": [],
      "source": [
        "# Example 1: Detect and Display Image\n",
        "\n",
        "# Define image path\n",
        "IMAGE_PATH = 'Taylor1.jpg'\n",
        "\n",
        "face_detector(IMAGE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTt7nAjf-EjY"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EmjZitO-dJI"
      },
      "source": [
        "Our `face_detector()` function had no trouble seeing Taylor Swift's face and putting a red \"bounding box\" around it. Here are the coordinates for the \"box\":\n",
        "\n",
        "~~~text\n",
        "'box': [154, 57, 291, 382],\n",
        "~~~\n",
        "\n",
        "And our `face_detector()` was very confident that it had correctly recognized a face:\n",
        "\n",
        "~~~text\n",
        "'confidence': 0.9988125562667847,\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpdWbN4tIly5"
      },
      "source": [
        "### Example 1B: Detect Face\n",
        "\n",
        "Does our `face_detector()` function work as well with a male face? Let's see how our function works with another person with the same first name `Taylor`, Taylor Eighmy -- The President of The University of Texas at San Antonio?\n",
        "\n",
        "The code in the cell below uses the function `face_detector()` to analyze an image of Taylor Eighmy (`TaylorEighmy.jpg`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WpsbaGdIly5"
      },
      "outputs": [],
      "source": [
        "# Example 1B: Detect Face\n",
        "\n",
        "# Define image path\n",
        "IMAGE_PATH = 'TaylorEighmy.jpg'\n",
        "\n",
        "face_detector(IMAGE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_MPup_GIly5"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1U6Wx34Ily5"
      },
      "source": [
        "Our `face_detector()` again has no trouble \"seeing\"  a face in the image.\n",
        "\n",
        "Here are the coordinates for the \"box\" the function placed around President Eighmy's face:\n",
        "\n",
        "~~~text\n",
        "'box': [204, 64, 167, 217]\n",
        "~~~\n",
        "\n",
        "Again, the confidence level of our `face_detector()` function is extremely high:\n",
        "\n",
        "~~~text\n",
        "confidence': 0.999735414981842\n",
        "~~~\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCUj1nWMEUMy"
      },
      "source": [
        "### **Exercise 1A: Detect Face**\n",
        "\n",
        "So far, we have only used images that contained a portrait of a person. Can our face detector find the face in an image of the whole person?\n",
        "\n",
        "In the cell below, use the function `face_detect()` to analyze an image of Taylor Swift where she is standing outside (`Taylor2.jpg`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlzjR0vOEUMz"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1A here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKRanTvYEUMz"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-IkkrAhEUMz"
      },
      "source": [
        "Our `face_detector()` function can clearly detect a face from a image showing most of a person in a somewhat cluttered environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6cwY6lH_8hq"
      },
      "source": [
        "### **Exercise 1B: Detect Face**\n",
        "\n",
        "An interesting question is \"How specific is our `face_detector()` function?\"\n",
        "\n",
        "For example, can it tell the difference between a human face and the face of a non-human primate like a baby gorilla?\n",
        "\n",
        "In the cell below, use the function `face_detector()` to analyze an image of a Woman holding a baby gorilla (`WomanGorilla.jpg`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uywh4_fD_8hr"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1B here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LEXedkc_8hr"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image06.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq1qhTTA_8hr"
      },
      "source": [
        "Our `face_detector()` function found both the face of the woman and the face of the baby gorilla. However, it should be noted that our function was somewhat less  certain about the baby gorilla (confidence': 0.9592615365982056) than it was for the woman's face (confidence': 0.9990089535713196)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pRWeYcqNNPK"
      },
      "source": [
        "### **Exercise 1C: Detect Face**\n",
        "\n",
        "What about a face that is clearly not human, but has some human-like features?\n",
        "\n",
        "In the cell below, use the function `face_detector()` to analyze an image of .**ET**, the Extra-Terrestrial, from the 1982 science fiction film directed by Steven Spielberg (`ET.jpg`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4FkdLdzNNPL"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1C here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_9H_oc0CumB"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa7AbYWy1l3h"
      },
      "source": [
        "Since there is no bounding box, and no printout, our `face_detector()` function didn't find any face when \"looking\" at ET's picture. So clearly there are limits to what is detected as a human face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx0J8u8jEOlD"
      },
      "source": [
        "### **Exercise 1D: Detect Faces**\n",
        "\n",
        "One final question we might want to ask is how good is our `face_detector()` function at identifying multiple faces of a group of people in a \"normal\" picture--a picture that you might take will your cell phone?\n",
        "\n",
        "In the cell below, use `face_detector()` to analyze an image of Taylor Swift, Travis Kelse and a third person in the image `TaylorTravis.jpg`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJe6Irh2nmID"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1D here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HMiiIUv3wxF"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image07.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-5Q4_R03s6u"
      },
      "source": [
        "Even though Taylor Swift isn't looking straight into the camera, our `face_detector()` function had no problem \"seeing\" her face along with the faces of the other two men in the picture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxP3hVaW6tL7"
      },
      "source": [
        "# **VGG16 Model**\n",
        "\n",
        "The **VGG16 model** is a convolutional neural network (CNN) architecture developed by the Visual Geometry Group (VGG) at the University of Oxford. It's widely used for image classification tasks. Here are some key points about VGG16:\n",
        "\n",
        "**Architecture**\n",
        "* **16 Layers:** The model has 16 layers with weights, including 13 convolutional layers and 3 fully connected layers.\n",
        "* **3x3 Filters:** It uses small 3x3 convolution filters throughout the network.\n",
        "* **Max Pooling:** It includes max pooling layers to reduce the spatial dimensions of the feature maps.\n",
        "* **Fully Connected Layers:** The final layers are fully connected, followed by a softmax activation function for classification.\n",
        "\n",
        "**Pre-trained Weights**\n",
        "* **ImageNet Pre-training:** The VGG16 model is often pre-trained on the ImageNet dataset, which contains over a million images across 1,000 categories.\n",
        "* **Transfer Learning:** This pre-trained model can be fine-tuned for specific tasks, making it a popular choice for transfer learning.\n",
        "\n",
        "**Applications***\n",
        "* **Image Classification:** VGG16 is used for classifying images into different categories, such as identifying objects, animals, or plants in images.\n",
        "* **Feature Extraction:** It can be used to extract features from images, which can then be used for other machine learning tasks1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMB4RajiVTlf"
      },
      "source": [
        "In order to use the VGG16 model, we need to create the 2 following functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg4n84yDIFRo"
      },
      "source": [
        "### **Create Function `load_image()`**\n",
        "\n",
        "The `load_image()` function is designed to load an image from a file, preprocess it, and prepare it for input into a neural network model.\n",
        "\n",
        "#### **Explanation of load_image Function:**\n",
        "1. **Load the Image:** The function uses the PIL library to open the image file specified by the filename and ensures that the image is in RGB format, which is essential for consistent processing.\n",
        "2. **Resize the Image:** It resizes the image to 224x224 pixels, which is the input size expected by many neural network models, like VGG16.\n",
        "3. **Convert Image to Numpy Array:** The function converts the image into a NumPy array, a common format used for numerical computations in machine learning.\n",
        "4. **Expand Dimensions:** It adds an extra dimension to the image array to match the expected input shape for the neural network. This extra dimension represents the batch size.\n",
        "5. **Preprocess the Image:** The function applies model-specific preprocessing to the image array. This step might normalize the pixel values to a range suitable for the neural network model.\n",
        "6. **Return the Preprocessed Image:** Finally, the function returns the preprocessed image array, ready to be used as input for the neural network model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2RvjM1WIItK"
      },
      "outputs": [],
      "source": [
        "# Create function load_image()\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess the image\n",
        "def load_image(filename):\n",
        "    img = Image.open(filename).convert('RGB')  # Ensure image is in RGB format\n",
        "    img = img.resize((224, 224))\n",
        "    img_array = np.array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "    return img_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb2oxGWn7pak"
      },
      "source": [
        "### **Create Function `predict_image()`**\n",
        "\n",
        "The code in the cell below creates a function called `predict_image()`. Here is a step-by-step explanation of this function:\n",
        "\n",
        "1. **Input Parameter:**\n",
        "* **img_array:** This is the input image array that you want to classify. It should be preprocessed and in the format expected by the VGG16 model.\n",
        "\n",
        "2. **Model Prediction:**\n",
        "* **preds = model_VGG16.predict(img_array):** This line uses the predict method of the model_VGG16 (a pre-trained VGG16 model) to make predictions on the input image array. The predict method returns a list of probabilities for each of the classes in the dataset.\n",
        "\n",
        "3, **Decode Predictions:**\n",
        "\n",
        "* **return decode_predictions(preds, top=5)[0]:** This line decodes the predicted probabilities into human-readable class names and probabilities. The decode_predictions function takes the following parameters:\n",
        "  - **preds:** The list of predicted probabilities returned by the model.\n",
        "  - **top=5:** This parameter specifies that we want the top 5 predictions.\n",
        "\n",
        "* The [0] at the end selects the top 5 predictions for the first image in the input array (assuming img_array could contain multiple images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hC-h6Bn5Cdo"
      },
      "outputs": [],
      "source": [
        "# Create function predict_image()\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Predict the image\n",
        "def predict_image(img_array):\n",
        "    preds = model_VGG16.predict(img_array)\n",
        "    return decode_predictions(preds, top=5)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObXuTlpb6bkD"
      },
      "source": [
        "### Example 2: Analyze Non-Facial Content\n",
        "\n",
        "The code in the cell below uses our 2 new functions `load_image()` and `predict_image()` to analyze the same picture of Taylor Swift that you used above in **Exercise 1A**. The VGG16 model is _not_ trained to find faces, but to analyze everything else it \"sees\" in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPdzrbnZVDdI"
      },
      "outputs": [],
      "source": [
        "# Example 2: Analyze Non-Facial Content\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
        "\n",
        "# Define Image path\n",
        "IMAGE_PATH = \"Taylor2.jpg\"\n",
        "\n",
        "# Load the VGG16 model\n",
        "model_VGG16 = VGG16(weights='imagenet', include_top=True)\n",
        "\n",
        "# Load image\n",
        "img_path = IMAGE_PATH\n",
        "img_array = load_image(img_path)\n",
        "\n",
        "# Make predictions\n",
        "predictions = predict_image(img_array)\n",
        "\n",
        "# Display the image and predictions\n",
        "plt.imshow(Image.open(img_path))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "for pred in predictions:\n",
        "    print(f\"{pred[1]}: {pred[2]*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNgzRXxi_snV"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image10.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52MMKDCi_56d"
      },
      "source": [
        "### **Analysis of the output**\n",
        "\n",
        "Let's take a closer look at the output since it gives us some insight into how the **VGG16** neural network extracts features and classifies them.\n",
        "\n",
        "#### **Downloaded Files**\n",
        "\n",
        "First, at the top of the output, you can see that the code downloaded two files from the Internet, (1) `vgg16_weights_tf_dim_ordering_tf_kernels.h5` and (2) `imagenet_class_index.json`.  \n",
        "\n",
        "1. The file `vgg16_weights_tf_dim_ordering_tf_kernels.h5` is a pre-trained weight file for the VGG16 model. Here's a breakdown of what the file name means:\n",
        "  - **vgg16_weights:** Indicates that it contains the weights for the VGG16 model.\n",
        "  - **tf_dim_ordering_tf_kernels:** Specifies that the weights are ordered for TensorFlow (tf) and use TensorFlow kernels.\n",
        "  - **h5:** The file format, which is HDF5, a data model, library, and file format for storing large and complex data.\n",
        "\n",
        ">This file can be used to load the pre-trained VGG16 model in TensorFlow or Keras, allowing you to leverage the model's learned features for your own image classification tasks without having to train it from scratch.\n",
        "\n",
        "2. The `imagenet_class_index.json` file is a **JSON file** that contains a mapping of ImageNet class indices to human-readable class names. ImageNet is a large visual database designed for use in visual object recognition research, and it contains millions of images categorized into thousands of different classes.\n",
        "\n",
        "#### **Image Analysis**\n",
        "\n",
        "By \"looking\" at the image of Taylor Swift, the **VGG16 model** correctly identifies that image contained a pair of jeans (jean: 70.75%), but since the image did not include a full view of her legs, the model thought there was a 7% chance that it \"saw\" a miniskirt (miniskirt: 6.84%).\n",
        "\n",
        "The model was unsure if it \"saw\" Taylor wearing a `cardigan` sweater or a sweatshirt. The VGG16 model didn't get either item exactly right, but Taylor wasn't really wearing her sweater in a typical manner either.\n",
        "\n",
        "Finally, Taylor Swift's heavy eye make-up made it appear to the model, that there was a small chance of a pair of sunglasses (sunglasses: 1.82%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaiVKBP5BwJy"
      },
      "source": [
        "### **Exercise 2: Analyze Non-Facial Content**\n",
        "\n",
        "In the cell below, use the VGG16 model to make predictions about the contents of an image of Kelse Travis in a football uniform (`Travis2.jpg`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FenbvF_KBwJy"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWQc3keKBwJz"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image09.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCSlpGtYBwJz"
      },
      "source": [
        "Again the output is interesting. The VGG16 model correctly identifies that image contained a football helmet (`football_helmet: 57.98%`) even though only a small part (the face mask) is actually visible in the image. Somewhat bizarrely, the model thought it \"saw\" Kelse wearing a \"basketball uniform\" (basketball: 29.98%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnxxTn9-AQ19"
      },
      "source": [
        "## **Face Extraction from Image**\n",
        "\n",
        "**Face extraction** in Convolutional Neural Networks (CNNs) refers to the process of detecting and isolating faces from an image before feeding them into a CNN for further processing, such as recognition or classification. This step is crucial because it ensures that the CNN focuses only on the relevant part of the image (the face) and ignores the background or other irrelevant detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FPd2_2hAQxO"
      },
      "source": [
        "### **Create Function `extract_face_from_image()`**\n",
        "\n",
        "The code in the cell below, creates a function called `extract_face_from_image()`. The function uses the MTCNN neural network to extract facial image(s) from a larger image and then returns the extracted face as an image to the program that called the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nHLLBKWaAYG"
      },
      "outputs": [],
      "source": [
        "# Create function extract_face_from_image()\n",
        "\n",
        "from numpy import asarray\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def extract_face_from_image(image_path, required_size=(224, 224)):\n",
        "  # load image and detect faces\n",
        "  image = plt.imread(image_path)\n",
        "  detector = MTCNN()\n",
        "  faces = detector.detect_faces(image)\n",
        "\n",
        "  face_images = []\n",
        "\n",
        "  for face in faces:\n",
        "    # extract the bounding box from the requested face\n",
        "    x1, y1, width, height = face['box']\n",
        "    x2, y2 = x1 + width, y1 + height\n",
        "\n",
        "    # extract the face\n",
        "    face_boundary = image[y1:y2, x1:x2]\n",
        "\n",
        "    # resize pixels to the model size\n",
        "    face_image = Image.fromarray(face_boundary)\n",
        "    face_image = face_image.resize(required_size)\n",
        "    face_array = asarray(face_image)\n",
        "    face_images.append(face_array)\n",
        "\n",
        "  return face_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNY3dULEVK4N"
      },
      "source": [
        "### Example 3: Extract Face from Image\n",
        "\n",
        "The code in the cell below, uses our function `extract_face_from_image()` to extract Taylor Swift's face from the image of her used above in Example 2.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorSwift2.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ris_faOBUv6h"
      },
      "outputs": [],
      "source": [
        "# Example 3: Extract face\n",
        "\n",
        "# Define Image path\n",
        "IMAGE_PATH = \"Taylor2.jpg\"\n",
        "\n",
        "extracted_face = extract_face_from_image(IMAGE_PATH)\n",
        "\n",
        "# Display the first face from the extracted faces\n",
        "plt.imshow(extracted_face[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5dLe-gOWlvY"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image11.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z5oHvq7XcHc"
      },
      "source": [
        "Our `extract_face_from_image()` function had no trouble with this image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bpwAezAWsgN"
      },
      "source": [
        "### **Exercise 3: Extract Face from Image**\n",
        "\n",
        "In the cell below, use our custom function `extract_face_from_image()` to extract Kelse Travis' face from the image of him standing in his football uniform (`Travis2.jpg`) that was used above in **Exercise 2**.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TravisKelce2.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzDMdWDkXMZs"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L40hVP6qXMZs"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image12.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbTO6ijrMj3h"
      },
      "source": [
        "It is noteworthy, that our `extract_face_from_image()` didn't include Kelsey's beard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6_CS8tVlNNj"
      },
      "source": [
        "## **CNNs to Automatically Blur Faces in Images**\n",
        "\n",
        "Automatically **blurring faces** in images is important for several reasons, especially when it comes to privacy, security, and ethical considerations:\n",
        "\n",
        "#### **Privacy Protection**\n",
        "* **Personal Privacy:** Blurring faces helps protect individuals' privacy by making them less recognizable in images. This is crucial in situations where individuals have not given their consent to be photographed or identified.\n",
        "* **Data Privacy Regulations:** Regulations like the General Data Protection Regulation (GDPR) in the EU emphasize the importance of protecting personal data. Automatically blurring faces ensures compliance with these regulations.\n",
        "\n",
        "#### **Security Concerns**\n",
        "* **Anonymity:** In sensitive contexts, such as protests or political gatherings, blurring faces can protect individuals from potential repercussions or surveillance.\n",
        "* **Witness Protection:** In law enforcement and legal contexts, blurring faces of witnesses and victims can protect their identities and ensure their safety.\n",
        "\n",
        "#### **Ethical Considerations**\n",
        "* **Consent:** It is ethically responsible to blur faces when sharing images of people who haven't explicitly consented to be photographed or identified. This is especially important in public places or when dealing with vulnerable populations, such as children.\n",
        "* **Minimizing Harm:** By blurring faces, content creators and organizations can minimize the potential harm that could come from individuals being identified without their permission.\n",
        "\n",
        "#### **Public Sharing and Media**\n",
        "* **Social Media:** Automatically blurring faces is particularly important for images shared on social media, where privacy settings might not be strict, and images can spread quickly.\n",
        "* **News and Journalism:** In journalism, blurring faces can protect the identities of individuals in sensitive or dangerous situations while still conveying important information.\n",
        "\n",
        "#### **Example Use Cases**\n",
        "* **CCTV Footage:** Automatically blurring faces in CCTV footage can help maintain the privacy of individuals who are not involved in any incidents being monitored.\n",
        "* **Photo Albums:** Photo-sharing platforms can use face blurring to respect the privacy of people in group photos before these images are made public."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jtJbrnRX7Bu"
      },
      "source": [
        "Before we can use the `face_recognition` package, we will need to install it first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv8Z0JMGNZYB"
      },
      "source": [
        "### **Install `face_recognition` package**\n",
        "\n",
        "The **face_recognition package** is a simple and easy-to-use facial recognition library for Python. It is built on top of **`dlib`** and **`OpenCV`**, leveraging `dlib's` state-of-the-art face recognition capabilities. Here are some key features and uses of the face_recognition package:\n",
        "\n",
        "#### **Key Features:**\n",
        "* **Face Detection:** It can detect faces in images and videos.\n",
        "* **Face Landmarks:** It can find and manipulate facial features such as eyes, nose, mouth, and chin.\n",
        "* **Face Encoding:** It can generate face encodings, which are numerical representations of faces that can be used for recognition.\n",
        "* **Face Recognition:** It can recognize and compare faces in images.\n",
        "* **Command-Line Tool:** It includes a simple command-line tool for performing face recognition on folders of images.\n",
        "\n",
        "#### **Typical Uses:**\n",
        "* **Photo Organization:** Automatically organizing photos by recognizing and grouping images of the same person.\n",
        "* **Security Systems:** Implementing access control systems that use facial recognition to grant or deny access.\n",
        "* **Social Media:** Identifying and tagging friends in photos.\n",
        "* **Real-Time Applications:** Building real-time face recognition systems for various applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZyPpjj-Okgm"
      },
      "source": [
        "Run the next cell to install the `face_recognition` software package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mUwCpnc2-Ks"
      },
      "outputs": [],
      "source": [
        "!pip install face_recognition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myEiBDYiO2vW"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "~~~text\n",
        "Collecting face_recognition\n",
        "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
        "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
        "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
        "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.1/100.1 MB 23.1 MB/s eta 0:00:00\n",
        "  Preparing metadata (setup.py) ... done\n",
        "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.7)\n",
        "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.2)\n",
        "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.26.4)\n",
        "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (10.4.0)\n",
        "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
        "Building wheels for collected packages: face-recognition-models\n",
        "  Building wheel for face-recognition-models (setup.py) ... done\n",
        "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566162 sha256=e4a2f3be70edc7779cb26510e9aeaa38290a0919ac05ed77bfe4d933cf4b60c1\n",
        "  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n",
        "Successfully built face-recognition-models\n",
        "Installing collected packages: face-recognition-models, face_recognition\n",
        "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu9D7KF0j6bw"
      },
      "source": [
        "### Example 4: Blur Faces in an Image\n",
        "\n",
        "Let's see what we can do with the `face_recognition` package. One practical function is to automatically find faces in an image and blur it.\n",
        "\n",
        "The code in the cell below uses the same image you used in **Exercise 1D** above.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorTravis.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkkW7tCqjzUH"
      },
      "outputs": [],
      "source": [
        "# Example 4: Blur Faces\n",
        "\n",
        "import face_recognition\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Path to group image\n",
        "GROUP_PHOTO = \"TaylorTravis.jpg\"\n",
        "\n",
        "# Load the image\n",
        "image = face_recognition.load_image_file(GROUP_PHOTO)\n",
        "\n",
        "# Find all face locations\n",
        "face_locations = face_recognition.face_locations(image)\n",
        "\n",
        "# Blur faces\n",
        "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "for (top, right, bottom, left) in face_locations:\n",
        "    face = image[top:bottom, left:right]\n",
        "    blurred_face = cv2.GaussianBlur(face, (99, 99), 30)\n",
        "    image[top:bottom, left:right] = blurred_face\n",
        "\n",
        "# Display the image with blurred faces\n",
        "cv2_imshow(image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb4GRHGomvIS"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image15.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4th29FCFZECQ"
      },
      "source": [
        "The code worked as expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyPyr1SUm-A2"
      },
      "source": [
        "### **Exercise 4: Blur Faces**\n",
        "\n",
        "In the cell below, finds faces and blur them in a group image called `TaylorGroup.jpg`.\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorGroup.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jHPFzVGTm-A2"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bcNeOoHm-A3"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image16.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX1lRHjeZjeR"
      },
      "source": [
        "## **Facial Analysis**\n",
        "\n",
        "Besides finding and blurring faces, the `facial_recognition` package can also be used to find facial features by identifying and returning the locations of facial landmarks such as eyes, nose, mouth, and chin.\n",
        "\n",
        "In order to utilize this capability, we need to create a new function called `analyze_facial_attribute()` in the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HRlmhwEMq2J"
      },
      "source": [
        "### **Create Function `analyze_facial_attributes()`**\n",
        "\n",
        "#### Explanation of the `analyze_facial_attributes` function:\n",
        "\n",
        "1. **Load the Image**: The function uses the `face_recognition` library to load an image from the specified file path.\n",
        "2. **Find Face Locations**: It detects all face locations in the image and stores the coordinates of these faces.\n",
        "3. **Find Facial Features**: The function identifies various facial features (landmarks) like eyes, nose, and mouth for each detected face.\n",
        "4. **Display the Image**: It uses the `PIL` library to open and display the image with `matplotlib`, turning off axis labels for a cleaner view.\n",
        "5. **Plot Facial Features**: For each set of facial features, it plots the points using `matplotlib`, connecting the landmarks with lines to visualize the features.\n",
        "6. **Show Image with Landmarks**: The function displays the image with the overlaid facial landmarks.\n",
        "7. **Return Results**: Finally, it returns the face locations and facial landmarks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H7mgt4zOYHW8"
      },
      "outputs": [],
      "source": [
        "# Create function analyze_facial_attributes()\n",
        "\n",
        "import face_recognition\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def analyze_facial_attributes(image_path):\n",
        "    # Load the image\n",
        "    image = face_recognition.load_image_file(image_path)\n",
        "\n",
        "    # Find all face locations in the image\n",
        "    face_locations = face_recognition.face_locations(image)\n",
        "\n",
        "    # Find all facial features in the image\n",
        "    face_landmarks_list = face_recognition.face_landmarks(image)\n",
        "\n",
        "    # Display the image with face landmarks\n",
        "    img = Image.open(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "    for face_landmarks in face_landmarks_list:\n",
        "        for feature, points in face_landmarks.items():\n",
        "            points = [tuple(point) for point in points]\n",
        "            x, y = zip(*points)\n",
        "            plt.plot(x, y, marker='o')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return face_locations, face_landmarks_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw35ct86ZBkq"
      },
      "source": [
        "If you are using an accelerated runtime, like the `L4 GPU`, you should not see any output after running the cell above.\n",
        "\n",
        "However, if you didn't change your runtime type and you are just using the CPU runtime, you would have seen the following error message:\n",
        "\n",
        "~~~text\n",
        "RuntimeError: Error while calling cudaGetDevice(&the_device_id) in file /root/.cache/uv/sdists-v4/pypi/dlib/19.24.2/9lV6imTrukkVJf3aTd5Ro/dlib-19.24.2.tar.gz/dlib/cuda/gpu_data.cpp:204. code: 35, reason: CUDA driver version is insufficient for CUDA runtime version\n",
        "~~~\n",
        "\n",
        "If you receive this error message, you will have to change your runtime to an accelerated runtime like `L4 GPU` which is available to both free and paid Colab users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkMhYBIEbR-W"
      },
      "source": [
        "### Example 5: Analyze Facial Attributes\n",
        "\n",
        "The code in the cell below uses our function `analyze_facial_attributes()` to identify and analyze the facial features in an image of Taylor Swift (`Taylor1.jpg`).\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/TaylorSwift1.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NURA8z1VYQUB"
      },
      "outputs": [],
      "source": [
        "# Example 5: Analyze Facial Attributes\n",
        "\n",
        "\n",
        "# Define Image path\n",
        "image_path = 'Taylor1.jpg'\n",
        "\n",
        "# Analyze facial attributes\n",
        "face_locations, face_landmarks_list = analyze_facial_attributes(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "148D3DY-c604"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image13.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwUJcycgQr8C"
      },
      "source": [
        "Each colored line resprents a different facial feature (attribute) extacted from the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT2OyILHdIr-"
      },
      "source": [
        "### **Exercise 5: Analyze Facial Attributes**\n",
        "\n",
        "In the cell below, use `analyze_facial_attributes()` to analyze the facial features in an image of Travis Kelse (`Travis3.jpg`).\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/TravisKelce3.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CS-KF743dIr_"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuaahuLndIr_"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image17.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH_s5KCmfwlH"
      },
      "source": [
        "## **Facial Recognition using `DeepFace`**\n",
        "\n",
        "**DeepFace** is a **deep learning facial recognition system** developed by a research group at [Facebook](https://en.wikipedia.org/wiki/Facebook). It was designed to identify human faces in digital images with high accuracy.\n",
        "\n",
        "Here are some key points about `DeepFace`:\n",
        "\n",
        "* **Architecture:** DeepFace uses a nine-layer neural network with over 120 million connection weights. This complex architecture allows it to achieve impressive accuracy in facial recognition tasks.\n",
        "\n",
        "* **Training Data:** The system was trained on four million images uploaded by Facebook users. This extensive dataset helped the model learn a wide variety of facial features and variations.\n",
        "\n",
        "* **Accuracy:** DeepFace has an accuracy of 97.35% on the Labeled Faces in the Wild (LFW) dataset, which is comparable to human performance. This means it can sometimes outperform humans in recognizing faces.\n",
        "\n",
        "* **Applications:** Initially, DeepFace was used to alert Facebook users when their face appeared in any photo posted on the platform. Users could then choose to remove their face from the photo if they wished.\n",
        "\n",
        "DeepFace represents a significant advancement in facial recognition technology and has influenced many subsequent developments in the field."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTCEzCC_gsdk"
      },
      "source": [
        "#### **Install `deepface`**\n",
        "\n",
        "Run the next cell to install DeepFace into your current Colab environment.\n",
        "\n",
        "The command `!pip install -U deepface` is used to install or upgrade the DeepFace library in a Python environment.\n",
        "\n",
        "Here’s a breakdown of what the command does:\n",
        "\n",
        "* **!pip:** The exclamation mark (!) indicates that this command should be executed in a Jupyter notebook or similar environment where the ! symbol is used to run shell commands.\n",
        "\n",
        "* **install:** This tells pip (the Python package installer) to install a package.\n",
        "\n",
        "* **-U:** This flag stands for \"upgrade\" and tells pip to upgrade the package to the latest version if it's already installed.\n",
        "\n",
        "* **deepface:** This specifies the name of the package to be installed or upgraded, which in this case is DeepFace.\n",
        "\n",
        "So, running this command will either install the DeepFace library if it’s not already present in your environment or upgrade it to the latest version if it is already installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UHYYNNywjxuK"
      },
      "outputs": [],
      "source": [
        "!pip install -U deepface\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIZdGlpITBJV"
      },
      "source": [
        "## **IMPORTANT**\n",
        "\n",
        "Do NOT include the output of the installation in the PDF that you will turn in for grading. Instead, clear the output now by selecting the icon at the top left of the output cell and selecting the `Clear selected outputs` option as shown in this image:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image18.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lAUzqOEUFPe"
      },
      "source": [
        "--------------------------------------------------------\n",
        "\n",
        "\n",
        "## **A Short History of Emotion Detection**\n",
        "\n",
        "The history of emotion detection using Convolutional Neural Networks (CNNs) reflects the broader advancements in both technology and our understanding of human emotions. Here's an overview:\n",
        "\n",
        "#### Early Days of Emotion Recognition\n",
        "- **19th Century**: The formal study of emotions can be traced back to Charles Darwin, who suggested that emotional expressions have evolved and serve social functions. Although his work did not benefit from modern technology, it laid the groundwork for understanding how behaviors related to emotions could be studied scientifically.\n",
        "- **1970s**: Researchers began to employ more systematic methods to study emotional expressions. The emergence of facial coding systems, such as Paul Ekman's Facial Action Coding System (FACS), transformed emotion recognition research. FACS categorized facial movements and expressions, allowing researchers to conduct more precise analysis based on visible emotional cues.\n",
        "\n",
        "#### Rise of Machine Learning and AI\n",
        "- **Late 20th Century**: The turning point for emotion recognition technologies came with the advent of machine learning (ML) and artificial intelligence (AI). By the early 2000s, the availability of more extensive datasets and more powerful computing resources allowed researchers to explore various algorithms for facial recognition and emotion detection.\n",
        "- **Early 2000s**: Researchers began to use machine learning techniques to automate emotion recognition. These early attempts relied on basic computer vision techniques but struggled to accurately interpret the nuances of human expressions.\n",
        "\n",
        "#### Advancements in Deep Learning\n",
        "- **2010s**: The introduction of deep learning and CNNs revolutionized emotion detection. CNNs, with their ability to learn hierarchical representations of data, proved to be highly effective in recognizing and classifying emotions from facial expressions.\n",
        "- **Recent Developments**: Modern CNNs have achieved impressive accuracy in emotion recognition tasks. Researchers have also explored the intrinsic ability of CNNs to represent the affective significance of visual input, suggesting that emotional perception might be an intrinsic property of the visual cortex.\n",
        "\n",
        "#### Current Trends and Applications\n",
        "- **Applications**: Emotion detection by CNNs is now used in various applications, including social media, customer service, healthcare, and security systems.\n",
        "- **Ethical Considerations**: As emotion detection technologies become more widespread, ethical considerations regarding privacy, consent, and the potential misuse of these technologies have come to the forefront.\n",
        "\n",
        "The history of emotion detection by CNNs showcases the rapid evolution of technology and its impact on our ability to understand and interact with human emotions. It's an exciting field that continues to grow and develop, offering new possibilities for enhancing human-computer interaction.\n",
        "\n",
        "-------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guRKZj85dNkA"
      },
      "source": [
        "## **Detect Emotion with `DeepFace`**\n",
        "\n",
        "The `DeepFace` system can analyze facial attributes to predict the age, gender, emotion, and race/ethnicity of the person in the image.\n",
        "\n",
        "In the cell below, we create a function called `detect_emotion()` that uses the DeepFace system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU62xYw4iVrk"
      },
      "source": [
        "### **Create function `detect_emotion()`**\n",
        "\n",
        "The code in the cell below creates a custom function called `detect_emotion()`. This code defines a function detect_emotion that takes an image file path as an input, analyzes the image for emotions using DeepFace, and then displays the detected emotions along with the image. Here's a breakdown of what the code does:\n",
        "\n",
        "* **Function Definition:** The function detect_emotion is defined with the parameter image_path, which represents the file path of the image to be analyzed.\n",
        "* **Analyze the Image**: The DeepFace library's analyze function is used to analyze the image for emotions. It takes the image path and a list of actions (in this case, ['emotion']) to perform the emotion analysis. The result is stored in the variable result.\n",
        "* **Print Detected Emotion:** The dominant emotion detected in the image is printed using print(f\"Detected emotion: {result[0]['dominant_emotion']}\"). It also prints a detailed emotion analysis showing the probabilities of different emotions in the image.\n",
        "* **Display the Image:** The image is opened using the Image.open function from the PIL (Python Imaging Library). It is then displayed using plt.imshow(img) from the matplotlib library, with the axis turned off using plt.axis('off') to avoid showing axis labels.\n",
        "* **Return Result:** The function returns the result variable, which contains the detailed emotion analysis.\n",
        "\n",
        "Overall, this function analyzes the emotions in the given image and provides a visual representation along with detailed emotion probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZC_FyIdKulFm"
      },
      "outputs": [],
      "source": [
        "# Create function `detect_emotion()`\n",
        "\n",
        "from deepface import DeepFace\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def detect_emotion(image_path):\n",
        "    # Analyze the image for emotions\n",
        "    result = DeepFace.analyze(img_path=image_path, actions=['emotion'])\n",
        "\n",
        "    # Print the detected emotion\n",
        "    print(f\"Detected emotion: {result[0]['dominant_emotion']}\")\n",
        "    print(f\"Emotion analysis: {result[0]['emotion']}\")\n",
        "\n",
        "    # Display the image\n",
        "    img = Image.open(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL_mRKTjOxf"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "~~~text\n",
        "24-11-07 16:54:48 - Directory /root/.deepface has been created\n",
        "24-11-07 16:54:48 - Directory /root/.deepface/weights has been created\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAPt7Pz6jXQe"
      },
      "source": [
        "### Example 6: Detect Emotion\n",
        "\n",
        "Let's see how well our `detect_emotion()` function works.\n",
        "\n",
        "The code in the cell below used the `detect_emotion()` function to analyze an image of Taylor Swift (`Taylor1.jpg`).\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorSwift1.jpg)\n",
        "\n",
        "Before you run the code, what emotion(s) do you think Taylor was feeling when this photograph was taken?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lq4JXcnvvEte"
      },
      "outputs": [],
      "source": [
        "# Example 6: Detect Emotion\n",
        "\n",
        "# Define Image path\n",
        "image_path = 'Taylor1.jpg'\n",
        "\n",
        "# Detect emotion\n",
        "emotion_attributes = detect_emotion(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXXhOPmRXHuw"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image19.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd_jjo_RXPtU"
      },
      "source": [
        "Here is what our `detect_emotion()` function predicted:\n",
        "\n",
        "~~~text\n",
        "Detected emotion: neutral\n",
        "Emotion analysis:\n",
        "{'angry': 1.2479273414866532,\n",
        "'disgust': 0.00167581684747923,\n",
        "'fear': 1.1876355633220412,\n",
        "'happy': 0.1236665601978636,\n",
        "'sad': 5.317516941876229,\n",
        "'surprise': 0.002416188507001907,\n",
        "'neutral': 92.11916280503051}\n",
        "~~~\n",
        "\n",
        "Here's how to interpret this output:\n",
        "\n",
        "### Detected Emotion:\n",
        "- **Neutral**: The dominant emotion detected in the image is \"neutral,\" which means the person's facial expression does not strongly convey any specific emotion.\n",
        "\n",
        "### Emotion Analysis:\n",
        "The analysis includes the probabilities (in percentages) of various emotions detected in the image:\n",
        "- **Angry**: 1.25%\n",
        "- **Disgust**: 0.002%\n",
        "- **Fear**: 1.19%\n",
        "- **Happy**: 0.12%\n",
        "- **Sad**: 5.32%\n",
        "- **Surprise**: 0.002%\n",
        "- **Neutral**: 92.12%\n",
        "\n",
        "The dominant emotion is \"neutral\" because it has the highest probability (92.12%), indicating that the person's expression is mostly neutral with some minor traces of other emotions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgXUQ2GVeczR"
      },
      "source": [
        "### **Exercise 6A: Detect Emotion**\n",
        "\n",
        "In Example 6, our `detect_emotion()` function really couldn't figure out what emotion Taylor Swift was feeling in the image.\n",
        "\n",
        "What if you try to analyze an image of Taylor Swift where she appears to be `angry`?\n",
        "\n",
        "In the cell below use the `detect_emotion()` function to analyze an image of Taylor Swift (`TaylorDisgust2.jpg`).\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorDisgust.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VFsLfgv4eczW"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 6A here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wqh2g2ReczW"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image21.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL4KxDqyeczW"
      },
      "source": [
        "Again, our `detect_emotion()` function failed to detect any emotion in the image of Taylor Swift.\n",
        "\n",
        "Here is what our `detect_emotion()` function predicted:\n",
        "\n",
        "~~~text\n",
        "Detected emotion: neutral\n",
        "Emotion analysis:\n",
        "{'angry': 0.7470636162906885,\n",
        "'disgust': 0.0009553421477903612,\n",
        "'fear': 4.207199439406395,\n",
        "'happy': 1.358166616410017,\n",
        "'sad': 6.161818280816078,\n",
        "'surprise': 1.8739817664027214,\n",
        "'neutral': 85.6508195400238}\n",
        "~~~\n",
        "\n",
        "Our `detect_emotion()` function concluded that there was a 85% probability that Taylor Swift's emotion was `neutral`, which means the person's facial expression does not strongly convey any specific emotion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQthEGU4o8IS"
      },
      "source": [
        "### **Exercise 6B: Detect Emotion**\n",
        "\n",
        "Maybe there is something \"unusual\" about Taylor Swift's expression? After all,Taylor Swift has been performing for over two decades. She began her career in 2006 with the release of her self-titled debut album. Since then, she has released numerous albums, won multiple awards, and captivated audiences worldwide with her remarkable talent as a singer-songwriter.\n",
        "\n",
        "Let's use an image that clear shows a person with strong emotions. Specifically, let's use an image of angry Chinese woman that was generated by AI.\n",
        "\n",
        "In the cell below, use the `detect_emotion()` function to analyze the image `ChineseAngry.jpg`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/ChineseAngry.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OmHltra3o8IT"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 6B here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT2mW9Rdo8IT"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image22.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYvvWQGYo8IT"
      },
      "source": [
        "This time our function worked as expected.\n",
        "\n",
        "Here is what our `detect_emotion()` function predicted after \"seeing\" the image:\n",
        "\n",
        "~~~text\n",
        "Detected emotion: angry\n",
        "Emotion analysis:\n",
        "{'angry': 94.17428970336914,\n",
        "'disgust': 0.007599566015414894,\n",
        "'fear': 5.48504963517189,\n",
        "'happy': 0.0066324246290605515,\n",
        "'sad': 0.21781865507364273,\n",
        "'surprise': 0.014071927580516785,\n",
        "'neutral': 0.09453678503632545}\n",
        "~~~\n",
        "\n",
        "Unlike the `neutral` emotion for the Taylor Swift images, this time the software predicted a 94% probability that the woman in the picture was angry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB2j3Z5UuMSz"
      },
      "source": [
        "### **Exercise 6C: Detect Emotion**\n",
        "\n",
        "The image used in **Exercise 6B** was AI generated which begs the question of whether our function works as well with a real image?\n",
        "\n",
        "In the cell below use the `detect_emotion()` function to analyze the image `TaylorEighmy.jpg`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorEighmy.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6sLotrBQuMSz"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 6C here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AHN81SiuMSz"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image23.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brCWZZWluMS0"
      },
      "source": [
        "Here is what our `detect_emotion()` function predicted after \"seeing\" the image of President Eighmy:\n",
        "\n",
        "~~~text\n",
        "Detected emotion: happy\n",
        "Emotion analysis:\n",
        "{'angry': 0.1631041503871471,\n",
        "'disgust': 0.7031595753526435,\n",
        "'fear': 0.34712506198883425,\n",
        "'happy': 96.58741930648203,\n",
        "'sad': 0.4291381942712184,\n",
        "'surprise': 0.042165323578362085,\n",
        "'neutral': 1.7278888361387752}\n",
        "~~~\n",
        "\n",
        "Our function worked very well, predicting that there was a 97% chance that the President was happy when this picture was taken."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-urnbzyfw55O"
      },
      "source": [
        "### **Exercise 6D: Detect Emotion**\n",
        "\n",
        "Humans are extremely good at detecting a \"fake smile\". A \"fake smile\" is an expression where a person smiles, but the smile is not genuine or sincere. A genuine smile, known as a Duchenne smile, involves the activation of the zygomatic major muscle (which raises the corners of the mouth) and the orbicularis oculi muscle (which causes the eyes to crinkle). This combination creates a natural and authentic smile. Authentic emotional expressions involve involuntary muscle movements that are difficult to consciously replicate. For example, a genuine smile engages the _orbicularis oculi_ muscle around the eyes, creating \"crow's feet,\" which is hard to fake. Audiences are generally adept at detecting insincerity. A performance that lacks genuine emotion can come across as forced or unconvincing, breaking the immersion and reducing the impact of the story.\n",
        "\n",
        "In the TV series, the Big Bang Series, the character Sheldon Cooper uses this insincerity for comic effect when I forces a patently exagerrated \"smile\" in this image.\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/SheldonSmile.jpg)\n",
        "\n",
        "\n",
        "Let's see what happens when your ask our function to analyze this image of Sheldon Cooper (`SheldonSmile.jpg`) from the Big Bang Series?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oW7_hW70w55P"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 6C here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SVTBgOQw55P"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image24.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOgbF1kmw55P"
      },
      "source": [
        "Here is what our `detect_emotion()` function predicted after \"seeing\" the image of a \"smiling\" Sheldon Cooper:\n",
        "\n",
        "~~~text\n",
        "Detected emotion: happy\n",
        "Emotion analysis:\n",
        "{'angry': 5.588822590482656e-11,\n",
        "'disgust': 4.056132971241158e-28,\n",
        "'fear': 2.447412559455601e-19,\n",
        "'happy': 99.99996423721313,\n",
        "'sad': 1.5831839877570924e-13,\n",
        "'surprise': 1.5042896706063402e-06,\n",
        "'neutral': 3.38719502224194e-05}\n",
        "~~~\n",
        "\n",
        "This is pretty funny! Our `detect_emotion()` function thought that there was a 99.9% chance that Sheldon was `happy` in this picture. Clearly, our `detect_emotion()` function is unable to spot a \"fake smile\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5xT3hfChnyl"
      },
      "source": [
        "## **FaceNet**\n",
        "\n",
        "**FaceNet** is a facial recognition system developed by researchers at Google, including Florian Schroff, Dmitry Kalenichenko, and James Philbin. It was first presented at the 2015 IEEE Conference on Computer Vision and Pattern Recognition.\n",
        "\n",
        "Here are some key points about FaceNet:\n",
        "\n",
        "* **Deep Convolutional Network:** FaceNet uses a deep convolutional neural network (CNN) to learn a mapping from face images to a 128-dimensional Euclidean space. This means that each face image is represented as a 128-dimensional vector, and the similarity between faces can be measured by the Euclidean distance between these vectors.\n",
        "* **Triplet Loss Function:** The system uses a triplet loss function to train the network. This involves comparing a \"triplet\" of images: an anchor image, a positive image (same person as the anchor), and a negative image (different person)1. The goal is to minimize the distance between the anchor and the positive while maximizing the distance between the anchor and the negative.\n",
        "* **High Accuracy:** FaceNet achieved an accuracy of 99.63% on the Labeled Faces in the Wild (LFW) dataset, which was the highest score at the time. This high accuracy makes it suitable for various applications, including face verification, recognition, and clustering.\n",
        "* **Efficiency:** By directly optimizing the embedding itself rather than using an intermediate bottleneck layer, FaceNet achieves greater representational efficiency. It can perform face recognition tasks using only 128 bytes per face.\n",
        "\n",
        "FaceNet has been influential in the field of facial recognition and has inspired many subsequent developments and implementations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gOEfeFulGKs"
      },
      "source": [
        "### Example 7: Verify Faces\n",
        "\n",
        "Another capability of `DeepFace` is its ability to identify and verify faces in digital images with high precision.\n",
        "\n",
        "The code in the cell below, uses `DeepFace` in combination with `FaceNet` to identify and verify faces using a `know_image` as a reference and an `unknown_image` as the test image.\n",
        "\n",
        "Here is the known image:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorSwift1.jpg)\n",
        "\n",
        "\n",
        "\n",
        "And here is the unknown image:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorDisgust2.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBv9JcOxnz7j"
      },
      "outputs": [],
      "source": [
        "# Example 7: Verify faces\n",
        "\n",
        "from deepface import DeepFace\n",
        "\n",
        "# Image path known person\n",
        "KNOWN_PERSON = 'Taylor1.jpg'\n",
        "\n",
        "# Image path to unknown person\n",
        "UNKNOWN_PERSON = 'TaylorDisgust2.jpg'\n",
        "\n",
        "# Perform face verification using Facenet\n",
        "result = DeepFace.verify(KNOWN_PERSON, UNKNOWN_PERSON, model_name='Facenet')\n",
        "\n",
        "# Print results\n",
        "if result[\"verified\"]:\n",
        "    print(\"Faces Matched\")\n",
        "else:\n",
        "    print(\"Faces Not Matched\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgHB5vrRpOt2"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "~~~text\n",
        "Faces Matched\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM7Ng04LqOra"
      },
      "source": [
        "### **Exercise 7A: Verify Faces**\n",
        "\n",
        "In the cell below, write the code to verify that Travis Kelce, shown in this picture:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TravisKelce3.jpg)\n",
        "\n",
        "\n",
        "is also seen in this picture:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorTravis.jpg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the filename `Travis3.jpg` for the KNOWN_PERSON and `TaylorTravis.jpg` for the UNKNOWN_PERSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT9WeMlsoV6t"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 7A here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8o_olfm2Tu0"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "~~~text\n",
        "Faces Matched\n",
        "~~~\n",
        "\n",
        "That's pretty impressive since Travis Kelce looked pretty different in the two images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_E9ZFiiuBV4"
      },
      "source": [
        "### **Exercise 7B: Verify Faces**\n",
        "\n",
        "Before we end this lesson, we should make sure that our software can also tell when two faces are **not** a \"match\".\n",
        "\n",
        "In the cell below, write the code to verify that Travis Kelce, shown in this picture:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TravisKelce3.jpg)\n",
        "\n",
        "\n",
        "with the picture of UTSA President, Taylor Eighmy:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TaylorEighmy.jpg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the filename `Travis3.jpg` for the KNOWN_PERSON and `TaylorEighmy.jpg` for the UNKNOWN_PERSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3EhYkyKuBV4"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 7A here\n",
        "\n",
        "from deepface import DeepFace\n",
        "\n",
        "# Image path known person\n",
        "KNOWN_PERSON = 'Travis3.jpg'\n",
        "\n",
        "# Image path to unknown person\n",
        "UNKNOWN_PERSON = 'TaylorEighmy.jpg'\n",
        "\n",
        "\n",
        "# Perform face verification using Facenet\n",
        "result = DeepFace.verify(KNOWN_PERSON, UNKNOWN_PERSON, model_name='Facenet')\n",
        "\n",
        "\n",
        "# Print results\n",
        "if result[\"verified\"]:\n",
        "    print(\"Faces Matched\")\n",
        "else:\n",
        "    print(\"Faces Not Matched\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCVPHe9GuBV4"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "~~~text\n",
        "Faces Not Matched\n",
        "~~~\n",
        "\n",
        "Our function got that one right!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UadJEsfGaomz"
      },
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells,use the **File --> Print.. --> Save to PDF** to generate a PDF of your JupyterLab notebook. Save your PDF as `Class_06_3.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}