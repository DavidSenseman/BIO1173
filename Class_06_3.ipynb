{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP/0b3zwogqLh+yni3vnLEW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_06_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ],
      "metadata": {
        "id": "ADXR45cKZecS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "5VJirvKJZgjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Change your Runtime Now!**\n",
        "\n",
        "For this lesson you must have a good **GPU** hardware accelerator like the `A-100`."
      ],
      "metadata": {
        "id": "2X-aE9YRaaWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Module 6: Advanced Topics**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 6 Material\n",
        "\n",
        "* Part 6.1: Reinforcement Learning\n",
        "* Part 6.2: ONNX Runtime Environment\n",
        "* **Part 6.3: Analysis of DICOM images with Pytorch**\n",
        "* Part 6.4: Agentic AI (Revisited)"
      ],
      "metadata": {
        "id": "TjHRztU-ZlDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ],
      "metadata": {
        "id": "6q7Ef2jTa8M7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b11DEz3IT9nn"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "Q7pr0vZbbNPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerated Run-time Check\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be running a Graphics Processing Unit (GPU) such as the `A100`."
      ],
      "metadata": {
        "id": "m_hIn8o4bPf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell second\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check for GPU\n",
        "def check_colab_gpu():\n",
        "    print(\"=== Colab GPU Check ===\")\n",
        "\n",
        "    # Check PyTorch\n",
        "    pt_gpu = torch.cuda.is_available()\n",
        "    print(f\"PyTorch GPU available: {pt_gpu}\")\n",
        "\n",
        "    if pt_gpu:\n",
        "        print(f\"PyTorch device count: {torch.cuda.device_count()}\")\n",
        "        print(f\"PyTorch current device: {torch.cuda.current_device()}\")\n",
        "        print(f\"PyTorch device name: {torch.cuda.get_device_name()}\")\n",
        "        print(\"You are good to go!\")\n",
        "\n",
        "    else:\n",
        "        print(\"No compatible device found\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now and start over!\")\n",
        "\n",
        "check_colab_gpu()\n"
      ],
      "metadata": {
        "id": "-6oSgM1UcMVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image01E.png)"
      ],
      "metadata": {
        "id": "uddp_Ek5bjgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Custom Function\n",
        "\n",
        "Run the code in the next cell to create the custom timing function."
      ],
      "metadata": {
        "id": "El2kWL0SUMoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to print out elapsed time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "cfT0lL00UPNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Packages\n",
        "\n",
        "Run the cell below to install the various packages needed for this lesson."
      ],
      "metadata": {
        "id": "kVVvtOMgbqiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydicom\n",
        "!pip install -q dropblock"
      ],
      "metadata": {
        "id": "taC2_W5pbs6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image02E.png)"
      ],
      "metadata": {
        "id": "6i1Rq784dSUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pneumonia Detection from DICOM Image Analysis**\n",
        "\n",
        "In this lesson we will introduce the following advanced techniques that have not been covered so far in this course:\n",
        "\n",
        "* **1. DICOM Image analysis**\n",
        "\n",
        "* **2. _Lazy_ Loading of image data**\n",
        "\n",
        "* **3. Training Deep Neural Networks using `PyTorch`**\n",
        "\n",
        "The images to be analyzed in this lesson are a subset of the NIH Chest‑X‑Ray‑8 (CXR‑8) dataset (https://data.nih.gov). This subset consists of `9937` images in **DICOM format** that are frontal-view chest-X-rays. Each image is a single‑channel (grayscale) image of roughly 1024 × 1024 pixels.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/DICOM_Images.png)\n",
        "**Figure:** The 3 different image classes in the DICOM dataset: **A** `Normal`, **B** `No Opacity/Not Normal` and **C** `Opacity`. Image classes **A** and **B** were assigned the `Target` value `0`(no pneumonia) while class **C** was assigned the `Target` value `1` (presence of pneumonia).\n",
        "\n",
        "The early detection and treatment of pneumonia is an important medical challenge. According to UNICEF, pneumonia and diarrhoea **kill 1.4 million children each year**, more than all other childhood illnesses combined.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_91HEIahkLI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DICOM Image Analysis**\n",
        "\n",
        "**DICOM (Digital Imaging and Communications in Medicine)** is a standard for handling, storing, printing, and transmitting medical images and related information. It was developed by the National Electrical Manufacturers Association (NEMA) as an international standard for healthcare informatics.\n",
        "\n",
        "#### **Key Features of DICOM**\n",
        "- **Standardized Format**: Ensures that medical images can be exchanged between different systems without loss of image quality or clinical relevant information.\n",
        "- **Metadata Storage**: Includes patient demographics, study details, imaging parameters, and other critical data in a structured format.\n",
        "- **Interoperability**: Facilitates communication between various medical devices, software applications, and healthcare providers.\n",
        "\n",
        "#### **Use Cases**\n",
        "1. **Image Transfer**: DICOM is used to transfer images from one modality (e.g., MRI, CT) to another or between different hospitals.\n",
        "2. **Storage and Retrieval**: For long-term storage of medical images in hospital networks, ensuring that images can be accessed when needed for review or consultation.\n",
        "3. **Quality Control**: In diagnostic imaging departments, DICOM allows for the verification and monitoring of image quality before distribution to clinicians.\n",
        "4. **Research and Education**: Used extensively in medical research and educational settings to analyze patient data across different studies and institutions.\n",
        "5. **Clinical Diagnostics**: Critical for clinical diagnosis as it provides detailed information about the imaging procedure, which is crucial for treatment planning and follow-up.\n",
        "\n",
        "#### **Importance for Students Learning Computational Biology**\n",
        "1. **Understanding Clinical Context**: Learning DICOM helps students to understand the context of medical images and their relevance in patient care.\n",
        "2. **Advanced Analysis**: Proficiency in handling DICOM images is essential for advanced analysis, such as image segmentation and machine learning applications in clinical diagnostics.\n",
        "3. **Research Skills**: In biocomputational fields, working with large datasets including DICOM images is common. Understanding how to manage these data sets enhances research capabilities.\n",
        "4. **Future Professional Development**: Knowledge of DICOM is becoming a standard requirement for many healthcare IT and bioinformatics positions, ensuring that students can meet industry standards post-graduation.\n",
        "\n",
        "#### **Why Learning DICOM is Important for Clinical Diagnostic Analysis**\n",
        "- **Accurate Diagnosis**: Understanding the metadata in DICOM images helps clinicians to make accurate diagnoses by reviewing imaging data in context with patient information.\n",
        "- **Treatment Planning**: Accurate image interpretation allows physicians to plan effective treatments, which can significantly impact patient outcomes.\n",
        "- **Interoperability and Integration**: In a healthcare environment where multiple systems are integrated, knowledge of DICOM ensures seamless communication between different medical devices and software applications.\n",
        "- **Legal and Ethical Considerations**: Proper handling and storage of DICOM images comply with legal and ethical standards in healthcare data management.\n",
        "\n",
        "### **Conclusion**\n",
        "**DICOM images** play a pivotal role in modern healthcare, especially in clinical diagnostic analysis. For biocomputational college students, mastering the use of DICOM is not only beneficial for academic learning but also crucial for future professional success in the field of healthcare informatics and bioinformatics.\n"
      ],
      "metadata": {
        "id": "ksSmv-2gdgXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Download DICOM Image Data\n",
        "\n",
        "This Python code in Example 1 downloads a zip file from a specified URL and save it to a local destination. The efficiency of this code comes from several aspects, primarily due to its design choices which optimize the process for both speed and resource usage:\n",
        "\n",
        "1. **Streamed Download:** By using requests.get(url, stream=True), the data is streamed directly from the server without downloading the entire file into memory at once. This is particularly useful when dealing with large files where loading the whole file could be impractical or consume excessive memory.\n",
        "\n",
        "2. **Chunk Size Optimization:** The parameter chunk_size (set to 8192 bytes by default) allows the code to write data in manageable chunks rather than all at once, which can improve performance on systems with limited memory or slow I/O operations.\n",
        "\n",
        "3. **Progressive Download Notification:** The print statement \"Downloading ... done\" provides feedback indicating that the download process has started and completed successfully, giving users a clear indication of progress without additional user interaction.\n",
        "\n",
        "4. **Error Handling:** r.raise_for_status() ensures that an HTTPError is raised if there was an issue with the request (e.g., 404 Not Found or 500 Server Error). This helps in debugging and ensuring that the script stops running when it encounters a problem, preventing potential corruption of partially downloaded files.\n",
        "\n",
        "5. **File Writing:** The file is opened in binary mode (\"wb\") to ensure compatibility with various types of data (including images, binaries, etc.) and to properly handle all bytes being transferred without alteration.\n",
        "\n",
        "Overall, this script is efficient because it handles large files efficiently by not loading them entirely into memory, allows for customization via parameters like chunk size, provides clear user feedback, includes error handling, and ensures that the file transfer respects data integrity through binary mode writing.\n",
        "\n",
        "**TIME WARNING:** This is a large datafile so it will take about 2-3 min to download to your Colab notebook."
      ],
      "metadata": {
        "id": "shpJIShkdgA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Download DICOM Image Data\n",
        "\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration\n",
        "URL = \"https://biologicslab.co/BIO1173/data/\"\n",
        "ZIP_FILENAME = \"pna_data.zip\"\n",
        "\n",
        "# Download the zip file (streamed, so it works with large files)\n",
        "cwd          = Path.cwd()            # current working directory\n",
        "zip_path     = cwd / ZIP_FILENAME\n",
        "extract_dir  = cwd / zip_path.stem   # e.g., /pna_data\n",
        "\n",
        "# Ensure the extraction directory exists\n",
        "extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Downloading {ZIP_FILENAME} to {zip_path}...\", end='')\n",
        "with requests.get(URL + ZIP_FILENAME, stream=True, timeout=30) as r:\n",
        "    r.raise_for_status()           # will raise for 4xx/5xx\n",
        "    with zip_path.open(\"wb\") as f_out:\n",
        "        for chunk in r.iter_content(chunk_size=8192):\n",
        "            if chunk:               # filter out keep-alive new chunks\n",
        "                f_out.write(chunk)\n",
        "print(\"done\")"
      ],
      "metadata": {
        "id": "H9lE89PVkFC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image04E.png)"
      ],
      "metadata": {
        "id": "VWyoeDmHqBGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Extract DICOM Image Data\n",
        "\n",
        "The code in the cell below unzips the DICOM image data and stores it in a new folder with the same name as the zip file."
      ],
      "metadata": {
        "id": "Ch-xquaNUMgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Extract DICOM Image Data\n",
        "\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Set Clean Up Flag\n",
        "clean_up = True  # Use lowercase for variable names\n",
        "\n",
        "# Configuration\n",
        "zip_filename = \"pna_data.zip\"\n",
        "\n",
        "# Unzip the downloaded archive into a named directory\n",
        "cwd = Path.cwd()  # current working directory\n",
        "extract_dir = cwd / zip_filename.replace('.zip', '')  # e.g., /pna_data\n",
        "\n",
        "# Ensure the extraction directory exists\n",
        "extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Unzipping {zip_filename} to {extract_dir}...\", end='')\n",
        "with zipfile.ZipFile(cwd / zip_filename, \"r\") as zf:\n",
        "    zf.extractall(path=extract_dir)\n",
        "print(\"done\")\n",
        "\n",
        "# Optional – delete the zip after extraction\n",
        "def clean_up_zip(zip_path: Path) -> None:\n",
        "    \"\"\"Delete the zip file – only if you no longer need it.\"\"\"\n",
        "    zip_path.unlink()\n",
        "    print(f\"Removed temporary archive: {zip_path}... done\")\n",
        "\n",
        "# Optionally call the clean-up function\n",
        "if clean_up:\n",
        "    clean_up_zip(Path(cwd / zip_filename))  # Ensure path matches file name case\n",
        "    print(f\"Files have been successfully extracted to {extract_dir} and the Zip file deleted\")\n",
        "else:\n",
        "    print(f\"Files have been successfully extracted to {extract_dir}\")\n"
      ],
      "metadata": {
        "id": "zTi1CbC2pSDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image03E.png)"
      ],
      "metadata": {
        "id": "gcKYM6rnqclc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Functions to Process DICOM Files\n",
        "\n",
        "The code in the cell below uses a special package called `pydicom`. **Pydicom** is a Python package for specifically handling DICOM (Digital Imaging and Communications in Medicine) files. It allows you to read, write, modify, and create DICOM files using Python code. DICOM is a standard used in healthcare imaging that defines file formats and communications protocols for medical imaging devices and information systems.\n",
        "\n",
        "Pydicom provides classes and functions to work with the data elements defined in DICOM standards, such as Patient Name, Study Date, Modality, and many others. It supports reading from and writing to DICOM files, handling pixel data (using NumPy arrays for efficient processing), and accessing metadata embedded within DICOM files.\n",
        "\n",
        "The code is divided into a series of functions (e.g. `read_dicom_file(file_path:str`) that will be called later.\n",
        "\n",
        "**Define the Path to the Data Root:**\n",
        "```python\n",
        "  # Path to the data root\n",
        "  data_root = os.path.join('.', 'pna_data')\n",
        "  print(f\"data_root = {data_root}\")\n",
        "```\n",
        "This line constructs a file path pointing to a directory named 'pna_data' located in the current working directory (.). The os.path.join function is used to join the current directory with the folder name, forming an absolute path like '/Users/username/project/./pna_data' on Unix-based systems or 'C:\\\\Users\\\\username\\\\project\\\\./pna_data' on Windows systems (depending on where your project and data are located).\n"
      ],
      "metadata": {
        "id": "M6Y1OAM6UMbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Functions to Process DICOM Files\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import warnings\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Global settings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "#  Helper: read a single DICOM file\n",
        "def read_dicom_file(file_path: str):\n",
        "    \"\"\"Read a DICOM file and extract image data + basic metadata.\"\"\"\n",
        "    ds = pydicom.dcmread(file_path)\n",
        "\n",
        "    # Basic metadata\n",
        "    metadata = {\n",
        "        'filename': os.path.basename(file_path),\n",
        "        'patient_name': getattr(ds, 'PatientName', 'Unknown'),\n",
        "        'patient_id': getattr(ds, 'PatientID', 'Unknown'),\n",
        "        'study_date': getattr(ds, 'StudyDate', 'Unknown'),\n",
        "        'study_time': getattr(ds, 'StudyTime', 'Unknown'),\n",
        "        'modality': getattr(ds, 'Modality', 'Unknown'),\n",
        "        'manufacturer': getattr(ds, 'Manufacturer', 'Unknown'),\n",
        "        'institution_name': getattr(ds, 'InstitutionName', 'Unknown'),\n",
        "        'series_description': getattr(ds, 'SeriesDescription', 'Unknown'),\n",
        "        'bits_allocated': getattr(ds, 'BitsAllocated', 'Unknown'),\n",
        "        'rows': getattr(ds, 'Rows', 'Unknown'),\n",
        "        'columns': getattr(ds, 'Columns', 'Unknown'),\n",
        "        'pixel_spacing': getattr(ds, 'PixelSpacing', 'Unknown')\n",
        "    }\n",
        "\n",
        "    # Image data\n",
        "    if hasattr(ds, 'pixel_array'):\n",
        "        image_array = ds.pixel_array\n",
        "\n",
        "        # Normalise to 0‑255 if needed\n",
        "        if image_array.dtype != np.uint8:\n",
        "            image_array = ((image_array - image_array.min()) /\n",
        "                           (image_array.max() - image_array.min()) * 255).astype(np.uint8)\n",
        "\n",
        "        metadata['image_available'] = True\n",
        "        metadata['image_shape'] = image_array.shape\n",
        "    else:\n",
        "        metadata['image_available'] = False\n",
        "        metadata['image_shape'] = 'No image data'\n",
        "\n",
        "    return ds, metadata\n",
        "\n",
        "#  Helper: fast drop‑check\n",
        "def is_file_dropped(file_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Quick guard that tells us whether a DICOM file is already\n",
        "    missing / unreadable.\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(file_path):\n",
        "        return True\n",
        "\n",
        "    if os.path.getsize(file_path) == 0:\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        pydicom.dcmread(file_path, stop_before_pixels=True)\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# ---- Load & preprocess – merge CSVs, keep only valid DICOM rows\n",
        "import os\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "\n",
        "def load_and_preprocess_data(\n",
        "    data_dir: str = '.',\n",
        "    log_dropped: bool = True,\n",
        "    max_files: int | None = None,\n",
        "    seed: int = 42\n",
        "):\n",
        "    \"\"\"\n",
        "    Load the two CSVs, merge on patient ID, filter out rows that don't have a\n",
        "    valid DICOM file and optionally subsample the result.\n",
        "\n",
        "    Returns:\n",
        "        image_df (pd.DataFrame):  DataFrame that contains a new column\n",
        "                                 `file_path` pointing to the DICOM file\n",
        "                                 and the original merged columns.\n",
        "        dropped_ids (list[str]):  Patient IDs that were dropped.\n",
        "    \"\"\"\n",
        "    # --- 1. Resolve the path & sanity‑check the CSVs\n",
        "    data_path   = pathlib.Path(data_dir).expanduser().resolve()\n",
        "    info_csv    = data_path / 'pna_detailed_class_info.csv'\n",
        "    labels_csv  = data_path / 'pna_train_labels.csv'\n",
        "\n",
        "    for fp in (info_csv, labels_csv):\n",
        "        if not fp.is_file():\n",
        "            raise FileNotFoundError(\n",
        "                f\"Required file not found: {fp}\\n\"\n",
        "                f\"Ensure `data_dir` points to the folder containing both CSVs.\"\n",
        "            )\n",
        "\n",
        "    # --- 2. Load the CSVs\n",
        "    info_df   = pd.read_csv(info_csv)\n",
        "    labels_df = pd.read_csv(labels_csv)\n",
        "\n",
        "    # --- 3. Merge on patient ID\n",
        "    info_id_col   = 'patientId'\n",
        "    labels_id_col = 'patientId'\n",
        "\n",
        "    merged_df = pd.merge(\n",
        "        info_df,\n",
        "        labels_df,\n",
        "        left_on=info_id_col,\n",
        "        right_on=labels_id_col,\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    # --- 4. Keep only rows that actually have a readable DICOM\n",
        "    dicom_dir   = data_path / 'pna_train_images'\n",
        "    valid_rows  = []\n",
        "    dropped_ids = []\n",
        "\n",
        "    for idx, row in merged_df.iterrows():\n",
        "        patient_id = row[info_id_col]\n",
        "        dicom_file = dicom_dir / f\"{patient_id}.dcm\"\n",
        "\n",
        "        if is_file_dropped(dicom_file):\n",
        "            dropped_ids.append(patient_id)\n",
        "        else:\n",
        "            valid_rows.append(idx)\n",
        "\n",
        "    image_df = merged_df.loc[valid_rows].copy()\n",
        "    if log_dropped:\n",
        "        print(f\"Dropped {len(dropped_ids)} rows (no valid DICOM).\")\n",
        "\n",
        "    # --- 5. (Optional) Random subsample\n",
        "    if max_files is not None:\n",
        "        if max_files == -1:\n",
        "            # Do nothing; use the full dataset\n",
        "            pass\n",
        "        elif len(image_df) > max_files:\n",
        "            image_df = image_df.sample(n=max_files, random_state=seed).reset_index(drop=True)\n",
        "            if log_dropped:\n",
        "                  print(f\"Randomly subsampled to {max_files} rows (seed={seed}).\")\n",
        "\n",
        "    # --- 6. Add `file_path` column\n",
        "    #   This mirrors the original behaviour – the rest of the code can still\n",
        "    #   reference `image_df['file_path']` exactly as before.\n",
        "    image_df['file_path'] = image_df['patientId'].apply(\n",
        "        lambda pid: str(dicom_dir / f\"{pid}.dcm\")\n",
        "    )\n",
        "\n",
        "    # print(f\"Filtered DataFrame shape (with valid DICOM files): {image_df.shape}\")\n",
        "    return image_df, dropped_ids\n",
        "\n",
        "#  Update load_and_preprocess_data to return only file‑paths + labels\n",
        "def load_file_paths_and_labels(image_df, max_samples=None):\n",
        "    \"\"\"\n",
        "    Return two lists: `file_paths` and `labels`.  The function keeps the\n",
        "    *original* logic that filtered out NaNs / duplicated rows – we simply\n",
        "    strip it down to the very few objects that the new lazy loader needs.\n",
        "    \"\"\"\n",
        "    file_paths = image_df[\"file_path\"].tolist()\n",
        "    labels = image_df[\"label\"].tolist()\n",
        "\n",
        "    if max_samples is not None:\n",
        "        file_paths = file_paths[:max_samples]\n",
        "        labels = labels[:max_samples]\n",
        "\n",
        "    return file_paths, labels\n",
        "\n",
        "\n",
        "#  Show a single DICOM image (for sanity checks)\n",
        "def display_dicom_image(file_path: str, figsize: tuple = (5, 5)):\n",
        "    \"\"\"Show a single DICOM image with proper orientation.\"\"\"\n",
        "    ds = pydicom.dcmread(file_path)\n",
        "    if hasattr(ds, 'pixel_array'):\n",
        "        img = ds.pixel_array\n",
        "        if getattr(ds, 'PhotometricInterpretation', None) == 'MONOCHROME1':\n",
        "            img = np.max(img) - img\n",
        "\n",
        "        plt.figure(figsize=figsize)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(ds.SOPClassUID)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"This DICOM has no pixel data.\")\n",
        "\n",
        "# Path to the data root\n",
        "data_root = os.path.join('.', 'pna_data')\n",
        "print(f\"data_root = {data_root}\")"
      ],
      "metadata": {
        "id": "qS6FnybtUmSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image05E.png)"
      ],
      "metadata": {
        "id": "zPQ84kgYn28Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Display a DICOM Image\n",
        "\n",
        "The code in the cell below uses the function `display_dicom_image()` to display one of three image choices based on the `user input`. The code catches errors and continues to prompt the user for an image choice until a valid name is entered. Your input choices are \"Normal\", \"No Lung Opacity/Not Normal\" and \"Lung Opacity\". Only the lung images that are labled as \"Lung Opacity\" exhibit signs of pneumonia.  "
      ],
      "metadata": {
        "id": "NUiC8xM8-qSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Display DICOM image\n",
        "\n",
        "# Extract path names\n",
        "zip_filename=ZIP_FILENAME\n",
        "folder_root =  zip_filename.replace('.zip', '')\n",
        "data_root = zip_filename.replace('_data.zip', '')\n",
        "image_folder = data_root+\"_train_images/\"\n",
        "\n",
        "# Generate path to image folder\n",
        "path =\"./\"+folder_root+\"/\"+image_folder\n",
        "\n",
        "print(path)\n",
        "\n",
        "image_paths = {\n",
        "    \"Normal\": path+\"c1edf42b-5958-47ff-a1e7-4f23d99583ba.dcm\",\n",
        "    \"No Lung Opacity/Not Normal\": path+\"00322d4d-1c29-4943-afc9-b6754be640eb.dcm\",\n",
        "    \"Lung Opacity\": path+\"c1ec14ff-f6d7-4b38-b0cb-fe07041cbdc8.dcm\"\n",
        "}\n",
        "\n",
        "# Set display size\n",
        "figsize = (6, 6)\n",
        "\n",
        "# Ask the user which class they want to see\n",
        "user_choice = input(\"Enter the class type to see (Normal, 'No Lung Opacity/Not Normal', or Lung Opacity): \")\n",
        "\n",
        "# Ensure the user inputs a valid choice\n",
        "while user_choice not in image_paths:\n",
        "    print(\"Invalid choice. Please enter one of the specified class types.\")\n",
        "    user_choice = input(\"Enter the class type to see (Normal, 'No Lung Opacity/Not Normal', or Lung Opacity): \")\n",
        "\n",
        "# Display the selected image\n",
        "img_dir = image_paths[user_choice]\n",
        "display_dicom_image(img_dir, figsize)"
      ],
      "metadata": {
        "id": "KBrG0GmVJf87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output depending upon your input choice. The example shown here is for the choice \"Lung Opacity\".\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image06E.png)"
      ],
      "metadata": {
        "id": "MzqOTnR5MmNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Select Image Number\n",
        "\n",
        "The code in the cell below allows the user to select **how many images** that were downloaded should be used in the analysis. The `pna_data` file has a total of `9937` images that are `1024x1024x3` pixels in size. Let's calculate how much System RAM would be necessary to store all `9937` of these DICOM images in memory:\n",
        "\n",
        "To determine the size of a data file that contains 9937 images, each with dimensions 1024x1024 and using 3 color channels (RGB), we can follow these steps:\n",
        "\n",
        "1. **Calculate the size of one image:**\n",
        "   - Each pixel is represented by 3 bytes (one for each color channel).\n",
        "   - The image resolution is 1024 x 1024 pixels.\n",
        "   - Therefore, the size of one image in bytes is:\n",
        "     $\n",
        "     1024 \\times 1024 \\times 3 = 3,145,728 \\text{ bytes}\n",
        "     $\n",
        "\n",
        "2. **Calculate the total size for all images:**\n",
        "   - There are 9937 such images in the data file.\n",
        "   - Therefore, the total size of the data file in bytes is:\n",
        "     $\n",
        "     9937 \\times 3,145,728 = 31,260,740,616 \\text{ bytes}\n",
        "     $\n",
        "\n",
        "To put this into perspective, we can convert it to more human-readable units:\n",
        "\n",
        "- **In kilobytes (KB):**\n",
        "  $\n",
        "  31,260,740,616 \\div 1024 = 30,528,067 \\text{ KB}\n",
        "  $\n",
        "\n",
        "- **In megabytes (MB):**\n",
        "  $\n",
        "  30,528,067 \\div 1024 = 30,007.9 \\text{ MB}\n",
        "  $\n",
        "\n",
        "- **In gigabytes (GB):**\n",
        "  $\n",
        "  30,007.9 \\div 1024 = 29.8 \\text{ GB}\n",
        "  $\n",
        "\n",
        "Therefore, the size of the data file would be approximately **29.8 gigabytes** if it contained 9937 images, each with dimensions `1024x1024` and using 3 color channels. That would require a large amount of system memory just to store the image data **BEFORE** it can be transferred to the GPU. Even with Colab's `High-RAM` option enabled, it is pretty easy to generate an **Out Of Memory (OOM)** error and crash the notebook!\n"
      ],
      "metadata": {
        "id": "lMFGfh0nUMVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Select Image Number\n",
        "\n",
        "MAX_FILES=-1  # -1 means all image files\n",
        "SET_SEED=586\n",
        "\n",
        "# generate image_df\n",
        "print(f\"Generating DICOM images with seed={SET_SEED}\")\n",
        "img_df, dropped = load_and_preprocess_data(\n",
        "    data_dir='./pna_data',\n",
        "    max_files=MAX_FILES,         # 10_000,\n",
        "    seed=SET_SEED,\n",
        "    log_dropped=True\n",
        ")\n",
        "display(img_df)"
      ],
      "metadata": {
        "id": "7WQ2nOHUVHIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output depending upon your input choice.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image07E.png)"
      ],
      "metadata": {
        "id": "6KoJBcX7Za77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Generate File Paths for Data Loader\n",
        "\n",
        "The code in the cell below generates two very important Python `lists`.\n",
        "As will be explained below, these file paths will tell the special data loader where to find the X-values (images) and y-values (Targets).\n"
      ],
      "metadata": {
        "id": "cEZZu4VTUMKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Generate File Paths for Data Loader\n",
        "\n",
        "# Build the two lists that the rest of your pipeline expects ----\n",
        "file_paths_list = img_df['file_path'].tolist()\n",
        "labels_list     = img_df['Target'].astype(int).tolist()\n",
        "\n",
        "# --- Show first 3 items in lists -------------------------------\n",
        "for i in range(3):\n",
        "    print(f\"X-value {i}: {file_paths_list[i]}\")\n",
        "    print(f\"y-value {i}:     {labels_list[i]}\\n\")"
      ],
      "metadata": {
        "id": "0VWd3Xg1VV4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output depending upon your input choice.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image08E.png)\n",
        "\n",
        "At first this output might appear rather odd, especially the `X-value`. Instead of actual DICOM images, the X-values are the _path names_ to each image. This is quite different than in your previous lessons where the X-values were numerical values in a `Numpy array`. The reason for generating path names and not a numerical array has to do with saving system RAM, as will be explained below."
      ],
      "metadata": {
        "id": "MO6WPTZ0fGgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------\n",
        "\n",
        "### **_Lazy Loading_: The Secret of Analyzing Large Image Datafiles**\n",
        "\n",
        "In this lesson we didn't generate a **huge NumPy array** to hold the image data; instead, we are using **filenames** and **_lazy loading_** techniques to feed the data loader. This approach significantly reduces memory usage because it avoids loading entire image arrays into `System RAM` at once. Instead, images are loaded on-the-fly as needed by the training process, which can be **much more memory efficient** for large datasets.\n",
        "\n",
        "Here's a breakdown of why _lazy loading_ is superior to generating an Numpy array in system RAM:\n",
        "\n",
        "* **Memory Efficiency:** By using filenames and a data loader that reads images on-the-fly, the code avoids consuming nearly 30 GB of RAM all at once. This is particularly important when dealing with large datasets or high-resolution images, where each image can consume substantial memory. Lazy loading defers the loading of image data until it's actually needed for processing, thus reducing peak memory usage.\n",
        "\n",
        "* **Scalability:** This approach scales better with larger datasets because you don't need to allocate a massive amount of RAM upfront. You only load as much data into memory as your application needs at any given time, which is ideal for scenarios where the dataset size far exceeds available system RAM.\n",
        "\n",
        "* **Efficiency:** Lazy loading can also improve processing efficiency by reducing I/O bottlenecks. When training a model, you often need to read and preprocess images multiple times (e.g., during each epoch of training). This repeated reading from disk is more efficiently managed when the data isn't fully loaded into memory at once.\n",
        "\n",
        "* **Simplicity:** The code can be simpler and cleaner because it doesn't involve managing large in-memory arrays. Instead, it relies on existing libraries for efficient file handling and data loading, which often have built-in optimizations for performance.\n",
        "\n",
        "In summary, the **lazy loading technique is superior** to loading all images into a `NumPy` array due to its memory efficiency, scalability, improved processing efficiency, and simplicity. This approach allows you to work with large datasets that would otherwise be impractical to handle using traditional methods that require massive amounts of RAM.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "-------------------------------------------------"
      ],
      "metadata": {
        "id": "-0_yKzGBzjlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Data Functions\n",
        "\n",
        "\n",
        "\n",
        "The code in the cell below creates one **class** and several **functions** that will perform a variety of tasks in preparation for train our `ResNet101` neural network.\n",
        "\n",
        "Here is a brief summary of the activities performed by the most important functions.\n",
        "\n",
        "------------------------------------------------\n",
        "* class: **DicomImageDataset(Dataset)**\n",
        "This code defines a custom dataset class named **DicomImageDataset** for handling DICOM image files. The class is designed to read and process these images along with their corresponding labels. Here's a brief synopsis of its functionality:\n",
        "\n",
        "* * **Initialization (__init__):** The constructor takes three parameters: file_paths, labels, and an optional transform. It asserts that the lengths of file_paths and labels are equal, as each file path must have a corresponding label. The class initializes these inputs into instance variables for later use.\n",
        "\n",
        "* * **Length (__len__):** This method returns the total number of items in the dataset, which is determined by the length of file_paths. It represents how many images are available in the dataset.\n",
        "\n",
        "* * **Get Item (__getitem__):** This method retrieves an item from the dataset at a specified index (idx). It first extracts the file path and label.\n",
        "\n",
        "--------------------------------------------\n",
        "\n",
        "* def function: **build_loaders( )**\n",
        "This function is designed to create train/validation DataLoaders from a dataset of DICOM images. It takes several parameters including file paths and labels, as well as optional settings for batch size, validation split ratio, number of worker threads, and whether memory should be pinned during data loading. The function returns two DataLoader objects: one for the training set and one for the validation set.\n",
        "\n",
        "* * **Parameters:**\n",
        "    - `file_paths`: List of absolute paths to the DICOM files.\n",
        "    - `labels`: Corresponding integer labels for each file path.\n",
        "    - `batch_size` (default: BATCH_SIZE): Number of samples per batch in the DataLoader.\n",
        "    - `val_split` (default: VAL_SPLIT): Proportion of the dataset to include in the validation set.\n",
        "    - `num_workers` (default: 0): Number of subprocesses to use for data loading.\n",
        "    - `pin_memory` (default: False): Whether to copy tensors into CUDA pinned memory before returning them.\n",
        "\n",
        "* * **Transforms:**\n",
        "    - Two transformations are obtained based on whether the dataset is for training or validation using the `get_transform` function. The training transformation includes data augmentation, while the validation transformation does not.\n",
        "\n",
        "* * **Dataset Splitting:**\n",
        "    - A full dataset object is created from the provided file paths and labels with the training transform.\n",
        "    - The size of the train and validation sets is calculated based on the `val_split` parameter.\n",
        "    - The dataset is split into a training set (`train_ds`) and a validation set (`val_ds`) using the `random_split` function.\n",
        "\n",
        "* * **Validation Transform Override:**\n",
        "    - The transform for the validation set (`val_ds`) is overridden to use the validation transformation obtained earlier.\n",
        "\n",
        "* * **DataLoaders:**\n",
        "    - Train and validation DataLoader objects are created with the specified batch size, shuffling, number of worker threads, and pinned memory settings.\n",
        "\n",
        "* * **Returns:**\n",
        "    - A tuple containing two DataLoader objects: `train_loader` for the training set and `val_loader` for the validation set.\n",
        "--------------------------------------------\n",
        "\n",
        "* def function: **get_resnet101( )**\n",
        "\n",
        "This function creates and returns a ResNet-101 model tailored for classification tasks, optionally with additional regularization layers such as dropout or DropBlock, followed by a final linear layer. The model can be initialized with pre-trained weights on ImageNet.\n",
        "\n",
        "* * **Parameters:**\n",
        "    - `num_classes`: Number of output classes the model should predict.\n",
        "    - `pretrained` (default=True): If True, initializes the model with pre-trained weights from ImageNet.\n",
        "    - `device` (default=None): Specifies the device (CPU or GPU) where the model will be loaded and run. If None, it automatically selects based on availability.\n",
        "    - `name` (default=None): Optional human-friendly name for the model, which is stored as `model.name`.\n",
        "    - `dropout_p` (default=0.5): Dropout probability to apply after the backbone layers.\n",
        "    - `dropblock_size` (default=5): Size of the spatial block to drop using DropBlock regularization. This parameter is only used if `dropblock_prob` is greater than 0.\n",
        "    - `dropblock_prob` (default=0.1): Probability that a block is dropped during training with DropBlock regularization.\n",
        "    - `add_norm` (default=False): If True, adds a BatchNorm layer after the backbone layers.\n",
        "\n",
        "* * **Device Handling:**\n",
        "    - If no device is specified, the function will automatically select either CPU or GPU based on whether CUDA is available.\n",
        "\n",
        "* * **Model Initialization:**\n",
        "    - The model starts by loading the ResNet-101 architecture, optionally using pre-trained weights from ImageNet.\n",
        "    - DropBlock regularization is applied to the final layer of the backbone if `dropblock_size` and `dropblock_prob` are provided.\n",
        "    - The fully connected (fc) layer of the original ResNet-101 is replaced with an identity mapping since we will add our own classification head.\n",
        "\n",
        "* * **Model Tail:**\n",
        "    - Additional layers such as BatchNorm and Dropout can be added based on the parameters provided.\n",
        "    - A new linear layer is appended to serve as the final classification layer for `num_classes`.\n",
        "    - The entire sequence of layers is moved to the specified device and returned as a single sequential model.\n",
        "    - If a name is provided, it is assigned to the model's `name` attribute for identification purposes.\n",
        "\n",
        "* * **Returns:**\n",
        "    - A PyTorch nn.Module object representing the ResNet-101 backbone with optional\n",
        "\n",
        "--------------------------------------------\n",
        "\n",
        "* def function: **train_one_epoch( )**\n",
        "\n",
        "This function is designed to train a neural network model for one complete pass through the training dataset. It takes several parameters including the model itself, a DataLoader object for the training data, a loss criterion (e.g., CrossEntropyLoss), an optimizer (e.g., Adam or SGD), a device (CPU or GPU) on which to perform computations, and optionally a `model_name` for bookkeeping purposes. The function returns the average loss per sample over the epoch.\n",
        "\n",
        "* * **Parameters:**\n",
        "    - `model`: Neural network model to be trained.\n",
        "    - `loader`: DataLoader object containing the training data.\n",
        "    - `criterion`: Loss criterion (e.g., CrossEntropyLoss) for computing the loss between predictions and targets.\n",
        "    - `optimizer`: Optimizer (e.g., Adam or SGD) used to update model parameters.\n",
        "    - `device`: Device (CPU or GPU) on which to perform computations.\n",
        "    - `model_name` (optional): A string to be stored as the model's name attribute for bookkeeping purposes.\n",
        "\n",
        "* * **Bookkeeping:**\n",
        "    - If a `model_name` is provided, it is assigned to `model.name`. This allows tracking and identification of the model during training or later inspection.\n",
        "\n",
        "* * **Training Process:**\n",
        "    - The model is set to train mode using `model.train()`.\n",
        "    - A variable `epoch_loss` is initialized to accumulate the total loss over the epoch.\n",
        "    - Iterating through the DataLoader, for each batch of images (`imgs`) and targets (ground truth labels), the following steps are performed:\n",
        "        - Move both `imgs` and `targets` to the specified device (CPU or GPU).\n",
        "        - Zero out the gradients from the previous step using `optimizer.zero_grad()`.\n",
        "        - Forward pass through the model to get predictions (`outputs`).\n",
        "        - Compute the loss between `outputs` and `targets` using the provided criterion.\n",
        "        - Backward pass to compute gradients of the loss with respect to model parameters.\n",
        "        - Update model parameters using the optimizer's step function.\n",
        "        - Accumulate the batch loss to `epoch_loss`.\n",
        "    - After all batches are processed, calculate and return the average epoch loss per sample (`epoch_loss / len(loader.dataset)`).\n",
        "--------------------------------------------\n",
        "\n",
        "* def function: **run_training_lazy( )**\n",
        "\n",
        "This function orchestrates the training of a neural network model using a lazy loading approach. It handles data loading, model creation, loss computation, optimization, learning rate scheduling, and early stopping. The function is designed to be flexible with various hyperparameters and can optionally use GPU acceleration if available.\n",
        "\n",
        "* * **Parameters:**\n",
        "    - `file_paths`: List of file paths to the training images.\n",
        "    - `labels`: Corresponding list of integer labels for each image.\n",
        "    - `num_epochs` (default: NUM_EPOCHS): Number of epochs to train the model.\n",
        "    - `batch_size` (default: BATCH_SIZE): Number of samples per batch in the DataLoader.\n",
        "    - `lr` (default: LEARNING_RATE): Learning rate for the optimizer.\n",
        "    - `weight_decay` (default: WEIGHT_DECAY): L2 regularization strength.\n",
        "    - `patience` (default: PATIENCE): Number of epochs with no improvement after which training will be stopped.\n",
        "    - `val_split` (default: VAL_SPLIT): Fraction of the dataset to use as validation set.\n",
        "    - `early_stop` (default: EARLY_STOPPING): Boolean flag indicating whether to stop early if validation loss does not improve.\n",
        "    - `device` (default: None): Device (CPU or GPU) where the model will be loaded and run. If None, it automatically selects based on availability.\n",
        "\n",
        "* * **Device Handling:**\n",
        "    - If no device is specified, the function will automatically select either CPU or GPU based on whether CUDA is available.\n",
        "\n",
        "* * **Data Loaders:**\n",
        "    - `train_loader` and `val_loader` are created using the `build_loaders` function with parameters for batch size, validation split, number of worker threads, and pinned memory.\n",
        "\n",
        "* * **Model Initialization:**\n",
        "    - A ResNet-101 model is instantiated with specified hyperparameters such as dropout probabilities, normalization addition, and a custom name. The number of classes (`num_classes`) is set based on the maximum label value plus one.\n",
        "\n",
        "* * **Loss/Optimizer/Scheduler:**\n",
        "    - CrossEntropyLoss is used for loss computation with optional label smoothing.\n",
        "    - AdamW optimizer is initialized with specified learning rate and weight decay.\n",
        "    - CosineAnnealingLR scheduler adjusts the learning rate during training.\n",
        "\n",
        "* * **Early Stopping:**\n",
        "    - A best model checkpoint is maintained to revert to if validation performance does not improve over a set number of epochs (`patience`).\n",
        "\n",
        "* * **Training Loop:**\n",
        "    - The function iterates through each epoch, performing both training and validation phases:\n",
        "        - During training, it computes gradients, updates weights, and accumulates loss.\n",
        "        - During validation, it evaluates the model without updating parameters, accumulating loss and accuracy metrics.\n",
        "    - Learning rate is adjusted using the scheduler at the end of each epoch.\n",
        "    - Early stopping is implemented to halt training if performance does not improve for a specified number of epochs.\n",
        "\n",
        "* * **Final Report:**\n",
        "    - After training, the best performing model (based on validation loss) is reported along with its metrics.\n",
        "    - Memory cache is cleared and garbage collected at the end of training to free up resources.\n",
        "\n",
        "* * **Returns:**\n",
        "    - A dictionary containing the training and validation losses, accuracies, as well as the best epoch and associated metrics.\n",
        "--------------------------------------------\n",
        "\n",
        "* def function: **plot_train_hist( )**\n",
        "\n",
        "This function generates a two-panel plot to visualize the training and validation performance metrics over epochs. The left panel displays accuracy trends during training and validation, while the right panel shows loss trends. Optionally, it highlights the epoch with the best model performance using a vertical red dashed line.\n",
        "\n",
        "* * **Parameters:**\n",
        "    - `history`: A dictionary containing keys for 'train_acc', 'val_acc', 'train_loss', and 'val_loss'. These keys should map to lists or arrays of metrics recorded over each epoch during training.\n",
        "\n",
        "* * **Plot Layout:**\n",
        "    - The plot consists of two subplots arranged horizontally: one on the left for accuracy, and one on the right for loss. Each subplot is configured with appropriate labels, titles, and limits as follows:\n",
        "        - Left panel (accuracy):\n",
        "            - Plots training and validation accuracy curves.\n",
        "            - Sets x-axis label to \"Epoch\", y-axis label to \"Accuracy\", and ensures the y-axis range is between 0 and 1 for better interpretability of the metrics.\n",
        "            - The legend indicates which curve corresponds to train and validation accuracy, positioned at the bottom right.\n",
        "        - Right panel (loss):\n",
        "            - Plots training and validation loss curves.\n",
        "            - Sets x-axis label to \"Epoch\", y-axis label to \"Loss\".\n",
        "            - Ensures the y-axis starts from 0 for better visual differentiation of trends in the loss values. The top limit is left to matplotlib's default setting, which generally adjusts it based on the data range.\n",
        "            - The legend indicates which curve corresponds to train and validation loss, positioned at the top right.\n",
        "\n",
        "* * **Highlight Best Epoch:**\n",
        "    - If a 'best_epoch' key exists in the `history` dictionary, indicating that early stopping was applied during training, a vertical red dashed line is drawn at this epoch on both subplots to highlight it as the optimal point where performance started improving significantly.\n",
        "    - The legend for each axis is updated dynamically to include the \"Best epoch\" label if not already present.\n",
        "\n",
        "* * **Figure Title and Layout:**\n",
        "    - A main title \"Training History\" is added at the top of the figure, ensuring it does not overlap with subplots.\n",
        "    - `fig.tight_layout(rect=[0, 0, 1, 0.95])` adjusts the layout to ensure all elements fit well without overlapping, leaving some space for the title.\n",
        "\n",
        "* * **Display the Plot:**\n",
        "    - The plot is displayed using `plt.show()`, which opens a window showing both subplots side by side."
      ],
      "metadata": {
        "id": "Y9qe072LVRRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Data Functions\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms, models\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from typing import Optional\n",
        "\n",
        "# Set Import Variables Here!\n",
        "NUM_EPOCHS: int = 50\n",
        "BATCH_SIZE: int = 64  # Reduced for memory management\n",
        "IMG_SIZE: int = 512  # Standard size for ResNet101\n",
        "LEARNING_RATE: float = 0.0010\n",
        "WEIGHT_DECAY: float  = 0.0010\n",
        "VAL_SPLIT: float = 0.2\n",
        "PATIENCE = 5\n",
        "EARLY_STOPPING = True\n",
        "\n",
        "# ------------------------------------------\n",
        "# Custom Dataset Class with Transforms\n",
        "# ------------------------------------------\n",
        "class DicomImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Load a DICOM image from disk and return (image_tensor, label).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_paths : list[str]\n",
        "        Absolute paths to the DICOM files.\n",
        "    labels    : list[int]\n",
        "        Integer labels that align with `file_paths`.\n",
        "    transform : callable | None\n",
        "        Optional transform that receives the `pydicom.dataset.FileDataset`\n",
        "        and returns a torch tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        assert len(file_paths) == len(labels), \\\n",
        "            f\"file_paths ({len(file_paths)}) != labels ({len(labels)})\"\n",
        "        self.file_paths = file_paths\n",
        "        self.labels    = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path  = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            dicom_img = pydicom.dcmread(path, force=True)   # <-- fails on corrupt files\n",
        "        except Exception as exc:\n",
        "            # Raise a RuntimeError that will be caught by the caller.\n",
        "            raise RuntimeError(f\"Could not read DICOM at {path!r}: {exc}\")\n",
        "\n",
        "        # Convert DICOM pixel array to proper format for transforms\n",
        "        if hasattr(dicom_img, 'pixel_array'):\n",
        "            img_array = dicom_img.pixel_array\n",
        "\n",
        "            # Handle different data types and normalize\n",
        "            if img_array.dtype != np.uint8:\n",
        "                if img_array.max() > 1.0:\n",
        "                    img_array = ((img_array - img_array.min()) /\n",
        "                               (img_array.max() - img_array.min()) * 255).astype(np.uint8)\n",
        "                else:\n",
        "                    img_array = (img_array * 255).astype(np.uint8)\n",
        "\n",
        "            # Convert to PIL Image for transforms\n",
        "            from PIL import Image\n",
        "            if len(img_array.shape) == 2:\n",
        "                # Single channel - convert to RGB by duplicating\n",
        "                pil_img = Image.fromarray(img_array, mode='L')\n",
        "                # Convert grayscale to RGB\n",
        "                pil_img = pil_img.convert('RGB')\n",
        "            else:\n",
        "                # Already multi-channel, convert to RGB if needed\n",
        "                pil_img = Image.fromarray(img_array)\n",
        "                if pil_img.mode != 'RGB':\n",
        "                    pil_img = pil_img.convert('RGB')\n",
        "\n",
        "            if self.transform is not None:\n",
        "                img = self.transform(pil_img)\n",
        "            else:\n",
        "                # Convert to tensor if no transform\n",
        "                img = torch.tensor(img_array, dtype=torch.float32)\n",
        "                if len(img.shape) == 2:\n",
        "                    img = img.unsqueeze(0)  # Add channel dimension\n",
        "        else:\n",
        "            raise RuntimeError(f\"No pixel array in DICOM file {path!r}\")\n",
        "\n",
        "        return img, label\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Helper: Check file paths\n",
        "# ------------------------------------------\n",
        "def filter_valid_dicom(paths, labels, max_attempts=5):\n",
        "    \"\"\"\n",
        "    Returns a pair (valid_paths, valid_labels) that only contains files that\n",
        "    can be successfully read with pydicom.\n",
        "    \"\"\"\n",
        "    valid_paths, valid_labels = [], []\n",
        "    for p, l in zip(paths, labels):\n",
        "        try:\n",
        "            pydicom.dcmread(p, force=True)\n",
        "        except Exception as exc:\n",
        "            # Log and skip\n",
        "            print(f\"Skipping {p!r}: {exc}\")\n",
        "            continue\n",
        "        valid_paths.append(p)\n",
        "        valid_labels.append(l)\n",
        "\n",
        "    if not valid_paths:\n",
        "        raise RuntimeError(\"All DICOM files failed to load – dataset is empty.\")\n",
        "    return valid_paths, valid_labels\n",
        "\n",
        "# ------------------------------------------\n",
        "# Helper: Build transforms\n",
        "# ------------------------------------------\n",
        "def get_transform(\n",
        "    img_size=IMG_SIZE,\n",
        "    is_train: bool = True,\n",
        "    crop_size=IMG_SIZE,\n",
        "    h_flip: bool = True,\n",
        "    augment: bool = False\n",
        ") -> transforms.Compose:\n",
        "    \"\"\"\n",
        "    Returns a torchvision transform chain.\n",
        "    \"\"\"\n",
        "    if is_train:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.RandomResizedCrop(crop_size) if augment else transforms.CenterCrop(crop_size),\n",
        "            transforms.RandomHorizontalFlip() if h_flip else transforms.Lambda(lambda x: x),\n",
        "            transforms.ToTensor(),  # This will convert to float and normalize [0,1]\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "    else:  # eval / test\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.CenterCrop(crop_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    return transform\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Helper: Build loaders from lazy datasets\n",
        "# ------------------------------------------------------------------\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import ResNet101_Weights\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import gc\n",
        "\n",
        "def build_loaders(\n",
        "    file_paths, labels,\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "    val_split:  float = VAL_SPLIT,\n",
        "    num_workers: int = 0,\n",
        "    pin_memory: bool = False\n",
        "):\n",
        "    \"\"\"Create train/validation DataLoaders from lazy dataset.\"\"\"\n",
        "    transform_train = get_transform(is_train=True)\n",
        "    transform_val   = get_transform(is_train=False)\n",
        "\n",
        "    full_dataset = DicomImageDataset(file_paths, labels, transform=transform_train)\n",
        "\n",
        "    train_size = int(len(full_dataset) * (1.0 - val_split))\n",
        "    val_size   = len(full_dataset) - train_size\n",
        "    train_ds, val_ds = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    # Override validation transform\n",
        "    val_ds.dataset.transform = transform_val\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# ------------------------------------------\n",
        "# Training loop\n",
        "# ------------------------------------------\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    model_name: str | None = None,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Train a single epoch.  The new ``model_name`` argument is *only* for\n",
        "    bookkeeping – it is stored on the model as ``model.name``.\n",
        "    \"\"\"\n",
        "    if model_name is not None:\n",
        "        # Store the name for later inspection\n",
        "        model.name = model_name\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for imgs, targets in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    return epoch_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# Measure validation loss during training\n",
        "# --------------------------------------------\n",
        "def validate(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> tuple[float, float]:\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in tqdm(loader, desc=\"Validation\", leave=False):\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            epoch_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == targets).sum().item()\n",
        "\n",
        "    val_loss = epoch_loss / len(loader.dataset)\n",
        "    val_acc = correct / len(loader.dataset)\n",
        "    return val_loss, val_acc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from dropblock import DropBlock2D  # pip install dropblock\n",
        "\n",
        "# --------------------------------------------\n",
        "# Create ResNet model\n",
        "# --------------------------------------------\n",
        "def get_resnet101(\n",
        "    num_classes: int,\n",
        "    pretrained: bool = True,\n",
        "    device: torch.device | None = None,\n",
        "    name: str | None = None,\n",
        "    dropout_p: float = 0.5,\n",
        "    dropblock_size: int | None = 5,\n",
        "    dropblock_prob: float | None = 0.1,\n",
        "    add_norm: bool = False,\n",
        "    **kwargs,\n",
        ") -> nn.Module:\n",
        "    \"\"\"\n",
        "    Return a ResNet‑101 backbone followed by optional regularisers\n",
        "    and a final linear head.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_classes : int\n",
        "        Number of output classes.\n",
        "    pretrained : bool, default=True\n",
        "        Load ImageNet pre‑trained weights.\n",
        "    device : torch.device | None, default=None\n",
        "        Device to move the model onto.\n",
        "    name : str | None, default=None\n",
        "        Optional human‑friendly name (`model.name = name`).\n",
        "    dropout_p : float, default=0.5\n",
        "        Drop‑out probability *after* the backbone.\n",
        "    dropblock_size : int | None, default=5\n",
        "        Size of the spatial block to drop (used only if dropblock_prob > 0).\n",
        "    dropblock_prob : float | None, default=0.1\n",
        "        Probability that a block is dropped.\n",
        "    add_norm : bool, default=False\n",
        "        Add a 1‑D BatchNorm layer *after* the backbone.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    nn.Module\n",
        "        Sequential model: `backbone → (optional layers) → Linear`\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load the pre‑trained ResNet‑101\n",
        "    backbone = models.resnet101(\n",
        "        weights=models.ResNet101_Weights.DEFAULT if pretrained else None\n",
        "    )\n",
        "\n",
        "    # Insert DropBlock *before* the global avg‑pool\n",
        "    if dropblock_size is not None and dropblock_prob is not None:\n",
        "        # The final residual block produces a 4‑D tensor (b, 2048, 7, 7)\n",
        "        backbone.layer4 = nn.Sequential(\n",
        "            backbone.layer4,\n",
        "            DropBlock2D(block_size=dropblock_size, drop_prob=dropblock_prob)\n",
        "        )\n",
        "\n",
        "    # Make the backbone output a 1‑D vector (2048‑dim)\n",
        "    in_features = backbone.fc.in_features\n",
        "    backbone.fc = nn.Identity()\n",
        "\n",
        "    # Build the tail\n",
        "    layers: list[nn.Module] = [backbone]\n",
        "\n",
        "    if add_norm:\n",
        "        layers.append(nn.BatchNorm1d(in_features))\n",
        "\n",
        "    if dropout_p > 0.0:\n",
        "        layers.append(nn.Dropout(p=dropout_p))\n",
        "\n",
        "    # Final classification head\n",
        "    layers.append(nn.Linear(in_features, num_classes))\n",
        "\n",
        "    # Stack everything\n",
        "    model = nn.Sequential(*layers).to(device)\n",
        "\n",
        "    if name is not None:\n",
        "        model.name = name\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Training routine\n",
        "# ------------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import copy, gc\n",
        "\n",
        "def run_training_lazy(\n",
        "    file_paths: list[str],\n",
        "    labels:     list[int],\n",
        "    num_epochs: int   = NUM_EPOCHS,\n",
        "    batch_size: int   = BATCH_SIZE,\n",
        "    lr:          float = LEARNING_RATE,\n",
        "    weight_decay:float = WEIGHT_DECAY,\n",
        "    patience:    int   = PATIENCE,\n",
        "    val_split:   float = VAL_SPLIT,\n",
        "    early_stop:  bool  = EARLY_STOPPING,\n",
        "    device:      torch.device = None\n",
        "):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # ----- Loaders\n",
        "    train_loader, val_loader = build_loaders(\n",
        "        file_paths, labels,\n",
        "        batch_size=batch_size,\n",
        "        val_split=val_split,\n",
        "        num_workers=0,\n",
        "        pin_memory=False\n",
        "    )\n",
        "\n",
        "    # ----- Model\n",
        "    num_classes = max(labels) + 1\n",
        "    # model = get_resnet101(num_classes=num_classes, pretrained=True, device=device)\n",
        "    model = get_resnet101(\n",
        "    num_classes=10,\n",
        "    pretrained=True,\n",
        "    dropout_p=0.5,\n",
        "    dropblock_size=5,\n",
        "    dropblock_prob=0.1,\n",
        "    add_norm=True,\n",
        "    name=\"resnet101_reg\"\n",
        ")\n",
        "\n",
        "    # ----- Loss/optimiser/scheduler\n",
        "    #criterion = nn.CrossEntropyLoss()\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    #optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=num_epochs, eta_min=lr * 0.01\n",
        "    )\n",
        "\n",
        "    # ----- Early‑stopping bookkeeping\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_epoch = 0\n",
        "    best_train_acc = best_val_acc = None\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    epochs_without_improve = 0\n",
        "\n",
        "    # ----- History\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs,   val_accs   = [], []\n",
        "\n",
        "    # ----- Epoch loop\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        # training\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        n_train = len(train_loader.dataset)\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item() * xb.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct_train += (preds == yb).sum().item()\n",
        "\n",
        "        epoch_train_loss /= n_train\n",
        "        epoch_train_acc  = correct_train / n_train\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        n_val = len(val_loader.dataset)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "\n",
        "                epoch_val_loss += loss.item() * xb.size(0)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                correct_val += (preds == yb).sum().item()\n",
        "\n",
        "        epoch_val_loss /= n_val\n",
        "        epoch_val_acc  = correct_val / n_val\n",
        "\n",
        "        # scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        # early stopping\n",
        "        if early_stop:\n",
        "            if epoch_val_loss < best_val_loss - 1e-5:\n",
        "                best_val_loss = epoch_val_loss\n",
        "                best_epoch = epoch\n",
        "                best_train_acc = epoch_train_acc\n",
        "                best_val_acc   = epoch_val_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                epochs_without_improve = 0\n",
        "            else:\n",
        "                epochs_without_improve += 1\n",
        "\n",
        "            if epochs_without_improve >= patience:\n",
        "                print(\"\\nEarly stopping triggered.\")\n",
        "                print(f\"Best epoch (before stopping): {best_epoch}\")\n",
        "                print(f\"  Train Acc: {best_train_acc:.4f} | Train Loss: {epoch_train_loss:.4f}\")\n",
        "                print(f\"  Valid Acc: {best_val_acc:.4f} | Valid Loss: {epoch_val_loss:.4f}\\n\")\n",
        "                model.load_state_dict(best_model_wts)\n",
        "                break\n",
        "\n",
        "        # log\n",
        "        print(\n",
        "            f\"Epoch {epoch}/{num_epochs} | \"\n",
        "            f\"Train Acc:  {epoch_train_acc:.4f} | Train Loss: {epoch_train_loss:.4f} | \"\n",
        "            f\"Val Acc:    {epoch_val_acc:.4f} | Val Loss: {epoch_val_loss:.4f}\"\n",
        "        )\n",
        "\n",
        "        # history\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "\n",
        "    # final report\n",
        "    if not early_stop or epochs_without_improve < patience:\n",
        "        best_epoch = epoch\n",
        "        best_train_acc = epoch_train_acc\n",
        "        best_val_acc   = epoch_val_acc\n",
        "        best_val_loss  = epoch_val_loss\n",
        "\n",
        "    print(\"\\nTraining finished.\")\n",
        "    print(f\"Best epoch: {best_epoch}\")\n",
        "    print(f\"  Train Acc: {best_train_acc:.4f} | Train Loss: {epoch_train_loss:.4f}\")\n",
        "    print(f\"  Val   Acc: {best_val_acc:.4f}   | Val   Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"train_acc\":   train_accs,\n",
        "        \"val_loss\":    val_losses,\n",
        "        \"val_acc\":     val_accs,\n",
        "        \"best_epoch\":  best_epoch,\n",
        "        \"best_train_acc\": best_train_acc,\n",
        "        \"best_val_acc\":   best_val_acc,\n",
        "        \"best_train_loss\": epoch_train_loss,\n",
        "        \"best_val_loss\":   epoch_val_loss,\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Plot training history\n",
        "# -------------------------------------------------------------\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_train_hist(history: dict) -> None:\n",
        "    \"\"\"\n",
        "    Plot a professional two-panel figure:\n",
        "        • Left panel – train & val accuracy.\n",
        "        • Right panel – train & val loss.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    history : dict\n",
        "        Must contain the keys 'train_acc', 'val_acc',\n",
        "        'train_loss', 'val_loss'.\n",
        "        Optional key: 'best_epoch' (1-based index).\n",
        "    \"\"\"\n",
        "    # Use a clean style\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    num_epochs = len(history[\"train_acc\"])\n",
        "    epochs = np.arange(1, num_epochs + 1)\n",
        "\n",
        "    # ---- 2-panel layout ---------------------------------------\n",
        "    fig, (ax_acc, ax_loss) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # ==========================================\n",
        "    # LEFT PANEL: ACCURACY\n",
        "    # ==========================================\n",
        "    ax_acc.plot(epochs, history[\"train_acc\"], label=\"Training Accuracy\",\n",
        "                color=\"#1f77b4\", marker='o', markersize=4, linewidth=2)\n",
        "    ax_acc.plot(epochs, history[\"val_acc\"],   label=\"Validation Accuracy\",\n",
        "                color=\"#ff7f0e\", marker='s', markersize=4, linewidth=2)\n",
        "\n",
        "    ax_acc.set_xlabel(\"Epoch\", fontsize=12, fontweight='bold')\n",
        "    ax_acc.set_ylabel(\"Accuracy\", fontsize=12, fontweight='bold')\n",
        "    ax_acc.set_ylim(0, 1.05)\n",
        "    ax_acc.set_title(\"Model Accuracy\", fontsize=14)\n",
        "    ax_acc.legend(loc=\"lower right\", frameon=True)\n",
        "    ax_acc.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # ==========================================\n",
        "    # RIGHT PANEL: LOSS\n",
        "    # ==========================================\n",
        "    ax_loss.plot(epochs, history[\"train_loss\"], label=\"Training Loss\",\n",
        "                 color=\"#2ca02c\", marker='o', markersize=4, linewidth=2)\n",
        "    ax_loss.plot(epochs, history[\"val_loss\"],   label=\"Validation Loss\",\n",
        "                 color=\"#d62728\", marker='s', markersize=4, linewidth=2)\n",
        "\n",
        "    ax_loss.set_xlabel(\"Epoch\", fontsize=12, fontweight='bold')\n",
        "    ax_loss.set_ylabel(\"Loss\", fontsize=12, fontweight='bold')\n",
        "    ax_loss.set_ylim(bottom=0)\n",
        "    ax_loss.set_title(\"Model Loss\", fontsize=14)\n",
        "    ax_loss.legend(loc=\"upper right\", frameon=True)\n",
        "    ax_loss.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # ==========================================\n",
        "    # HIGHLIGHT BEST EPOCH\n",
        "    # ==========================================\n",
        "    if \"best_epoch\" in history:\n",
        "        best_ep = history[\"best_epoch\"]\n",
        "        best_idx = best_ep - 1 # Convert 1-based epoch to 0-based index\n",
        "\n",
        "        # 1. Vertical Line on both plots\n",
        "        for ax in (ax_acc, ax_loss):\n",
        "            ax.axvline(best_ep, color=\"gray\", linestyle=\"--\", linewidth=1.5, alpha=0.8)\n",
        "            # Add text label at the top of the line\n",
        "            ax.text(best_ep, ax.get_ylim()[1]*0.95, f' Best Epoch: {best_ep}',\n",
        "                    color=\"gray\", fontweight='bold', ha='left')\n",
        "\n",
        "        # 2. Annotate value on Accuracy Plot\n",
        "        best_val_acc = history[\"val_acc\"][best_idx]\n",
        "        ax_acc.plot(best_ep, best_val_acc, marker='*', color='gold', markersize=15, markeredgecolor='black')\n",
        "        ax_acc.annotate(f\"{best_val_acc:.2f}\",\n",
        "                        (best_ep, best_val_acc),\n",
        "                        textcoords=\"offset points\", xytext=(-20, 10), ha='center',\n",
        "                        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", alpha=0.8))\n",
        "\n",
        "        # 3. Annotate value on Loss Plot\n",
        "        best_val_loss = history[\"val_loss\"][best_idx]\n",
        "        ax_loss.plot(best_ep, best_val_loss, marker='*', color='gold', markersize=15, markeredgecolor='black')\n",
        "        ax_loss.annotate(f\"{best_val_loss:.2f}\",\n",
        "                        (best_ep, best_val_loss),\n",
        "                        textcoords=\"offset points\", xytext=(-20, 10), ha='center',\n",
        "                        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", alpha=0.8))\n",
        "\n",
        "    # Main Title\n",
        "    fig.suptitle(\"Training History Metrics\", fontsize=18, fontweight='bold', y=0.98)\n",
        "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YTXmganaVo-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should _not_ see any output."
      ],
      "metadata": {
        "id": "gACON8wvqq5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Device\n",
        "\n",
        "The code in the cell below double-checks to see if there is a Nvidia GPU available to PyTorch to train the neural network.  The code then issues a warning if no GPU is available."
      ],
      "metadata": {
        "id": "JwqwLG3LVRJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device\n",
        "\n",
        "import torch\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if device == torch.device(\"cuda\"):\n",
        "    print(f\"Using device: {device} for neural network training\")\n",
        "elif device == torch.device(\"cpu\"):\n",
        "   print(f\"WARNING: While this code can run on a CPU, it will take many hours to run a single epoch!\")\n",
        "else:\n",
        "  print(\"No device found!\")"
      ],
      "metadata": {
        "id": "JLjEp3lwV1cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct and you are using a GPU, you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image09E.png)\n"
      ],
      "metadata": {
        "id": "4fh33XVw4xq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Run Training\n",
        "\n",
        "This code snippet initiates the training process for a classification model, measuring and printing the total elapsed time taken to complete the training across a specified number of epochs (NUM_EPOCHS).\n",
        "\n",
        "The key steps involved are:\n",
        "\n",
        "* **Start Training Notification:** It prints a message indicating that the training is starting for the given number of epochs, along with other relevant details such as batch size and learning rate.\n",
        "\n",
        "* **Time Measurement:** It captures the current time before commencing the training using time.time() to serve as the start time. This will be used later to calculate the total elapsed time during and after the training process.\n",
        "\n",
        "* **Training Execution:** The function `run_training_lazy` is called with parameters for file paths, labels, epochs, batch size, learning rate, weight decay, patience (for early stopping), validation split fraction, and a flag indicating whether to use early stopping. This function performs the actual training of the model using these inputs. The suffix `lazy` refers to the fact that it is a \"lazy\" or \"delayed evaluation\" function. In programming, a `lazy function` is one that defers the computation or execution of its result until it is explicitly demanded, typically on request (e.g., when accessed). This approach can be particularly useful for optimizing performance by reducing unnecessary operations and improving efficiency, especially with large datasets or complex computations.\n",
        "\n",
        "* **End Time Recording:** After the training is complete, it records the current time again using time.time() to calculate the total elapsed time by subtracting the start time from this end time.\n",
        "\n",
        "* **Elapsed Time Output:** The code prints out the total elapsed time in a human-readable format (hours:minutes:seconds) using the function hms_string(elapsed_time) which likely converts the elapsed seconds into hours, minutes, and seconds for easier readability. This helps users understand how much time was spent on training.\n",
        "\n",
        "The purpose of this code is to provide user feedback regarding both the progress (training epochs completed so far) and efficiency (total time taken for all epochs), thus aiding in performance monitoring and debugging if needed.\n",
        "\n",
        "### **TIME ALERT**\n",
        "\n",
        "You will be training a relatively large dataset. You can expect training to require **more than 1 hr**."
      ],
      "metadata": {
        "id": "qXBP0g0FV7RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Training\n",
        "\n",
        "import time\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Start training\n",
        "# ------------------------------------------------------------------------\n",
        "print(f\"-- Training (classification) is starting for {NUM_EPOCHS} epochs----------------------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Run training\n",
        "history = run_training_lazy(\n",
        "    file_paths=file_paths_list,\n",
        "    labels=labels_list,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    patience=PATIENCE,\n",
        "    val_split=VAL_SPLIT,\n",
        "    early_stop=EARLY_STOPPING\n",
        ")\n",
        "\n",
        "# Record end time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")"
      ],
      "metadata": {
        "id": "DHm9uEmCWOxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image10E.png)\n"
      ],
      "metadata": {
        "id": "LFJxbEeLJsNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7: Plot Training History\n",
        "\n",
        "The code in the cell below generates two graphs. The graph on the left shows the change during each epoch in the model's accuracy in predicting the correct label for each training image (`train_acc`) and for each validation image (`val_acc`) while the graph on the right shows model's loss for each epoch for the training images (`train_loss`) and for the validation images (`val loss`). If the model was stopped early (`EarlyStopping`), a vertical red line indicates the `best epoch`.\n",
        "\n",
        "It should be noted that `EarlyStopping` is best on the lowest `val_loss` value, not the highest `val_acc` value. There are a variety of reasons for doing it this way which are beyond the scope of this class lesson."
      ],
      "metadata": {
        "id": "9tQMWwuPV7Kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: Plot Training History\n",
        "plot_train_hist(history=history)\n"
      ],
      "metadata": {
        "id": "nmVOTKaBWyZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_3_image11E.png)\n",
        "\n",
        "### **Analysis of Training History Metrics**\n",
        "\n",
        "The graphs above visualize the performance of the machine learning model over roughly 21 epochs. The plots track two key metrics: **Accuracy** (higher is better) and **Loss** (lower is better) for both the training and validation datasets.\n",
        "\n",
        "##### **1. Model Accuracy (Left Panel)**\n",
        "\n",
        "* **Training Accuracy (Blue Line):** The model learns the training data extremely fast. By Epoch 6, it reaches nearly **100% accuracy (1.0)** and stays there. This indicates the model has effectively memorized the training examples.\n",
        "* **Validation Accuracy (Orange Line):** The model's performance on unseen data improves initially but plateaus around **91% (0.91)**. It does not improve significantly after Epoch 7.\n",
        "* **The \"Best Epoch\":** The star annotation highlights **Epoch 17**, which achieved the maximum validation accuracy of **0.91**.\n",
        "\n",
        "##### **2. Model Loss (Right Panel)**\n",
        "\n",
        "* **Training Loss (Green Line):** The loss decreases rapidly and smoothly, stabilizing around **0.5**, confirming that the model is effectively minimizing errors on the training set.\n",
        "* **Validation Loss (Red Line):** This metric is much \"noisier\" (bumpy) than the training loss. It drops initially but hits a floor around **0.73** and stops improving. Crucially, it stays consistently higher than the training loss.\n",
        "\n",
        "##### **3. Key Diagnosis: Overfitting**\n",
        "The most distinct feature of these graphs is the large **gap** (divergence) between the training curves and the validation curves.\n",
        "\n",
        "* **The Gap:** The model performs significantly better on training data (Accuracy ~1.0) than on validation data (Accuracy ~0.91).\n",
        "* **Interpretation:** This is a classic signature of **Overfitting**. The model has learned the specific details or \"noise\" of the training data too well, preventing it from generalizing perfectly to new data.\n",
        "* **Conclusion:** While 91% accuracy is a strong result, the flatline in the validation metrics suggests the model has reached its capacity. Training for more epochs beyond this point is unlikely to yield better results."
      ],
      "metadata": {
        "id": "idPYp50NV7Di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook if you are using a Mac. If you are using a Windows computer, then use the **File --> Print.. --> Microsoft Print to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_06_3.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "dMPEzhHMV66i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lizard Tail**\n",
        "\n",
        "## **Nano Banana Pro**\n",
        "\n",
        "#### **1. Introduction: The Supercomputer in Your Pocket**\n",
        "\n",
        "In the grand history of scientific tooling, there are moments of distinct punctuation—points where a barrier to entry completely collapses. The microscope allowed anyone to see the invisible. The polymerase chain reaction (PCR) allowed anyone to amplify the infinitesimal. Today, we are living through the next great punctuation: the democratization of high-performance artificial intelligence.\n",
        "\n",
        "We refer to this ecosystem as the **\"Nano Banana Pro.\"**\n",
        "\n",
        "While this may sound whimsical, the name encapsulates a profound shift in how biology and medicine are practiced.\n",
        "* **\"Nano\"** represents accessibility and ubiquity. It requires no massive server racks, no expensive dedicated workstations, and no complex installation processes. It runs on the \"thin clients\" we carry every day—our laptops, tablets, and phones.\n",
        "* **\"Banana\"** symbolizes the organic, flexible, and user-friendly nature of the platform. Just as a banana comes in its own wrapper and is ready to consume, this tech stack is pre-packaged (via Google Colab) and ready to \"peel\" open and use immediately.\n",
        "* **\"Pro\"** belies the accessibility. Beneath the simple interface lies the raw power of Google’s TPUs (Tensor Processing Units) and the Gemini multimodal foundation models—tools that rival the computing power available to top research institutions just a decade ago.\n",
        "\n",
        "For the biology major or premed student, the Nano Banana Pro is not just a calculator; it is a limitless lab partner, a tireless research assistant, and a creative engine. This analysis explores the history, mechanics, and boundless applications of this ecosystem, serving as a roadmap for your future in the bio-digital age.\n",
        "\n",
        "#### **2. A History of Convergence: From Command Lines to Chatbots**\n",
        "\n",
        "To understand the power you hold in your hands with a simple API key, we must understand the \"Dark Ages\" that preceded it.\n",
        "\n",
        "### The Era of \"Iron and Syntax\" (1990s–2010s)\n",
        "For decades, computational biology (bioinformatics) was a gated community. To analyze a genome or model a protein, you needed three things that most biology undergraduates did not have:\n",
        "1.  **Heavy Iron:** Physical access to a Linux cluster or a high-performance workstation.\n",
        "2.  **Arcane Knowledge:** Fluency in the command line (BASH), Perl, or C++.\n",
        "3.  **Patience:** Setting up software environments often took longer than the experiment itself.\n",
        "\n",
        "Biology students were taught that \"wet lab\" (pipettes and petri dishes) and \"dry lab\" (code and servers) were separate worlds. If you wanted to cross over, you had to leave your biology intuition behind and become a computer scientist.\n",
        "\n",
        "### The Jupyter Revolution\n",
        "The first crack in the wall came with the **Jupyter Notebook**. It allowed code to be written in blocks, mixed with text and images, and run interactively. It turned coding from a \"script\" into a \"narrative.\" Python became the lingua franca of science because of its readability.\n",
        "\n",
        "### The Cloud Shift: Google Colab\n",
        "In late 2017, Google released Colaboratory (Colab). It was a Jupyter notebook environment running entirely in the cloud. Crucially, Google offered free access to GPUs and TPUs. Suddenly, a student with a $200 Chromebook could train a deep learning model that previously required a $5,000 workstation. The hardware barrier was obliterated.\n",
        "\n",
        "### The Cognitive Leap: The Transformer & Gemini\n",
        "While Colab solved the *hardware* problem, the *software* problem remained: you still had to know how to write code perfectly. A missing semicolon or a misspelled variable would crash your simulation.\n",
        "\n",
        "This changed with the advent of the **Transformer architecture** (the \"T\" in ChatGPT) and the rise of Large Language Models (LLMs). Google's Gemini represents the pinnacle of this evolution. Unlike earlier models that were \"text-only,\" Gemini is **multimodal**—native in text, code, images, and video.\n",
        "\n",
        "When we combine the compute of Colab with the cognition of Gemini, we get the **Nano Banana Pro**. The barrier of syntax is removed. You no longer need to speak \"Computer\" fluently; the computer now speaks \"Biologist.\"\n",
        "\n",
        "#### **3. The \"Nano Banana Pro\" in the Professional Realm**\n",
        "\n",
        "For students aiming for medical school, graduate research, or the biotech industry, mastery of this ecosystem is a career accelerant. Here is how it applies to professional workflows.\n",
        "\n",
        "### A. The \"Pocket Pathologist\": Medical Imaging\n",
        "In modern medicine, the volume of diagnostic imaging (X-rays, CT scans, Histology slides) is outpacing the number of human radiologists. AI is becoming the standard \"first pass\" triage tool.\n",
        "\n",
        "Using Gemini's vision capabilities within Colab, a student can prototype diagnostic tools.\n",
        "* **Scenario:** You are studying hematology.\n",
        "* **Workflow:** You upload an image of a blood smear. You ask the Nano Banana Pro agent to \"Identify and count the basophils and eosinophils.\"\n",
        "* **Result:** The model returns a structured count and highlights the cells.\n",
        "* **Professional Skill:** Understanding how to validate these AI outputs—knowing where the AI fails and where it succeeds—is a critical skill for the doctors of tomorrow. You will likely be working alongside these \"AI Residents\" in your future practice.\n",
        "\n",
        "\n",
        "### B. Structural Biology and Drug Discovery\n",
        "Understanding how proteins fold and interact is the \"Holy Grail\" of pharmacology. Tools like AlphaFold have revolutionized this field.\n",
        "\n",
        "* **Scenario:** You want to understand why a specific mutation causes Cystic Fibrosis.\n",
        "* **Workflow:** You use your Colab environment to retrieve the protein structure prediction for the CFTR protein. You then ask the AI to \"Highlight the Phenylalanine-508 residue and visualize the hydrophobic pocket.\"\n",
        "* **Result:** You get a 3D interactive visualization.\n",
        "* **Professional Skill:** This moves you from memorizing pathway diagrams to visualizing molecular mechanics. In the pharmaceutical industry, this \"in silico\" (computer simulation) testing saves billions of dollars before \"in vitro\" (test tube) experiments begin.\n",
        "\n",
        "\n",
        "### C. Genomic Data Wrangling\n",
        "Genomics produces \"Big Data\"—terabytes of A, C, T, and Gs. \"Wrangling\" this data (cleaning it, formatting it, finding patterns) is 80% of a bioinformatician's job.\n",
        "\n",
        "* **Scenario:** You have a messy text file of patient gene sequences mixed with clinical notes.\n",
        "* **Workflow:** You paste the mess into Colab and tell the agent: *\"Extract the gene sequences, convert them to FASTA format, and create a JSON file linking each patient's age to their specific mutation.\"*\n",
        "* **Result:** What used to take 4 hours of writing Regular Expressions (RegEx) now takes 30 seconds.\n",
        "* **Professional Skill:** Speed. In a clinical research setting, the ability to clean data instantly makes you the most valuable person on the team.\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/DNA_image.png)\n",
        "\n",
        "\n",
        "#### **4. The \"Nano Banana Pro\" for Personal Productivity**\n",
        "\n",
        "You don't need to be curing cancer to use these tools. The Nano Banana Pro is also a massive upgrade to your personal study habits and daily life as a student.\n",
        "\n",
        "### A. The Socratic Study Buddy\n",
        "Biology requires memorizing massive amounts of information. Static flashcards are boring and lack context.\n",
        "\n",
        "* **Use Case:** You are studying for a Biochemistry final on the Krebs Cycle.\n",
        "* **Action:** You initialize a \"Tutor Agent\" in Colab with a system instruction: *\"You represent the enzyme Citrate Synthase. Quiz me on my inputs and outputs. If I get it wrong, give me a hint related to molecular energy.\"*\n",
        "* **Benefit:** This creates an active recall loop that is personalized to your weak points. It turns studying into a conversation rather than a chore.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/TCA.jpg)\n",
        "\n",
        "\n",
        "### B. The \"De-Jargonizer\"\n",
        "Academic papers are notoriously difficult to read. They are dense, filled with acronyms, and written in passive voice.\n",
        "\n",
        "* **Use Case:** You need to read five papers on \"Epigenetic Methylation\" for a seminar tomorrow.\n",
        "* **Action:** You paste the abstract and results sections into your Colab notebook. You prompt the AI: *\"Explain these findings using an analogy involving a library and locked books. Summarize the statistical significance of their results.\"*\n",
        "* **Benefit:** The AI translates \"high-science\" into concepts you already understand, allowing you to grasp the *logic* of the paper before you dive into the technical details.\n",
        "\n",
        "### C. Coding for \"Life Hacks\"\n",
        "Because the Nano Banana Pro can write Python code for you, you can automate mundane tasks even if you aren't a programmer.\n",
        "\n",
        "* **Use Case:** You have a spreadsheet of your lab hours or grades that you need to visualize to track your progress.\n",
        "* **Action:** You upload the CSV file and ask: *\"Plot my cumulative GPA over the last 4 semesters and project what grades I need to get a 3.8 average.\"*\n",
        "* **Benefit:** You gain data literacy and actionable insights into your own life without needing to master Excel macros.\n",
        "\n",
        "\n",
        "#### **5. Strengths of the Ecosystem: Why This Matters**\n",
        "\n",
        "Why choose this specific \"Nano Banana Pro\" stack (Colab + Gemini) over just using ChatGPT or a standard textbook?\n",
        "\n",
        "### Strength 1: Reproducibility and Documentation\n",
        "In science, if you didn't write it down, it didn't happen. When you work in a Colab notebook, you are creating a **living document**. You have your code, your AI prompts, the AI's output, and your own notes all in one file. This is the gold standard for scientific reproducibility. You can hand your notebook to a peer, and they can run it and get the exact same results.\n",
        "\n",
        "### Strength 2: Multimodality is Biological Reality\n",
        "Biology is not text. Biology is visual (histology, anatomy), spatial (protein folding), and numerical (population statistics). A text-only AI is blind to half of biology. The Gemini model's ability to \"see\" images and graphs makes it uniquely suited for the life sciences.\n",
        "\n",
        "\n",
        "\n",
        "### Strength 3: The \"Sandbox\" Safety\n",
        "In a wet lab, if you make a mistake, you might waste expensive reagents or create a hazardous spill. In the Nano Banana Pro ecosystem, the cost of failure is zero. You can crash the code, delete the data, or hallucinate a simulation—and fix it with a simple \"Restart Runtime.\" This psychological safety encourages play, and play is the highest form of research.\n",
        "\n",
        "\n",
        "#### **6. Ethical Considerations for the Premed Student**\n",
        "\n",
        "With great power comes great responsibility. Using AI in biology requires a new set of ethics.\n",
        "\n",
        "1.  **Patient Privacy (HIPAA):** Never, ever put real patient names or identifying data into a public cloud model. In our exercises, we use synthetic (fake) data. In your future career, you must be strictly aware of data privacy laws.\n",
        "2.  **Hallucination Check:** LLMs can \"hallucinate\"—they can confidently state biological facts that are wrong. You must always act as the **\"Human in the Loop.\"** Never trust the AI blindly for clinical decisions. Use it to generate ideas, but verify them with primary literature.\n",
        "3.  **Intellectual Integrity:** Using AI to write your lab report essay is plagiarism. Using AI to analyze your data and generating code to visualize it is **computational science**. Learn the difference.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. The Future: Agentic Biology**\n",
        "\n",
        "Where is the Nano Banana Pro going next? We are moving from \"Chatbots\" to \"Agents.\"\n",
        "\n",
        "Currently, you ask a question, and the AI answers. In the near future, you will give a **goal**, and the AI will execute a series of steps to achieve it.\n",
        "* **Current:** \"Write code to plot this data.\"\n",
        "* **Future:** \"Analyze this dataset, find the outliers, search PubMed for potential reasons why these outliers exist, and draft a preliminary hypothesis.\"\n",
        "\n",
        "We are entering an era of **Agentic Biology**, where software agents will run autonomous experiments in the cloud (or even drive robotic wet labs) while you direct the overall strategy. Learning to prompt and guide these agents today is the best preparation for the scientific leadership roles of tomorrow.\n",
        "\n",
        "\n",
        "### **Conclusion: Peel the Banana**\n",
        "\n",
        "The \"Nano Banana Pro\" is not a product you buy; it is a mindset you adopt. It is the realization that the barrier between \"thinking about an experiment\" and \"running an experiment\" has dissolved.\n",
        "\n",
        "For the BIO 1173 student, this ecosystem offers a superpower. It allows you to punch above your weight class. You can analyze data like a PhD student, visualize concepts like a professional illustrator, and build tools like a software engineer—all while sitting in your dorm room.\n",
        "\n",
        "The history of biology is the history of tools that extend human perception. The microscope extended our sight. The Nano Banana Pro extends our cognition. The tool is ready, powerful, and sitting in your browser. All you have to do is peel it open and start building.s"
      ],
      "metadata": {
        "id": "jQgBl33E6vzh"
      }
    }
  ]
}