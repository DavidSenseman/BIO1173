{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Introduction to Large Language Models (LLMs)\n",
        "* Part 4.2: Chatbots\n",
        "* Part 4.3: Image Generation with StableDiffusion\n",
        "* **Part 4.4: Image Generation with DALL-E**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded.\n"
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DALL·E Generative Models**\n",
        "\n",
        "Recent advances in **generative AI**, particularly diffusion models like **OpenAI's DALL·E**, have begun transforming **computational biology**—especially in the field of **protein design**.\n",
        "\n",
        "DALL·E is a **text-to-image diffusion model** developed by OpenAI that generates images from natural language prompts. Its underlying architecture has inspired similar models in biology that generate **molecular and protein structures** instead of pictures.\n",
        "\n",
        "#### **Applications in Biology**\n",
        "\n",
        "Biotech labs are now adapting DALL·E-like models to **design novel proteins** with specific shapes, sizes, and functions. These proteins:\n",
        "- **Do not exist in nature**\n",
        "- Can be tailored for **therapeutic purposes**\n",
        "- May accelerate **drug discovery** and **biomedical innovation**\n",
        "\n",
        "#### **Key Projects**\n",
        "- **Chroma** by Generate Biomedicines: Dubbed the *“DALL·E 2 of biology”*, it uses diffusion models to generate symmetrical and functional protein structures.\n",
        "- **RoseTTAFold Diffusion** by the University of Washington: Developed by David Baker’s lab, this model designs proteins from scratch with high precision.\n",
        "\n",
        "#### **Impact**\n",
        "These tools allow scientists to:\n",
        "- Explore a **vast design space** beyond natural proteins\n",
        "- Create **custom proteins** for targeted medical applications\n",
        "- Reduce the time and cost of **drug development**\n",
        "\n",
        "> “We can discover in minutes what took evolution millions of years.” — Gevorg Grigoryan, CTO of Generate Biomedicines\n",
        "\n",
        "#### **References**\n",
        "- [MIT Technology Review](https://www.technologyreview.com/2022/12/01/1064023/biotech-labs-are-using-ai-inspired-by-dall-e-to-invent-new-drugs/)[1](https://www.technologyreview.com/2022/12/01/1064023/biotech-labs-are-using-ai-inspired-by-dall-e-to-invent-new-drugs/)\n",
        "- [Interesting Engineering](https://interestingengineering.com/science/ai-inspires-labs-new-medicine)[2](https://interestingengineering.com/science/ai-inspires-labs-new-medicine)\n"
      ],
      "metadata": {
        "id": "TCSQnN-szCNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optional YorTube Video**\n",
        "\n",
        "If you are interested in knowing more about OpenAI `DALL-E` run the next cell to watch this YouTube video."
      ],
      "metadata": {
        "id": "d3EASu-A3-Xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Extracted video ID from the new URL\n",
        "video_id = \"qTgPSKKjfVg\"\n",
        "\n",
        "# Construct the proper embed URL\n",
        "embed_url = f\"https://www.youtube.com/embed/{video_id}\"\n",
        "\n",
        "# Display the embedded video\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\" src=\"{embed_url}\"\n",
        "frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "allowfullscreen></iframe>\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "NEzVc9Ja5sw3",
        "outputId": "2c7efba3-8be4-4f20-c747-bddd43037018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/qTgPSKKjfVg\"\n",
              "frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
              "allowfullscreen></iframe>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Your OPENAI_API_KEY\n",
        "\n",
        "In order to run the code in this lesson you will need to have your secret `OEPNAI_API_KEY` installed in your **Secrets** on this Colab notebook. Detailed steps for purchasing your `OPENAI_API_KEY` and installing it in your Colab notebook Secrets was provide in `Class_04_1`.\n",
        "\n",
        "Run the code in the next cell to see if your `OPENAI_API_KEY` is installed correctly. You make have to Grant Access for your notebook to use your API key."
      ],
      "metadata": {
        "id": "0UHav6bt1FoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for OpenAI API Key\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Create local API variable\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Retrieve the OpenAI API key and store it in a variable\n",
        "#OPENAI_KEY = userdata.get('OPENAI_KEY')\n",
        "\n",
        "# Ensure that the API key is correctly set\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\")\n",
        "else:\n",
        "  print(f\"Your secret OPENAI_API_KEY =\", OPENAI_KEY)"
      ],
      "metadata": {
        "id": "_gXbyT2T2Pt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you OPENAI_API_KEY is correctly installed, you should see\n",
        "~~~type\n",
        "Your secret OPENAI_API_KEY = sk-proj-DHILXTfLOz.....\n",
        "~~~\n",
        "\n",
        "However, if the output says\n",
        "```type\n",
        "\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\n",
        "```\n",
        "You must correct this error before you can continue with this lesson. Again, details instructions were given in Class_04_1."
      ],
      "metadata": {
        "id": "Jswm7Qbk2W1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `LangChain` packages\n",
        "\n",
        "Run the code in the following cell to install the `langchain-openai` and related packages."
      ],
      "metadata": {
        "id": "jbDi38qW2gaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install langchain-openai packages\n",
        "\n",
        "!pip install -q langchain openai langchain_openai > /dev/null"
      ],
      "metadata": {
        "id": "XZ7PS51U2o6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any output."
      ],
      "metadata": {
        "id": "O3O1FivA2zwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generating Images with DALL·E**\n",
        "\n",
        "**DALL·E** is an advanced generative AI model developed by `OpenAI` that creates images from textual descriptions. It is designed to interpret and visually represent detailed prompts, producing diverse and creative imagery ranging from realistic to abstract. `DALL·E` is capable of synthesizing a wide variety of artistic styles, objects, and environments based on the user's input.\n",
        "\n",
        "API access to DALL·E is available through `OpenAI`, allowing developers to integrate image generation capabilities into their applications. This API allows users to provide text prompts, specifying what kind of image they want, and `DALL·E` responds by generating corresponding visuals. The API is designed for scalability and ease of integration, making it a powerful tool for developers seeking to add creative image generation features to web applications, mobile apps, or other digital platforms.\n",
        "\n",
        "By leveraging `DALL·E` through the `OpenAI API`, users gain access to a state-of-the-art tool capable of turning imagination into highly detailed visuals, applicable in industries ranging from design and marketing to entertainment and education.\n",
        "\n",
        "### **DALL·E 2 vs DALL·E 3**\n",
        "\n",
        "**DALL·E 2** and **DALL·E 3** are successive versions of OpenAI's powerful image generation models, each building upon the strengths and capabilities of its predecessor.\n",
        "\n",
        "`DALL·E 2`, released in 2022, represented a significant leap from the original `DALL·E` model. It was capable of creating high-resolution, highly detailed images from textual prompts, with a particular focus on creative abstraction and imaginative combinations. Some key features of `DALL·E 2` include:\n",
        "\n",
        "* **Realism and Artistic Expression**: `DALL·E 2` excelled at generating both photorealistic images and artistic interpretations based on the text input. It had a remarkable ability to blend various elements (e.g., surreal, abstract, or realistic objects) into coherent compositions.\n",
        "* **Inpainting**: Users could modify specific parts of an image by describing the desired change, allowing for more interactive control over image generation.\n",
        "* **Style Variations**: DALL·E 2 could mimic different art styles, such as painting or photography, providing flexibility for creative projects.\n",
        "\n",
        "While `DALL·E 2` was highly versatile and generated impressive imagery, its prompt comprehension and adherence to nuanced text could sometimes fall short. It struggled with complex or ambiguous instructions, occasionally misinterpreting subtle details in the user input.\n",
        "\n",
        "**`DALL·E 3`**, released in 2023, improved upon the limitations of `DALL·E 2` and introduced new capabilities that made it more accessible and precise:\n",
        "\n",
        "* **Enhanced Text Comprehension**: One of the standout features of DALL·E 3 is its improved ability to understand and accurately follow more complex or specific instructions. It does a much better job of capturing nuances, making it easier to generate images that align with the user’s expectations, even with detailed or intricate prompts.\n",
        "* **Deeper Integration with ChatGPT**: DALL·E 3 was deeply integrated with ChatGPT, enabling users to interact with the model conversationally, refining their prompts and requests iteratively. This integration simplifies the creative process, allowing for more intuitive prompt adjustments.\n",
        "* **Reduction of Known Limitations**: DALL·E 3 improved its handling of difficult tasks such as rendering human hands and faces, areas where previous models often produced distorted or unrealistic results.\n",
        "Inpainting Enhancements: DALL·E 3 continues the development of inpainting, providing even greater control over modifications to specific areas of generated images, allowing users to refine portions of images without starting over from scratch.\n",
        "\n",
        "The Images API provides three methods for interacting with images:\n",
        "\n",
        "* Creating images from scratch based on a text prompt (DALL·E 3 and DALL·E 2)\n",
        "* Creating edited versions of images by having the model replace some areas of a pre-existing image, based on a new text prompt (DALL·E 2 only)\n",
        "* Creating variations of an existing image (DALL·E 2 only)"
      ],
      "metadata": {
        "id": "sHQ0dd8027wP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Generate Single Image with DALL·E 2\n",
        "\n",
        "The code in the cell below generates 3 differnt images of a \"a white siamese cat\"."
      ],
      "metadata": {
        "id": "7Zd5QduZ4S0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Generate single image\n",
        "\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set prompt\n",
        "PROMPT=\"a white siamese cat\"\n",
        "\n",
        "# Retrieve the OpenAI API key\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure that the API key is correctly set\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\")\n",
        "\n",
        "# Initialize the OpenAI client with the API key\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "# Generate one image\n",
        "response = client.images.generate(\n",
        "    model=\"dall-e-2\",\n",
        "    prompt=PROMPT,\n",
        "    size=\"1024x1024\",\n",
        "    n=1,\n",
        ")\n",
        "\n",
        "# Get the image URL\n",
        "image_url = response.data[0].url\n",
        "\n",
        "# Fetch the image from the URL\n",
        "response2 = requests.get(image_url)\n",
        "img = Image.open(BytesIO(response2.content))\n",
        "\n",
        "# Save the image as a JPG to disk\n",
        "img.save(\"cat_image.jpg\", \"JPEG\")\n",
        "\n",
        "# Display the image using matplotlib\n",
        "plt.imshow(img)\n",
        "plt.axis('off')  # Hide the axes for a cleaner view\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aP9CGt1F5Cv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image06A.png)\n"
      ],
      "metadata": {
        "id": "oTGf-Red7LDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Generate Single Image with DALL·E 2**\n",
        "\n",
        "In the cell below write the code to generate a single image of a \"a roadrunner eating a lizard\"."
      ],
      "metadata": {
        "id": "HJgX3nyj7zCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KKBWP9TL7zCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image07A.png)\n"
      ],
      "metadata": {
        "id": "s3GmuwHe7zCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generating Multiple Images**\n",
        "\n",
        "DALL·E 2 allows you to generate multiple images, which we will see in this section.\n",
        "\n",
        "You are allowed to select the resolution, by the `OpenAI` library; however, not all of these resolutions will be available for every model:\n",
        "\n",
        "* 1024x512\n",
        "* 256x256\n",
        "* 512x512\n",
        "* 1024x1024\n",
        "* 1024x1792\n",
        "* 1792x1024\n",
        "\n",
        "`DALL·E 3` allows an additional quality of \"hd\"; whereas `DALL·E 2` only supports \"df\".\n",
        "\n",
        "* standard\n",
        "* hd\n"
      ],
      "metadata": {
        "id": "BK--evkIckfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Generate Multiple Images with DALL·E 2\n",
        "\n",
        "The code in the cell below generates 3 differnt images of a \"a white siamese cat\"."
      ],
      "metadata": {
        "id": "_tQW2l57_VS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Generate multiple images\n",
        "\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set prompt\n",
        "PROMPT=\"a white siamese cat\"\n",
        "\n",
        "# Retrieve the OpenAI API key\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure that the API key is correctly set\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\")\n",
        "\n",
        "# Initialize the OpenAI client with the API key\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "# Generate 3 images\n",
        "response = client.images.generate(\n",
        "    model=\"dall-e-2\",\n",
        "    prompt=PROMPT,\n",
        "    size=\"1024x1024\",\n",
        "    n=3,\n",
        ")\n",
        "\n",
        "# Loop through the generated images, display them with matplotlib, and save as JPG\n",
        "for i, data in enumerate(response.data):\n",
        "    image_url = data.url\n",
        "    img_response = requests.get(image_url)\n",
        "    img = Image.open(BytesIO(img_response.content))\n",
        "    img.save(f\"cat_image_{i+1}.jpg\", \"JPEG\")\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Image {i+1}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "KdDkInBd_VS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image01A.png)\n"
      ],
      "metadata": {
        "id": "a5WDQpSL_VS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Generate Multiple Images with DALL·E 2**\n",
        "\n",
        "In the cell below write the code to generate 3 images of a \"a roadrunner eating a lizard\"."
      ],
      "metadata": {
        "id": "9RGbrpkf_VS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n"
      ],
      "metadata": {
        "id": "BKQqlHdX_VS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image05A.png)\n"
      ],
      "metadata": {
        "id": "y6o4OTh0_VTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DALL·E Existing Images**\n",
        "\n",
        "**`DALL·E`** has the ability to extend images by generating additional content based on the original image and a provided prompt. This feature, often called \"outpainting,\" allows users to expand the boundaries of an image, adding context or completing a scene beyond the original frame. The original image serves as the foundation, while the mask image is used to define the areas where new content will be generated. The mask, typically a transparent or blocked-out region, signals to DALLE which parts of the canvas need to be filled in. By specifying a new prompt, the user guides DALLE on how to interpret the masked area, dictating the nature of the extension in alignment with the style, context, or subject matter of the original image.\n",
        "\n",
        "In practice, outpainting leverages the original image and prompt to maintain coherence between the existing and generated content. For instance, if the original image is a landscape, the prompt can guide DALLE to extend that landscape with consistent colors, textures, and thematic elements. The system seamlessly blends the generated pixels into the original, creating an expanded scene that appears natural. The prompt plays a crucial role in steering the generation process, describing the desired extension in terms of objects, settings, or creative directions, while the mask ensures that the new content integrates smoothly into the intended area without overwriting the existing details."
      ],
      "metadata": {
        "id": "7Nu1xScG-sOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Modify Existing Image\n",
        "\n",
        "The code in the cell below uses this original image of a sea turtle\n",
        "\n",
        "Original image:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image13A.png)\n",
        "\n",
        "and adds an element to the image based on a text prompt.The image location for the placement will occur depends upon a \"mask\" image.\n",
        "\n",
        "A **mask image** is a type of image used in computer graphics and image processing to selectively control which parts of another image are visible, edited, or affected by operations. It acts like a stencil or filter. In this picture of the mask image the white area shows the image location that will be \"edited\" by `OpenAI`.\n",
        "    \n",
        "Mask image:  \n",
        "    \n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image31A.png)\n",
        "\n",
        "In this example an extemely simple text prompt was used--just the word \"cat\"."
      ],
      "metadata": {
        "id": "dG8mlchId-fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Modify Existing Image\n",
        "\n",
        "import requests\n",
        "from PIL import Image, ImageEnhance\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from openai import OpenAI\n",
        "from google.colab import files\n",
        "\n",
        "# Define prompt\n",
        "PROMPT = \"cat\"\n",
        "print(f\"PROMPT = {PROMPT}\")\n",
        "\n",
        "# Retrieve the OpenAI API key\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set.\")\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "# Function to fetch image from URL\n",
        "def fetch_image(url, save_path=None):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to download image from {url}: {response.status_code}\")\n",
        "        return None\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    if save_path:\n",
        "        img.save(save_path, format=\"PNG\")\n",
        "    return img\n",
        "\n",
        "# URLs for image and mask\n",
        "image_url = \"https://biologicslab.co/BIO1173/images/class_04/class_04_4_image_01.png\"\n",
        "mask_url =  \"https://biologicslab.co/BIO1173/images/class_04/class_04_4_mask_01.png\"\n",
        "\n",
        "# Save images without converting to RGBA\n",
        "image_path = \"image.png\"\n",
        "mask_path = \"mask.png\"\n",
        "fetch_image(image_url, image_path)\n",
        "fetch_image(mask_url, mask_path)\n",
        "\n",
        "# Call OpenAI API\n",
        "with open(image_path, \"rb\") as image_file, open(mask_path, \"rb\") as mask_file:\n",
        "    response = client.images.edit(\n",
        "        image=image_file,\n",
        "        mask=mask_file,\n",
        "        prompt=PROMPT,\n",
        "        n=2,\n",
        "        size=\"1024x1024\"\n",
        "    )\n",
        "\n",
        "# Display result\n",
        "edited_image_url = response.data[0].url\n",
        "edited_image = fetch_image(edited_image_url)\n",
        "\n",
        "try:\n",
        "    plt.imshow(edited_image)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Edited Image\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"Error displaying the edited image: {e}\")\n"
      ],
      "metadata": {
        "id": "HECDndYV1svE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image32A.png)\n",
        "\n",
        "The `Edited Image` (`output`) can be quite variable. If rerun the above cell you we see quite different outputs. I had to run the cell above several times to generate this particular image.\n",
        "\n",
        "You should notice that the cat addeb by DALL-E nicely blends into the background and is completely surrounded by water. In other words, DALL-E is doing much more than a simple \"copy-and-paste\"."
      ],
      "metadata": {
        "id": "l5OlY4p13Swf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Modify Existing Image**\n",
        "\n",
        "The output (`Edited Image`) in Example 3 was quite variable. We also used a very simple prompt `cat`.\n",
        "\n",
        "In **Exericse 3** you will examine the effect(s) of using a more detailed prompt.\n",
        "\n",
        "In the cell below use the code and the same images of the sea turtle and the same mask that were used in Example 5 but only change the prompt.\n",
        "\n",
        "Use instead this more detailed prompt:\n",
        "\n",
        "```type\n",
        "# Define prompt\n",
        "PROMPT = (\n",
        "    \"A domestic short-haired cat lying comfortably on a vibrant coral reef \"\n",
        "    \"in an underwater scene, alongside a sea turtle swimming above. \"\n",
        "    \"The cat appears relaxed and is gazing toward the viewer, blending \"\n",
        "    \"naturally into the marine environment despite the surreal setting.\"\n",
        ")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "zYQ2KcpWSQL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n"
      ],
      "metadata": {
        "id": "qEFletpeSQL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image38A.png)\n",
        "\n",
        "Even with the more detailed prompt, the `Edited Image` (`output`) can still be variable albeit much less variable that the output with the extremely simple prompt \"cat\". Rerun the above cell a few times to see the range of different outputs.\n"
      ],
      "metadata": {
        "id": "lNRHhCB2SQL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AeiUDGCgYpwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Modify Existing Image\n",
        "\n",
        "The code in the cell below uses the same image of a sea turtle\n",
        "\n",
        "Original image:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image13A.png)\n",
        "\n",
        "but changes the location of **editible region** in the mask image (i.e. the white area).\n",
        "    \n",
        "Mask image:  \n",
        "    \n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image34A.png)\n",
        "\n",
        "Since the **editible area** (i.e. white oval) in the mask covers the sea turtle, we would expect the turtle will be replaced by the object defined in the text `prompt`.\n",
        "\n",
        "In Example 4 we will be using the following prompt that is moderately detailed:\n",
        "\n",
        "```text\n",
        "PROMPT = (\n",
        "    \" A cute young baby girl swimming towards the camera.\"\n",
        "    \" She is extemely close so her face is very large in the scene\"\n",
        "    \" She is holding her breath and is not wearing any goggles or face mask\"\n",
        ")  \n",
        "```\n",
        "\n",
        "Run the code in the next cell to see what happens when we give DALL-E the new prompt."
      ],
      "metadata": {
        "id": "KZGoJKQrYqfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Modify Existing Image\n",
        "\n",
        "import requests\n",
        "from PIL import Image, ImageEnhance\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from openai import OpenAI\n",
        "from google.colab import files\n",
        "\n",
        "# Define prompt\n",
        "# Define prompt\n",
        "PROMPT = (\n",
        "    \" A cute young baby girl swimming towards the camera.\"\n",
        "    \" She is extemely close so her face is very large in the scene\"\n",
        "    \" She is holding her breath and is not wearing any goggles or face mask\"\n",
        ")\n",
        "print(f\"PROMPT = {PROMPT}\")\n",
        "\n",
        "# Retrieve the OpenAI API key\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set.\")\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "# Function to fetch image from URL\n",
        "def fetch_image(url, save_path=None):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to download image from {url}: {response.status_code}\")\n",
        "        return None\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    if save_path:\n",
        "        img.save(save_path, format=\"PNG\")\n",
        "    return img\n",
        "\n",
        "# URLs for image and mask\n",
        "image_url = \"https://biologicslab.co/BIO1173/images/class_04/class_04_4_image_01.png\"\n",
        "mask_url =  \"https://biologicslab.co/BIO1173/images/class_04/class_04_4_mask_02.png\"\n",
        "\n",
        "# Save images without converting to RGBA\n",
        "image_path = \"image.png\"\n",
        "mask_path = \"mask.png\"\n",
        "fetch_image(image_url, image_path)\n",
        "fetch_image(mask_url, mask_path)\n",
        "\n",
        "# Call OpenAI API\n",
        "with open(image_path, \"rb\") as image_file, open(mask_path, \"rb\") as mask_file:\n",
        "    response = client.images.edit(\n",
        "        image=image_file,\n",
        "        mask=mask_file,\n",
        "        prompt=PROMPT,\n",
        "        n=2,\n",
        "        size=\"1024x1024\"\n",
        "    )\n",
        "\n",
        "# Display result\n",
        "edited_image_url = response.data[0].url\n",
        "edited_image = fetch_image(edited_image_url)\n",
        "\n",
        "try:\n",
        "    plt.imshow(edited_image)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Edited Image\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"Error displaying the edited image: {e}\")\n"
      ],
      "metadata": {
        "id": "0nwRiA7EYqfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image37A.png)\n",
        "\n",
        "As before, the `Edited Image` (`output`) can be quite variable. If you rerun the above cell several times you will get quite different outputs."
      ],
      "metadata": {
        "id": "LTooeodUYqfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Modify Existing Image**\n",
        "\n",
        "For **Exercise 4** reuse the code from Example 4. However, change the `prompt` to insert an **inanimate object** into the scene. You are free to choose any inanimate object that you would like. Here is a chance to show your creativity!"
      ],
      "metadata": {
        "id": "lxFQkc8yYqfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n"
      ],
      "metadata": {
        "id": "75Kr5Q8mYqfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is output obtained using the following `prompt`:\n",
        "\n",
        "```text\n",
        "# Define prompt\n",
        "PROMPT = (\n",
        "    \" A British classic MGA sports car.\"\n",
        "    \" The car's color is British Racing Green\"\n",
        ")\n",
        "```\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image39A.png)\n",
        "\n",
        "Your output will depend upon the `prompt` you created.\n"
      ],
      "metadata": {
        "id": "1TH2gZv4YqfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating Variations**\n",
        "\n",
        "**DALL-E** offers the ability to create variations of images, a feature that allows users to generate alternate versions of an existing image with slight modifications while maintaining the overall theme or structure. This functionality is available via the API in DALLE 2, where developers can provide an image input and request multiple variations based on it. The API enables integration into applications that require creative content generation or visual brainstorming. However, this image variation feature is not yet supported in DALLE 3, meaning users must still rely on DALLE 2 for this specific capability."
      ],
      "metadata": {
        "id": "yMXxIHygI6bA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Functions\n",
        "\n",
        "The code below creates 3 functions we will need to demonstrate `DALL-E's` create variation capability:\n",
        "\n",
        "1. `display_image(image, title)`\n",
        "2. `load_image_from_url(url)`\n",
        "3. `generate_variations( )`"
      ],
      "metadata": {
        "id": "NJlarVcM1qts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create functions\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_image(image, title=None):\n",
        "    \"\"\"Display an image using matplotlib.\"\"\"\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def load_image_from_url(url):\n",
        "    \"\"\"Download an image from a URL and return a file-like object.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        img = Image.open(BytesIO(response.content)).convert(\"RGBA\")\n",
        "        img_byte_array = BytesIO()\n",
        "        img.save(img_byte_array, format='PNG')\n",
        "        img_byte_array.seek(0)\n",
        "        return img_byte_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_variations(client, image_file, n=2, size=\"1024x1024\", save_images=True):\n",
        "    \"\"\"Generate and display image variations using DALL·E 2.\"\"\"\n",
        "    try:\n",
        "        response = client.images.create_variation(\n",
        "            image=image_file,\n",
        "            n=n,\n",
        "            size=size\n",
        "        )\n",
        "        for i, data in enumerate(response.data):\n",
        "            variation_url = data.url\n",
        "            variation_response = requests.get(variation_url)\n",
        "            variation_image = Image.open(BytesIO(variation_response.content))\n",
        "            display_image(variation_image, title=f\"Variation {i+1}\")\n",
        "            if save_images:\n",
        "                variation_image.save(f\"variation_image_{i+1}.png\", \"PNG\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating variations: {e}\")\n"
      ],
      "metadata": {
        "id": "5v8L9-hj1sD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Create Variations\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image42A_little.png)\n",
        "\n",
        "In Example 5 we are going to demonstrate `DALL-E 2's` ability to generate \"variations\" of an image using this picture of `Grogu`, _aka_ \"Baby Yoda\"."
      ],
      "metadata": {
        "id": "kGvwNhffKAuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Create variations\n",
        "\n",
        "from openai import OpenAI\n",
        "from image_utils import load_image_from_url, generate_variations\n",
        "\n",
        "# Set number of variations\n",
        "var_num = 2\n",
        "\n",
        "# Retrieve the OpenAI API key\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set.\")\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "# Main execution\n",
        "image_url = \"https://biologicslab.co/BIO1173/images/class_04/class_04_4_image42A.png\"\n",
        "image_file = load_image_from_url(image_url)\n",
        "\n",
        "if image_file:\n",
        "    generate_variations(client, image_file, n=var_num, size=\"1024x1024\", save_images=True)\n"
      ],
      "metadata": {
        "id": "C59l3Jul23Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image43A.png)"
      ],
      "metadata": {
        "id": "EzOXuhv6VD2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Create Variations**\n",
        "\n",
        "In the cell below write the code to create variations on a cartoon image of Grogu (\"Baby Yoda\") surrounded by Ewoks. You can reuse the code in Example 5 but you will need to change the URL for the image.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image44A_little.png)\n",
        "\n",
        "Here is the URL you need to use to use the correct initial image:\n",
        "\n",
        "```text\n",
        "# Main execution\n",
        "image_url = \"https://biologicslab.co/BIO1173/images/class_04/class_04_4_image44A.png\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "t1fzOao4XevS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n"
      ],
      "metadata": {
        "id": "nkJaoU6O3332"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image45A.png)"
      ],
      "metadata": {
        "id": "TNsbdHvSXevU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Image to Art**\n",
        "\n",
        "\n",
        "Using this multimodal approach, we can also generate prompts that describe an image in a way that allows us to render it as a cartoon. By inputting an image alongside a text description, the model can analyze its visual components and then generate a creative, detailed prompt specifically crafted to produce a cartoon-like rendering. Once we have this prompt, we can pass it to DALLE, OpenAI's image generation model, to transform the visual data into a stylized cartoon version. This workflow allows for automated, creative transformations of images, combining the interpretive power of a multimodal model with the generative capabilities of DALLE, enabling users to create custom visual outputs from real-world images."
      ],
      "metadata": {
        "id": "SvuazqVwaZYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6A: Generate Prompt from Image\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image42A_little.png)\n",
        "\n",
        "The code in the cell below uses `OpenAI` to analyze this image of `Grogu` and return a text description of the image's content. We will use this text description in Example 6B to regenerate the iamge."
      ],
      "metadata": {
        "id": "QlLIz3D5b56f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6A: Generate Prompt from Image\n",
        "\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "import base64\n",
        "import httpx\n",
        "import textwrap\n",
        "\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "image_url = \"https://biologicslab.co/BIO1173/images/class_04/class_04_4_image42A.png\"\n",
        "prompt = \"Describe this image.\"\n",
        "\n",
        "# Retrieve the OpenAI API key\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set.\")\n",
        "\n",
        "# Initialize the GPT model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_KEY)\n",
        "\n",
        "# Fetch image data and encode it in base64\n",
        "image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
        "\n",
        "# Create a message with both text and the image\n",
        "message = HumanMessage(\n",
        "    content=[\n",
        "        {\"type\": \"text\", \"text\": prompt},\n",
        "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}},\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Get response with a modified prompt from GPT\n",
        "response = model.invoke([message])\n",
        "\n",
        "# Wrap the text output to avoid scrolling off the screen in Colab\n",
        "wrapped_output = textwrap.fill(response.content, width=80)\n",
        "print(wrapped_output)\n"
      ],
      "metadata": {
        "id": "nOUHThDOb6Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image46A.png)"
      ],
      "metadata": {
        "id": "6ok97vAx_hll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6B: Generate Image from Text\n",
        "\n",
        "Now that we have a textual description of our image, we can use it as a prompt to generate 3 new images using the same code demonstrated in Example 2 above.\n",
        "\n",
        "The only _tricky_ part of the process is making sure to format the prompt correctly. If you try to simply `copy-and-paste` the output text as is you will most likely generate an error. This is because Python will interprets the carriage-returns at the end of each line as a new command, not part of the text.\n",
        "\n",
        "The solution is to rewrite the text output shown in the output above as follows:\n",
        "\n",
        "```text\n",
        "# Set prompt\n",
        "PROMPT = (\n",
        "\"The image shows a small, humanoid figure with green skin and large, expressive\"\n",
        "\"eyes. It has prominent, oversized ears and is wearing a cozy, beige cloak with a\"\n",
        "\"hood. The figure appears to be seated on a mossy surface, surrounded by natural\"\n",
        "\"greenery. The overall look gives off a sense of innocence and curiosity.\"\n",
        ")\n",
        "```\n",
        "\n",
        "This prompt was created simply by `copying-and-pasting` the text between two parantheses and adding double quotation marks (`#`) at the start and the end of each line of text. The parentheses `( )` that enclose the prompt tells Python to consider all of the separate lines as a single string."
      ],
      "metadata": {
        "id": "RpOoJ84g4XTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6B: Generate image from text\n",
        "\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set prompt\n",
        "PROMPT = (\n",
        "\"The image shows a small, humanoid figure with green skin and large, expressive\"\n",
        "\"eyes. It has prominent, oversized ears and is wearing a cozy, beige cloak with a\"\n",
        "\"hood. The figure appears to be seated on a mossy surface, surrounded by natural\"\n",
        "\"greenery. The overall look gives off a sense of innocence and curiosity.\"\n",
        ")\n",
        "\n",
        "# Retrieve the OpenAI API key\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure that the API key is correctly set\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\")\n",
        "\n",
        "# Initialize the OpenAI client with the API key\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "# Generate 3 images\n",
        "response = client.images.generate(\n",
        "    model=\"dall-e-2\",\n",
        "    prompt=PROMPT,\n",
        "    size=\"1024x1024\",\n",
        "    n=3,\n",
        ")\n",
        "\n",
        "# Loop through the generated images, display them with matplotlib, and save as JPG\n",
        "for i, data in enumerate(response.data):\n",
        "    image_url = data.url\n",
        "    img_response = requests.get(image_url)\n",
        "    img = Image.open(BytesIO(img_response.content))\n",
        "    img.save(f\"cat_image_{i+1}.jpg\", \"JPEG\")\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Image {i+1}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ZKhTOcAe4zuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image50A.png)"
      ],
      "metadata": {
        "id": "tgRuHrS6_OXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6A: Generate Prompt from Image**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image44A_little.png)\n",
        "\n",
        "This is a carton image of `Grogu` in a forest surrounded by baby Ewoks. Here is the URL for this image:\n",
        "\n",
        "```text\n",
        "image_url = \"https://biologicslab.co/BIO1173/images/class_04/class_04_4_image44A.png\"\n",
        "```\n",
        "In the cell below write the code to generate a text prompt from an analysis of this image."
      ],
      "metadata": {
        "id": "G32l8WYC_zf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6A here\n",
        "\n"
      ],
      "metadata": {
        "id": "v253rcx1_zf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image47A.png)"
      ],
      "metadata": {
        "id": "fel-4izh_zf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6B: Generate Image from Text**\n",
        "\n",
        "In the cell below write the code to generate 3 images using the prompt generated above in **Exercise 6A**. If your prompt generates an error message, you should carefully re-read Example 6B."
      ],
      "metadata": {
        "id": "XDv_jy-MAMrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6B here:\n",
        "\n"
      ],
      "metadata": {
        "id": "P7UDKPgXANu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image53A.png)"
      ],
      "metadata": {
        "id": "T3pUuzT0EiJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating Cartoon Images**\n",
        "\n",
        "In this last part of this lesson we demonstrate how to convert a photorealtistic image into a cartoon image.\n",
        "\n",
        "### **Why Convert a Photorealistic Image into a Cartoon Image?**\n",
        "\n",
        "Converting a photorealistic image into a cartoon image can serve a variety of creative, practical, and communicative purposes. Here are some common reasons:\n",
        "\n",
        "#### **1. Artistic Expression**\n",
        "- Stylizes images to emphasize features or emotions.\n",
        "- Transforms mundane photos into engaging artwork.\n",
        "\n",
        "#### **2. Branding and Marketing**\n",
        "- Cartoon images are memorable and approachable.\n",
        "- Ideal for logos, mascots, and promotional materials.\n",
        "\n",
        "#### **3. Social Media and Content Creation**\n",
        "- Popular for avatars, thumbnails, and storytelling.\n",
        "- Helps creators stand out with a consistent visual identity.\n",
        "\n",
        "#### **4. Privacy and Anonymity**\n",
        "- Obscures identities while retaining personality.\n",
        "- Useful for online profiles and public presentations.\n",
        "\n",
        "#### **5. Educational and Instructional Use**\n",
        "- Simplified visuals make complex subjects more accessible.\n",
        "- Common in infographics, children’s books, and tutorials.\n",
        "\n",
        "#### **6. Entertainment and Media**\n",
        "- Widely used in animation, comics, and video games.\n",
        "- Enables exaggerated expressions and imaginative scenarios.\n"
      ],
      "metadata": {
        "id": "E2kxcTQwW6yR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Functions for Cartoon Art\n",
        "\n",
        "The code in the cell below creates 3 functions we will need for creating cartoon art:\n",
        "\n",
        "1. `create_messages()`\n",
        "2. `generate_cartoon prompt()`\n",
        "3. `generate_cartoon_images()`"
      ],
      "metadata": {
        "id": "1MEgjT0YQHs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create functions for creating cartoon art\n",
        "\n",
        "import base64, requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from openai import OpenAI\n",
        "\n",
        "# Retrieve the OpenAI API key\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "if not OPENAI_KEY:\n",
        "    raise RuntimeError(\"OPENAI_KEY not found in environment.\")\n",
        "\n",
        "# One global client for DALL·E (created with the key above)\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "def create_messages(image_url: str):\n",
        "    \"\"\"\n",
        "    Return a single HumanMessage that contains:\n",
        "        • a short text prompt, and\n",
        "        • the image encoded as a data‑URL.\n",
        "    \"\"\"\n",
        "    prompt = \"Output a prompt that would render this image as a cartoon.\"\n",
        "\n",
        "    r = requests.get(image_url)\n",
        "    if r.status_code != 200:\n",
        "        print(f\"[⚠] Could not download image: {image_url}\")\n",
        "        return None\n",
        "\n",
        "    img = Image.open(BytesIO(r.content))\n",
        "    buf = BytesIO()\n",
        "    img.save(buf, format=\"JPEG\")\n",
        "    buf.seek(0)\n",
        "    b64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
        "\n",
        "    return [\n",
        "        HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "                {\"type\": \"image_url\",\n",
        "                 \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64}\"}},\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "def generate_cartoon_prompt(image_url: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Uses GPT‑4o to turn the image into a text prompt for a cartoon.\n",
        "    \"\"\"\n",
        "    msgs = create_messages(image_url)\n",
        "    if not msgs:\n",
        "        return None\n",
        "\n",
        "    chat = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        api_key=OPENAI_KEY,          # <-- explicit key\n",
        "        temperature=0.7\n",
        "    )\n",
        "    response = chat.invoke(msgs)\n",
        "    return response.content\n",
        "\n",
        "def generate_cartoon_image(prompt: str) -> Image.Image | None:\n",
        "    \"\"\"\n",
        "    Sends *prompt* to DALL·E‑3, downloads the image,\n",
        "    shows it inline, and returns the PIL.Image.\n",
        "    \"\"\"\n",
        "    resp = client.images.generate(\n",
        "        model=\"dall-e-3\",\n",
        "        prompt=prompt,\n",
        "        size=\"1024x1024\",\n",
        "        quality=\"standard\",\n",
        "        n=1,\n",
        "    )\n",
        "\n",
        "    img_url = resp.data[0].url\n",
        "    img_bytes = requests.get(img_url).content\n",
        "    img = Image.open(BytesIO(img_bytes))\n",
        "\n",
        "    # Quick preview\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "wrR6cuJyN6uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any ouput."
      ],
      "metadata": {
        "id": "27kPE_Y3XRw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7: generate Cartoon Image\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image54A_smaller.png)\n",
        "\n",
        "The code in the cell below demonstrates how to turn this actual image of a bulldog sitting on a chaise lounge into a cartoon‑style image using OpenAI’s GPT‑4o for prompt generation and DALL·E 3 for image synthesis."
      ],
      "metadata": {
        "id": "LltlejxpaZLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: Create cartoon art\n",
        "\n",
        "# Select Image\n",
        "IMAGE_URL = \"https://biologicslab.co/BIO1173/images/class_04/class_04_4_image54A.jpg\"\n",
        "\n",
        "# Ask GPT‑4o for a cartoon prompt\n",
        "cartoon_prompt = generate_cartoon_prompt(IMAGE_URL)\n",
        "\n",
        "if not cartoon_prompt:\n",
        "    raise RuntimeError(\"Failed to generate a cartoon prompt.\")\n",
        "\n",
        "print(\"\\n Cartoon Prompt:\\n\", cartoon_prompt)\n",
        "\n",
        "# Generate the cartoon image\n",
        "cartoon_img = generate_cartoon_image(cartoon_prompt)\n",
        "\n",
        "if cartoon_img:\n",
        "    # Save to the Colab filesystem\n",
        "    cartoon_img.save(\"cartoon_result.jpg\")\n",
        "    print(\"\\n Cartoon image saved as 'cartoon_result.jpg'\")\n",
        "\n",
        "    # Optional: let the user download the file\n",
        "    #from google.colab import files\n",
        "    #files.download(\"cartoon_result.jpg\")\n",
        "else:\n",
        "    print(\"[⚠] DALL·E returned no image.\")"
      ],
      "metadata": {
        "id": "VtLUNTITRLjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image59A.png)"
      ],
      "metadata": {
        "id": "bQWuaOhTIsHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7: Generate Cartoon Image**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image56A_smaller.png)\n",
        "\n",
        "In the cell below write the code to turn this actual image of a clothing store scene into a cartoon‑style image using OpenAI’s GPT‑4o for prompt generation and DALL·E 3 for image synthesis.\n",
        "\n",
        "Here is the URL for this image:\n",
        "\n",
        "```text\n",
        "\"https://biologicslab.co/BIO1173/images/class_04/class_04_4_image56A.jpg\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "SY9wL-XYJDkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 7 here\n",
        "\n"
      ],
      "metadata": {
        "id": "2pRxQq5BOgE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image60A.png)"
      ],
      "metadata": {
        "id": "UwpPFek_JDkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Class_04_4.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "iG6D_KMDNdIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lizard Tail**\n",
        "## **NVIDIA**\n",
        "\n",
        "### **Entrance of Endeavor headquarters building in 2018**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/7/75/2788-2888_San_Tomas_Expwy.jpg)\n",
        "\n",
        "**Nvidia Corporation** (/ɛnˈvɪdiə/ en-VID-ee-ə) is an American multinational corporation and technology company headquartered in Santa Clara, California, and incorporated in Delaware. Founded in 1993 by Jensen Huang (president and CEO), Chris Malachowsky, and Curtis Priem, it is a software company which designs and supplies graphics processing units (GPUs), application programming interfaces (APIs) for data science and high-performance computing, and system on a chip units (SoCs) for mobile computing and the automotive market. Nvidia is also the dominant supplier of artificial intelligence (AI) hardware and software. Nvidia outsources the manufacturing of the hardware it designs.\n",
        "\n",
        "Nvidia's professional line of GPUs are used for edge-to-cloud computing and in supercomputers and workstations for applications in fields such as architecture, engineering and construction, media and entertainment, automotive, scientific research, and manufacturing design. Its GeForce line of GPUs are aimed at the consumer market and are used in applications such as video editing, 3D rendering, and PC gaming. With a market share of 80.2% in the second quarter of 2023, Nvidia leads the market for discrete desktop GPUs by a wide margin. The company expanded its presence in the gaming industry with the introduction of the Shield Portable (a handheld game console), Shield Tablet (a gaming tablet), and Shield TV (a digital media player), as well as its cloud gaming service GeForce Now.\n",
        "\n",
        "In addition to GPU design and outsourcing manufacturing, Nvidia provides the CUDA software platform and API that allows the creation of massively parallel programs which utilize GPUs. They are deployed in supercomputing sites around the world. In the late 2000s, Nvidia had moved into the mobile computing market, where it produces Tegra mobile processors for smartphones and tablets and vehicle navigation and entertainment systems. Its competitors include AMD, Intel,[19] Qualcomm, and AI accelerator companies such as Cerebras and Graphcore. It also makes AI-powered software for audio and video processing (e.g., Nvidia Maxine).\n",
        "\n",
        "Nvidia's offer to acquire Arm from SoftBank in September 2020 failed to materialize following extended regulatory scrutiny, leading to the termination of the deal in February 2022 in what would have been the largest semiconductor acquisition. In 2023, Nvidia became the seventh public U.S. company to be valued at over \\$1 trillion, and the company's valuation has increased rapidly since then as the company became a leader in data center chips with AI capabilities in the midst of the AI boom. In June 2024, for one day, Nvidia overtook Microsoft as the world's most valuable publicly traded company, with a market capitalization of over \\$3.3 trillion.\n",
        "\n",
        "## **History**\n",
        "\n",
        "**Founding**\n",
        "\n",
        "Nvidia was founded on April 5, 1993, by Jensen Huang (who, as of 2024, remains CEO), a Taiwanese-American electrical engineer who was previously the director of CoreWare at LSI Logic and a microprocessor designer at AMD; Chris Malachowsky, an engineer who worked at Sun Microsystems; and Curtis Priem, who was previously a senior staff engineer and graphics chip designer at IBM and Sun Microsystems. The three men agreed to start the company in a meeting at a Denny's roadside diner on Berryessa Road in East San Jose.\n",
        "\n",
        "At the time, Malachowsky and Priem were frustrated with Sun's management and were looking to leave, but Huang was on \"firmer ground\", in that he was already running his own division at LSI. The three co-founders discussed a vision of the future which was so compelling that Huang decided to leave LSI and become the chief executive officer of their new startup.\n",
        "\n",
        "In 1993, the three co-founders envisioned that the ideal trajectory for the forthcoming wave of computing would be in the realm of accelerated computing, specifically in graphics-based processing. This path was chosen due to its unique ability to tackle challenges that eluded general-purpose computing methods.[36] As Huang later explained: \"We also observed that video games were simultaneously one of the most computationally challenging problems and would have incredibly high sales volume. Those two conditions don’t happen very often. Video games was our killer app — a flywheel to reach large markets funding huge R&D to solve massive computational problems.\" With \\$40,000 in the bank, the company was born. The company subsequently received \\$20 million of venture capital funding from Sequoia Capital, Sutter Hill Ventures and others.\n",
        "\n",
        "During the late 1990s, Nvidia was one of 70 startup companies chasing the idea that graphics acceleration for video games was the path to the future. Only two survived: Nvidia and ATI Technologies, the latter of which merged into AMD.\n",
        "\n",
        "Nvidia initially had no name and the co-founders named all their files NV, as in \"next version\". The need to incorporate the company prompted the co-founders to review all words with those two letters. At one point, Malachowsky and Priem wanted to call the company NVision, but that name was already taken by a manufacturer of toilet paper. Huang suggested the name Nvidia, from \"invidia\", the Latin word for \"envy\". The company's original headquarters office was in Sunnyvale, California.\n",
        "\n",
        "**First graphics accelerator**\n",
        "\n",
        "Nvidia's first graphics accelerator, the NV1, was designed to process quadrilateral primitives (forward texture mapping), a feature that set it apart from competitors, who preferred triangle primitives. However, when Microsoft introduced the DirectX platform, it chose not to support any other graphics software and announced that its Direct3D API would exclusively support triangles. As a result, the NV1 failed to gain traction in the market.\n",
        "\n",
        "Nvidia had also entered into a partnership with Sega to supply the graphics chip for the Dreamcast console and worked on the project for about a year. However, Nvidia's technology was already lagging behind competitors. This placed the company in a difficult position: continue working on a chip that was likely doomed to fail or abandon the project, risking financial collapse.\n",
        "\n",
        "In a pivotal moment, Sega's president, Shoichiro Irimajiri, visited Huang in person to inform him that Sega had decided to choose another vendor for the Dreamcast. However, Irimajiri believed in Nvidia's potential and persuaded Sega’s management to invest $5 million into the company. Huang later reflected that this funding was all that kept Nvidia afloat, and that Irimajiri's \"understanding and generosity gave us six months to live\".\n",
        "\n",
        "In 1996, Huang laid off more than half of Nvidia's employees—thereby reducing headcount from 100 to 40—and focused the company's remaining resources on developing a graphics accelerator product optimized for processing triangle primitives: the RIVA 128. By the time the RIVA 128 was released in August 1997, Nvidia had only enough money left for one month’s payroll. The sense of impending failure became so pervasive that it gave rise to Nvidia's unofficial company motto: \"Our company is thirty days from going out of business.\" Huang began internal presentations to Nvidia staff with those words for many years.\n",
        "\n",
        "Nvidia sold about a million RIVA 128 units within four months, and used the revenue to fund development of its next generation of products. In 1998, the release of the RIVA TNT helped solidify Nvidia’s reputation as a leader in graphics technology.\n",
        "\n",
        "**Public company**\n",
        "\n",
        "Nvidia went public on January 22, 1999. Investing in Nvidia after it had already failed to deliver on its contract turned out to be Irimajiri's best decision as Sega's president. After Irimajiri left Sega in 2000, Sega sold its Nvidia stock for \\$15 million.\n",
        "\n",
        "In late 1999, Nvidia released the GeForce 256 (NV10), its first product expressly marketed as a GPU, which was most notable for introducing onboard transformation and lighting (T&L) to consumer-level 3D hardware. Running at 120 MHz and featuring four-pixel pipelines, it implemented advanced video acceleration, motion compensation, and hardware sub-picture alpha blending. The GeForce outperformed existing products by a wide margin.\n",
        "\n",
        "Due to the success of its products, Nvidia won the contract to develop the graphics hardware for Microsoft's Xbox game console, which earned Nvidia a \\$200 million advance. However, the project took many of its best engineers away from other projects. In the short term this did not matter, and the GeForce2 GTS shipped in the summer of 2000. In December 2000, Nvidia reached an agreement to acquire the intellectual assets of its one-time rival 3dfx, a pioneer in consumer 3D graphics technology leading the field from the mid-1990s until 2000. The acquisition process was finalized in April 2002.\n",
        "\n",
        "In 2001, Standard & Poor's selected Nvidia to replace the departing Enron in the S&P 500 stock index, meaning that index funds would need to hold Nvidia shares going forward.\n",
        "\n",
        "In July 2002, Nvidia acquired Exluna for an undisclosed sum. Exluna made software-rendering tools and the personnel were merged into the Cg project. In August 2003, Nvidia acquired MediaQ for approximately US$70 million. It launched GoForce the follow year. On April 22, 2004, Nvidia acquired iReady, also a provider of high-performance TCP offload engines and iSCSI controllers. In December 2004, it was announced that Nvidia would assist Sony with the design of the graphics processor (RSX) for the PlayStation 3 game console. On December 14, 2005, Nvidia acquired ULI Electronics, which at the time supplied third-party southbridge parts for chipsets to ATI, Nvidia's competitor. In March 2006, Nvidia acquired Hybrid Graphics. In December 2006, Nvidia, along with its main rival in the graphics industry AMD (which had acquired ATI), received subpoenas from the U.S. Department of Justice regarding possible antitrust violations in the graphics card industry.\n",
        "\n",
        "# NVIDIA Corporation: A Pillar of AI and LLM Innovation\n",
        "\n",
        "## 🏢 Company Overview\n",
        "\n",
        "**Founded:** 1993  \n",
        "**Headquarters:** Santa Clara, California  \n",
        "**CEO:** Jensen Huang  \n",
        "**Valuation (2025):** $4 Trillion  \n",
        "**Employees:** ~36,000  \n",
        "**Market Share:** 92% in discrete graphics segment\n",
        "\n",
        "NVIDIA began as a graphics chip designer and revolutionized the gaming industry with the invention of the **GPU (Graphics Processing Unit)** in 1999. Over time, it evolved into a full-stack computing company, now leading the charge in AI, data centers, robotics, autonomous vehicles, and scientific computing.[1](https://www.thomasnet.com/insights/nvidia-company-overview/)\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Role in AI and LLM Development\n",
        "\n",
        "### 1. **Hardware Leadership**\n",
        "- **GPUs for AI Training**: NVIDIA's H100 and Blackwell GB200 chips are optimized for training and inference of massive LLMs.\n",
        "- **Blackwell Architecture**: Introduced in 2024, it supports ultra-efficient 4-bit precision inference, enabling real-time deployment of trillion-parameter models.[2](https://blogs.nvidia.com/blog/openai-gpt-oss/)\n",
        "- **DGX Systems**: High-performance AI supercomputers used by enterprises and research institutions globally.\n",
        "\n",
        "### 2. **Software Ecosystem**\n",
        "- **CUDA Platform**: Over 450 million downloads; enables parallel computing on NVIDIA GPUs.\n",
        "- **TensorRT-LLM**: Optimizes LLM inference for speed and efficiency.\n",
        "- **NeMo Framework**: Supports training and customization of LLMs, including Megatron 530B.[3](https://nvidianews.nvidia.com/news/nvidia-launches-large-language-model-cloud-services-to-advance-ai-and-digital-biology)\n",
        "\n",
        "### 3. **Cloud Services**\n",
        "- **NeMo LLM Service**: Allows developers to fine-tune foundation models using prompt learning.\n",
        "- **BioNeMo LLM Service**: Extends LLM capabilities to biology and chemistry, aiding in drug discovery and genomics.[3](https://nvidianews.nvidia.com/news/nvidia-launches-large-language-model-cloud-services-to-advance-ai-and-digital-biology)\n",
        "\n",
        "### 4. **Strategic Collaborations**\n",
        "- **OpenAI Partnership**: NVIDIA GPUs power OpenAI’s open-weight models like gpt-oss-120b, achieving 1.5 million tokens/sec on Blackwell systems.[2](https://blogs.nvidia.com/blog/openai-gpt-oss/)\n",
        "- **Global Developer Ecosystem**: 6.5 million developers across 250 countries use NVIDIA’s AI stack.\n",
        "\n",
        "---\n",
        "\n",
        "## 🌍 Industry Impact\n",
        "\n",
        "NVIDIA’s technologies are transforming multiple sectors:\n",
        "- **Healthcare**: Accelerating diagnostics and drug discovery.\n",
        "- **Automotive**: Enabling autonomous driving through the DRIVE platform.\n",
        "- **Robotics**: Powering industrial and humanoid robots.\n",
        "- **Climate Science**: Earth-2 digital twin predicts climate change impacts.\n",
        "- **Metaverse**: Omniverse Cloud supports virtual world creation.\n",
        "- **Quantum Research**: New center in Boston focuses on quantum computing applications.[1](https://www.thomasnet.com/insights/nvidia-company-overview/)\n",
        "\n",
        "---\n",
        "\n",
        "## 📈 Future Outlook\n",
        "\n",
        "Despite challenges like chip shortages and competition, NVIDIA is projected to maintain dominance in AI and data center markets. Its continued investment in R&D, acquisitions, and infrastructure positions it as a cornerstone of the AI revolution.\n",
        "\n",
        "> “I want to turn NVIDIA into one giant AI.”  \n",
        "> — Jensen Huang, CEO[1](https://www.thomasnet.com/insights/nvidia-company-overview/)\n",
        "\n",
        "---\n",
        "\n",
        "## 🔗 References\n",
        "- [NVIDIA Blog on OpenAI Collaboration](https://blogs.nvidia.com/blog/openai-gpt-oss/)[2](https://blogs.nvidia.com/blog/openai-gpt-oss/)  \n",
        "- [Thomasnet Company Overview](https://www.thomasnet.com/insights/nvidia-company-overview/)[1](https://www.thomasnet.com/insights/nvidia-company-overview/)  \n",
        "- [NVIDIA NeMo and BioNeMo Services](https://nvidianews.nvidia.com/news/nvidia-launches-large-language-model"
      ],
      "metadata": {
        "id": "6x8gMtgQxMMH"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}