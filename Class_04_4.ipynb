{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Introduction to Large Language Models (LLMs)\n",
        "* Part 4.2: Chatbots\n",
        "* Part 4.3: Image Generation with StableDiffusion\n",
        "* **Part 4.4: AI Agents (Agentic AI)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ERfMETL_-NoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5756673d-b013-4e4e-e4f7-3de4318116ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: Using Google CoLab\n",
            "david.senseman@gmail.com\n"
          ]
        }
      ],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded.\n"
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to Agentic AI**\n",
        "\n",
        "Watch this YouTube video to learn more about Agentic AI."
      ],
      "metadata": {
        "id": "alhVuqaQPWfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"EDb37y_MhRw?si=DAYdcJv5fwlTHZ_T\"\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\">\n",
        "</iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "6LRXnrPmPXVq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "9d85104a-0d98-490b-d01b-0145c713c2b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"560\" height=\"315\"\n",
              "  src=\"https://www.youtube.com/embed/EDb37y_MhRw?si=DAYdcJv5fwlTHZ_T\"\n",
              "  title=\"YouTube video player\"\n",
              "  frameborder=\"0\"\n",
              "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
              "  allowfullscreen\n",
              "  referrerpolicy=\"strict-origin-when-cross-origin\">\n",
              "</iframe>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup GENAI**\n",
        "\n",
        "Run the code in the next cell to setup `GENAI` for this lesson."
      ],
      "metadata": {
        "id": "5eR6_NXZ5tH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install & Configure\n",
        "!pip install -q -U google-genai\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV-lHc384gNe",
        "outputId": "4ca5df3c-0508-4a89-9203-0241f5be1ab5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.1/719.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.9/234.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.47.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image01D.png)\n",
        "\n",
        "Don't worry about the error message."
      ],
      "metadata": {
        "id": "7r8p9EHirbor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Your OPENAI_API_KEY\n",
        "\n",
        "In order to run the code in this lesson you will need to have your secret `GENAI_API_KEY` installed in your **Secrets** on this Colab notebook. Detailed steps for purchasing your `GENAI_API_KEY` and installing it in your Colab notebook Secrets were already provided in a separate hand-out.\n",
        "\n",
        "Run the code in the next cell to see if your `GENAI_API_KEY` is installed correctly. You make have to Grant Access for your notebook to use your API key."
      ],
      "metadata": {
        "id": "0UHav6bt1FoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Verify API Key\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "# Check if API key is properly loaded\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    print(\"API key loaded successfully!\")\n",
        "    print(f\"Key length: {len(GEMINI_API_KEY)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading API key: {e}\")\n",
        "    print(\"Please set your API key in Google Colab:\")\n",
        "    print(\"1. Go to Secrets in the left sidebar\")\n",
        "    print(\"2. Create a new secret named 'openai_api_key'\")\n",
        "    print(\"3. Paste your OpenAI API key\")\n"
      ],
      "metadata": {
        "id": "_gXbyT2T2Pt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f5ee50-dffe-4f5f-d4f7-1b692c52ac88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key loaded successfully!\n",
            "Key length: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. You may see this message when you run this cell:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image08C.png)\n",
        "\n",
        "If you do see this popup just click on `Grant access`.\n",
        "\n",
        "\n",
        "2. If your `GENAI_API_KEY` is correctly installed you should see the following output.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image09C.png)\n",
        "\n",
        "3. However, if you see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image10C.png)\n",
        "\n",
        "You will need to correct the error before you can continue. Ask your Instructor or TA for help if you can resolve the error yourself."
      ],
      "metadata": {
        "id": "Jswm7Qbk2W1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Agentic AI**\n",
        "\n",
        "**Agentic AI** represents a fundamental shift in how artificial intelligence systems interact with the world. Unlike traditional AI models that respond to prompts with static outputs, agentic AI systems can autonomously plan, reason, and take actions to accomplish complex goals. These systems don't just answer questions—they actively work toward objectives, breaking down tasks, using tools, and adapting their approach based on feedback.\n",
        "\n",
        "### **What Makes AI \"Agentic\"?**\n",
        "\n",
        "At its core, agentic AI is characterized by several key capabilities. First, these systems exhibit goal-directed behavior, meaning they can take a high-level objective and determine the steps needed to achieve it. Second, they possess the ability to use tools—whether that's browsing the web, executing code, reading files, or interacting with APIs. Third, agentic systems demonstrate iterative reasoning, allowing them to evaluate their progress, recognize when something isn't working, and adjust their strategy accordingly.\n",
        "\n",
        "Consider the difference between asking a traditional chatbot to help you research a topic versus asking an agentic system. The chatbot provides information from its training data. The agentic system might search the web for recent sources, synthesize findings across multiple articles, organize the information into a structured document, and save it for you—all from a single request.\n",
        "\n",
        "### **Why Agentic AI Matters**\n",
        "\n",
        "The importance of agentic AI lies in its potential to dramatically expand what humans can accomplish with AI assistance. Rather than serving as sophisticated autocomplete tools, agentic systems function more like capable assistants that can handle multi-step workflows independently.\n",
        "\n",
        "For individuals, this means being able to delegate tedious, time-consuming tasks that previously required manual effort across multiple applications. For organizations, agentic AI opens possibilities for automating complex business processes that involve judgment calls and dynamic decision-making—tasks that rule-based automation could never handle.\n",
        "\n",
        "The technology also matters because it represents a more natural way for humans to work with AI. Instead of crafting perfect prompts and manually orchestrating a sequence of interactions, users can express their intent at a higher level and trust the system to figure out the details.\n",
        "\n",
        "### **Challenges and Considerations**\n",
        "\n",
        "Of course, increased autonomy brings increased responsibility. Agentic AI systems must be designed with robust safety measures, clear boundaries, and appropriate human oversight. Questions of trust, verification, and control become paramount when AI systems can take actions in the real world. The most effective agentic systems are those that balance capability with transparency—powerful enough to be genuinely useful, but designed to keep humans informed and in control.\n",
        "\n",
        "### **Looking Ahead**\n",
        "\n",
        "Agentic AI is still in its early stages, but its trajectory suggests a future where AI becomes less of a tool we use and more of a collaborator we work alongside. As these systems become more capable and more widely deployed, they have the potential to reshape how we approach work, creativity, and problem-solving. Understanding what agentic AI is and how it operates is increasingly essential for anyone looking to leverage the next generation of artificial intelligence."
      ],
      "metadata": {
        "id": "GuDrwbqQtEWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This lesson has been divided into different parts. In each part we will build a different type of AI agent that solves a particular use case that you might encounter in your own work."
      ],
      "metadata": {
        "id": "TcX-wnMAt_0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: The Literature Agent (Reasoning)**\n",
        "\n",
        "Concept: An agent isn't just a chatbot; it's a reasoning engine. In this example, we will act as a \"Literature Review Agent\" that can synthesize complex biological mechanisms into clear summaries."
      ],
      "metadata": {
        "id": "NBnB-qTE--mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 1: The Mechanism Explainer\n",
        "prompt = \"\"\"\n",
        "You are a Biology Professor Agent.\n",
        "Explain the mechanism of CRISPR-Cas9 gene editing.\n",
        "Focus on:\n",
        "1. The role of gRNA.\n",
        "2. The role of the Cas9 enzyme.\n",
        "3. The outcome of the double-strand break (NHEJ vs HDR).\n",
        "\n",
        "Keep it concise (under 200 words) and suitable for a biology undergraduate.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "zFi4Iy9X481I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "f312d325-6932-48a7-84e7-3c56ecae4fe8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'client' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4095840273.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \"\"\"\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m response = client.models.generate_content(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image.png)\n"
      ],
      "metadata": {
        "id": "oTGf-Red7LDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: The Vaccine Mechanism**\n",
        "\n",
        "\n",
        "Task: Copy the code above. Change the prompt to explain the mechanism of mRNA vaccines (like Pfizer/Moderna). Requirements:\n",
        "\n",
        "Focus on: Lipid Nanoparticles entry, Translation of Spike Protein, and Immune recognition.\n",
        "\n",
        "Keep the same \"Biology Professor Agent\" persona.\n",
        "\n"
      ],
      "metadata": {
        "id": "HJgX3nyj7zCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KKBWP9TL7zCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image.png)\n"
      ],
      "metadata": {
        "id": "s3GmuwHe7zCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: The Vision Agent (Multimodal Analysis)**\n",
        "\n",
        "Concept: \"Multimodal\" means the agent can see. We will use a Vision Agent to identify structures in biological diagrams or microscopy images."
      ],
      "metadata": {
        "id": "aLwTpl8G-u9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Vision Agent (Multimodal Analysis)\n",
        "\n",
        "\n",
        "Concept: \"Multimodal\" means the agent can see. We will use a Vision Agent to identify structures in biological diagrams or microscopy images.\n"
      ],
      "metadata": {
        "id": "_tQW2l57_VS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 2 (Fixed Accuracy): High-Precision Cell Analyst\n",
        "import PIL.Image\n",
        "import PIL.ImageDraw\n",
        "import PIL.ImageFont\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "from google.genai import types\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "\n",
        "# We explicitly ask for 0-1000 scale integers now\n",
        "class BoundingBox(BaseModel):\n",
        "    ymin: int\n",
        "    xmin: int\n",
        "    ymax: int\n",
        "    xmax: int\n",
        "\n",
        "class DetectedObject(BaseModel):\n",
        "    label: str\n",
        "    box: BoundingBox\n",
        "\n",
        "class DetectionResult(BaseModel):\n",
        "    objects: List[DetectedObject]\n",
        "\n",
        "def draw_labels_on_image(image_input, detection_result: DetectionResult):\n",
        "     annotated_image = image_input.copy()\n",
        "     draw = PIL.ImageDraw.Draw(annotated_image)\n",
        "     width, height = annotated_image.size\n",
        "\n",
        "     try:\n",
        "         font = PIL.ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\", 18)\n",
        "     except IOError:\n",
        "         font = PIL.ImageFont.load_default()\n",
        "\n",
        "     for obj in detection_result.objects:\n",
        "         # CONVERT 0-1000 SCALE TO PIXELS\n",
        "         # We divide by 1000.0 to get the percentage, then multiply by image dim\n",
        "         y1 = (obj.box.ymin / 1000.0) * height\n",
        "         x1 = (obj.box.xmin / 1000.0) * width\n",
        "         y2 = (obj.box.ymax / 1000.0) * height\n",
        "         x2 = (obj.box.xmax / 1000.0) * width\n",
        "\n",
        "         # Colors\n",
        "         color = \"#00FF00\" # Green\n",
        "         if \"nucleus\" in obj.label.lower():\n",
        "             color = \"#FFFF00\" # Yellow for Nucleus (High contrast)\n",
        "         if \"rbc\" in obj.label.lower():\n",
        "             color = \"#FF0000\" # Red for RBC\n",
        "\n",
        "         # Draw Box\n",
        "         draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
        "\n",
        "         # Draw Label\n",
        "         text = obj.label\n",
        "         text_pos = (x1, y1 - 22)\n",
        "         if text_pos[1] < 0: text_pos = (x1, y1 + 5)\n",
        "\n",
        "         bbox = draw.textbbox(text_pos, text, font=font)\n",
        "         draw.rectangle(bbox, fill=color)\n",
        "         draw.text(text_pos, text, fill=\"black\", font=font)\n",
        "\n",
        "     return annotated_image\n",
        "\n",
        "# --- 2. EXECUTION ---\n",
        "\n",
        "# Load Image\n",
        "url = \"https://biologicslab.co/BIO1173/images/class_04/Neutrophil.jpg\"\n",
        "response = requests.get(url)\n",
        "original_image = PIL.Image.open(BytesIO(response.content))\n",
        "\n",
        "print(\"--- Original Image ---\")\n",
        "display(original_image)\n",
        "\n",
        "# --- REFINED PROMPT ---\n",
        "# We add Visual Descriptions to guide the agent's eyes\n",
        "prompt = \"\"\"\n",
        "Analyze this microscopy image.\n",
        "Detect the following objects with high spatial accuracy:\n",
        "\n",
        "1. **Neutrophil**: The entire central white blood cell (including the cytoplasm).\n",
        "2. **RBC**: Identify 3 surrounding red blood cells (pink circles).\n",
        "\n",
        "**FORMAT INSTRUCTIONS:**\n",
        "- Return bounding boxes using the **0 to 1000 integer scale**.\n",
        "- ymin=0 is top, ymax=1000 is bottom.\n",
        "- xmin=0 is left, xmax=1000 is right.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Analysis... ---\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[original_image, prompt],\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=DetectionResult\n",
        "    )\n",
        ")\n",
        "\n",
        "try:\n",
        "    detection_data = DetectionResult.model_validate_json(response.text)\n",
        "    final_image = draw_labels_on_image(original_image, detection_data)\n",
        "    print(\"--- Labeled Result ---\")\n",
        "    display(final_image)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(response.text)"
      ],
      "metadata": {
        "id": "89P_5SNA3cxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Generate Multiple Images with DALL·E 2**\n",
        "\n",
        "In the cell below write the code to generate 3 images of a \"a roadrunner eating a lizard\"."
      ],
      "metadata": {
        "id": "9RGbrpkf_VS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 2 (Fixed Accuracy): High-Precision Cell Analyst\n",
        "import PIL.Image\n",
        "import PIL.ImageDraw\n",
        "import PIL.ImageFont\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "from google.genai import types\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "\n",
        "# We explicitly ask for 0-1000 scale integers now\n",
        "class BoundingBox(BaseModel):\n",
        "    ymin: int\n",
        "    xmin: int\n",
        "    ymax: int\n",
        "    xmax: int\n",
        "\n",
        "class DetectedObject(BaseModel):\n",
        "    label: str\n",
        "    box: BoundingBox\n",
        "\n",
        "class DetectionResult(BaseModel):\n",
        "    objects: List[DetectedObject]\n",
        "\n",
        "def draw_labels_on_image(image_input, detection_result: DetectionResult):\n",
        "     annotated_image = image_input.copy()\n",
        "     draw = PIL.ImageDraw.Draw(annotated_image)\n",
        "     width, height = annotated_image.size\n",
        "\n",
        "     try:\n",
        "         font = PIL.ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\", 18)\n",
        "     except IOError:\n",
        "         font = PIL.ImageFont.load_default()\n",
        "\n",
        "     for obj in detection_result.objects:\n",
        "         # CONVERT 0-1000 SCALE TO PIXELS\n",
        "         # We divide by 1000.0 to get the percentage, then multiply by image dim\n",
        "         y1 = (obj.box.ymin / 1000.0) * height\n",
        "         x1 = (obj.box.xmin / 1000.0) * width\n",
        "         y2 = (obj.box.ymax / 1000.0) * height\n",
        "         x2 = (obj.box.xmax / 1000.0) * width\n",
        "\n",
        "         # Colors\n",
        "         color = \"#00FF00\" # Green\n",
        "         if \"nucleus\" in obj.label.lower():\n",
        "             color = \"#FFFF00\" # Yellow for Nucleus (High contrast)\n",
        "         if \"rbc\" in obj.label.lower():\n",
        "             color = \"#FF0000\" # Red for RBC\n",
        "\n",
        "         # Draw Box\n",
        "         draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
        "\n",
        "         # Draw Label\n",
        "         text = obj.label\n",
        "         text_pos = (x1, y1 - 22)\n",
        "         if text_pos[1] < 0: text_pos = (x1, y1 + 5)\n",
        "\n",
        "         bbox = draw.textbbox(text_pos, text, font=font)\n",
        "         draw.rectangle(bbox, fill=color)\n",
        "         draw.text(text_pos, text, fill=\"black\", font=font)\n",
        "\n",
        "     return annotated_image\n",
        "\n",
        "# --- 2. EXECUTION ---\n",
        "\n",
        "# Load Image\n",
        "url = \"https://biologicslab.co/BIO1173/images/class_04/Neuron.jpg\"\n",
        "response = requests.get(url)\n",
        "original_image = PIL.Image.open(BytesIO(response.content))\n",
        "\n",
        "print(\"--- Original Image ---\")\n",
        "display(original_image)\n",
        "\n",
        "# --- REFINED PROMPT ---\n",
        "# We add Visual Descriptions to guide the agent's eyes\n",
        "prompt = \"\"\"\n",
        "Analyze this microscopy image.\n",
        "Detect the following objects with high spatial accuracy:\n",
        "\n",
        "1. **Nucleus**: The dark spot inside the central cell body.\n",
        "2. **Axon**: The largest process leaving the cell body.\n",
        "3. **Dendrites**: Identify 3 smaller processes leaving the cell body.\n",
        "\n",
        "**FORMAT INSTRUCTIONS:**\n",
        "- Return bounding boxes using the **0 to 1000 integer scale**.\n",
        "- ymin=0 is top, ymax=1000 is bottom.\n",
        "- xmin=0 is left, xmax=1000 is right.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Analyzing with Precision... ---\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[original_image, prompt],\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=DetectionResult\n",
        "    )\n",
        ")\n",
        "\n",
        "try:\n",
        "    detection_data = DetectionResult.model_validate_json(response.text)\n",
        "    final_image = draw_labels_on_image(original_image, detection_data)\n",
        "    print(\"--- Labeled Result ---\")\n",
        "    display(final_image)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(response.text)"
      ],
      "metadata": {
        "id": "oLbo0slE3JSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image.png)\n"
      ],
      "metadata": {
        "id": "y6o4OTh0_VTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: The Clinical Tool Agent (Function Calling)**\n",
        "\n",
        "Concept: Models are bad at math. \"Function Calling\" allows the Agent to use a Python function (a \"tool\") to perform accurate calculations, like a calculator."
      ],
      "metadata": {
        "id": "KYJWJ4Ye-Jcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Example 3: BMI Calculator Tool\n",
        "\n",
        "We define a Python function for BMI, and the Agent decides when to use it.\n"
      ],
      "metadata": {
        "id": "mKGjNmbe7DWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3:\n",
        "\n",
        "# 1. Define the tool (Python Function)\n",
        "def calculate_bmi(weight_kg: float, height_m: float) -> float:\n",
        "    \"\"\"Calculates Body Mass Index (BMI).\"\"\"\n",
        "    return weight_kg / (height_m ** 2)\n",
        "\n",
        "# 2. Create the Agent with the tool\n",
        "# We pass the function to the 'tools' parameter\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"My patient is 1.75 meters tall and weighs 70 kg. What is their BMI and is it healthy?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[calculate_bmi] # Give the agent the tool\n",
        "    )\n",
        ")\n",
        "\n",
        "# 3. Print the result\n",
        "# The agent calls the tool automatically behind the scenes in the SDK\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "SfWTZ8qj6-8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Kidney Function Tool (Creatinine Clearance)**\n",
        "\n",
        "Task: Copy the code above. Replace the BMI tool with a tool to estimate Creatinine Clearance (Cockcroft-Gault formula). Formula: ((140 - age) * weight_kg) / (72 * serum_creatinine) Prompt: \"Calculate the creatinine clearance for a 60-year-old male patient weighing 72kg with a serum creatinine of 1.2 mg/dL.\"\n"
      ],
      "metadata": {
        "id": "zYQ2KcpWSQL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n",
        "# @title Solution: Creatinine Clearance Agent\n",
        "from google.genai import types\n",
        "\n",
        "# 1. Define the specific clinical tool (The Cockcroft-Gault Formula)\n",
        "def calculate_creatinine_clearance(age: int, weight_kg: float, serum_creatinine: float) -> float:\n",
        "    \"\"\"\n",
        "    Calculates Creatinine Clearance (CrCl) using the Cockcroft-Gault formula.\n",
        "    Formula: ((140 - age) * weight_kg) / (72 * serum_creatinine)\n",
        "    \"\"\"\n",
        "    numerator = (140 - age) * weight_kg\n",
        "    denominator = 72 * serum_creatinine\n",
        "    return numerator / denominator\n",
        "\n",
        "# 2. Create the Agent with the tool\n",
        "# We pass the function to the 'tools' parameter in the config\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate the creatinine clearance for a 60-year-old male patient weighing 72kg with a serum creatinine of 1.2 mg/dL.\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[calculate_creatinine_clearance]\n",
        "    )\n",
        ")\n",
        "\n",
        "# 3. Print the result\n",
        "print(\"--- Clinical Agent Response ---\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "qEFletpeSQL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image.png)\n",
        "\n",
        "Even with the more detailed prompt, the `Edited Image` (`output`) can still be variable albeit much less variable that the output with the extremely simple prompt \"cat\". Rerun the above cell a few times to see the range of different outputs.\n"
      ],
      "metadata": {
        "id": "lNRHhCB2SQL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AeiUDGCgYpwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: The Research Agent (Grounding)**\n",
        "\n",
        "Concept: Models can hallucinate (make things up). \"Grounding\" connects the Agent to Google Search to get real-time, factual medical data.\n"
      ],
      "metadata": {
        "id": "kGvwNhffKAuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Disease Outbreak Tracker\n",
        "\n",
        "We will ask the agent to find current information about a disease.\n"
      ],
      "metadata": {
        "id": "KZGoJKQrYqfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4:\n",
        "\n",
        "# We enable 'google_search' as a tool\n",
        "google_search_tool = types.Tool(\n",
        "    google_search=types.GoogleSearch()\n",
        ")\n",
        "\n",
        "# Define prompt\n",
        "prompt = \"What are the latest updates on the spread of H5N1 Bird Flu in mammals in 2024-2025? Cite your sources.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[google_search_tool]\n",
        "    )\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "# Inspect the 'grounding_metadata' to see source links\n",
        "# print(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ],
      "metadata": {
        "id": "pTmZEXb48XDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image.png)\n",
        "\n",
        "As before, the `Edited Image` (`output`) can be quite variable. If you rerun the above cell several times you will get quite different outputs."
      ],
      "metadata": {
        "id": "LTooeodUYqfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Malaria Treatment Search**\n",
        "\n",
        "Task: Copy the code above. Change the prompt to research \"Approved Malaria vaccines and their efficacy rates in 2025\". Requirements: Ensure the agent searches for the most recent R21/Matrix-M data."
      ],
      "metadata": {
        "id": "lxFQkc8yYqfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n",
        "# We enable 'google_search' as a tool\n",
        "google_search_tool = types.Tool(\n",
        "    google_search=types.GoogleSearch()\n",
        ")\n",
        "\n",
        "# Define prompt\n",
        "prompt = \"Approved Malaria vaccines and their efficacy rates in 2025? Search recent R21/Matrix-M data.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[google_search_tool]\n",
        "    )\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "# Inspect the 'grounding_metadata' to see source links\n",
        "# print(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ],
      "metadata": {
        "id": "75Kr5Q8mYqfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is output obtained using the following `prompt`:\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image.png)\n",
        "\n",
        "Your output will depend upon the `prompt` you created.\n"
      ],
      "metadata": {
        "id": "1TH2gZv4YqfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 5: The Triage Agent (Structured Data Extraction)**\n",
        "\n",
        "Concept: In medicine, we often have messy doctor's notes. We need an Agent to extract structured data (JSON) for a database."
      ],
      "metadata": {
        "id": "aCU9nzw7_s5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# 1. Define the Structure we want\n",
        "class PatientVitals(BaseModel):\n",
        "    patient_id: str\n",
        "    systolic_bp: int\n",
        "    diastolic_bp: int\n",
        "    heart_rate: int\n",
        "    is_critical: bool\n",
        "\n",
        "# 2. The messy input data\n",
        "clinical_note = \"\"\"\n",
        "Patient John Doe (ID: PT-99283) presented to ER at 14:00.\n",
        "Observed highly agitated. BP was recorded at 160 over 95, which is concerning.\n",
        "Pulse is tachycardia at 110 bpm. Respiration 20.\n",
        "\"\"\"\n",
        "\n",
        "# 3. Run the Agent with 'response_schema'\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=f\"Extract vitals from this note: {clinical_note}\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=PatientVitals\n",
        "    )\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "id": "V36DTAz3o2PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Medication Extraction**\n",
        "\n",
        "Task: Copy the code above.\n",
        "\n",
        "    Define a new class MedicationOrder with fields: drug_name (str), dosage (str), frequency (str).\n",
        "\n",
        "    Use this note: \"Patient prescribed Amoxicillin 500mg to be taken three times daily for 7 days.\"\n",
        "\n",
        "    Extract the data."
      ],
      "metadata": {
        "id": "t1fzOao4XevS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# 1. Define the Structure we want\n",
        "class MedicationOrder(BaseModel):\n",
        "    drug_name: str\n",
        "    dosage: str\n",
        "    frequency: str\n",
        "\n",
        "\n",
        "# 2. The messy input data\n",
        "clinical_note = \"\"\"\n",
        "Patient prescribed Amoxicillin 500mg to be taken three times daily for 7 days.\n",
        "\"\"\"\n",
        "\n",
        "# 3. Run the Agent with 'response_schema'\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=f\"Extract vitals from this note: {clinical_note}\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=MedicationOrder\n",
        "    )\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "nkJaoU6O3332"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following\n",
        "\n"
      ],
      "metadata": {
        "id": "TNsbdHvSXevU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 6: Video Analysis**"
      ],
      "metadata": {
        "id": "zkxug0v2B3Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 0. Install & Configure\n",
        "!pip install -q -U google-genai\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve API Key\n",
        "try:\n",
        "    API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Error: Please add 'GEMINI_API_KEY' to your Colab Secrets.\")\n",
        "\n",
        "# Initialize Client\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "MODEL_ID = \"gemini-2.0-flash-exp\" # The Agentic Model\n",
        "\n",
        "print(\"✅ Environment Ready.\")"
      ],
      "metadata": {
        "id": "M7YbMPo1B-JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 6: Generative AI (The \"Reactive\" Assistant)**\n",
        "\n",
        "Concept: As explained in the video, Generative AI is \"reactive.\" It waits for a prompt and generates content based on patterns. It is excellent for summarizing complex medical topics for patients."
      ],
      "metadata": {
        "id": "NSLvXT4MCVVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Patient Education Generator\n",
        "\n",
        "We will act as a reactive system generating a patient handout."
      ],
      "metadata": {
        "id": "K7lOK0tvClD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 1: The Patient Explainer\n",
        "prompt = \"\"\"\n",
        "You are a helpful medical assistant.\n",
        "Create a simple, 3-bullet point handout explaining 'Hypertension' (High Blood Pressure) to a patient with low health literacy.\n",
        "Focus on:\n",
        "1. What it is.\n",
        "2. Why it is dangerous.\n",
        "3. One lifestyle change to fix it.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "rf5XosQYCv2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 7: Chain of Thought (The \"Reasoning\" Engine)**\n",
        "\n",
        "Concept: The video states that LLMs serve as the \"Reasoning Engine\" for agents, using \"Chain of Thought\" to break down complex problems step-by-step. This is critical for Differential Diagnosis."
      ],
      "metadata": {
        "id": "yuFTA9SfDR1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7: Diagnostic Reasoning\n",
        "\n",
        "We will ask the model to \"think out loud\" to diagnose a set of symptoms."
      ],
      "metadata": {
        "id": "leTNUkZoDcOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: The Differential Diagnosis\n",
        "\n",
        "symptoms = \"Patient is a 24-year-old male presenting with sharp, pleuritic chest pain, shortness of breath, and tachycardia after a long flight.\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Act as a Clinical Reasoning Agent.\n",
        "Analyze the following case: \"{symptoms}\"\n",
        "\n",
        "Use 'Chain of Thought' reasoning:\n",
        "1. Identify the key risk factors present.\n",
        "2. List 3 potential diagnoses (differential).\n",
        "3. Identify the most likely diagnosis and explain why.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "Ri_5wwVgCV93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7: Abdominal Pain Reasoning**\n",
        "\n",
        "Task: Copy the code above. Change the symptoms variable to: \"Patient is a 45-year-old female presenting with right upper quadrant abdominal pain that radiates to the shoulder, triggered after eating a fatty meal.\" Goal: See if the agent correctly identifies Cholecystitis (Gallstones) as a primary differential."
      ],
      "metadata": {
        "id": "WSvD62SCD0FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "symptoms = \"Patient is a 45-year-old female presenting with right upper quadrant abdominal pain that radiates to the shoulder, triggered after eating a fatty meal.\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Act as a Clinical Reasoning Agent.\n",
        "Analyze the following case: \"{symptoms}\"\n",
        "\n",
        "Use 'Chain of Thought' reasoning:\n",
        "1. Identify the key risk factors present.\n",
        "2. List 3 potential diagnoses (differential).\n",
        "3. Identify the most likely diagnosis and explain why.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "LM8tUWPPD-re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 8: Agentic Search (The \"Proactive\" Researcher)**\n",
        "\n",
        "Concept: Agentic AI is \"Proactive.\" It perceives a gap in knowledge and takes action (like searching the web) to fill it. This is essential for keeping up with new treatments or drug interactions."
      ],
      "metadata": {
        "id": "sheE_Kg-EWL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 8: Drug Interaction Scout\n",
        "\n",
        "We will use Google Search grounding to find real-time medical data."
      ],
      "metadata": {
        "id": "pXuV6tMVEhWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 8: Interaction Checker\n",
        "\n",
        "# We enable the Search Tool\n",
        "google_search_tool = types.Tool(\n",
        "    google_search=types.GoogleSearch()\n",
        ")\n",
        "\n",
        "drug_query = \"Are there any adverse interactions between taking Warfarin and eating Spinach? Explain the mechanism.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=drug_query,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[google_search_tool]\n",
        "    )\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "sKFuNvvGEh8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 8: Supplement Interaction Search**\n",
        "\n",
        "Task: Copy the code above. Change the drug_query to check for interactions between \"St. John's Wort\" and \"Oral Contraceptives\". Goal: Understand how the agent finds the mechanism (CYP450 enzyme induction)."
      ],
      "metadata": {
        "id": "Qx_XSPmXE0L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 8 here\n",
        "\n",
        "# Example 8: Interaction Checker\n",
        "\n",
        "# We enable the Search Tool\n",
        "google_search_tool = types.Tool(\n",
        "    google_search=types.GoogleSearch()\n",
        ")\n",
        "\n",
        "drug_query = \"St. John's Wort\" and \"Oral Contraceptives\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=drug_query,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[google_search_tool]\n",
        "    )\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "kDRExi70E641"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 9: Agentic Tools (The \"Execution\" Phase)**\n",
        "\n",
        "Concept: The video explains that agents \"Execute actions.\" In medicine, this often means precise calculations. We will give the Agent a Python function (a tool) to calculate a patient's kidney function (GFR)."
      ],
      "metadata": {
        "id": "yve977JIFcOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 9: The GFR Calculator Agent\n",
        "\n",
        "The agent will decide when to use the math tool."
      ],
      "metadata": {
        "id": "NskOZosRFqHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 4: Renal Function Agent\n",
        "# 1. Define the Tool (Python Function)\n",
        "def calculate_gfr(creatinine: float, age: int, is_female: bool) -> float:\n",
        "    \"\"\"Calculates GFR. Formula: 175 * (Scr)^-1.154 * (Age)^-0.203 * (0.742 if female)\"\"\"\n",
        "    gfr = 175 * (creatinine ** -1.154) * (age ** -0.203)\n",
        "    if is_female:\n",
        "        gfr *= 0.742\n",
        "    return gfr\n",
        "\n",
        "# 2. Run Agent with the Tool\n",
        "prompt = \"My patient is a 55-year-old female with a creatinine level of 1.5 mg/dL. Calculate her GFR and tell me if she has kidney failure.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[calculate_gfr] # Giving the agent the tool\n",
        "    )\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "WMfJ9y5GFqv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 9: The Anion Gap Agent**\n",
        "\n",
        "Task: Copy the code above.\n",
        "\n",
        "    Replace the calculate_gfr function with a new function calculate_anion_gap(sodium, chloride, bicarbonate).\n",
        "\n",
        "        Formula: Sodium - (Chloride + Bicarbonate)\n",
        "\n",
        "    Change the prompt: \"Patient has Sodium 140, Chloride 100, and Bicarbonate 15. Calculate the Anion Gap. Is it elevated (>12)?\""
      ],
      "metadata": {
        "id": "gcyTvy_qF-tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 9 here\n",
        "\n",
        "# 1. Define the Tool (Python Function)\n",
        "def calculate_anion_gap(sodium, chloride, bicarbonate) -> float:\n",
        "    \"\"\"Calculates Anion Gap. Formula: Sodium - (Chloride + Bicarbonate)\"\"\"\n",
        "    Anion_Gap = sodium - (chloride + bicarbonate)\n",
        "# 2. Run Agent with the Tool\n",
        "prompt = \"Patient has Sodium 140, Chloride 100, and Bicarbonate 15. Calculate the Anion Gap. Is it elevated (>12)?\"\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[calculate_gfr] # Giving the agent the tool\n",
        "    )\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "Gd8xZA97FdKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 10: Analyzing the Video (The \"Collaborator\")**\n",
        "\n",
        "Concept: The video concludes that the future is \"Intelligent Collaboration.\" We will now use the Agent to analyze the actual transcript of the video to extract definitions, acting as a research assistant."
      ],
      "metadata": {
        "id": "n8xn4DHtIOrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 10: Video Transcript Analyst\n",
        "\n",
        "We will feed the transcript of the video into the model and ask it to contrast the two AI types."
      ],
      "metadata": {
        "id": "ZCp-D0PtIUaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 10: The Video Analyst\n",
        "\n",
        "# Transcript from the IBM Video\n",
        "video_transcript = \"\"\"\n",
        "[00:00:00] what's the difference between generative Ai and agentic AI well they're two distinct approaches...\n",
        "[00:01:33] now agentic AI systems by contrast those are not reactive they are pro-active systems...\n",
        "[00:01:56] an agentic system basically goes through a life cycle: it perceives, decides, executes, and learns...\n",
        "[00:05:40] chain of thought reasoning is where the agent breaks down a complex task into smaller logical steps...\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Analyze this video transcript text:\n",
        "{video_transcript}\n",
        "\n",
        "1. Create a comparison table (text based) between Generative AI and Agentic AI based ONLY on this text.\n",
        "2. Extract the specific \"Life Cycle\" steps of an agent mentioned in the video.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "gGoDSfG9IVKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 10: Extracting The \"Conference\" Example**\n",
        "\n",
        "Task: Copy the code above.\n",
        "\n",
        "    Keep the video_transcript.\n",
        "\n",
        "    Change the prompt to: \"Based on the transcript, explain how an Agent would plan a conference. List the specific 'internal dialogue' steps mentioned in the video.\"\n",
        "\n",
        "    This tests your ability to extract specific logic from unstructured data—a key skill for analyzing medical records."
      ],
      "metadata": {
        "id": "dd7NMFO3Izek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 10: The Video Analyst\n",
        "\n",
        "# Transcript from the IBM Video\n",
        "video_transcript = \"\"\"\n",
        "[00:00:00] what's the difference between generative Ai and agentic AI well they're two distinct approaches...\n",
        "[00:01:33] now agentic AI systems by contrast those are not reactive they are pro-active systems...\n",
        "[00:01:56] an agentic system basically goes through a life cycle: it perceives, decides, executes, and learns...\n",
        "[00:05:40] chain of thought reasoning is where the agent breaks down a complex task into smaller logical steps...\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Analyze this video transcript text:\n",
        "{video_transcript}\n",
        "\n",
        "\"Based on the transcript, explain how an Agent would plan a conference. List the specific 'internal dialogue' steps mentioned in the video.\"\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "173owwIxI0F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-In**\n",
        "\n",
        "When you have completed and run all of the code cells, use the `File --> Print.. --> Microsoft Print to PDF` to generate your PDF if you are running `MS Windows`. If you have a Mac, use the `File --> Print.. --> Save as PDF`\n",
        "\n",
        "In either case, save your PDF as Copy of Class_04_4.lastname.pdf where lastname is your last name, and upload the file to Canvas.\n",
        "\n",
        "**NOTE TO WINDOWS USERS:** Your grade will be reduced by 10% if your PDF is found to be missing pages when it is being graded in Canvas. This penalty is simply meant to prevent the grader from having to take the additional steps of (1) downloading your PDF, (2) printing it out using the `Microsoft Print to PDF` and (3) having to resubmit to Canvas so they can grade it."
      ],
      "metadata": {
        "id": "iG6D_KMDNdIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lizard Tail**\n",
        "## **NVIDIA**\n",
        "\n",
        "### **Entrance of Endeavor headquarters building in 2018**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/7/75/2788-2888_San_Tomas_Expwy.jpg)\n",
        "\n",
        "**Nvidia Corporation** (/ɛnˈvɪdiə/ en-VID-ee-ə) is an American multinational corporation and technology company headquartered in Santa Clara, California, and incorporated in Delaware. Founded in 1993 by Jensen Huang (president and CEO), Chris Malachowsky, and Curtis Priem, it is a software company which designs and supplies graphics processing units (GPUs), application programming interfaces (APIs) for data science and high-performance computing, and system on a chip units (SoCs) for mobile computing and the automotive market. Nvidia is also the dominant supplier of artificial intelligence (AI) hardware and software. Nvidia outsources the manufacturing of the hardware it designs.\n",
        "\n",
        "Nvidia's professional line of GPUs are used for edge-to-cloud computing and in supercomputers and workstations for applications in fields such as architecture, engineering and construction, media and entertainment, automotive, scientific research, and manufacturing design. Its GeForce line of GPUs are aimed at the consumer market and are used in applications such as video editing, 3D rendering, and PC gaming. With a market share of 80.2% in the second quarter of 2023, Nvidia leads the market for discrete desktop GPUs by a wide margin. The company expanded its presence in the gaming industry with the introduction of the Shield Portable (a handheld game console), Shield Tablet (a gaming tablet), and Shield TV (a digital media player), as well as its cloud gaming service GeForce Now.\n",
        "\n",
        "In addition to GPU design and outsourcing manufacturing, Nvidia provides the CUDA software platform and API that allows the creation of massively parallel programs which utilize GPUs. They are deployed in supercomputing sites around the world. In the late 2000s, Nvidia had moved into the mobile computing market, where it produces Tegra mobile processors for smartphones and tablets and vehicle navigation and entertainment systems. Its competitors include AMD, Intel,[19] Qualcomm, and AI accelerator companies such as Cerebras and Graphcore. It also makes AI-powered software for audio and video processing (e.g., Nvidia Maxine).\n",
        "\n",
        "Nvidia's offer to acquire Arm from SoftBank in September 2020 failed to materialize following extended regulatory scrutiny, leading to the termination of the deal in February 2022 in what would have been the largest semiconductor acquisition. In 2023, Nvidia became the seventh public U.S. company to be valued at over \\$1 trillion, and the company's valuation has increased rapidly since then as the company became a leader in data center chips with AI capabilities in the midst of the AI boom. In June 2024, for one day, Nvidia overtook Microsoft as the world's most valuable publicly traded company, with a market capitalization of over \\$3.3 trillion.\n",
        "\n",
        "## **History**\n",
        "\n",
        "**Founding**\n",
        "\n",
        "Nvidia was founded on April 5, 1993, by Jensen Huang (who, as of 2024, remains CEO), a Taiwanese-American electrical engineer who was previously the director of CoreWare at LSI Logic and a microprocessor designer at AMD; Chris Malachowsky, an engineer who worked at Sun Microsystems; and Curtis Priem, who was previously a senior staff engineer and graphics chip designer at IBM and Sun Microsystems. The three men agreed to start the company in a meeting at a Denny's roadside diner on Berryessa Road in East San Jose.\n",
        "\n",
        "At the time, Malachowsky and Priem were frustrated with Sun's management and were looking to leave, but Huang was on \"firmer ground\", in that he was already running his own division at LSI. The three co-founders discussed a vision of the future which was so compelling that Huang decided to leave LSI and become the chief executive officer of their new startup.\n",
        "\n",
        "In 1993, the three co-founders envisioned that the ideal trajectory for the forthcoming wave of computing would be in the realm of accelerated computing, specifically in graphics-based processing. This path was chosen due to its unique ability to tackle challenges that eluded general-purpose computing methods.[36] As Huang later explained: \"We also observed that video games were simultaneously one of the most computationally challenging problems and would have incredibly high sales volume. Those two conditions don’t happen very often. Video games was our killer app — a flywheel to reach large markets funding huge R&D to solve massive computational problems.\" With \\$40,000 in the bank, the company was born. The company subsequently received \\$20 million of venture capital funding from Sequoia Capital, Sutter Hill Ventures and others.\n",
        "\n",
        "During the late 1990s, Nvidia was one of 70 startup companies chasing the idea that graphics acceleration for video games was the path to the future. Only two survived: Nvidia and ATI Technologies, the latter of which merged into AMD.\n",
        "\n",
        "Nvidia initially had no name and the co-founders named all their files NV, as in \"next version\". The need to incorporate the company prompted the co-founders to review all words with those two letters. At one point, Malachowsky and Priem wanted to call the company NVision, but that name was already taken by a manufacturer of toilet paper. Huang suggested the name Nvidia, from \"invidia\", the Latin word for \"envy\". The company's original headquarters office was in Sunnyvale, California.\n",
        "\n",
        "**First graphics accelerator**\n",
        "\n",
        "Nvidia's first graphics accelerator, the NV1, was designed to process quadrilateral primitives (forward texture mapping), a feature that set it apart from competitors, who preferred triangle primitives. However, when Microsoft introduced the DirectX platform, it chose not to support any other graphics software and announced that its Direct3D API would exclusively support triangles. As a result, the NV1 failed to gain traction in the market.\n",
        "\n",
        "Nvidia had also entered into a partnership with Sega to supply the graphics chip for the Dreamcast console and worked on the project for about a year. However, Nvidia's technology was already lagging behind competitors. This placed the company in a difficult position: continue working on a chip that was likely doomed to fail or abandon the project, risking financial collapse.\n",
        "\n",
        "In a pivotal moment, Sega's president, Shoichiro Irimajiri, visited Huang in person to inform him that Sega had decided to choose another vendor for the Dreamcast. However, Irimajiri believed in Nvidia's potential and persuaded Sega’s management to invest $5 million into the company. Huang later reflected that this funding was all that kept Nvidia afloat, and that Irimajiri's \"understanding and generosity gave us six months to live\".\n",
        "\n",
        "In 1996, Huang laid off more than half of Nvidia's employees—thereby reducing headcount from 100 to 40—and focused the company's remaining resources on developing a graphics accelerator product optimized for processing triangle primitives: the RIVA 128. By the time the RIVA 128 was released in August 1997, Nvidia had only enough money left for one month’s payroll. The sense of impending failure became so pervasive that it gave rise to Nvidia's unofficial company motto: \"Our company is thirty days from going out of business.\" Huang began internal presentations to Nvidia staff with those words for many years.\n",
        "\n",
        "Nvidia sold about a million RIVA 128 units within four months, and used the revenue to fund development of its next generation of products. In 1998, the release of the RIVA TNT helped solidify Nvidia’s reputation as a leader in graphics technology.\n",
        "\n",
        "**Public company**\n",
        "\n",
        "Nvidia went public on January 22, 1999. Investing in Nvidia after it had already failed to deliver on its contract turned out to be Irimajiri's best decision as Sega's president. After Irimajiri left Sega in 2000, Sega sold its Nvidia stock for \\$15 million.\n",
        "\n",
        "In late 1999, Nvidia released the GeForce 256 (NV10), its first product expressly marketed as a GPU, which was most notable for introducing onboard transformation and lighting (T&L) to consumer-level 3D hardware. Running at 120 MHz and featuring four-pixel pipelines, it implemented advanced video acceleration, motion compensation, and hardware sub-picture alpha blending. The GeForce outperformed existing products by a wide margin.\n",
        "\n",
        "Due to the success of its products, Nvidia won the contract to develop the graphics hardware for Microsoft's Xbox game console, which earned Nvidia a \\$200 million advance. However, the project took many of its best engineers away from other projects. In the short term this did not matter, and the GeForce2 GTS shipped in the summer of 2000. In December 2000, Nvidia reached an agreement to acquire the intellectual assets of its one-time rival 3dfx, a pioneer in consumer 3D graphics technology leading the field from the mid-1990s until 2000. The acquisition process was finalized in April 2002.\n",
        "\n",
        "In 2001, Standard & Poor's selected Nvidia to replace the departing Enron in the S&P 500 stock index, meaning that index funds would need to hold Nvidia shares going forward.\n",
        "\n",
        "In July 2002, Nvidia acquired Exluna for an undisclosed sum. Exluna made software-rendering tools and the personnel were merged into the Cg project. In August 2003, Nvidia acquired MediaQ for approximately US$70 million. It launched GoForce the follow year. On April 22, 2004, Nvidia acquired iReady, also a provider of high-performance TCP offload engines and iSCSI controllers. In December 2004, it was announced that Nvidia would assist Sony with the design of the graphics processor (RSX) for the PlayStation 3 game console. On December 14, 2005, Nvidia acquired ULI Electronics, which at the time supplied third-party southbridge parts for chipsets to ATI, Nvidia's competitor. In March 2006, Nvidia acquired Hybrid Graphics. In December 2006, Nvidia, along with its main rival in the graphics industry AMD (which had acquired ATI), received subpoenas from the U.S. Department of Justice regarding possible antitrust violations in the graphics card industry.\n",
        "\n",
        "# NVIDIA Corporation: A Pillar of AI and LLM Innovation\n",
        "\n",
        "## 🏢 Company Overview\n",
        "\n",
        "**Founded:** 1993  \n",
        "**Headquarters:** Santa Clara, California  \n",
        "**CEO:** Jensen Huang  \n",
        "**Valuation (2025):** $4 Trillion  \n",
        "**Employees:** ~36,000  \n",
        "**Market Share:** 92% in discrete graphics segment\n",
        "\n",
        "NVIDIA began as a graphics chip designer and revolutionized the gaming industry with the invention of the **GPU (Graphics Processing Unit)** in 1999. Over time, it evolved into a full-stack computing company, now leading the charge in AI, data centers, robotics, autonomous vehicles, and scientific computing.[1](https://www.thomasnet.com/insights/nvidia-company-overview/)\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Role in AI and LLM Development\n",
        "\n",
        "### 1. **Hardware Leadership**\n",
        "- **GPUs for AI Training**: NVIDIA's H100 and Blackwell GB200 chips are optimized for training and inference of massive LLMs.\n",
        "- **Blackwell Architecture**: Introduced in 2024, it supports ultra-efficient 4-bit precision inference, enabling real-time deployment of trillion-parameter models.[2](https://blogs.nvidia.com/blog/openai-gpt-oss/)\n",
        "- **DGX Systems**: High-performance AI supercomputers used by enterprises and research institutions globally.\n",
        "\n",
        "### 2. **Software Ecosystem**\n",
        "- **CUDA Platform**: Over 450 million downloads; enables parallel computing on NVIDIA GPUs.\n",
        "- **TensorRT-LLM**: Optimizes LLM inference for speed and efficiency.\n",
        "- **NeMo Framework**: Supports training and customization of LLMs, including Megatron 530B.[3](https://nvidianews.nvidia.com/news/nvidia-launches-large-language-model-cloud-services-to-advance-ai-and-digital-biology)\n",
        "\n",
        "### 3. **Cloud Services**\n",
        "- **NeMo LLM Service**: Allows developers to fine-tune foundation models using prompt learning.\n",
        "- **BioNeMo LLM Service**: Extends LLM capabilities to biology and chemistry, aiding in drug discovery and genomics.[3](https://nvidianews.nvidia.com/news/nvidia-launches-large-language-model-cloud-services-to-advance-ai-and-digital-biology)\n",
        "\n",
        "### 4. **Strategic Collaborations**\n",
        "- **OpenAI Partnership**: NVIDIA GPUs power OpenAI’s open-weight models like gpt-oss-120b, achieving 1.5 million tokens/sec on Blackwell systems.[2](https://blogs.nvidia.com/blog/openai-gpt-oss/)\n",
        "- **Global Developer Ecosystem**: 6.5 million developers across 250 countries use NVIDIA’s AI stack.\n",
        "\n",
        "---\n",
        "\n",
        "## 🌍 Industry Impact\n",
        "\n",
        "NVIDIA’s technologies are transforming multiple sectors:\n",
        "- **Healthcare**: Accelerating diagnostics and drug discovery.\n",
        "- **Automotive**: Enabling autonomous driving through the DRIVE platform.\n",
        "- **Robotics**: Powering industrial and humanoid robots.\n",
        "- **Climate Science**: Earth-2 digital twin predicts climate change impacts.\n",
        "- **Metaverse**: Omniverse Cloud supports virtual world creation.\n",
        "- **Quantum Research**: New center in Boston focuses on quantum computing applications.[1](https://www.thomasnet.com/insights/nvidia-company-overview/)\n",
        "\n",
        "---\n",
        "\n",
        "## 📈 Future Outlook\n",
        "\n",
        "Despite challenges like chip shortages and competition, NVIDIA is projected to maintain dominance in AI and data center markets. Its continued investment in R&D, acquisitions, and infrastructure positions it as a cornerstone of the AI revolution.\n",
        "\n",
        "> “I want to turn NVIDIA into one giant AI.”  \n",
        "> — Jensen Huang, CEO[1](https://www.thomasnet.com/insights/nvidia-company-overview/)\n",
        "\n",
        "---\n",
        "\n",
        "## 🔗 References\n",
        "- [NVIDIA Blog on OpenAI Collaboration](https://blogs.nvidia.com/blog/openai-gpt-oss/)[2](https://blogs.nvidia.com/blog/openai-gpt-oss/)  \n",
        "- [Thomasnet Company Overview](https://www.thomasnet.com/insights/nvidia-company-overview/)[1](https://www.thomasnet.com/insights/nvidia-company-overview/)  \n",
        "- [NVIDIA NeMo and BioNeMo Services](https://nvidianews.nvidia.com/news/nvidia-launches-large-language-model"
      ],
      "metadata": {
        "id": "6x8gMtgQxMMH"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}