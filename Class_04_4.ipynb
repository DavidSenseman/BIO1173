{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51zKHFIGdtp0"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9SaQYwLdtp0"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrem8Zfndtp0"
      },
      "source": [
        "##### **Module 4: Training for Tabular Data**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Encoding a Feature Vector for Keras Deep Learning\n",
        "* Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC\n",
        "* Part 4.3: Keras Regression for Deep Neural Networks with RMSE\n",
        "* **Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Neural Network Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRMAKKUjdtp1"
      },
      "source": [
        "### Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running the correct version of TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# YOU MUST RUN THIS CELL FIRST\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-PFQjj6dtp1"
      },
      "source": [
        "# Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Neural Network Training\n",
        "\n",
        "In 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published an article in [Nature](https://corgi.genomelab.utsa.edu/BIO1173/images/Nature.pdf) with the title _Learning representations by back-propagating errors_. Since then, **_backpropagation_** has become one of the most popular methods for training neural networks. Programmers frequently train deep neural networks with backpropagation because it scales really well when run on graphical processing units (GPUs). To understand this algorithm for neural networks, we must examine how to train it as well as how it processes a pattern.\n",
        "\n",
        "More recently, researchers have extended classic backpropagation and modified it giving rise to many different training algorithms. In this lesson we will look at a few of the most important training paradigms, beginning with classic backpropagation and ending with stochastic gradient descent (SGD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWnnSHkfdtp1"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "[**_Backpropagation_**](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf) is the primary means of determining a neural network's weights during training. Backpropagation works by calculating a weight change amount ($v_t$) for every weight ($\\theta$, theta) in the neural network. This value is subtracted from every weight by the following equation:\n",
        "\n",
        "$$ \\theta_t = \\theta_{t-1} - v_t $$\n",
        "\n",
        "We repeat this process for every iteration ($t$). The training algorithm determines how we calculate the weight change. Classic backpropagation calculates a gradient ($\\nabla$, nabla) for every weight in the neural network for the neural network's error function ($J$). We scale the gradient by a learning rate ($\\eta$, eta).\n",
        "\n",
        "$$ v_t = \\eta \\nabla_{\\theta_{t-1}} J(\\theta_{t-1}) $$\n",
        "\n",
        "The **_learning rate_** is an important concept for backpropagation training. Setting the learning rate can be complex:\n",
        "\n",
        "* Too low a learning rate will usually converge to a reasonable solution; however, the process will be prolonged.\n",
        "* Too high of a learning rate will either fail outright or converge to a higher error than a better learning rate.\n",
        "\n",
        "Common values for learning rate are: 0.1, 0.01, 0.001, etc.\n",
        "\n",
        "Backpropagation is a gradient descent type, and many texts will use these two terms interchangeably. Gradient descent refers to calculating a gradient on each weight in the neural network for each training element. Gradient descent is an optimization algorithm used in training neural networks to minimize the loss function by iteratively updating the parameters (weights and biases) of the network.\n",
        "\n",
        "The **_gradient_** of the loss function with respect to the parameters is computed, indicating the direction in which the parameters should be adjusted to reduce the loss. The parameters are then updated in the opposite direction of the gradient, taking into account a learning rate that controls the size of the step. This process is repeated iteratively until convergence is reached, leading to the optimal set of parameters that minimize the loss function. If the neural network did output exactly what was expected, the gradient for each weight would be 0, indicating that no change to the weight is necessary.\n",
        "\n",
        "The gradient is the **_derivative_** of the error function at the weight's current value. The gradient represents the derivative of the error function with respect to the weights of the neural network at their current values. It indicates the rate of change of the error with respect to each weight parameter, showing the direction in which the weights should be adjusted to minimize the error function. By computing the gradient, we can determine how each weight contributes to the overall error and update the weights in the direction that reduces the error, ultimately improving the performance of the neural network during training.\n",
        "The error function measures the distance of the neural network's output from the expected output. We can use gradient descent, a process in which each weight's gradient value can reach even lower values of the error function.\n",
        "  \n",
        "The gradient is the partial derivative of each **_weight_** in the neural network concerning the error function. Each weight has a gradient that is the slope of the error function. The term _weight_ refers to the strength of the connection between two neurons. Calculating the gradient of the error function allows the training method to determine whether it should increase or decrease the weight (connection strength). In turn, this determination will decrease the error of the neural network. The **_error_** is the difference between the expected output and actual output of the neural network. Many different training methods called propagation-training algorithms utilize gradients. In all of them, the sign of the gradient tells the neural network the following information:\n",
        "\n",
        "* Zero gradient - The weight does not contribute to the neural network's error.\n",
        "* Negative gradient - The algorithm should _increase_ the weight to lower error.\n",
        "* Positive gradient - The algorithm should _decrease_ the weight to lower error.\n",
        "\n",
        "\n",
        "Because many algorithms depend on gradient calculation, we will begin with an analysis of this process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "TXMISeSjdtp1"
      },
      "source": [
        "### Gradient Calculations\n",
        "\n",
        "First of all, let's examine the gradient. Essentially, training is a search for the set of weights that will cause the neural network to have the lowest error for a training set. If we had infinite computation resources, we would try every possible combination of weights to determine the one that provided the lowest error during the training.\n",
        "\n",
        "Because we do not have unlimited computing resources, we have to use some shortcuts to prevent the need to examine every possible weight combination. These training methods utilize clever techniques to avoid performing a brute-force search of all weight values. This type of exhaustive search would be impossible because even small networks have an infinite number of weight combinations.\n",
        "\n",
        "In training neural networks, gradient calculations are made using the backpropagation algorithm. The process involves computing the gradient of the loss function with respect to the weights of the network by applying the chain rule of calculus.\n",
        "\n",
        "* **Forward Pass:** The input data is passed through the network, and the output is computed. The loss function is then evaluated using the predicted output and the actual labels.\n",
        "* **Backward Pass (Backpropagation):** The gradient of the loss function with respect to the output of the network is calculated first, and then the gradients are recursively propagated backward through the network layers. This process involves computing the gradient of the loss with respect to the activations of each layer, and then using these gradients to compute the gradients with respect to the weights of the network.\n",
        "* **Gradient Descent:** Once the gradients with respect to the weights are calculated, the weights are updated using an optimization algorithm like gradient descent. The weights are adjusted in the direction opposite to the gradient, with the magnitude of the update determined by the learning rate.\n",
        "\n",
        "By iteratively updating the weights based on the calculated gradients, the neural network learns to minimize the loss function and improve its performance on the training data.\n",
        "\n",
        "Consider the chart below that shows the error of a neural network for each possible weight. This chart is a graph showing the error for a single weight:\n",
        "\n",
        "![Derivative](https://biologicslab.co/BIO1173/images/class_2_deriv2.png \"Derivative\")\n",
        "\n",
        "Looking at this chart, you can easily see that the **_optimal weight_** is where the black line has the lowest y-value (Optimal wt). The problem is that we see only the error for the current value of the weight; we do not see the entire graph because that process would require an exhaustive search.\n",
        "\n",
        "We can, however, determine the slope of the error curve at a particular weight. In the above chart, we can see the slope of the error curve at 1.5. The straight blue line, that barely touches the error curve at 1.5, gives the slope. In this case, the slope, or gradient, is -0.5622. The negative slope indicates that an _increase_ in the weight will _lower_ the error.\n",
        "\n",
        "The gradient is the instantaneous slope of the error function at the specified weight. The derivative of the error curve at that point gives the gradient. This line tells us the steepness of the error function at the given weight.  \n",
        "\n",
        "Derivatives are one of the most fundamental concepts in calculus. For this course, you need to understand that a derivative provides the slope of a function at a specific point. A training technique and this slope can give you the information to adjust the weight for a lower error. Using our working definition of the gradient, we will show how to calculate it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UchC2zpcdtp2"
      },
      "source": [
        "## Momentum Backpropagation\n",
        "\n",
        "In physics, **_momentum_** is a vector quantity that represents the motion of an object and is calculated as the product of its mass and velocity. It describes the quantity of motion an object possesses and the difficulty in stopping it.\n",
        "\n",
        "In the context of optimization algorithms such as gradient descent, momentum is a technique that aims to accelerate convergence by adding a fraction of the previous update to the current update. This helps to smooth out variations in the gradient and speed up convergence, especially in the presence of noisy or sparse gradients.\n",
        "\n",
        "Momentum adds another term to the calculation of $v_t$:\n",
        "\n",
        "$$ v_t = \\eta \\nabla_{\\theta_{t-1}} J(\\theta_{t-1}) + \\lambda v_{t-1} $$\n",
        "\n",
        "Like the learning rate, momentum adds another training parameter that scales the effect of momentum. Momentum backpropagation has two training parameters: learning rate ($\\eta$, eta) and momentum ($\\lambda$, lambda). Momentum adds the scaled value of the previous weight change amount ($v_{t-1}$) to the current weight change amount($v_t$).\n",
        "\n",
        "This technique has the effect of adding additional force behind the direction a weight is moving. In other words, momentum **_keeps_** pushing for a change in the weight, even when the slope might be zero. In certain situations, unless the weight is changed, the model might get \"trapped\" within local minimum that is far from the true minimum. The figure below shows how momentum might allow the weight to escape local minima.\n",
        "\n",
        "![Momentum](https://biologicslab.co/BIO1173/images/class_5_momentum.png \"Momentum \")\n",
        "\n",
        "A typical value for momentum is 0.9.\n",
        "\n",
        "Using momentum in training a neural network can have some drawbacks, including:\n",
        "\n",
        "* **Overshooting:** High momentum values can lead to overshooting, where the optimization algorithm oscillates or overshoots the minimum point, causing instability and slower convergence.\n",
        "* **Local Minima:** Momentum can sometimes make it harder for the optimization algorithm to escape local minima, as the accumulated momentum may prevent the algorithm from exploring other regions of the parameter space.\n",
        "* **Sensitivity to Hyperparameters:** Choosing the right momentum value can be crucial, as selecting a suboptimal value can negatively impact the training process and result in poor convergence.\n",
        "* **Increased Complexity:** Incorporating momentum adds an additional hyperparameter to tune, increasing the complexity of the training process and potentially requiring more computational resources.\n",
        "* **Difficulty in Interpretation:** Momentum may introduce additional complexity in interpreting and understanding the training behavior of the neural network, making it harder to diagnose issues or optimize the training process effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "I5ArQLTrdtp2"
      },
      "source": [
        "## Batch and Online Backpropagation\n",
        "\n",
        "While it is possible to calculate gradients for element in a training set, gradients can also be summed together into **_batches_**, and the weights updated once per batch.\n",
        "\n",
        "In training neural networks, batches refer to dividing the training data into smaller subsets to update the model's parameters more frequently during optimization. This process is known as _mini-batch gradient descent_, where the gradients are computed for each batch and the model parameters are updated accordingly.\n",
        "\n",
        "Batches are useful in the following scenarios:\n",
        "* **Efficiency:** Training neural networks on large datasets can be computationally intensive. Using batches allows for more frequent parameter updates, making the optimization process more efficient compared to updating the parameters after processing the entire dataset (batch gradient descent).\n",
        "* **Memory Constraints:** Processing the entire dataset at once may exceed the memory capacity of the system. Batching the data helps in managing memory constraints by loading and processing smaller subsets at a time.\n",
        "* **Generalization:** Batching introduces noise into the gradient estimates, which can help prevent the optimization process from getting stuck in local minima and improve the generalization of the model.\n",
        "* **Parallelism:** Training on batches enables parallelization of computations, allowing for faster training on hardware with multiple processors or GPUs.\n",
        "* **Stability:** Using batches can improve the stability of the training process by reducing the variance of parameter updates, leading to smoother convergence and improved optimization performance.\n",
        "\n",
        "When using batches, additional parameters need to be specified:\n",
        "\n",
        "* **Online Training** - Update the weights based on gradients calculated from a single training set element.\n",
        "* **Batch Training** - Update the weights based on the sum of the gradients over all training set elements.\n",
        "* **Batch Size** - Update the weights based on the sum of some batch size of training set elements.\n",
        "* **Mini-Batch Training** - The same as batch size, but with minimal batch size.  Mini-batches are very popular, often in the 32-64 element range.\n",
        "\n",
        "Because the batch size is smaller than the full training set size, it may take several batches to make it completely through the training set.  \n",
        "\n",
        "* **Step/Iteration** - The number of processed batches.\n",
        "* **Epoch** - The number of times the algorithm processed the complete training set.\n",
        "\n",
        "Batch processing will be very important when we start processing images with convolutional neural networks later in this course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZH6p2uPdtp2"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "**_Stochastic gradient descent (SGD)_** is currently one of the most popular neural network training algorithms.  It works very similarly to Batch/Mini-Batch training, except that the batches are made up of a random set of training elements.\n",
        "\n",
        "Stochastic gradient descent is commonly used in training neural networks when dealing with large datasets as it offers faster convergence compared to traditional batch gradient descent. Additionally, stochastic gradient descent is useful when working with noisy data or when the loss function is flat, as it introduces randomness that can help escape local minima. However, it may result in higher variance in the learning process due to its stochastic nature.\n",
        "\n",
        "This technique leads to a very irregular convergence in error during training, as shown in the figure below.\n",
        "\n",
        "![SGD  Error](https://biologicslab.co/BIO1173/images/class_5_sgd_error.png \"SGD Error\")\n",
        "\n",
        "\n",
        "[Image from Wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
        "\n",
        "Because the neural network is trained on a random sample of the complete training set each time, the error does not make a smooth transition downward.  However, the error usually does go down.\n",
        "\n",
        "Advantages to SGD include:\n",
        "\n",
        "* **Computationally efficient.**  Each training step can be relatively fast, even with a huge training set.\n",
        "* **Decreases overfitting** by focusing on only a portion of the training set each step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8mikUnwdtp2"
      },
      "source": [
        "## Other Techniques\n",
        "\n",
        "One problem with simple backpropagation training algorithms is that they are susceptible to learning rate and momentum. This technique is difficult because:\n",
        "\n",
        "* **Learning rate** must be adjusted to a small enough level to train an accurate neural network.\n",
        "* **Momentum** must be large enough to overcome local minima yet small enough not to destabilize the training.\n",
        "* **A single learning rate/momentum** is often not good enough for the entire training process. It is often helpful to automatically decrease the learning rate as the training progresses.\n",
        "* **All weights** share a single learning rate/momentum.\n",
        "\n",
        "Other training techniques:\n",
        "\n",
        "* **Resilient Propagation** - Use only the magnitude of the gradient and allow each neuron to learn at its rate. There is no need for learning rate/momentum; however, it only works in full batch mode.\n",
        "* **Nesterov accelerated gradient** - Helps mitigate the risk of choosing a bad mini-batch.\n",
        "* **Adagrad** - Allows an automatically decaying per-weight learning rate and momentum concept.\n",
        "* **Adadelta** - Extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.\n",
        "* **Non-Gradient Methods** - Non-gradient methods can *sometimes* be useful, though rarely outperform gradient-based backpropagation methods.  These include: [simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing), [genetic algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm), [particle swarm optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization), [Nelder Mead](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method), and [many more](https://en.wikipedia.org/wiki/Category:Optimization_algorithms_and_methods)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD61Rf6Pdtp2"
      },
      "source": [
        "## ADAM Update\n",
        "\n",
        "**_ADAM_** is the first training algorithm you should try.  It is very effective.  Kingma and Ba (2014) introduced the Adam update rule that derives its name from the adaptive moment estimates. [[Cite:kingma2014adam]](https://arxiv.org/abs/1412.6980)  Adam estimates the first (mean) and second (variance) moments to determine the weight corrections.  Adam begins with an exponentially decaying average of past gradients (m):\n",
        "\n",
        "$$ m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t $$\n",
        "\n",
        "This average accomplishes a similar goal as classic momentum update; however, its value is calculated automatically based on the current gradient ($g_t$).  The update rule then calculates the second moment ($v_t$):\n",
        "\n",
        "$$ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 $$\n",
        "\n",
        "The values $m_t$ and $v_t$ are estimates of the gradients' first moment (the mean) and the second moment (the uncentered variance).  However, they will be strongly biased towards zero in the initial training cycles.  The first moment’s bias is corrected as follows.\n",
        "\n",
        "$$ \\hat{m}_t = \\frac{m_t}{1-\\beta^t_1} $$\n",
        "\n",
        "Similarly, the second moment is also corrected:\n",
        "\n",
        "$$ \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t} $$\n",
        "\n",
        "These bias-corrected first and second moment estimates are applied to the ultimate Adam update rule, as follows:\n",
        "\n",
        "$$ \\theta_t = \\theta_{t-1} - \\frac{\\alpha \\cdot \\hat{m}_t}{\\sqrt{\\hat{v}_t}+\\eta} \\hat{m}_t $$\n",
        "\n",
        "Adam is very tolerant to initial learning rate ($\\alpha$) and other training parameters. Kingma and Ba (2014)  propose default values of 0.9 for $\\beta_1$, 0.999 for $\\beta_2$, and 10-8 for $\\eta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlFxrCXLdtp2"
      },
      "source": [
        "The choice of optimizer for Keras regression neural networks can vary depending on the specific dataset, network architecture, and training requirements. However, some commonly used optimizers that have been effective for regression tasks in Keras include:\n",
        "\n",
        "* **Adam:** Adam (Adaptive Moment Estimation) is a popular optimizer that combines the benefits of adaptive learning rates and momentum. It is known for its fast convergence and good performance across a wide range of tasks.\n",
        "* **RMSprop:** RMSprop (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm that divides the learning rate by the square root of the exponentially weighted moving average of squared gradients. RMSprop is suitable for dealing with non-stationary objectives in regression tasks.\n",
        "* **SGD with Momentum:** Stochastic Gradient Descent (SGD) with momentum accelerates convergence by adding a fraction of the update from the previous time step. This helps to smooth out variations in the gradient and improve convergence speed.\n",
        "* **Adagrad:** Adagrad is an adaptive learning rate optimization algorithm that scales the learning rate based on the frequency of feature occurrences. It performs well in regression tasks with sparse data or varying feature importance.\n",
        "* **Nadam:** Nadam is an extension of Adam that incorporates the benefits of Nesterov Accelerated Gradient (NAG) into the Adam optimizer. It combines ideas from Adam and NAG to provide faster convergence and better generalization for regression tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCLZFJuYdtp3"
      },
      "source": [
        "## Methods Compared\n",
        "\n",
        "The following image shows an animation of how each of these algorithms train. It can be accessed from here: [https://bit.ly/3kykkbn](https://bit.ly/3kykkbn).\n",
        "\n",
        "![Training Techniques](https://biologicslab.co/BIO1173/images/contours_evaluation_optimizers.gif \"Training Techniques\")\n",
        "\n",
        "Image credits: [Alec Radford](https://scholar.google.com/citations?user=dOad5HoAAAAJ&hl=en)\n",
        "\n",
        "## Specifying the Update Rule in Keras\n",
        "\n",
        "Keras offers various update rules that can be used during the optimization process. Some of the update rules allowed by Keras include:\n",
        "\n",
        "* **Adam (Adaptive Moment Estimation):** A popular optimization algorithm that combines the benefits of momentum and AdaGrad while alleviating their limitations.\n",
        "* **SGD (Stochastic Gradient Descent):** A basic optimization algorithm that updates the model parameters based on the average of the gradients computed from a mini-batch of data.\n",
        "* **RMSprop (Root Mean Square Propagation):** An adaptive learning rate method that divides the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.\n",
        "* **Adagrad (Adaptive Gradient Algorithm):** An adaptive learning rate method that adapts the learning rate for each parameter based on the historical gradients for that parameter.\n",
        "* **Adadelta:** An extension of Adagrad that seeks to alleviate Adagrad's aggressive, monotonically decreasing learning rate by using a running average of parameter updates.\n",
        "* **Adamax:** A variant of the Adam optimizer that replaces the L2 norm with the L infinity norm while updating the parameters.\n",
        "\n",
        "These update rules provide flexibility and options for optimizing neural network models with different characteristics and training requirements. Different update rules may perform better for specific problems or in different network architectures, and Keras allows users to experiment with and choose the most suitable update rule for their training tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6wKr4xDdtp3"
      },
      "source": [
        "# Changing optimizer and learning rates\n",
        "\n",
        "In the last section of this lesson, we look at the effect of changing the optimizer and the learning rate on neural network training.\n",
        "\n",
        "For this section, we will use the Obesity Data Set that we have seen in previous lessons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dexPTyzmdtp3"
      },
      "source": [
        "### **Obesity Data Set**\n",
        "\n",
        "[Obesity Data Set](https://archive.ics.uci.edu/ml/datasets/)\n",
        "\n",
        "\n",
        "![_____](https://biologicslab.co/BIO1173/images/obesity.jpg)\n",
        "\n",
        "**Description:**\n",
        "\n",
        "The Obesity Data Set includes data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. The data contains 17 attributes and 2111 records, the records are labeled with the class variable NObesity (Obesity Level), that allows classification of the data using the values of `Insufficient Weight`, `Normal Weight`, `Overweight Level I`, `Overweight Level II`, `Obesity Type I`, `Obesity Type II` and `Obesity Type III`.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "* **Gender-** Female/Male\n",
        "* **Age-** Numeric value\n",
        "* **Height-** Numeric value in meters\n",
        "* **Weight-** Numeric value in kilograms\n",
        "* **family_history_with_overweight-** Has a family member suffered or suffers from overweight - Yes/No\n",
        "* **FAVC-** Do you eat high caloric food frequently - Yes/No\n",
        "* **FCVC-** Do you usually eat vegetables in your meals - Never/Sometimes/Always\n",
        "* **NCP-** How many main meals do you have daily - Between 1 y 2/Three/More than three\n",
        "* **CAEC-** Do you eat any food between meals? - No/Sometimes/Frequently/Always\n",
        "* **SMOKE-** Do you smoke? - Yes/No\n",
        "* **CH2O-** How much water do you drink daily? - Less than a liter/Between 1 and 2 L/More than 2 L\n",
        "* **SCC-** Do you monitor the calories you eat daily - Yes/No\n",
        "* **FAF-** How often do you have physical activity? - I do not have/1 or 2 days/2 or 4 days/4 or 5 days\n",
        "* **TUE-** How much time do you use technological devices such as cell phone, videogames, television, computer and others - 0–2 hours/3–5 hours/More than 5 hours\n",
        "* **CALC-** How often do you drink alcohol? - I do not drink/Sometimes/Frequently/Always\n",
        "* **MTRANS-** Which transportation do you usually use? Automobile/Motorbike/Bike/Public Transportation/Walking\n",
        "* **NObeyesdad-** Obesity levels: 'Insufficient_Weight', 'Obesity_Type_III', 'Normal_Weight', 'Obesity_Type_II', 'Overweight_Level_I', 'Obesity_Type_I', 'Overweight_Level_II'\n",
        "\n",
        "For our regression neural network, the response variable (Y) will be the column `NObeyesdad`. Since this column contains 7 categorical variables, we are going to map these variables to strings as follows:\n",
        "\n",
        "* 'Insufficient_Weight' = 0\n",
        "* 'Normal_Weight'       = 1\n",
        "* 'Overweight_Level_I'  = 2\n",
        "* 'Overweight_Level_II' = 3\n",
        "* 'Obesity_Type_I'      = 4\n",
        "* 'Obesity_Type_II'     = 5\n",
        "* 'Obesity_Type_III'    = 6\n",
        "\n",
        "In other words, instead of having a response variable that is a continuous numerical value (e.g. 5.321 to 8.786), the response variable for the Obesity Data Set will be a discrete integer, between 0 and 6. As you will see later, this will produce a rather unique regression chart below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muvj-1fSdtp3"
      },
      "source": [
        "### Define Function to Plot Regression Chart\n",
        "\n",
        "The code in the cell below will be used later to generate a Lift Chart for each regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1E3_wNO5dtp3"
      },
      "outputs": [],
      "source": [
        "# Regression chart.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def chart_regression(pred, y, lr, sort=True):\n",
        "    stitle=\"Learning Rate = \" + str(lr)\n",
        "    t = pd.DataFrame({'pred': pred, 'y': y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'], inplace=True)\n",
        "    plt.plot(t['y'].tolist(), label='expected')\n",
        "    plt.plot(t['pred'].tolist(), label='prediction')\n",
        "    plt.ylabel('output')\n",
        "    plt.title(stitle)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu4gTwa4dtp3"
      },
      "source": [
        "### Read in the data\n",
        "\n",
        "The code in the cell below reads in the data from the course HTTPS server and creates a DataFrame called `df`. Assigning `df` as the name of DataFrame is often seen in Python code examples.  \n",
        "\n",
        "Once the data is read and the DataFrame created, the data is shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbst57-Adtp3"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read data and create DataFrame-------------------------------------\n",
        "df = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "#   index_col=0,\n",
        "    sep=',',\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "# Set the random seed to 42\n",
        "np.random.seed(42)\n",
        "\n",
        "# Use random.permutation function for shuffling & reindexing\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "\n",
        "# Set max rows and max columns\n",
        "pd.set_option('display.max_rows', 6)\n",
        "pd.set_option('display.max_columns', 6)  # Zero means all columns\n",
        "\n",
        "# Display DataFrame-------------------------------------------------\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BLaLKfodtp3"
      },
      "source": [
        "If your code is correct your should see something similar to the following table:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04_4_Ob.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N23fnVQ4dtp3"
      },
      "source": [
        "### Preprocess the data\n",
        "\n",
        "The code in the cell below converts categorical variables into numerical values either by (1) mapping string values to integers or (2) using one-hot encoding to create dummy columns. The mapping of the response column `NObeyesdad` to integers `0`-`6` should be noted. Numeric values in the columns `Height` and `Weight` have been standardized to their Z-scores.\n",
        "\n",
        "The numeric values in all of the columns, with the exception of the response column `NObeyesdad`, are used to generate the X feature vector.\n",
        "\n",
        "As a final step, the data is split into training and test sets and the X-values for the first 3 subjects in the test set (`xtest`) are printed out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ6Wq76Xdtp3"
      },
      "outputs": [],
      "source": [
        "# Preprocess data\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Map Gender\n",
        "mapping = {'Male': 1, 'Female': 0}\n",
        "df['Gender'] = df['Gender'].map(mapping)\n",
        "\n",
        "# Map family_history_with_overweight\n",
        "mapping = {'yes': 1, 'no': 0}\n",
        "df['family_history_with_overweight'] = df['family_history_with_overweight'].map(mapping)\n",
        "\n",
        "# Map FAVC\n",
        "mapping = {'yes': 1, 'no': 0}\n",
        "df['FAVC'] = df['FAVC'].map(mapping)\n",
        "\n",
        "# Map SMOKE\n",
        "mapping = {'yes': 1, 'no': 0}\n",
        "df['SMOKE'] = df['SMOKE'].map(mapping)\n",
        "\n",
        "# Map SCC\n",
        "mapping = {'yes': 1, 'no': 0}\n",
        "df['SCC'] = df['SCC'].map(mapping)\n",
        "\n",
        "# Map NObeyesdad\n",
        "mapping2 = {'Insufficient_Weight': 0,\n",
        "            'Normal_Weight': 1,\n",
        "            'Overweight_Level_I': 2,\n",
        "            'Overweight_Level_II': 3,\n",
        "            'Obesity_Type_I': 4,\n",
        "            'Obesity_Type_II': 5,\n",
        "            'Obesity_Type_III': 6}\n",
        "\n",
        "df['NObeyesdad'] = df['NObeyesdad'].map(mapping2)\n",
        "\n",
        "# Generate dummies for CAEC\n",
        "df = pd.concat([df,pd.get_dummies(df['CAEC'],prefix=\"CAEC\")],axis=1)\n",
        "df.drop('CAEC', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for CALC\n",
        "df = pd.concat([df,pd.get_dummies(df['CALC'],prefix=\"CALC\")],axis=1)\n",
        "df.drop('CALC', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for MTRANS\n",
        "df = pd.concat([df,pd.get_dummies(df['MTRANS'],prefix=\"MTRANS\")],axis=1)\n",
        "df.drop('MTRANS', axis=1, inplace=True)\n",
        "\n",
        "# Standardize ranges\n",
        "df['Height'] = zscore(df['Height'])\n",
        "df['Weight'] = zscore(df['Weight'])\n",
        "\n",
        "# Generate X feature vector\n",
        "x_columns = df.columns.drop('NObeyesdad')\n",
        "x = df[x_columns].values\n",
        "x = np.asarray(x).astype('float32')\n",
        "\n",
        "# Generate Y feature vector\n",
        "y = df['NObeyesdad'].values\n",
        "y = np.asarray(y).astype('float32')\n",
        "\n",
        "# Split data\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.25, random_state=408)\n",
        "\n",
        "# Print\n",
        "print (y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMbUVwFZdtp4"
      },
      "source": [
        "If your code is correct you should see something similar to the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPNZ8XS0dtp4"
      },
      "source": [
        "~~~text\n",
        "[1. 5. 6. 2. 2. 3. 6. 5. 0. 3. 6. 6. 3. 4. 6. 0. 5. 4. 2. 3. 6. 3. 2. 4.\n",
        " 3. 1. 0. 0. 2. 6. 3. 5. 1. 5. 1. 5. 6. 6. 6. 6. 1. 0. 3. 5. 3. 1. 5. 4.\n",
        " 6. 4. 5. 1. 1. 1. 2. 3. 4. 5. 1. 4. 3. 2. 3. 5. 0. 0. 4. 5. 0. 3. 1. 1.\n",
        " 4. 6. 2. 1. 0. 2. 3. 1. 3. 0. 4. 0. 3. 5. 1. 4. 4. 1. 4. 3. 6. 3. 4. 5.\n",
        " 1. 4. 6. 1. 3. 5. 3. 0. 3. 3. 6. 6. 4. 6. 3. 6. 6. 0. 5. 6. 5. 3. 4. 2.\n",
        " 3. 6. 1. 5. 3. 4. 5. 4. 1. 0. 1. 4. 5. 2. 2. 5. 6. 6. 3. 2. 2. 1. 5. 3.\n",
        " 0. 4. 3. 2. 0. 4. 3. 3. 4. 6. 6. 0. 6. 2. 0. 0. 3. 4. 4. 1. 3. 3. 4. 5.\n",
        " 0. 6. 4. 0. 1. 2. 1. 0. 6. 0. 2. 1. 6. 1. 3. 0. 6. 5. 0. 2. 4. 6. 3. 3.\n",
        " 2. 3. 6. 4. 1. 5. 6. 1. 5. 5. 1. 5. 4. 5. 6. 3. 3. 5. 6. 6. 0. 2. 5. 3.\n",
        " 4. 6. 6. 4. 4. 3. 4. 5. 1. 4. 6. 5. 4. 3. 4. 3. 4. 4. 5. 2. 0. 2. 2. 6.\n",
        " 4. 0. 0. 1. 2. 4. 4. 1. 6. 6. 5. 2. 6. 3. 4. 1. 5. 0. 4. 6. 5. 4. 2. 5.\n",
        " 4. 6. 6. 0. 4. 6. 1. 3. 4. 6. 5. 0. 5. 5. 1. 6. 6. 4. 5. 1. 6. 4. 2. 0.\n",
        " 1. 5. 6. 6. 1. 3. 3. 6. 4. 1. 3. 0. 5. 3. 4. 4. 3. 3. 1. 2. 5. 3. 3. 6.\n",
        " 0. 2. 4. 4. 3. 6. 3. 5. 1. 4. 0. 2. 4. 5. 6. 3. 2. 2. 2. 2. 2. 1. 5. 2.\n",
        " 5. 4. 0. 5. 3. 5. 5. 2. 0. 5. 4. 4. 4. 3. 1. 6. 5. 4. 0. 3. 6. 0. 4. 3.\n",
        " 2. 2. 4. 4. 1. 5. 2. 0. 2. 4. 4. 4. 1. 0. 6. 1. 0. 3. 0. 3. 6. 1. 5. 4.\n",
        " 2. 6. 3. 1. 2. 3. 3. 6. 1. 1. 2. 1. 5. 6. 1. 1. 2. 5. 4. 6. 4. 1. 1. 5.\n",
        " 2. 1. 6. 4. 6. 5. 2. 2. 0. 0. 1. 6. 4. 0. 2. 1. 4. 0. 4. 6. 2. 4. 2. 0.\n",
        " 4. 4. 4. 1. 6. 4. 1. 6. 5. 2. 3. 2. 5. 1. 1. 0. 3. 6. 4. 1. 6. 4. 2. 6.\n",
        " 0. 4. 6. 4. 0. 2. 0. 3. 4. 1. 2. 5. 1. 2. 5. 1. 5. 1. 3. 5. 0. 4. 4. 5.\n",
        " 2. 0. 4. 6. 0. 2. 0. 3. 0. 4. 2. 0. 1. 2. 0. 2. 4. 4. 4. 5. 6. 2. 0. 1.\n",
        " 2. 0. 2. 1. 2. 2. 1. 3. 1. 1. 4. 3. 2. 6. 6. 5. 2. 3. 3. 5. 4. 3. 4. 4.]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akgqll4Xdtp4"
      },
      "source": [
        "The above array represent **all** of the actual Y values in the test data set. As you can see, the Y-values are discrete integers ranging from `0` for `Insufficient_Weight` to `6` for 'Obesity_Type_III'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TakaqB0mdtp4"
      },
      "source": [
        "### Example 1: Optimizer = Adam, Learning rate = 0.01\n",
        "\n",
        "The code in the cell below constructs a regression neural network with the `Adam` optimizer and a learning rate = 0.01. The learning rate is set with the variable `lrate` at the start of the code cell. The `Adam` optimizer is set in the compiler with the following line of code:\n",
        "~~~text\n",
        "optimizer=keras.optimizers.Adam(learning_rate=lrate)`\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": true,
        "id": "c_1QOg19dtp4",
        "outputId": "f37a5457-8ced-4238-817e-e6e587341da5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING MODEL...Please be patient!\n",
            "The variable verbose=0 so you won't see any output until the training is completed.\n",
            "Epoch 56: early stopping\n",
            "Restoring model weights from the end of the best epoch: 46.\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Example 1: Optimizer = adam, Learning rate = 0.01\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Nadam\n",
        "from keras.metrics import Precision, Recall\n",
        "\n",
        "# Set learning rate\n",
        "lrate = 0.01\n",
        "\n",
        "# Construct model\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(x.shape[1],)))  # Input\n",
        "model.add(Dense(25, activation='relu')) # Hidden 1\n",
        "model.add(Dense(10, activation='relu')) # Hidden 2\n",
        "model.add(Dense(1)) # Output\n",
        "\n",
        "# Compile with DIFFERENT optimizers\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=Adam(learning_rate=lrate)\n",
        "             )\n",
        "\n",
        "# Create monitor\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10,\n",
        "                        verbose=1, mode='auto', restore_best_weights=True)\n",
        "\n",
        "print(\"TRAINING MODEL...Please be patient!\")\n",
        "print(\"The variable verbose=0 so you won't see any output until the training is completed.\")\n",
        "\n",
        "# Fit model\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
        "          callbacks=[monitor],verbose=0,epochs=1000)\n",
        "\n",
        "# Plot the chart\n",
        "pred = model.predict(x_test)\n",
        "chart_regression(pred.flatten(),y_test, lrate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnGfGGgrdtp4"
      },
      "source": [
        "If your code is correct you should see something similar the following Lift Chart.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04_Exe1.png)\n",
        "\n",
        "The solid blue line are the actual Y-values of **all** subjects in the test set (_n_=530). The values have been sorted from lowest to highest and then plotted from left to right. There are exactly `7` \"steps\" corresponding to the `7` categorical levels in the response variable `NObeyesdad`. For example, the first \"step\" at Y=0, are the approximately 75 subjects in the test set who were classified as being 'Insufficient_Weight'. The last, and highest step, at Y=6, correspond to the 75 test subjects who were classified as being `Obesity_Type_III`.\n",
        "\n",
        "The \"jaggy\" orange line shows the model's corresponding predicted value for each test subject. By inspection you can see that the model predicted Y-values follow the stair-step increase in the actual Y-values, albeit in a rather noisy fashion.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_M0k62hdtp4"
      },
      "source": [
        "### **Exercise 1: Optimizer = Adam, Learning rate = 0.1**\n",
        "\n",
        "In the cell below, write the code to repeat Example 1 again using the `Adam` optimizer, but this time increase the learing rate to 0.1.\n",
        "\n",
        "Set `verbose=0` when you fit the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc4a5Znmdtp4"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I73HMoL3HegP"
      },
      "source": [
        "If your code is correct you should see something similiar the following Lift Chart.\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image01.png)\n",
        "\n",
        "Increasing the Learning Rate from 0.01 to 0.1 may (or may not) prevent training. In the Lift Chart shown above, the result looks similar to the result for Example 1 with the Adam optimizer and a learning rate set to 0.1.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drcYEARHdtp4"
      },
      "source": [
        "Or if your code is correct you might see something similiar the following Lift Chart.\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_04_Exe2A.png)\n",
        "\n",
        "Increasing the Learning Rate from 0.01 to 0.1 may (or may not) prevent training.\n",
        "\n",
        "In the Lift Chart shown above, the model simply predicted 3.1519866, basically the average (mean) value between 0 and 6, for all subjects in the test set.\n",
        "\n",
        "When the learning rate is set too high any one of the following might occur:\n",
        "* The training process may become unstable and the model may fail to converge to an optimal solution.\n",
        "* The model may overshoot the minimum and start oscillating or diverging.\n",
        "* The updates to the weights may be too large, causing the model to skip over the optimal solution.\n",
        "* The loss function may fluctuate significantly, making it difficult to determine if the model is improving.\n",
        "* The model may never reach a good solution and perform poorly on new data.\n",
        "\n",
        "It is also possible that you could get a similar result as that shown in Example 1 with the Adam optimizer and a learning rate set to 0.1.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs-YPfSjdtp5"
      },
      "source": [
        "### **Exercise 2: Optimizer = SGD, Learning rate = 0.001**\n",
        "\n",
        "In the cell below, write the code to repeat Example 1 again but this time using the `SGD` optimizer (Stochastic Gradient Descent) with the learing rate set to 0.001. Just change the word `Adam` to `SGD` in the compiler and the variable `lrate`.\n",
        "\n",
        "Be prepared to wait for the training to complete since 0.001 is a slower rate than before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PowgoDPvdtp5"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9an9gXEdtp5"
      },
      "source": [
        "If your code is correct you should see something similiar the following Lift Chart.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04_Exe3.png)\n",
        "\n",
        "The model's predictions using the Stocastic Descent Graient (SGD) optimizer appear more \"jagged\" than before with the Adam optimizer. This \"jaggedness\" means the predictions are more noisy, overshooting and undershooting the actual prediction.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HKS5ww9dtp5"
      },
      "source": [
        "### **Exercise 3: Optimizer = SGD, Learning rate = 0.01**\n",
        "\n",
        "In the cell below, write the code to repeat **Exercise 2**, again using the `SGD` optimizer (Stochastic Gradient Descent), but set the learing rate set to 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_k1KND3dtp8"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v74Dpj8zJrlB"
      },
      "source": [
        "If your code is correct you should see something similiar the following Lift Chart.\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image02.png)\n",
        "\n",
        "Increasing the Learning Rate from 0.01 to 0.1 may (or may not) prevent training. In the Lift Chart shown above, the result looks vaguely similar to the result for Example 1 with the Adam optimizer and a learning rate set to 0.1.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyZAJebdtp8"
      },
      "source": [
        "Or, you might see something more similiar the following Lift Chart.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04_Exe4.png)\n",
        "\n",
        "In this example, a learning rate of 0.01 was too fast for the Stochastic Gradient Descent optimizer. Again, all of the the model's predictions were about the average value (3) of the actual Y values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4bL_YRqdtp8"
      },
      "source": [
        "### **Exercise 4: Optimizer = Nadam, Learning rate = 0.01**\n",
        "\n",
        "In the cell below, write the code to repeat **Exercise 2**, again using the `Nadam` optimizer with a learning rate set to 0.01.\n",
        "\n",
        "As explained above, **_Nadam_** is an extension of Adam that incorporates the benefits of Nesterov Accelerated Gradient (NAG) into the Adam optimizer. It combines ideas from Adam and NAG to provide faster convergence and better generalization for regression tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diz2WqC5dtp8"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF7yrdqvdtp8"
      },
      "source": [
        "If your code is correct you should see something similiar the following Lift Chart.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04_Exe5.png)\n",
        "\n",
        "Using the Nadam optimizer doesn't appear to improve the model's predictions based on the \"jagginess\" of the orange line. If anything, the predictions are more noisy. However, it should be keep in mind that the effectiveness of any optimizer depends on the neural network model and the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPJeYeISdtp8"
      },
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_04_4.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}