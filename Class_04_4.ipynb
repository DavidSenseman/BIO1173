{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Introduction to Large Language Models (LLMs)\n",
        "* Part 4.2: Chatbots\n",
        "* Part 4.3: Image Generation with StableDiffusion\n",
        "* **Part 4.4: Agentic AI**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded.\n"
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify GEMINI_API_KEY\n",
        "\n",
        "In order to run the code in this lesson you will need to have your `GEMINI_API_KEY` properly installed in your Colab **Secrets**. Detailed steps for purchasing your `GENAI_API_KEY` and installing it in your Colab notebook Secrets was provided previously in a separate hand-out.\n",
        "\n",
        "Run the code in the next cell to see if your `GEMINI_API_KEY` is installed correctly. Make sure to Grant Access for your notebook to use your API key if Colab requests this action."
      ],
      "metadata": {
        "id": "0UHav6bt1FoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify GEMINI_API_KEY\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "# Check if API key is properly loaded\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    print(\"API key loaded successfully!\")\n",
        "    print(f\"Key length: {len(GOOGLE_API_KEY)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading API key: {e}\")\n",
        "    print(\"Please set your API key in Google Colab:\")\n",
        "    print(\"1. Go to Secrets in the left sidebar\")\n",
        "    print(\"2. Create a new secret named 'GEMINI_API_KEY'\")\n",
        "    print(\"3. Paste your GEMINI API key\")\n"
      ],
      "metadata": {
        "id": "_gXbyT2T2Pt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. You may see this message when you run this cell:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image08C.png)\n",
        "\n",
        "> If you do see this popup just click on `Grant access`.\n",
        "\n",
        "\n",
        "2. If your `GEMINI_API_KEY` is correctly installed you should see something _similar_ to the following output.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image09C.png)\n",
        "\n",
        "3. However, if you see the following output...\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image10C.png)\n",
        "\n",
        "> you will need to correct the error before you can continue. Ask your Instructor or TA for help if you can resolve the error yourself."
      ],
      "metadata": {
        "id": "Jswm7Qbk2W1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List Current Google GenAI Models\n",
        "\n",
        "At this point in time, new generative AI (genAI) models are being released and old models are being retired. This true for all of the major AI providers including Google. The code below lists the current Google AI models that you can access using your `GEMINI_API_KEY`."
      ],
      "metadata": {
        "id": "IpnVVHmG11hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List Current Google GenAI Models\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure with your API key from Colab secrets\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# List all available models with display names\n",
        "print(f\"{'Model Name':<45} {'Display Name'}\")\n",
        "print(\"-\" * 75)\n",
        "for model in genai.list_models():\n",
        "    print(f\"{model.name:<45} {model.display_name}\")\n"
      ],
      "metadata": {
        "id": "jxYlOUck3_oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Google Genai packages\n",
        "\n",
        "Run the next code cell to install the software packages needed for this lesson."
      ],
      "metadata": {
        "id": "NuCbGs2PxxQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install & Configure\n",
        "\n",
        "!pip install -q -U google-genai\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve API Key\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "except Exception as e:\n",
        "    print(\"Error: Please add 'GEMINI_API_KEY' to your Colab Secrets.\")\n",
        "\n",
        "# Initialize Client\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "MODEL_ID = \"gemini-2.0-flash\" # The Agentic Model\n",
        "\n",
        "print(\"Environment Ready.\")"
      ],
      "metadata": {
        "id": "M7YbMPo1B-JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "```text\n",
        "Environment Ready.\n",
        "```\n",
        "\n",
        "Or you might see the following:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image01D.png)\n",
        "\n",
        "Don't worry about the error message."
      ],
      "metadata": {
        "id": "ZZHmF2eywAOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Video**\n",
        "\n",
        "Run the next cell to watch a YouTube video about Agentic AI."
      ],
      "metadata": {
        "id": "d3EASu-A3-Xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"EDb37y_MhRw\"\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\"> </iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "hUNaOu-eNAUq",
        "outputId": "6387eeb9-ae56-4bd9-c259-238fa8c91294"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"560\" height=\"315\"\n",
              "  src=\"https://www.youtube.com/embed/EDb37y_MhRw\"\n",
              "  title=\"YouTube video player\"\n",
              "  frameborder=\"0\"\n",
              "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
              "  allowfullscreen\n",
              "  referrerpolicy=\"strict-origin-when-cross-origin\"> </iframe>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Agentic AI**\n",
        "\n",
        "**Agentic AI** represents a fundamental shift in how artificial intelligence systems interact with the world. Unlike traditional AI models that respond to prompts with static outputs, agentic AI systems can autonomously plan, reason, and take actions to accomplish complex goals. These systems don't just answer questions—they actively work toward objectives, breaking down tasks, using tools, and adapting their approach based on feedback.\n",
        "\n",
        "### **What Makes AI \"Agentic\"?**\n",
        "\n",
        "At its core, agentic AI is characterized by several key capabilities. First, these systems exhibit goal-directed behavior, meaning they can take a high-level objective and determine the steps needed to achieve it. Second, they possess the ability to use tools—whether that's browsing the web, executing code, reading files, or interacting with APIs. Third, agentic systems demonstrate iterative reasoning, allowing them to evaluate their progress, recognize when something isn't working, and adjust their strategy accordingly.\n",
        "\n",
        "Consider the difference between asking a traditional chatbot to help you research a topic versus asking an agentic system. The chatbot provides information from its training data. The agentic system might search the web for recent sources, synthesize findings across multiple articles, organize the information into a structured document, and save it for you—all from a single request.\n",
        "\n",
        "### **Why Agentic AI Matters**\n",
        "\n",
        "The importance of agentic AI lies in its potential to dramatically expand what humans can accomplish with AI assistance. Rather than serving as sophisticated autocomplete tools, agentic systems function more like capable assistants that can handle multi-step workflows independently.\n",
        "\n",
        "For individuals, this means being able to delegate tedious, time-consuming tasks that previously required manual effort across multiple applications. For organizations, agentic AI opens possibilities for automating complex business processes that involve judgment calls and dynamic decision-making—tasks that rule-based automation could never handle.\n",
        "\n",
        "The technology also matters because it represents a more natural way for humans to work with AI. Instead of crafting perfect prompts and manually orchestrating a sequence of interactions, users can express their intent at a higher level and trust the system to figure out the details.\n",
        "\n",
        "### **Challenges and Considerations**\n",
        "\n",
        "Of course, increased autonomy brings increased responsibility. Agentic AI systems must be designed with robust safety measures, clear boundaries, and appropriate human oversight. Questions of trust, verification, and control become paramount when AI systems can take actions in the real world. The most effective agentic systems are those that balance capability with transparency—powerful enough to be genuinely useful, but designed to keep humans informed and in control.\n",
        "\n",
        "### **Looking Ahead**\n",
        "\n",
        "Agentic AI is still in its early stages, but its trajectory suggests a future where AI becomes less of a tool we use and more of a collaborator we work alongside. As these systems become more capable and more widely deployed, they have the potential to reshape how we approach work, creativity, and problem-solving. Understanding what agentic AI is and how it operates is increasingly essential for anyone looking to leverage the next generation of artificial intelligence."
      ],
      "metadata": {
        "id": "-bVqYX94vbgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **AI Agents**\n",
        "\n",
        "In this lesson, we are moving beyond simple \"chatbots\" to explore **AI Agents**. While a standard Large Language Model (LLM) like ChatGPT waits for you to ask a question, an **AI Agent** is designed to act, reason, and complete multi-step goals with minimal human intervention.\n",
        "\n",
        "### **What is an AI Agent?**\n",
        "\n",
        "An AI Agent is an autonomous system that can **perceive** its environment, **reason** about how to achieve a goal, and **execute** actions using external tools (like Python scripts, web searches, or database queries).\n",
        "\n",
        "In the context of Python programming, think of an agent as a \"wrapper\" around an LLM that gives it:\n",
        "* **Planning:** The ability to break a complex prompt (e.g., \"Find the IL10 gene sequence and check for common mutations\") into smaller sub-tasks.\n",
        "* **Tool Use:** The ability to write and run code, call APIs (like NCBI or PubMed), or read local files.\n",
        "* **Memory:** The ability to remember what worked or failed in previous steps to refine its strategy.\n",
        "\n",
        "### **Why are AI Agents Important for Science?**\n",
        "\n",
        "For biology and pre-med students, AI agents are becoming \"digital lab partners.\" They are important because:\n",
        "\n",
        "* **Handling Complexity:** Biological data is massive and fragmented (genomics, proteomics, clinical notes). Agents can bridge these silos.\n",
        "* **Reproducibility:** An agent can document every step of a data analysis pipeline, ensuring the exact same code is used every time.\n",
        "* **Efficiency:** They automate repetitive tasks—like literature reviews or data cleaning—freeing up researchers to focus on hypothesis generation.\n",
        "\n",
        "### **Potential Use Cases in Biology & Medicine**\n",
        "\n",
        "As you learn to code in Python, you can build agents to handle the following scenarios:\n",
        "\n",
        "| Use Case | How the Agent Works | Python Tools Involved |\n",
        "| :--- | :--- | :--- |\n",
        "| **Literature Review** | Scans PubMed for papers on a specific protein, summarizes findings, and flags conflicting results. | `Biopython`, `Bio.Entrez`, `LangChain` |\n",
        "| **Genomic Assistant** | Fetches DNA sequences from NCBI, runs a Python script to calculate GC content, and searches for specific motifs. | `pandas`, `Biopython`, `NCBI API` |\n",
        "| **Clinical Decision Support** | Analyzes patient lab results against clinical guidelines and suggests potential diagnoses for review. | `Pydantic`, `JSON`, `scikit-learn` |\n",
        "| **Lab Automation** | Connects to lab hardware to monitor a PCR run and sends alerts if parameters fluctuate. | `pyserial`, `Flask`, `OpenCV` |\n",
        "\n",
        "### **Key Concept: The \"Reasoning\" Loop**\n",
        "\n",
        "When you write Python code for an agent, you aren't just writing a linear script. You are building a **loop**:\n",
        "1.  **Observation:** The agent reads the current state (e.g., \"The data file has missing values\").\n",
        "2.  **Thought:** The agent decides what to do (e.g., \"I should use the `pandas` mean-imputation method\").\n",
        "3.  **Action:** The agent writes and executes the Python code.\n",
        "4.  **Evaluation:** The agent checks if the code worked and decides the next step.\n"
      ],
      "metadata": {
        "id": "d-YYllAOG4rC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: The Literature Agent (Reasoning)**\n",
        "\n",
        "### The Agent as a Reasoning Engine\n",
        "\n",
        "An agent isn't just a chatbot that mimics human conversation; it functions as a **reasoning engine**. Unlike a simple search engine that returns a list of links, an agent can:\n",
        "* **Synthesize:** Combine multiple complex concepts into a unified explanation.\n",
        "* **Contextualize:** Tailor its explanation based on the specific audience (e.g., adapting the technical depth for a pre-med student vs. a PhD researcher).\n",
        "* **Structure:** Organize biological pathways into logical steps (Input $\\rightarrow$ Mechanism $\\rightarrow$ Result).\n",
        "\n",
        "In this example, we will act as a **\"Literature Review Agent\"**. Its goal is to synthesize the complex molecular machinery of CRISPR-Cas9 into a clear, academic summary.\n",
        "\n",
        "### Python Implementation: The Mechanism Explainer\n",
        "Below is the code to initialize the agent's prompt. Notice how we provide the agent with a **Persona** (\"Biology Professor Agent\") and a **Specific Task Structure** to guide its reasoning.\n"
      ],
      "metadata": {
        "id": "NBnB-qTE--mc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Literure Agent\n",
        "\n",
        "The code in the cell below shows how to create a literature agent"
      ],
      "metadata": {
        "id": "4SxBvjDLjP04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Literature Agent\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, Markdown\n",
        "from google import genai\n",
        "\n",
        "# Get API key from Colab secrets\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Define the client\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Set the model\n",
        "MODEL_ID = \"gemini-2.0-flash\"  # Fast and capable\n",
        "\n",
        "# Create the prompt\n",
        "prompt = \"\"\"\n",
        "You are a Biology Professor Agent.\n",
        "Explain the mechanism of CRISPR-Cas9 gene editing.\n",
        "Focus on:\n",
        "1. The role of gRNA.\n",
        "2. The role of the Cas9 enzyme.\n",
        "3. The outcome of the double-strand break (NHEJ vs HDR).\n",
        "\n",
        "Keep it concise (under 500 words) and suitable for a biology undergraduate.\n",
        "\"\"\"\n",
        "\n",
        "# Send the prompt to the LLM\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "# Render the formatted response\n",
        "display(Markdown(response.text))\n"
      ],
      "metadata": {
        "id": "vnZo0nlE49sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image19D.png)\n"
      ],
      "metadata": {
        "id": "oTGf-Red7LDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: The Vaccine Mechanism**\n",
        "\n",
        "In the cell below, write the code to generate a literature agent. Copy the code above.\n",
        "\n",
        "Change the prompt to explain\n",
        "```text\n",
        "the mechanism of mRNA vaccines like Pfizer/Moderna.\n",
        "```\n",
        "Also tell your Literature Agent to:\n",
        "```text\n",
        "Focus on:\n",
        "1. Lipid Nanoparticles entry\n",
        "2. Translation of Spike Protein\n",
        "3. Immune recognition.\n",
        "```\n",
        "Keep the same \"Biology Professor Agent\" persona and cap the output at 500 words."
      ],
      "metadata": {
        "id": "HJgX3nyj7zCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n"
      ],
      "metadata": {
        "id": "Aau7fw6mz0ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image20D.png)\n"
      ],
      "metadata": {
        "id": "elP9c0Js07gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: The Vision Agent (Multimodal Analysis)**\n",
        "\n",
        "### **Concept: What is \"Multimodal\" AI?**\n",
        "\n",
        "In the biological sciences, much of our data isn't text—it's visual. We look at Western blots, histological slides, and complex pathway diagrams. \"Multimodal\" means the agent can \"see\" and process visual information alongside text.\n",
        "\n",
        "A **Vision Agent** doesn't just recognize an image; it can **reason** about it. For example, instead of just saying \"this is a cell,\" it can identify the phase of mitosis or locate a specific organelle based on its morphology.\n",
        "\n",
        "### Why Vision Agents Matter for Bio/Pre-Med Students\n",
        "* **Microscopy Automation:** Agents can assist in identifying cell types or counting colonies in a Petri dish.\n",
        "* **Diagram Interpretation:** They can translate a complex metabolic pathway diagram into a step-by-step summary or a Python dictionary of proteins.\n",
        "* **Clinical Diagnostics:** In a pre-med context, agents can be trained to flag anomalies in X-rays or identify specific pathologies in tissue biopsies.\n"
      ],
      "metadata": {
        "id": "aLwTpl8G-u9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Vision Agent (Multimodal Analysis)\n",
        "\n",
        "\n",
        "The code in the cell below uses a Vision Agent to analyze a histological image of a blood sample containing a white blood cell. The agent is asked to identify which cells are normal red blood cells (RBCs) and which type of white blood cells does it see.\n"
      ],
      "metadata": {
        "id": "_tQW2l57_VS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Vision Agent (Multimodal Analysis)\n",
        "\n",
        "import PIL.Image\n",
        "import PIL.ImageDraw\n",
        "import PIL.ImageFont\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "from google.genai import types\n",
        "from google import genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# API Setup\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "MODEL_ID = \"gemini-2.0-flash\"\n",
        "\n",
        "# SETUP ----------------------\n",
        "\n",
        "class BoundingBox(BaseModel):\n",
        "    ymin: int\n",
        "    xmin: int\n",
        "    ymax: int\n",
        "    xmax: int\n",
        "\n",
        "class DetectedObject(BaseModel):\n",
        "    label: str\n",
        "    box: BoundingBox\n",
        "\n",
        "class DetectionResult(BaseModel):\n",
        "    objects: List[DetectedObject]\n",
        "\n",
        "def draw_labels_on_image(image_input, detection_result: DetectionResult):\n",
        "     annotated_image = image_input.copy()\n",
        "     draw = PIL.ImageDraw.Draw(annotated_image)\n",
        "     width, height = annotated_image.size\n",
        "\n",
        "     try:\n",
        "         font = PIL.ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\", 18)\n",
        "     except IOError:\n",
        "         font = PIL.ImageFont.load_default()\n",
        "\n",
        "     for obj in detection_result.objects:\n",
        "         # CONVERT 0-1000 SCALE TO PIXELS\n",
        "         y1 = (obj.box.ymin / 1000.0) * height\n",
        "         x1 = (obj.box.xmin / 1000.0) * width\n",
        "         y2 = (obj.box.ymax / 1000.0) * height\n",
        "         x2 = (obj.box.xmax / 1000.0) * width\n",
        "\n",
        "         # Colors\n",
        "         color = \"#00FF00\"  # Green\n",
        "         if \"nucleus\" in obj.label.lower():\n",
        "             color = \"#FFFF00\"  # Yellow for Nucleus\n",
        "         if \"rbc\" in obj.label.lower():\n",
        "             color = \"#FF0000\"  # Red for RBC\n",
        "\n",
        "         # Draw Box\n",
        "         draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
        "\n",
        "         # Draw Label\n",
        "         text = obj.label\n",
        "         text_pos = (x1, y1 - 22)\n",
        "         if text_pos[1] < 0:\n",
        "             text_pos = (x1, y1 + 5)\n",
        "\n",
        "         bbox = draw.textbbox(text_pos, text, font=font)\n",
        "         draw.rectangle(bbox, fill=color)\n",
        "         draw.text(text_pos, text, fill=\"black\", font=font)\n",
        "\n",
        "     return annotated_image\n",
        "\n",
        "# EXECUTION --------------\n",
        "\n",
        "# Load Image\n",
        "url = \"https://biologicslab.co/BIO1173/images/class_04/Neutrophil.jpg\"\n",
        "response = requests.get(url)\n",
        "original_image = PIL.Image.open(BytesIO(response.content))\n",
        "\n",
        "print(\"--- Original Image ---\")\n",
        "display(original_image)\n",
        "\n",
        "# REFINED PROMPT ---------------\n",
        "prompt = \"\"\"\n",
        "Analyze this microscopy image.\n",
        "Detect the following objects:\n",
        "\n",
        "1. **Neutrophil**: The entire central white blood cell (including the cytoplasm).\n",
        "2. **RBC**: Identify 3 surrounding red blood cells (pink circles).\n",
        "\n",
        "**FORMAT INSTRUCTIONS:**\n",
        "- Return bounding boxes using the **0 to 1000 integer scale**.\n",
        "- ymin=0 is top, ymax=1000 is bottom.\n",
        "- xmin=0 is left, xmax=1000 is right.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Analyzing... ---\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[original_image, prompt],\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=DetectionResult\n",
        "    )\n",
        ")\n",
        "\n",
        "try:\n",
        "    detection_data = DetectionResult.model_validate_json(response.text)\n",
        "    final_image = draw_labels_on_image(original_image, detection_data)\n",
        "    print(\"--- Labeled Result ---\")\n",
        "    display(final_image)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(response.text)\n"
      ],
      "metadata": {
        "id": "s9t-ARRdAZ3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image04D.png)\n"
      ],
      "metadata": {
        "id": "T_idGQRH2t6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Cell Analysis**\n",
        "\n",
        "In the cell below write the code to analyze a photomicrograph of a nerve cell that you can download from this URL: (\"https://biologicslab.co/BIO1173/images/class_04/Neuron.jpg\").\n",
        "\n",
        "Detect these objects in the photomicrograph of the nerve cell:\n",
        "\n",
        "1. **Nucleus**: The dark spot inside the central cell body.\n",
        "2. **Axon**: The largest process leaving the cell body.\n",
        "3. **Dendrites**: Identify 3 smaller processes leaving the cell body."
      ],
      "metadata": {
        "id": "9RGbrpkf_VS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oLbo0slE3JSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image05D.png )\n"
      ],
      "metadata": {
        "id": "y6o4OTh0_VTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: The Clinical Tool Agent (Function Calling)**\n",
        "\n",
        "\n",
        "### **Concept: Giving the Agent a \"Tool Belt\"**\n",
        "\n",
        "While Large Language Models (LLMs) are incredibly smart at language, they are notoriously unreliable at precise mathematics or real-time data retrieval—they \"hallucinate\" numbers. **Function Calling** (or \"Tool Use\") solves this by allowing the agent to stop and say: *\"I don't know the math, but I have a Python function that does.\"*\n",
        "\n",
        "In this scenario, the agent acts as the **Brain**, and the Python function acts as the **Calculator**.\n",
        "\n",
        "### Why Function Calling is Vital in Medicine\n",
        "Precision is non-negotiable in clinical settings. You cannot rely on an LLM to \"guess\" a drug dosage based on body weight.\n",
        "* **Safe Calculations:** The agent identifies the user's intent, extracts the variables (like weight or age), and passes them to a hard-coded, verified Python function.\n",
        "* **Database Access:** An agent can use a tool to look up the most recent drug-drug interaction data from a trusted medical database.\n",
        "* **Standardization:** It ensures that every student or clinician using the tool gets the same mathematical result every time.\n",
        "\n"
      ],
      "metadata": {
        "id": "KYJWJ4Ye-Jcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: AI Agent Tool Use\n",
        "\n",
        "For Example 3. we define a `tool` for our AI Agent to use. In this case, we construct a simple Python function to compute a person's `Body Mass Index` )BMI), and the let our Agent decide when to use it.\n"
      ],
      "metadata": {
        "id": "mKGjNmbe7DWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3:\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# API Setup\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "MODEL_ID = \"gemini-2.0-flash\"\n",
        "\n",
        "# 1. Define the tool (Python Function)\n",
        "def calculate_bmi(weight_kg: float, height_m: float) -> float:\n",
        "    \"\"\"Calculates Body Mass Index (BMI).\"\"\"\n",
        "    return weight_kg / (height_m ** 2)\n",
        "\n",
        "# 2. Create the Agent with the tool\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"My patient is 1.75 meters tall and weighs 70 kg. \"\n",
        "             \"What is their BMI and is it healthy?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[calculate_bmi]\n",
        "    )\n",
        ")\n",
        "\n",
        "# 3. Display the result as formatted Markdown\n",
        "display(Markdown(\"### Clinical Agent Response\"))\n",
        "display(Markdown(response.text))\n"
      ],
      "metadata": {
        "id": "OYJL5eu1Ew3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "<span style=\"font-family: sans-serif; padding: 10px; display: block;\">\n",
        "<b>Clinical Agent Response</b><br>\n",
        "The patient's BMI is 22.86. A BMI between 18.5 and 24.9 is considered healthy. So, the patient has a healthy BMI.\n",
        "</span>\n",
        "\n",
        "You should note that the code in this example did _not_ provide any information about the health status of any particular BMI value. Our AI Agent's response that the patient was \"healthy\" was based on information that our Agent accessed from the LLM (`gemini-2.0-flash`). While an AI Agent might not be that good at doing `math`, they tend to be pretty good at getting basic information from a LLM.\n"
      ],
      "metadata": {
        "id": "FYDQui1qI16p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Kidney Function Tool (Creatinine Clearance)**\n",
        "\n",
        "\n",
        "In the cell below, write the code to create a kidney function tool that can compute creatinine clearance. Then let the Agent use the tool for its analysis of this prompt:\n",
        "\n",
        "```python\n",
        "# 2. Create the Agent with the tool\n",
        "# We pass the function to the 'tools' parameter in the config\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate the creatinine clearance for a 60-year-old male patient weighing 72kg with a serum creatinine of 1.2 mg/dL.\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[calculate_creatinine_clearance]\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Here is the code for genrating the tool:\n",
        "\n",
        "```python\n",
        "def calculate_creatinine_clearance(age: int, weight_kg: float, serum_creatinine: float) -> float:\n",
        "    \"\"\"\n",
        "    Calculates Creatinine Clearance (CrCl) using the Cockcroft-Gault formula.\n",
        "    Formula: ((140 - age) * weight_kg) / (72 * serum_creatinine)\n",
        "    \"\"\"\n",
        "    numerator = (140 - age) * weight_kg\n",
        "    denominator = 72 * serum_creatinine\n",
        "    return numerator / denominator\n",
        "```\n",
        "\n",
        "Task: Copy the code above. Replace the BMI tool with a tool to estimate Creatinine Clearance (Cockcroft-Gault formula). Formula: ((140 - age) * weight_kg) / (72 * serum_creatinine) Prompt: \"Calculate the creatinine clearance for a 60-year-old male patient weighing 72kg with a serum creatinine of 1.2 mg/dL.\"\n"
      ],
      "metadata": {
        "id": "zYQ2KcpWSQL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gCZLm1vgG3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "<span style=\"font-family: sans-serif; padding: 10px; display: block;\">\n",
        "<b>Clinical Agent Response</b><br>\n",
        "The creatinine clearance for this patient is approximately 66.67 mL/min.\n",
        "Generally, creatinine clearance values above 60 mL/min are considered normal. A value of 66.67 mL/min suggests mild renal dysfunction.\n",
        "</span>\n",
        "\n",
        "Again, your AI Agent went to the LLM to see the health status of the patient's creatine clearance value."
      ],
      "metadata": {
        "id": "lNRHhCB2SQL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: The Research Agent (Grounding)**\n",
        "\n",
        "### **Solving the \"Hallucination\" Problem**\n",
        "\n",
        "Large Language Models (LLMs) are like extremely well-read students who sometimes misremember facts. Because they predict the \"next most likely word,\" they can occasionally invent a convincing-sounding but entirely fake citation or drug side effect. This is called **hallucination**.\n",
        "\n",
        "**Grounding** is the process of anchoring an agent’s responses to a \"Source of Truth\"—in this case, the live internet via **Google Search**. Instead of relying solely on its internal training data, the agent becomes a researcher: it searches for information, reads the top results, and cites its sources.\n",
        "\n",
        "#### **Why Grounding is Essential for Biology & Medicine**\n",
        "In a field where information updates daily (e.g., new COVID-19 variants or FDA drug approvals), an ungrounded model is stuck in the past.\n",
        "* **Real-Time Data:** Fetch the latest clinical trial results from 2024 or 2025.\n",
        "* **Citation & Trust:** Medical students need to see *where* a fact came from (e.g., Mayo Clinic, CDC, or a Nature article).\n",
        "* **Fact-Checking:** If an agent makes a claim about a rare genetic mutation, it can perform a search to verify that the mutation actually exists in the literature.\n",
        "\n",
        "#### **Python Implementation:** The Grounded Medical Researcher\n",
        "In this code, we enable the `Google Search` tool. This allows the agent to generate search queries and read web snippets before answering.\n"
      ],
      "metadata": {
        "id": "kGvwNhffKAuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Grounded Medical Researcher\n",
        "\n",
        "For Example 4, we will ask the \"Grounded Medical Reseracher\" to find current information about the most recent treatments for Alzheimer's disease. To guard against hallucinations, we also ask the agent to cite its sources.\n"
      ],
      "metadata": {
        "id": "KZGoJKQrYqfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Grounded Medical Researcher\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "# API Setup\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "MODEL_ID = \"gemini-2.0-flash\"\n",
        "\n",
        "# 1. Define the Google Search tool correctly using the types module\n",
        "search_tool = types.Tool(\n",
        "    google_search=types.GoogleSearch()\n",
        ")\n",
        "\n",
        "prompt = \"\"\"\n",
        "What are the most recent (2024-2025) FDA-approved treatments for Alzheimer's disease?\n",
        "Please provide the drug names and their primary mechanism of action.\n",
        "Cite your sources with URLs.\n",
        "\"\"\"\n",
        "\n",
        "# 2. Pass the tool inside 'GenerateContentConfig'\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[search_tool]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Render the response with clickable links\n",
        "display(Markdown(response.text))\n"
      ],
      "metadata": {
        "id": "CI9i1vU9J2qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image06D.png)\n",
        "\n",
        "Since we \"grounded\" out agent, we can be more confident in the accuracy of the results compared to simply asking the LLM directly for this information."
      ],
      "metadata": {
        "id": "LTooeodUYqfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Malaria Treatment Search**\n",
        "\n",
        "For **Exercise 4**, copy the code above and change the prompt to research \"Approved Malaria vaccines and their efficacy rates in 2025\".\n",
        "\n",
        "Make sure the agent searches for the most recent R21/Matrix-M data."
      ],
      "metadata": {
        "id": "lxFQkc8yYqfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n"
      ],
      "metadata": {
        "id": "li64vqqYZMmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image07D.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "1TH2gZv4YqfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 5: The Triage Agent (Structured Data Extraction)**\n",
        "\n",
        "\n",
        "### **Turning Messy Notes into Clean Data**\n",
        "\n",
        "In a hospital setting, the most valuable data is often trapped in **unstructured text**—handwritten scrawls, rushed \"doctor's notes,\" or recorded patient intake conversations. While humans can read these, computers cannot easily perform statistics or trigger alerts on them.\n",
        "\n",
        "A **Triage Agent** acts as a sophisticated parser. It uses **Named Entity Recognition (NER)** to scan a block of text, identify clinical entities (like symptoms, medications, or vitals), and organize them into a **JSON (JavaScript Object Notation)** format. JSON is the standard language for modern databases and web applications.\n",
        "\n",
        "### Why Structured Data is a Medical Necessity\n",
        "* **Database Integration:** You can't plot a graph of a patient's blood pressure over time if the values are buried in paragraphs of text.\n",
        "* **Emergency Prioritization:** An agent can instantly extract \"Chest Pain\" and \"Shortness of Breath\" from a note and flag that patient as a \"Level 1\" priority in the ER queue.\n",
        "* **Error Reduction:** By extracting specific dosages and frequencies into structured fields, agents help prevent \"transcription errors\" that occur during manual data entry.\n",
        "\n"
      ],
      "metadata": {
        "id": "aCU9nzw7_s5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Triage Agent\n",
        "\n",
        "This example demostrates how to convert a messy clinical note into a clean data. We used an optical character reader (OCR) could be used to scan the doctor's note and stored it on the course file server at this URL: \"https://biologicslab.co/BIO1173/images/class_04/DoctorNote.png\"\n"
      ],
      "metadata": {
        "id": "R3Am4b3gbEke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: The Medical JSON Generator (Modified for Image Input)\n",
        "\n",
        "import json\n",
        "from IPython.display import Markdown\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# API Setup\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "MODEL_ID = \"gemini-2.0-flash\"\n",
        "\n",
        "# Load the doctor's note image from URL\n",
        "image_url = \"https://biologicslab.co/BIO1173/images/class_04/DoctorNote.png\"\n",
        "\n",
        "# Download the image\n",
        "response_img = requests.get(image_url)\n",
        "img = Image.open(BytesIO(response_img.content))\n",
        "\n",
        "# Display the image to verify it loaded correctly\n",
        "display(img)\n",
        "\n",
        "# We ask for a \"JSON schema\" - a specific structure the agent must follow\n",
        "prompt = \"\"\"\n",
        "Analyze the doctor's note in this image and extract the data into a JSON format.\n",
        "\n",
        "The JSON must include these keys:\n",
        "- 'patient_name': string\n",
        "- 'patient_age': int\n",
        "- 'patient_sex': string\n",
        "- 'address': string\n",
        "- 'date': string\n",
        "- 'prescription_medication': string\n",
        "- 'dosage': string\n",
        "- 'sig': string (instructions)\n",
        "- 'physician_signature': string\n",
        "- 'license_numbers': object (Lic No, PTR No, S2 No)\n",
        "\n",
        "Return ONLY the raw JSON.\n",
        "\"\"\"\n",
        "\n",
        "# Send both the image and prompt to the model\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[prompt, img]\n",
        ")\n",
        "\n",
        "# Parse the string response into a real Python dictionary\n",
        "try:\n",
        "    # Removing potential markdown backticks if the agent included them\n",
        "    clean_json = response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "    data = json.loads(clean_json)\n",
        "\n",
        "    print(\"Successfully Extracted Structured Data:\")\n",
        "    print(json.dumps(data, indent=4))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error parsing JSON: {e}\")\n",
        "    print(f\"Raw response: {response.text}\")\n"
      ],
      "metadata": {
        "id": "mj_Bi-vy-qsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image08D.png)\n",
        "\n",
        "Converting a doctor's handwritten notes into structured JSON output offers several important benefits:\n",
        "\n",
        "1. Interoperability\n",
        "JSON is a universal format that can be read by virtually any electronic health record (EHR) system, database, or healthcare application\n",
        "Enables seamless data exchange between hospitals, clinics, pharmacies, and insurance companies\n",
        "2. Reduced Errors\n",
        "Eliminates misinterpretation of illegible handwriting (a major cause of medical errors)\n",
        "Standardizes terminology and formatting\n",
        "Reduces medication errors from misread prescriptions\n",
        "3. Searchability & Analytics\n",
        "Structured data can be easily searched, filtered, and queried\n",
        "Enables population health analytics (e.g., \"How many patients have diabetes?\")\n",
        "Supports clinical research and trend identification\n",
        "4. Automation\n",
        "Triggers automatic workflows (e.g., prescription sent to pharmacy, lab order submitted)\n",
        "Enables clinical decision support alerts\n",
        "Facilitates automated billing and insurance claims\n",
        "5. Accessibility\n",
        "Multiple providers can access the same structured information instantly\n",
        "Patients can view their records through patient portals\n",
        "Emergency responders can quickly retrieve critical information\n",
        "6. Compliance & Auditing\n",
        "Creates consistent, timestamped records for regulatory compliance\n",
        "Simplifies auditing and quality assurance processes\n",
        "Supports HIPAA documentation requirements\n",
        "\n",
        "\n",
        "This transformation makes the data actionable, shareable, and far less prone to dangerous misinterpretation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2fwBncjq_3js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Medication Extraction**\n",
        "\n",
        "Copy the code above and change the image to \"DoctorNote2.png\"."
      ],
      "metadata": {
        "id": "t1fzOao4XevS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exericse 5 here\n",
        "\n"
      ],
      "metadata": {
        "id": "04yE6MXaBI7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image09D.png)\n",
        "\n",
        "As above, the ability of your AI Agent to convert a hand-written `script` for a prescription into JSON code has a number of important advantages."
      ],
      "metadata": {
        "id": "uyeJK-K0Ccva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 6: Patient Education Assistant**\n",
        "\n",
        "An AI Patient Education Assistant would be useful in a medical environment primarily because it bridges the gap between complex medical jargon and patient understanding, acting as a **translator and facilitator**.\n",
        "\n",
        "Here is a breakdown of its key benefits:\n",
        "\n",
        "##### **1. Overcoming \"Health Literacy\" Gaps**\n",
        "Many patients struggle to understand medical terminology.\n",
        "*   **The Problem:** Doctors often use technical terms (e.g., \"tachycardia,\" \"hypertension\") that patients may nod at but don't actually comprehend.\n",
        "*   **The Solution:** An AI can instantly rephrase these terms into plain English.\n",
        "\n",
        "##### **2. 24/7 Availability**\n",
        "Doctors and nurses are busy and cannot answer every question a patient has while in the exam room.\n",
        "*   **The Problem:** Patients often wait until they get home to ask questions, leading to anxiety.\n",
        "*   **The Solution:** An AI assistant can provide answers immediately after a consultation.\n",
        "\n",
        "##### **3. Empowering Self-Management**\n",
        "Patient education is crucial for chronic disease management.\n",
        "*   **The Problem:** A doctor might explain instructions once, and the patient might forget.\n",
        "*   **The Solution:** An AI can provide reminders, explain side effects, and give step-by-step guides.\n",
        "\n",
        "##### **4. Reducing Anxiety**\n",
        "Fear of the unknown often causes unnecessary ER visits.\n",
        "*   **The Problem:** A vague diagnosis can be terrifying.\n",
        "*   **The Solution:** The AI can provide clear, accurate, and empathetic explanations to demystify the illness.\n",
        "\n",
        "##### **5. Standardization of Information**\n",
        "*   **The Problem:** Medical information can vary depending on the source.\n",
        "*   **The Solution:** An AI can be programmed to strictly follow the latest clinical guidelines.\n",
        "\n",
        "#### **Summary**\n",
        "In short, an AI Patient Education Assistant transforms medical information from a **confusing barrier** into a **useful tool**. It empowers the patient to take an active role in their own health.\n"
      ],
      "metadata": {
        "id": "NSLvXT4MCVVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Patient Explainer\n",
        "\n",
        "The code in the cell below creates an Patient Assistant that generates a small handout explaining \"Hypertension\" to a patient with **low** health literacy."
      ],
      "metadata": {
        "id": "K7lOK0tvClD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Patient Explainer\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "# API Setup\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "MODEL_ID = \"gemini-2.0-flash\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "You are a helpful medical assistant.\n",
        "Create a simple, 3-bullet point handout explaining 'Hypertension' (High Blood Pressure) to a patient with low health literacy.\n",
        "Focus on:\n",
        "1. What it is.\n",
        "2. Why it is dangerous.\n",
        "3. One lifestyle change to fix it.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "# Render the response with clickable links\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "rf5XosQYCv2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image10D.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "KXzGY9tQDHr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6: Patient Explainer**\n",
        "\n",
        "The code in the cell below creates an Patient Assistant that generates a small handout explaining \"Hypertension\" to a patient with **high** health literacy."
      ],
      "metadata": {
        "id": "pU_tnEjUFJVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6 here\n",
        "\n"
      ],
      "metadata": {
        "id": "mVAvpZhGFJVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image11D.png)"
      ],
      "metadata": {
        "id": "yFtcgDScFJVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 7: Chain of Thought (The \"Reasoning\" Engine)**\n",
        "\n",
        "### **\"Think Step-by-Step\"**\n",
        "\n",
        "Standard AI models often provide a final answer immediately. However, for high-stakes biology and medicine, we need to see the **why** and the **how**. **Chain of Thought (CoT)** is a reasoning technique where the agent is forced to decompose a complex problem into intermediate, logical steps before arriving at a conclusion.\n",
        "\n",
        "Think of it as \"showing your work\" on a math test. In medicine, this is the digital equivalent of a **Differential Diagnosis (DDx)**.\n",
        "\n",
        "### **Why CoT is Critical for Differential Diagnosis**\n",
        "\n",
        "Differential diagnosis is the systematic process of distinguishing between two or more conditions that share similar symptoms.\n",
        "1.  **Evidence Summarization:** The agent first identifies pertinent \"positives\" (symptoms present) and \"negatives\" (symptoms absent).\n",
        "2.  **Hypothesis Generation:** It creates a broad list of possible conditions using frameworks like **VINDICATE** (Vascular, Infectious, Neoplastic, etc.).\n",
        "3.  **Iterative Refinement:** It uses logical \"if-then\" reasoning to rule out conditions based on specific data (e.g., \"If the patient has no fever, an infectious cause is less likely\").\n",
        "4.  **Transparency:** If the agent makes a mistake, you can trace the \"Chain\" to see exactly where the logic failed."
      ],
      "metadata": {
        "id": "yuFTA9SfDR1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7: CoT Diagnosis Agent\n",
        "\n",
        "The code in the cell below shows how to create and use a differential diagnosis agent that uses Chain of Thought (CoT) reasoning."
      ],
      "metadata": {
        "id": "yUkuT8ozrCaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: CoT Diagnosis Agent\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# A complex patient presentation\n",
        "case_study = \"\"\"\n",
        "Patient: 58yo Female.\n",
        "Chief Complaint: Sudden onset shortness of breath and sharp chest pain that worsens with deep breaths.\n",
        "History: Recent 12-hour flight from overseas. No history of heart disease.\n",
        "Vitals: HR 115 (Tachycardia), BP 110/70, O2 Sats 91% (Hypoxia).\n",
        "Exam: Right calf is swollen and tender to touch.\n",
        "\"\"\"\n",
        "\n",
        "# We explicitly ask the agent to use 'Chain of Thought'\n",
        "prompt = f\"\"\"\n",
        "You are a Senior Diagnostic Resident. Analyze this case using Chain of Thought reasoning.\n",
        "Case: {case_study}\n",
        "\n",
        "Follow these steps:\n",
        "1. Summarize key clinical findings.\n",
        "2. List 3 potential differential diagnoses (DDx) using the VINDICATE framework.\n",
        "3. Evaluate each DDx based on the evidence (Rule-in/Rule-out).\n",
        "4. Recommend the single most high-yield next diagnostic test.\n",
        "\n",
        "Let's think step by step.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "id": "ENs4g2gGqWdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image12D.png)"
      ],
      "metadata": {
        "id": "ntVO6OMFGMyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7A: CoT Diagnosis Agent**\n",
        "\n",
        "In the cell below create a CoT Diagnosis Agent for a `Senior Pediatric Resident`.\n",
        "\n",
        "Use the following as your case study:\n",
        "\n",
        "```Python\n",
        "# Case study\n",
        "case_study = \"\"\"\n",
        "Patient: 7yo Male.\n",
        "Chief Complaint: Mother reports extreme thirst (polydipsia) and frequent urination (polyuria) for 2 weeks.\n",
        "History: Significant weight loss despite increased appetite. Today, the child is lethargic and breathing deeply/rapidly.\n",
        "Vitals: HR 125, BP 90/60, Temp 98.6F.\n",
        "Exam: \"Fruity\" odor on breath. Abdominal tenderness present.\n",
        "\"\"\"\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "leTNUkZoDcOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 7A here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I20d3Sp0uPqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image13D.png)\n",
        "\n",
        "Your CoT Diagnosis Agent suspects that the patient is experiencing diabetic ketoacidosis (DKA) but wants to measure the patient's blood glucose with a simple fingerstick to determine if he's hyperglycemic. This is an excellent response."
      ],
      "metadata": {
        "id": "frZbw-mUH7uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7B: CoT Diagnosis Agent**\n",
        "\n",
        "Copy the code above and change the symptoms variable to: \"Patient is a 45-year-old female presenting with right upper quadrant abdominal pain that radiates to the shoulder, triggered after eating a fatty meal.\" Goal: See if the agent correctly identifies Cholecystitis (Gallstones) as a primary differential."
      ],
      "metadata": {
        "id": "WSvD62SCD0FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 7B here\n",
        "\n"
      ],
      "metadata": {
        "id": "j2hNrQWBHIYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image14D.png)\n",
        "\n",
        "Again, your CoT Diagnosis Agent did a great job! The agent correctly diagnosed that patient's symptoms were likely caused gall stones (cholelithiasis)."
      ],
      "metadata": {
        "id": "FctgZ_4UILtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 8: Agentic Search (The \"Proactive\" Researcher)**\n",
        "\n",
        "### **Reactive vs. Proactive AI**\n",
        "\n",
        "Most AI interactions are **reactive**: you ask a question, and the AI answers based on what it already knows. **Agentic Search** is **proactive**. When an agent encounters a \"knowledge gap\"—a piece of information it doesn't have or is unsure about—it doesn't guess. Instead, it pauses its reasoning, realizes it needs more data, and takes the initiative to search the web or a database to fill that gap.\n",
        "\n",
        "In science, this is the difference between a student who says \"I don't know\" and a researcher who says \"I don't know yet, let me find out.\"\n",
        "\n",
        "### Why Proactive Search is Essential for Medicine\n",
        "The medical field moves faster than any AI training cycle.\n",
        "* **Emerging Pathogens:** During an outbreak, an agent can proactively look for the latest CDC or WHO situation reports rather than relying on year-old training data.\n",
        "* **Drug Interactions:** If a patient is prescribed a brand-new medication, an agentic search can find the most recent FDA \"black box\" warnings or peer-reviewed studies on contraindications.\n",
        "* **Personalized Medicine:** Agents can search for specific genomic variants in databases like ClinVar to see if a newly discovered mutation has been recently classified as pathogenic.\n"
      ],
      "metadata": {
        "id": "sheE_Kg-EWL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 8: The Proactive Research Agent\n",
        "\n",
        "The code in the cell below shows how to build a Proactive Research Agent to find the latest targeted therapies for a particular genetic mutation."
      ],
      "metadata": {
        "id": "SBQhy5NMw-YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 8: The Proactive Research Agent\n",
        "\n",
        "from IPython.display import Markdown\n",
        "from google.genai import types\n",
        "\n",
        "# Define the search tool (as seen in Part 4)\n",
        "search_tool = types.Tool(\n",
        "    google_search=types.GoogleSearch()\n",
        ")\n",
        "\n",
        "# A prompt that requires 'proactive' discovery of recent events\n",
        "prompt = \"\"\"\n",
        "A patient with a rare GATA3 gene mutation is asking about any\n",
        "newly published (2024-2025) clinical trials for targeted therapies.\n",
        "\n",
        "1. Check for any recent clinical trial announcements.\n",
        "2. Summarize the focus of these trials.\n",
        "3. Provide the ClinicalTrials.gov identifier if available.\n",
        "\"\"\"\n",
        "\n",
        "# The agent perceives the '2024-2025' requirement and chooses to search\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[search_tool]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "eg41OHUZw5Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image15D.png)\n",
        "\n",
        "Imagine that you are an overworked 3rd year medical resident in a large, urban teaching hospital. Do you think you would have the time to perform a literature yourself even using sources like Google Scholar or PubMed to help this patient?"
      ],
      "metadata": {
        "id": "L1bF93rXItmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 8: The Proactive Research Agent**\n",
        "\n",
        "In the cell below write the code to generate a Proactive Research Agent.\n",
        "\n",
        "Use this prompt:\n",
        "\n",
        "```python\n",
        "# Prompt\n",
        "prompt = \"\"\"\n",
        "You are a Clinical Pharmacogenomics Agent.\n",
        "A patient has been identified as a CYP2C19 'Poor Metabolizer' (*2/*2 genotype).\n",
        "\n",
        "1. Search for the most recent (2024-2025) CPIC or FDA clinical guidelines\n",
        "   regarding the use of Clopidogrel (Plavix) for this patient.\n",
        "2. Identify if there are recommended alternative antiplatelet medications\n",
        "   (e.g., Prasugrel or Ticagrelor).\n",
        "3. Briefly explain the 'why'—how does this specific genetic variant\n",
        "   affect the drug's activation?\n",
        "\n",
        "Cite your sources with direct links.\n",
        "\"\"\"\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "pXuV6tMVEhWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 8 here\n",
        "\n"
      ],
      "metadata": {
        "id": "HkgLR4n9xoT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image16D.png)"
      ],
      "metadata": {
        "id": "R8mUO8vUJ03A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 9: Connecting to the \"Source of Truth\"**\n",
        "\n",
        "Why would you **not** want to ask a LLM for a protein sequence? The answer lies in the difference between **Predictive Text** and **Data Retrieval**.\n",
        "\n",
        "#### **Eliminating \"Sequence Hallucinations\"**\n",
        "\n",
        "Standard LLMs (like GPT-4 or Gemini) are trained to predict the most likely \"next word.\" When you ask for a DNA sequence, they often \"invent\" a sequence that *looks* correct but is biologically impossible or contains fatal errors.\n",
        "* **The Risk:** A single nucleotide hallucination can change an entire amino acid, leading to a false research conclusion.\n",
        "* **The Agent Solution:** The agent uses Python to \"handshake\" with the NCBI database. It retrieves the **actual** sequence stored on government servers. It isn't \"guessing\"; it is **reading**.\n",
        "\n",
        "#### **Real-Time \"Grounding\" (2026 and Beyond)**\n",
        "\n",
        "Biological knowledge is not static. The human \"Reference Genome\" is updated (e.g., from GRCh37 to GRCh38), and new gene variants are reclassified as \"Pathogenic\" or \"Benign\" every day.\n",
        "* **The Static Model:** A standard AI's knowledge is frozen in the past (its training cutoff).\n",
        "* **The Agentic Agent:** By using the NCBI API, the agent is **grounded** in the present. If a new paper was published this morning or a sequence was updated an hour ago, the agent will see the most current version.\n",
        "\n",
        "#### **Handling Biological Big Data**\n",
        "\n",
        "The human **DMD** gene (Dystrophin) is over **2.4 million base pairs long**. A standard AI cannot \"read\" or \"remember\" a file that large in its conversation window.\n",
        "* **The Agent Advantage:** The agent uses Python as its \"hands.\" It can:\n",
        "    1. **Download** the massive file in the background.\n",
        "    2. **Scan** for specific motifs (like \"TATA\" boxes) using efficient code.\n",
        "    3. **Summarize** only the critical findings for you.\n",
        "\n",
        "\n",
        "#### **Comparison Summary**\n",
        "\n",
        "| Feature | Standard AI (Chatbot) | AI Agent (NCBI Tool) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Source** | Internal \"Memory\" (Static) | Live NCBI Libraries (Dynamic) |\n",
        "| **Accuracy** | High risk of \"Scientific Fiction\" | 100% Data Integrity |\n",
        "| **Citations** | May invent fake papers | Provides real Accession Numbers (e.g., NM_000492) |\n",
        "| **Capability** | Can only talk about biology | Can **perform** bioinformatics analysis |\n",
        "\n",
        "---\n",
        "\n",
        "#### **Key Takeaway**\n",
        "\n",
        "In medicine and research, **\"close enough\" is dangerous**. Using Python to build an agent that consults the \"Source of Truth\" ensures that your diagnostic logic or research hypothesis is built on a foundation of verified fact, not a probabilistic guess."
      ],
      "metadata": {
        "id": "yve977JIFcOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 9: Agent Connected to the \"Source of Truth\"\n",
        "\n",
        "The National Center for Biotechnology Information (NCBI) is the world's largest repository of genetic data. Instead of asking an AI to \"remember\" a sequence, we give it a **Tool** to fetch the exact, peer-reviewed data directly from the NCBI `nucleotide` database.\n",
        "\n",
        "In this workflow:\n",
        "1. **The User** asks for a gene (e.g., \"Human BRCA1\").\n",
        "2. **The Agent** recognizes it needs a sequence and calls the `fetch_ncbi_sequence` tool.\n",
        "3. **Python** connects to NCBI, downloads the data, and hands it back to the Agent.\n",
        "4. **The Agent** analyzes the raw data for the user."
      ],
      "metadata": {
        "id": "NskOZosRFqHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 9: The Live NCBI Research Agent\n",
        "\n",
        "!pip install -q biopython\n",
        "\n",
        "from Bio import Entrez, SeqIO\n",
        "from IPython.display import Markdown\n",
        "import io\n",
        "\n",
        "# 1. Setup NCBI Access (NCBI requires an email address for API use)\n",
        "Entrez.email = \"utsa_student@utsa.edu\"\n",
        "\n",
        "def fetch_ncbi_sequence(gene_name: str):\n",
        "    \"\"\"Searches NCBI Nucleotide database and returns the FASTA sequence.\"\"\"\n",
        "    try:\n",
        "        # Search for the gene\n",
        "        handle = Entrez.esearch(db=\"nucleotide\", term=f\"{gene_name}[Gene Name] AND human[Organism]\", retmax=1)\n",
        "        record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        if not record[\"IdList\"]:\n",
        "            return \"No sequence found.\"\n",
        "\n",
        "        # Fetch the actual sequence data\n",
        "        gi_id = record[\"IdList\"][0]\n",
        "        fetch_handle = Entrez.efetch(db=\"nucleotide\", id=gi_id, rettype=\"fasta\", retmode=\"text\")\n",
        "        fasta_data = fetch_handle.read()\n",
        "        fetch_handle.close()\n",
        "\n",
        "        return fasta_data\n",
        "    except Exception as e:\n",
        "        return f\"Error connecting to NCBI: {e}\"\n",
        "\n",
        "# 2. The Agent Logic\n",
        "target_gene = \"BRCA1\"\n",
        "print(f\"Agent: 'I am fetching the live sequence for {target_gene} from NCBI...'\")\n",
        "raw_fasta = fetch_ncbi_sequence(target_gene)\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are a Genomic Research Agent.\n",
        "I have retrieved the following FASTA data from NCBI for the {target_gene} gene:\n",
        "{raw_fasta[:1000]}... (sequence truncated for brevity)\n",
        "\n",
        "1. Identify the Accession Number from the header.\n",
        "2. Explain the clinical significance of mutations in this gene.\n",
        "3. Why is it important to use live NCBI data rather than relying on your training memory?\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "4vR3uBMO3ViV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image17D.png)\n",
        "\n",
        "#### **The Significance of this Output**\n",
        "\n",
        "This output is much more reliable because the agent was **\"grounded\"** in reality. The AI Agent didn't just guess; it looked at the specific data provided.\n",
        "\n",
        "The concept of a **Source of Truth** in AI agents is powerful because it solves the fundamental problem of **Hallucination**.\n",
        "\n",
        "Here is why a \"Source of Truth\" agent is good, structured by how it improves the agent's performance:\n",
        "\n",
        "##### **1. Solves the Hallucination Problem**\n",
        "\n",
        "LLMs, like ChatGPT and Gemini are essentially prediction engines. They predict the next token based on patterns in their training data. This means they can confidently state things that are false (hallucinations). By fetching live data first, the agent is forced to \"see\" the specific document or data source. It cannot hallucinate facts that contradict the source.\n",
        "\n",
        "##### **2. Real-Time Accuracy (Timeliness)**\n",
        "\n",
        "Training data has a cutoff date. The agent's memory might know that BRCA1 was discovered in 1994,, but it might not know about the latest clinical trials or specific mutations identified in 2026. By fetching live data, you ensure the agent is working with the **current state of the world**, not a snapshot from years ago.\n",
        "\n",
        "##### **3. Contextual Precision**\n",
        "\n",
        "If you ask a generic question about BRCA1, you get a generic answer. However, by feeding the agent the *specific* FASTA header, you are effectively giving it a \"Context Window\" of the exact sequence it is analyzing. This allows the agent to perform specific analysis (like identifying the exact Accession Number) that is impossible to do with general knowledge.\n",
        "\n",
        "##### **4. Verifiability**\n",
        "\n",
        "Because the agent saw the data, you can trust its conclusions. If the agent says, \"The sequence starts with `>NM_007294.4`, which is the RefSeq ID,\" you can verify that by checking the header of the returned FASTA. This builds trust in the tool.\n",
        "\n",
        "#### **Summary**\n",
        "\n",
        "The \"Source of Truth\" agent is good because it turns the AI from a **memory-based chatbot** into a **research tool**. It leverages the AI's ability to *understand and synthesize* information while relying on the computer's ability to *find and store* that information accurately.\n"
      ],
      "metadata": {
        "id": "vEy6oQWBKXUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 9: Agent Connected to the \"Source of Truth\"**\n",
        "\n",
        "In the cell below, write the code to search the NCBI for the RefSeq for 'NM_000492`, the human CFTR (Cystic Fibrosis Tranport Regulator) gene.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Use this code for the \"fetch\" definition:\n",
        "```python\n",
        "def fetch_cftr_data():\n",
        "    \"\"\"Fetches the human CFTR reference sequence from NCBI.\"\"\"\n",
        "    # We search specifically for the RefSeq 'NM_000492', which is the standard CFTR transcript\n",
        "    handle = Entrez.efetch(db=\"nucleotide\", id=\"NM_000492\", rettype=\"fasta\", retmode=\"text\")\n",
        "    fasta_data = handle.read()\n",
        "    handle.close()\n",
        "```\n",
        "\n",
        "Use this code for the Agent Execution:\n",
        "\n",
        "```python\n",
        "# 2. Agent Execution\n",
        "print(\"Agent: 'Accessing NCBI RefSeq database for CFTR transcript...'\")\n",
        "cftr_fasta = fetch_cftr_data()\n",
        "```\n",
        "\n",
        "Use this code for the prompt:\n",
        "\n",
        "```python\n",
        "# We show the agent the first 500 characters of the live sequence\n",
        "prompt = f\"\"\"\n",
        "You are a Clinical Geneticist Agent.\n",
        "I have retrieved the official RefSeq FASTA for the human CFTR gene:\n",
        "{cftr_fasta[:500]}...\n",
        "\n",
        "Based on this live data:\n",
        "1. Identify the 'NM' accession number and explain what a 'RefSeq' is.\n",
        "2. The most common mutation is DeltaF508 (a 3-base pair deletion).\n",
        "   Explain how a 'small' change like a 3-bp deletion can lead to a 'large' systemic disease like Cystic Fibrosis.\n",
        "3. Calculate roughly how many 'screens' of text this gene would take if the full transcript is ~6,000 bases long.\n",
        "\"\"\"\n",
        "```"
      ],
      "metadata": {
        "id": "pVR5Wr4sKe9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 9 here\n",
        "\n"
      ],
      "metadata": {
        "id": "X-_Cm_C-3rAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_4_image18D.png)"
      ],
      "metadata": {
        "id": "FN_xcm04NH7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Summary**\n",
        "\n",
        "This lesson introduced you to **Agentic AI**—artificial intelligence systems that go beyond simple question-answering to actively perform tasks, use tools, and interact with external data sources.\n",
        "\n",
        "#### **What is Agentic AI?**\n",
        "\n",
        "Unlike standard chatbots that only generate text responses, agentic AI systems can take autonomous actions: searching databases, running calculations, analyzing images, and retrieving real-time information. The lesson demonstrates this through the Google Gemini API with various agent configurations.\n",
        "\n",
        "#### **Key Concepts Covered**\n",
        "\n",
        "##### 1. **Literature Agents**\n",
        "Students learn to create AI agents that explain complex biological concepts, such as mRNA vaccine mechanisms (lipid nanoparticle entry, spike protein translation, and immune recognition). These agents adopt specific personas like \"Biology Professor\" to tailor explanations appropriately.\n",
        "\n",
        "##### 2. **Vision Agents (Multimodal Analysis)**\n",
        "Vision agents can analyze microscopy images, identifying cell types (like neutrophils vs. red blood cells) and labeling structures (nucleus, axon, dendrites). This capability is valuable for automating microscopy tasks and interpreting histological slides.\n",
        "\n",
        "##### 3. **Function Calling (Tool Use)**\n",
        "LLMs are notoriously unreliable at precise calculations. Function calling solves this by allowing the agent to invoke Python functions for exact computations. Examples include BMI calculators and creatinine clearance calculations using the Cockcroft-Gault formula—critical for clinical applications where precision is non-negotiable.\n",
        "\n",
        "##### 4. **Grounded Research Agents**\n",
        "To combat \"hallucination\" (when AI invents plausible-sounding but false information), grounded agents use Google Search to anchor responses in real sources. Students query for current FDA-approved treatments, with the agent citing actual URLs rather than relying solely on training data.\n",
        "\n",
        "##### 5. **Triage Agents (Structured Data Extraction)**\n",
        "These agents extract structured JSON data from unstructured sources like handwritten prescriptions—identifying patient names, medications, dosages, and physician signatures. This bridges the gap between messy real-world clinical notes and computer-readable databases.\n",
        "\n",
        "##### 6. **Patient Education Assistants**\n",
        "Agents can adapt explanations to different health literacy levels, creating simple handouts for patients with low health literacy versus more technical explanations for those with medical backgrounds.\n",
        "\n",
        "##### 7. **Chain of Thought Reasoning**\n",
        "For diagnostic scenarios, agents use systematic reasoning frameworks (like VINDICATE) to work through differential diagnoses step-by-step, mimicking how clinicians approach complex cases with pulmonary embolism, pneumonia, or diabetic ketoacidosis.\n",
        "\n",
        "##### 8. **Proactive Research Agents**\n",
        "Rather than waiting for users to provide all information, proactive agents recognize knowledge gaps and autonomously search for current clinical guidelines, such as pharmacogenomics recommendations for CYP2C19 poor metabolizers.\n",
        "\n",
        "##### 9. **Connecting to Source of Truth (NCBI)**\n",
        "The lesson culminates with agents that query the NCBI database directly, fetching live genetic sequences (like BRCA1 or CFTR) rather than relying on potentially outdated training data. This ensures 100% data integrity for genomic research.\n",
        "\n",
        "### **Why This Matters**\n",
        "\n",
        "In medicine and biology, \"close enough\" is dangerous. Agentic AI ensures that diagnostic logic, drug calculations, and genetic data are grounded in verified sources rather than probabilistic guesses. Students learn that the future of AI in healthcare lies not in replacement of human judgment, but in building reliable tools that consult authoritative databases and show their reasoning transparently."
      ],
      "metadata": {
        "id": "H3c7I4hvOFjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-In**\n",
        "\n",
        "When you have completed and run all of the code cells, use the `File --> Print.. --> Microsoft Print to PDF` to generate your PDF if you are running `MS Windows`. If you have a Mac, use the `File --> Print.. --> Save as PDF`\n",
        "\n",
        "In either case, save your PDF as Copy of Class_04_4.lastname.pdf where lastname is your last name, and upload the file to Canvas.\n",
        "\n",
        "**NOTE TO WINDOWS USERS:** Your grade will be reduced by 10% if your PDF is found to be missing pages when it is being graded in Canvas. This penalty is simply meant to prevent the grader from having to take the additional steps of (1) downloading your PDF, (2) printing it out using the `Microsoft Print to PDF` and (3) having to resubmit to Canvas so they can grade it."
      ],
      "metadata": {
        "id": "iG6D_KMDNdIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lizard Tail**\n",
        "## **NVIDIA**\n",
        "\n",
        "### **Entrance of Endeavor headquarters building in 2018**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/7/75/2788-2888_San_Tomas_Expwy.jpg)\n",
        "\n",
        "**Nvidia Corporation** (/ɛnˈvɪdiə/ en-VID-ee-ə) is an American multinational corporation and technology company headquartered in Santa Clara, California, and incorporated in Delaware. Founded in 1993 by Jensen Huang (president and CEO), Chris Malachowsky, and Curtis Priem, it is a software company which designs and supplies graphics processing units (GPUs), application programming interfaces (APIs) for data science and high-performance computing, and system on a chip units (SoCs) for mobile computing and the automotive market. Nvidia is also the dominant supplier of artificial intelligence (AI) hardware and software. Nvidia outsources the manufacturing of the hardware it designs.\n",
        "\n",
        "Nvidia's professional line of GPUs are used for edge-to-cloud computing and in supercomputers and workstations for applications in fields such as architecture, engineering and construction, media and entertainment, automotive, scientific research, and manufacturing design. Its GeForce line of GPUs are aimed at the consumer market and are used in applications such as video editing, 3D rendering, and PC gaming. With a market share of 80.2% in the second quarter of 2023, Nvidia leads the market for discrete desktop GPUs by a wide margin. The company expanded its presence in the gaming industry with the introduction of the Shield Portable (a handheld game console), Shield Tablet (a gaming tablet), and Shield TV (a digital media player), as well as its cloud gaming service GeForce Now.\n",
        "\n",
        "In addition to GPU design and outsourcing manufacturing, Nvidia provides the CUDA software platform and API that allows the creation of massively parallel programs which utilize GPUs. They are deployed in supercomputing sites around the world. In the late 2000s, Nvidia had moved into the mobile computing market, where it produces Tegra mobile processors for smartphones and tablets and vehicle navigation and entertainment systems. Its competitors include AMD, Intel,[19] Qualcomm, and AI accelerator companies such as Cerebras and Graphcore. It also makes AI-powered software for audio and video processing (e.g., Nvidia Maxine).\n",
        "\n",
        "Nvidia's offer to acquire Arm from SoftBank in September 2020 failed to materialize following extended regulatory scrutiny, leading to the termination of the deal in February 2022 in what would have been the largest semiconductor acquisition. In 2023, Nvidia became the seventh public U.S. company to be valued at over \\$1 trillion, and the company's valuation has increased rapidly since then as the company became a leader in data center chips with AI capabilities in the midst of the AI boom. In June 2024, for one day, Nvidia overtook Microsoft as the world's most valuable publicly traded company, with a market capitalization of over \\$3.3 trillion.\n",
        "\n",
        "## **History**\n",
        "\n",
        "**Founding**\n",
        "\n",
        "Nvidia was founded on April 5, 1993, by Jensen Huang (who, as of 2024, remains CEO), a Taiwanese-American electrical engineer who was previously the director of CoreWare at LSI Logic and a microprocessor designer at AMD; Chris Malachowsky, an engineer who worked at Sun Microsystems; and Curtis Priem, who was previously a senior staff engineer and graphics chip designer at IBM and Sun Microsystems. The three men agreed to start the company in a meeting at a Denny's roadside diner on Berryessa Road in East San Jose.\n",
        "\n",
        "At the time, Malachowsky and Priem were frustrated with Sun's management and were looking to leave, but Huang was on \"firmer ground\", in that he was already running his own division at LSI. The three co-founders discussed a vision of the future which was so compelling that Huang decided to leave LSI and become the chief executive officer of their new startup.\n",
        "\n",
        "In 1993, the three co-founders envisioned that the ideal trajectory for the forthcoming wave of computing would be in the realm of accelerated computing, specifically in graphics-based processing. This path was chosen due to its unique ability to tackle challenges that eluded general-purpose computing methods.[36] As Huang later explained: \"We also observed that video games were simultaneously one of the most computationally challenging problems and would have incredibly high sales volume. Those two conditions don’t happen very often. Video games was our killer app — a flywheel to reach large markets funding huge R&D to solve massive computational problems.\" With \\$40,000 in the bank, the company was born. The company subsequently received \\$20 million of venture capital funding from Sequoia Capital, Sutter Hill Ventures and others.\n",
        "\n",
        "During the late 1990s, Nvidia was one of 70 startup companies chasing the idea that graphics acceleration for video games was the path to the future. Only two survived: Nvidia and ATI Technologies, the latter of which merged into AMD.\n",
        "\n",
        "Nvidia initially had no name and the co-founders named all their files NV, as in \"next version\". The need to incorporate the company prompted the co-founders to review all words with those two letters. At one point, Malachowsky and Priem wanted to call the company NVision, but that name was already taken by a manufacturer of toilet paper. Huang suggested the name Nvidia, from \"invidia\", the Latin word for \"envy\". The company's original headquarters office was in Sunnyvale, California.\n",
        "\n",
        "**First graphics accelerator**\n",
        "\n",
        "Nvidia's first graphics accelerator, the NV1, was designed to process quadrilateral primitives (forward texture mapping), a feature that set it apart from competitors, who preferred triangle primitives. However, when Microsoft introduced the DirectX platform, it chose not to support any other graphics software and announced that its Direct3D API would exclusively support triangles. As a result, the NV1 failed to gain traction in the market.\n",
        "\n",
        "Nvidia had also entered into a partnership with Sega to supply the graphics chip for the Dreamcast console and worked on the project for about a year. However, Nvidia's technology was already lagging behind competitors. This placed the company in a difficult position: continue working on a chip that was likely doomed to fail or abandon the project, risking financial collapse.\n",
        "\n",
        "In a pivotal moment, Sega's president, Shoichiro Irimajiri, visited Huang in person to inform him that Sega had decided to choose another vendor for the Dreamcast. However, Irimajiri believed in Nvidia's potential and persuaded Sega’s management to invest $5 million into the company. Huang later reflected that this funding was all that kept Nvidia afloat, and that Irimajiri's \"understanding and generosity gave us six months to live\".\n",
        "\n",
        "In 1996, Huang laid off more than half of Nvidia's employees—thereby reducing headcount from 100 to 40—and focused the company's remaining resources on developing a graphics accelerator product optimized for processing triangle primitives: the RIVA 128. By the time the RIVA 128 was released in August 1997, Nvidia had only enough money left for one month’s payroll. The sense of impending failure became so pervasive that it gave rise to Nvidia's unofficial company motto: \"Our company is thirty days from going out of business.\" Huang began internal presentations to Nvidia staff with those words for many years.\n",
        "\n",
        "Nvidia sold about a million RIVA 128 units within four months, and used the revenue to fund development of its next generation of products. In 1998, the release of the RIVA TNT helped solidify Nvidia’s reputation as a leader in graphics technology.\n",
        "\n",
        "**Public company**\n",
        "\n",
        "Nvidia went public on January 22, 1999. Investing in Nvidia after it had already failed to deliver on its contract turned out to be Irimajiri's best decision as Sega's president. After Irimajiri left Sega in 2000, Sega sold its Nvidia stock for \\$15 million.\n",
        "\n",
        "In late 1999, Nvidia released the GeForce 256 (NV10), its first product expressly marketed as a GPU, which was most notable for introducing onboard transformation and lighting (T&L) to consumer-level 3D hardware. Running at 120 MHz and featuring four-pixel pipelines, it implemented advanced video acceleration, motion compensation, and hardware sub-picture alpha blending. The GeForce outperformed existing products by a wide margin.\n",
        "\n",
        "Due to the success of its products, Nvidia won the contract to develop the graphics hardware for Microsoft's Xbox game console, which earned Nvidia a \\$200 million advance. However, the project took many of its best engineers away from other projects. In the short term this did not matter, and the GeForce2 GTS shipped in the summer of 2000. In December 2000, Nvidia reached an agreement to acquire the intellectual assets of its one-time rival 3dfx, a pioneer in consumer 3D graphics technology leading the field from the mid-1990s until 2000. The acquisition process was finalized in April 2002.\n",
        "\n",
        "In 2001, Standard & Poor's selected Nvidia to replace the departing Enron in the S&P 500 stock index, meaning that index funds would need to hold Nvidia shares going forward.\n",
        "\n",
        "In July 2002, Nvidia acquired Exluna for an undisclosed sum. Exluna made software-rendering tools and the personnel were merged into the Cg project. In August 2003, Nvidia acquired MediaQ for approximately US$70 million. It launched GoForce the follow year. On April 22, 2004, Nvidia acquired iReady, also a provider of high-performance TCP offload engines and iSCSI controllers. In December 2004, it was announced that Nvidia would assist Sony with the design of the graphics processor (RSX) for the PlayStation 3 game console. On December 14, 2005, Nvidia acquired ULI Electronics, which at the time supplied third-party southbridge parts for chipsets to ATI, Nvidia's competitor. In March 2006, Nvidia acquired Hybrid Graphics. In December 2006, Nvidia, along with its main rival in the graphics industry AMD (which had acquired ATI), received subpoenas from the U.S. Department of Justice regarding possible antitrust violations in the graphics card industry.\n",
        "\n",
        "# NVIDIA Corporation: A Pillar of AI and LLM Innovation\n",
        "\n",
        "## 🏢 Company Overview\n",
        "\n",
        "**Founded:** 1993  \n",
        "**Headquarters:** Santa Clara, California  \n",
        "**CEO:** Jensen Huang  \n",
        "**Valuation (2025):** $4 Trillion  \n",
        "**Employees:** ~36,000  \n",
        "**Market Share:** 92% in discrete graphics segment\n",
        "\n",
        "NVIDIA began as a graphics chip designer and revolutionized the gaming industry with the invention of the **GPU (Graphics Processing Unit)** in 1999. Over time, it evolved into a full-stack computing company, now leading the charge in AI, data centers, robotics, autonomous vehicles, and scientific computing.[1](https://www.thomasnet.com/insights/nvidia-company-overview/)\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Role in AI and LLM Development\n",
        "\n",
        "### 1. **Hardware Leadership**\n",
        "- **GPUs for AI Training**: NVIDIA's H100 and Blackwell GB200 chips are optimized for training and inference of massive LLMs.\n",
        "- **Blackwell Architecture**: Introduced in 2024, it supports ultra-efficient 4-bit precision inference, enabling real-time deployment of trillion-parameter models.[2](https://blogs.nvidia.com/blog/openai-gpt-oss/)\n",
        "- **DGX Systems**: High-performance AI supercomputers used by enterprises and research institutions globally.\n",
        "\n",
        "### 2. **Software Ecosystem**\n",
        "- **CUDA Platform**: Over 450 million downloads; enables parallel computing on NVIDIA GPUs.\n",
        "- **TensorRT-LLM**: Optimizes LLM inference for speed and efficiency.\n",
        "- **NeMo Framework**: Supports training and customization of LLMs, including Megatron 530B.[3](https://nvidianews.nvidia.com/news/nvidia-launches-large-language-model-cloud-services-to-advance-ai-and-digital-biology)\n",
        "\n",
        "### 3. **Cloud Services**\n",
        "- **NeMo LLM Service**: Allows developers to fine-tune foundation models using prompt learning.\n",
        "- **BioNeMo LLM Service**: Extends LLM capabilities to biology and chemistry, aiding in drug discovery and genomics.[3](https://nvidianews.nvidia.com/news/nvidia-launches-large-language-model-cloud-services-to-advance-ai-and-digital-biology)\n",
        "\n",
        "### 4. **Strategic Collaborations**\n",
        "- **OpenAI Partnership**: NVIDIA GPUs power OpenAI’s open-weight models like gpt-oss-120b, achieving 1.5 million tokens/sec on Blackwell systems.[2](https://blogs.nvidia.com/blog/openai-gpt-oss/)\n",
        "- **Global Developer Ecosystem**: 6.5 million developers across 250 countries use NVIDIA’s AI stack.\n",
        "\n",
        "---\n",
        "\n",
        "## 🌍 Industry Impact\n",
        "\n",
        "NVIDIA’s technologies are transforming multiple sectors:\n",
        "- **Healthcare**: Accelerating diagnostics and drug discovery.\n",
        "- **Automotive**: Enabling autonomous driving through the DRIVE platform.\n",
        "- **Robotics**: Powering industrial and humanoid robots.\n",
        "- **Climate Science**: Earth-2 digital twin predicts climate change impacts.\n",
        "- **Metaverse**: Omniverse Cloud supports virtual world creation.\n",
        "- **Quantum Research**: New center in Boston focuses on quantum computing applications.[1](https://www.thomasnet.com/insights/nvidia-company-overview/)\n",
        "\n",
        "---\n",
        "\n",
        "## 📈 Future Outlook\n",
        "\n",
        "Despite challenges like chip shortages and competition, NVIDIA is projected to maintain dominance in AI and data center markets. Its continued investment in R&D, acquisitions, and infrastructure positions it as a cornerstone of the AI revolution.\n",
        "\n",
        "> “I want to turn NVIDIA into one giant AI.”  \n",
        "> — Jensen Huang, CEO[1](https://www.thomasnet.com/insights/nvidia-company-overview/)\n",
        "\n",
        "---\n",
        "\n",
        "## 🔗 References\n",
        "- [NVIDIA Blog on OpenAI Collaboration](https://blogs.nvidia.com/blog/openai-gpt-oss/)[2](https://blogs.nvidia.com/blog/openai-gpt-oss/)  \n",
        "- [Thomasnet Company Overview](https://www.thomasnet.com/insights/nvidia-company-overview/)[1](https://www.thomasnet.com/insights/nvidia-company-overview/)  \n",
        "- [NVIDIA NeMo and BioNeMo Services](https://nvidianews.nvidia.com/news/nvidia-launches-large-language-model"
      ],
      "metadata": {
        "id": "6x8gMtgQxMMH"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}