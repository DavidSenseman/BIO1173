{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEEW13uCtiJ"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/master/Class_05_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BIO 1173: Intro Computational Biology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module 5: Regularization and Dropout**\n",
    "\n",
    "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
    "\n",
    "### Module 5 Material\n",
    "\n",
    "* Part 5.1: Part 5.1: Introduction to Regularization: Ridge and Lasso\n",
    "* Part 5.2: Using K-Fold Cross Validation with Keras\n",
    "* Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting\n",
    "* **Part 5.4: Drop Out for Keras to Decrease Overfitting**\n",
    "* Part 5.5: Benchmarking Keras Deep Learning Regularization Techniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKQylnEiLDUM"
   },
   "source": [
    "### Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n",
    "  Running the following code will map your GDrive to ```/content/drive```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seXFCYH4LDUM",
    "outputId": "c05015aa-871e-4779-9265-5ad07e8bf617"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Setup\n",
    "\n",
    "Run the next code cell to load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You MUST run this code cell first\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "path = '/'\n",
    "memory = shutil.disk_usage(path)\n",
    "dirpath = os.getcwd()\n",
    "print(\"Your current working directory is : \" + dirpath)\n",
    "print(\"Disk\", memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions\n",
    "\n",
    "The cell below creates the function(s) needed for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_xChO0Fkn4S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple function to print out elasped time\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "# Simple function to change column name in a dataframe\n",
    "def rename_col_by_index(dataframe, index_mapping):\n",
    "    dataframe.columns = [index_mapping.get(i, col) for i, col in enumerate(dataframe.columns)]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5.4: Drop Out for Keras to Decrease Overfitting\n",
    "\n",
    "Hinton, Srivastava, Krizhevsky, Sutskever, & Salakhutdinov (2012) introduced the **_dropout regularization_** algorithm. [[Cite:srivastava2014dropout]](http://www.jmlr.org/papers/volume15/nandan14a/nandan14a.pdf) \n",
    "\n",
    "Although dropout works differently than L1 and L2, it accomplishes the same goal—the prevention of overfitting. However, the algorithm does the task by actually _removing_ neurons and connections—at least temporarily. Unlike L1 and L2, no weight penalty is added. Dropout does not directly seek to train small weights.\n",
    "\n",
    "Dropout works by causing hidden neurons of the neural network to be unavailable during part of the training. Dropping part of the neural network causes the remaining portion to be trained to still achieve a good score even without the dropped neurons. This technique decreases co-adaptation between neurons, which results in less overfitting. \n",
    "\n",
    "Most neural network frameworks implement dropout as a separate layer. Dropout layers function like a regular, densely connected neural network layer. The only difference is that the dropout layers will periodically drop some of their neurons during training. You can use dropout layers on regular feedforward neural networks. \n",
    "\n",
    "A _program_ can implement a dropout layer as a dense layer that can eliminate some of its neurons. Contrary to popular belief about the dropout layer, such a program does not permanently remove these discarded neurons. In other words, a dropout layer does **_not_** lose any of its neurons during the training process, and it will still have the same number of neurons after training. In this way, the program only _temporarily masks_ the neurons rather than dropping them. \n",
    "\n",
    "Figure 5.DROPOUT shows how a dropout layer might be situated with other layers.\n",
    "\n",
    "**Figure 5.DROPOUT: Dropout Regularization**\n",
    "![Dropout Regularization](https://biologicslab.co/BIO1173/images/class_9_dropout.png \"Dropout Regularization\")\n",
    "\n",
    "The discarded neurons and their connections are shown as dashed lines. The input layer has two input neurons as well as a bias neuron. The second layer is a dense layer with three neurons and a bias neuron. The third layer is a dropout layer with six regular neurons even though the program has dropped 50% of them. \n",
    "\n",
    "While the program drops these neurons, it neither calculates nor trains them. However, the final neural network will use _all_ of these neurons for the output. As previously mentioned, the program only temporarily discards the neurons. \n",
    "\n",
    "The program chooses different sets of neurons from the dropout layer during subsequent training iterations. Although we chose a probability of 50% for dropout, the computer will not necessarily drop three neurons. It is as if we flipped a coin for each of the dropout candidate neurons to choose if that neuron was dropped out. You must know that the program should never drop the bias neuron. Only the regular neurons on a dropout layer are candidates.\n",
    "\n",
    "The implementation of the training algorithm influences the process of discarding neurons. The dropout set frequently changes once per training iteration or batch. The program can also provide intervals where all neurons are present. Some neural network frameworks give additional hyper-parameters to allow you to specify exactly the rate of this interval. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does Dropout work? \n",
    "\n",
    "Why dropout is capable of decreasing overfitting is a common question. The answer is that dropout can reduce the chance of **_codependency_** developing between two neurons. Two neurons that develop codependency will not be able to operate effectively when one is dropped out. As a result, the neural network can no longer rely on the presence of every neuron, and it trains accordingly. This characteristic decreases its ability to memorize the information presented, thereby forcing generalization.\n",
    "\n",
    "Dropout also decreases overfitting by **_forcing a bootstrapping process_** upon the neural network. Bootstrapping is a prevalent ensemble technique. Ensembling is a technique of machine learning that combines multiple models to produce a better result than those achieved by individual models. The ensemble is a term that originates from the musical ensembles in which the final music product that the audience hears is the combination of many instruments.  \n",
    "\n",
    "**_Bootstrapping_** is one of the most simple ensemble techniques. The bootstrapping programmer simply trains several neural networks to perform precisely the same task. However, each neural network will perform differently because of some training techniques and the random numbers used in the neural network weight initialization. The difference in weights causes the performance variance. The output from this ensemble of neural networks becomes the average output of the members taken together. This process decreases overfitting through the consensus of differently trained neural networks.  \n",
    "\n",
    "Dropout works somewhat like bootstrapping. You might think of each neural network that results from a different set of neurons being dropped out as an individual member in an ensemble. As training progresses, the program creates more neural networks in this way. However, dropout does not require the same amount of processing as bootstrapping. The new neural networks created are temporary; they exist only for a training iteration. The final result is also a single neural network rather than an ensemble of neural networks to be averaged together.\n",
    "\n",
    "This short YouTube video shows how dropout works: [Dropout tutorial](https://youtu.be/NhZVe50QwPM?si=Zr-6qrPdE9YXTj3Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Dropout for Keras\n",
    "\n",
    "For Example 1 we will create a neural network with 3 hidden layers and 2 dropout layers to demonstrate how to implement dropout layers in classification neural network. The dataset for Example 1 is the [Body Performance Dataset](https://www.kaggle.com/datasets/kukuroo3/body-performance-data) that we have used in previous lessons (e.g. Class_05_2, Class_05_3). \n",
    "\n",
    "This dataset has the following 12 categories:\n",
    "\n",
    "* **age:** 20 ~64\n",
    "* **gender:** M,F\n",
    "* **height_cm:** (If you want to convert to feet, divide by 30.48)\n",
    "* **weight_kg:**\n",
    "* **body fat_%:**\n",
    "* **diastolic:** diastolic blood pressure (min)\n",
    "* **systolic:** systolic blood pressure (min)\n",
    "* **gripForce:**\n",
    "* **sit and bend forward_cm:**\n",
    "* **sit-ups counts:**\n",
    "* **broad jump_cm:**\n",
    "* **class:** A,B,C,D ( A: best) / stratified\n",
    "\n",
    "To help follow the code examples, Example 1 has been divided into 3 sections, A`,`B` and `C`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1A: Create feature vector\n",
    "\n",
    "The code in the cell below reads the Body Performance dataset, `bodyPerformance.csv` from the course HTTPS server and creates a new DataFrame called `bpBigDF`. To speedup training, only 30% of `bpBigDF` we used to create the DataFrame for this example, `bpDF`. \n",
    "\n",
    "The column `gender` is mapped (`M`=`0`,`F`=`1`). While only the columns `age`, `height_cm`, `weight_kg`, `diastolic`, `systolic` and `gripForce` are standardized, all of the columns, except `class` are used for creating the x-value variable. Since we will are building a classification neural network, column `classes` is One-Hot encoded to gererate the y-values. \n",
    "\n",
    "As always, both the x-values and the y-values must be converted to type `float32` to avoid errors during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1A: Create feature vector\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Read the data set\n",
    "bpBigDF = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/bodyPerformance.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Only use 30% for neural network\n",
    "bpDF=bpBigDF.sample(frac=0.30, random_state=2)\n",
    "\n",
    "# Map Gender\n",
    "mapping =  {'M': 0,\n",
    "            'F': 1}\n",
    "bpDF['gender'] = bpDF['gender'].map(mapping)\n",
    "\n",
    "# Standardize ranges\n",
    "bpDF['age'] = zscore(bpDF['age'])\n",
    "bpDF['height_cm'] = zscore(bpDF['height_cm'])\n",
    "bpDF['weight_kg'] = zscore(bpDF['weight_kg'])\n",
    "bpDF['diastolic'] = zscore(bpDF['diastolic'])\n",
    "bpDF['systolic'] = zscore(bpDF['systolic'])\n",
    "bpDF['gripForce'] = zscore(bpDF['gripForce'])\n",
    "\n",
    "# Generate list of columns for x\n",
    "bpX_columns = bpDF.columns.drop('class')  # `class` is y-value \n",
    "\n",
    "# Generate x-values as numpy array\n",
    "bpX = bpDF[bpX_columns].values\n",
    "\n",
    "# Convert x-values to float 32\n",
    "bpX = np.asarray(bpX).astype('float32')\n",
    "\n",
    "# One-Hot encode column containing y-values\n",
    "dummies = pd.get_dummies(bpDF['class']) # Classification\n",
    "BpCategories = dummies.columns\n",
    "bpY = dummies.values\n",
    "\n",
    "# Convert y-values to float 32\n",
    "bpY = np.asarray(bpY).astype('float32')\n",
    "\n",
    "# Print y categorical names\n",
    "print(*BpCategories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the 4 different fitness levels in your output:\n",
    "~~~text\n",
    "A B C D\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1B: Keras with dropout for Classification\n",
    "\n",
    "The code in the cell below creates a sequential neural network with 3 hidden layers of densely connected neurons. A dropout layer is added after the first and second hidden layers, but _not_ after the last hidden (3rd) layer. A dropout layer is not usually added after the last hidden layer. \n",
    "\n",
    "Since this neural network is designed for classification, the output layer has 4 neurons -- one neuron for each fitness class. The number of output neurons is set by the argument `bpY.shape[1]` as demonstrated in the following code chunk:\n",
    "~~~text\n",
    "model.add(Dense(bpY.shape[1],activation='softmax')) # Output layer\n",
    "~~~\n",
    "The code is setup to take advantage of **_K_-fold Cross Validation_**. The advantages of using this technique and how to implement it were demonstrated previously, in Class_05_2. The number of _K_-folds to be employed is set by the variable `numK=5`. During each _K_ turn of the `for loop`, a new neural network is created and trained for the number of epochs specified in the variable `EPOCHS=100`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1B: Keras with dropout for Classification L1\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Set variables\n",
    "EPOCHS=100 # number of epochs for each loop\n",
    "numK=5     # number of K-folds\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Cross-validate\n",
    "kf = KFold(numK, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize arrays and counter\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "# Start loop ------------------------------------------#\n",
    "\n",
    "for train, test in kf.split(bpX):\n",
    "    fold+=1\n",
    "    print(f\"Starting Fold #{fold}...\")\n",
    "\n",
    "    # Split data for this fold\n",
    "    x_train = bpX[train]\n",
    "    y_train = bpY[train]\n",
    "    x_test = bpX[test]\n",
    "    y_test = bpY[test]\n",
    "    \n",
    "    # To use L2 regularization, substitute\n",
    "    # kernel_regularizer=regularizers.l2(0.01)\n",
    "    \n",
    "    # Create new model for this K fold\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=bpX.shape[1], activation='relu')) # Hidden 1\n",
    "    model.add(Dropout(0.5))  # Add dropout after Hidden 1\n",
    "    model.add(Dense(25, activation='relu', \\\n",
    "                activity_regularizer=regularizers.l1(1e-4))) # Hidden 2\n",
    "    model.add(Dropout(0.5))  # Add dropout after Hidden 2\n",
    "    model.add(Dense(10, activation='relu', \\\n",
    "                activity_regularizer=regularizers.l1(1e-4))) # Hidden 3\n",
    "    # Usually don't add dropout after last hidden layer\n",
    "    model.add(Dense(bpY.shape[1],activation='softmax')) # Output layer\n",
    "    \n",
    "    # Compile model for classification\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),\\\n",
    "              verbose=0,epochs=EPOCHS)\n",
    "    \n",
    "    # Use model to make predictions\n",
    "    pred = model.predict(x_test)\n",
    "    # Add actual y-values for the data used this fold\n",
    "    oos_y.append(y_test)\n",
    "    # raw probabilities to chosen class (highest probability)\n",
    "    pred = np.argmax(pred,axis=1) \n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure this fold's accuracy\n",
    "    y_compare = np.argmax(y_test,axis=1) # For accuracy calculation\n",
    "    score = metrics.accuracy_score(y_compare, pred)\n",
    "    print(f\"Fold score (accuracy): {score}\")\n",
    "\n",
    "# End loop ---------------------------------------------#\n",
    "\n",
    "# Final Processing\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "oos_y_compare = np.argmax(oos_y,axis=1) # For accuracy calculation\n",
    "score = metrics.accuracy_score(oos_y_compare, oos_pred)\n",
    "print(f\"Final score (accuracy): {score}\")    \n",
    "    \n",
    "# Write the cross-validated prediction\n",
    "oos_y = pd.DataFrame(oos_y)\n",
    "oos_pred = pd.DataFrame(oos_pred)\n",
    "oosDF = pd.concat( [oos_pred, oos_y],axis=1 )\n",
    "\n",
    "# Print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will run pretty slowly unless you have GPU. On my Windows machine with an NVIDIA graphics card, it took about 4 minutes to complete 5 folds with 100 epochs per fold.\n",
    "\n",
    "If your code is correct, you should see something similar to the following output:\n",
    "~~~text\n",
    "Starting Fold #1...\n",
    "26/26 [==============================] - 0s 2ms/step\n",
    "Fold score (accuracy): 0.5883084577114428\n",
    "Starting Fold #2...\n",
    "26/26 [==============================] - 0s 1ms/step\n",
    "Fold score (accuracy): 0.5808457711442786\n",
    "Starting Fold #3...\n",
    "26/26 [==============================] - 0s 1ms/step\n",
    "Fold score (accuracy): 0.5833333333333334\n",
    "Starting Fold #4...\n",
    "26/26 [==============================] - 0s 2ms/step\n",
    "Fold score (accuracy): 0.5815691158156912\n",
    "Starting Fold #5...\n",
    "26/26 [==============================] - 0s 2ms/step\n",
    "Fold score (accuracy): 0.6239103362391034\n",
    "Final score (accuracy): 0.5915878546540567\n",
    "Elapsed time: 0:04:20.22\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1C: Print out actual and predicted y-values\n",
    "\n",
    "The code in the cell below prints out the predicted and the actual Body Performance classes for the out-of-sample (oos) individuals. The function `new_column_mapping()` created at the start of this lesson is used make the column labels more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1C: Print out actual and predicted y-values \n",
    "\n",
    "# Rename columns\n",
    "new_column_mapping = {0: 'Predicted Fitness Class', 1: 'Actual: 0'}\n",
    "oosDF = rename_col_by_index(oosDF, new_column_mapping)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_rows', 8)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "\n",
    "# Display DataFrame\n",
    "display(oosDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see something similar to the following table:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_05_4_Exm1C.png)\n",
    "\n",
    "For the 8 predictions shown in the above table, 4 were correct and 4 were incorrect. That error rate is what you might expect with a final accuracy score of around 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Exercise 1: Dropout for Keras**\n",
    "\n",
    "For **Exercise 1** you are to create a classification neural network with 3 hidden layers and 2 dropout layers. The dataset you should use is the [Obesity Prediction Dataset](https://www.kaggle.com/datasets/mrsimple07/obesity-prediction) that we have seen previously in lessons Class_05_2 and  Class_05_3. \n",
    "\n",
    "The 7 categories of obesity/demographics measurements are:\n",
    "\n",
    "* **Age:** The age of the individual, expressed in years (Mean=49.9 yrs +/- 18.1)\n",
    "* **Gender:** The gender of the individual coded Male and Female\n",
    "* **Height:** The height of the individual measured in centimeters (Mean=170 cm +/- 10.3)\n",
    "* **Weight:** The weight of the individual measured in kilograms (Mean=71.2 kg +/- 15.5)\n",
    "* **BMI:** Body mass index, a calculated metric derived from the individual's weight and height (Mean=24.9 +/- 6.19)\n",
    "* **PhysicalActivityLevel:** This variable quantifies the individual's level of physical activity (Mean=2.53 +/- 1.12)\n",
    "* **ObesityCategory:** Categorization of individuals based on their BMI into different obesity categories. The Obesity Categories are: `Underweight`, `Normal weight`, `Overweight`, and `Obese`. \n",
    "\n",
    "You are to construct a neural network that can predict the correct `ObesityCategory`. \n",
    "\n",
    "To help you in your coding, **Exercise 1** has been divided into 3 sections, A`, `B` and `C`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1A: Create feature vector**\n",
    "\n",
    "In the cell below, write the code to read the Obesity Prediction dataset `obesity_prediction.csv` and create a new DataFrame called `opDF`. Since this dataset isn't too large, use the **entire** dataset to create `opDF` (i.e. don't sample it).\n",
    "\n",
    "You will need to map the column `Gender` to convert the strings `Male` and `Female` to integers. Also standardize the columns `Age`, `Height`, `Weight` and `BMI` to their Zscores. \n",
    "\n",
    "Since the column, `ObesityCategory`, will be the y-value for training your neural network, make sure to drop it when generating your list of column names for generatung x-values (`opX_columns`). You can use the following code chunk to create your x-values:\n",
    "~~~text\n",
    "# Generate x-values as numpy array\n",
    "opX = opDF[opX_columns].values\n",
    "~~~\n",
    "Since you are building a neural network for classification, you will need to One-Hot encode the `ObesityCategory` column. Use the `dummies.values`, created as part of your One-Hot encoding, as your y-value, `opY`. You should also save the `dummies.columns` in a variable called `ObCategories`. \n",
    "\n",
    "Don't forget to convert all your x-values and y-values to `float32` or you will get an error message when you try to train your neural network. \n",
    "\n",
    "Finally, print out the categorical values (names) that were One-Hot encoded using the \"starred\" print statement:\n",
    "~~~text\n",
    "# Print y categorical names\n",
    "print(*ObCategories)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 1A here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the names of the 5 obesity categories:\n",
    "~~~text\n",
    "Normal weight Obese Overweight Underweight\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1B: Keras with dropout for Classification**\n",
    "\n",
    "In the cell below, create a sequential neural network with 3 hidden layers of densely connected neurons. Add a dropout layer after the first and second hidden layers, but not after the 3rd hidden layer. \n",
    "\n",
    "Since this neural network is designed for classification, make sure your output layer has 5 neurons -- one neuron for each Obesity Category. You can set the correct number of output neurons using the following code chunk:\n",
    "~~~text\n",
    "model.add(Dense(opY.shape[1],activation='softmax')) # Output layer\n",
    "~~~\n",
    "Setup your code to take advantage of _K_-fold Cross Validation. Set the number of K-folds to `5` and the number of epochs to `100` for each time through the loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 1B here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of your neural network should run faster that the one in Example 1. \n",
    "\n",
    "If your code is correct, you should see the following output:\n",
    "~~~text\n",
    "Starting Fold #1...\n",
    "7/7 [==============================] - 0s 2ms/step\n",
    "Fold score (accuracy): 0.95\n",
    "Starting Fold #2...\n",
    "7/7 [==============================] - 0s 2ms/step\n",
    "Fold score (accuracy): 0.97\n",
    "Starting Fold #3...\n",
    "7/7 [==============================] - 0s 1ms/step\n",
    "Fold score (accuracy): 0.935\n",
    "Starting Fold #4...\n",
    "7/7 [==============================] - 0s 2ms/step\n",
    "Fold score (accuracy): 0.965\n",
    "Starting Fold #5...\n",
    "7/7 [==============================] - 0s 1ms/step\n",
    "Fold score (accuracy): 0.91\n",
    "Final score (accuracy): 0.946\n",
    "Elapsed time: 0:01:30.00\n",
    "~~~\n",
    "\n",
    "The final accuracy score for your neural network is about 95%, which is very good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1C: Print out actual and predicted y-values**\n",
    "\n",
    "In the cell below, print out the predicted and the actual Obesity Catagories for the out-of-sample (oos) individuals. \n",
    "\n",
    "Use the following code chunk to change the column names to make them easier to interpret.\n",
    "~~~text\n",
    "# Rename columns\n",
    "new_column_mapping = {0: 'Predicted Obesity Category', 1: 'Actual: 0'}\n",
    "oosDF = rename_col_by_index(oosDF, new_column_mapping)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 1C here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see something similar to the following table:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_05_4_Exe1C.png)\n",
    "\n",
    "All 8 predictions shown in the above table are correct. Almost perfect predictions is what you might expect with a final accuracy score of around 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lesson Turn-in**\n",
    "\n",
    "When you have completed all of the code cells, and run them in sequential order (the last code cell should be number 9) use the **File --> Print.. --> Save to PDF** to generate a PDF of your JupyterLab notebook. Save your PDF as `Class_05_4.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
