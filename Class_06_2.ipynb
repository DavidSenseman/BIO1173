{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPofNZr9P5acTkTAhfCqU9G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_06_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ],
      "metadata": {
        "id": "NssSnfzn2vAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "pGAuC6fFfBbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Module 6: Advanced Topics**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 6 Material\n",
        "\n",
        "* Part 6.1: Reinforcment Learning\n",
        "* **Part 6.2: ONNX Runtime Environment**\n",
        "* Part 6.3: Analysis of DICOM images with Pytorch\n"
      ],
      "metadata": {
        "id": "AfDqayZnfKoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Change your Runtime Now!**\n",
        "\n",
        "For this lesson you must have a **GPU** hardware accelerator (e.g. T4 High-RAM).\n",
        "NOTE: There is no need to use an \"expensive\" GPU like the A-100 for this lesson."
      ],
      "metadata": {
        "id": "EjHLH6yJZnEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ],
      "metadata": {
        "id": "pbG8yxbEfqO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ],
      "metadata": {
        "id": "DWfA2_Wj2nCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "Iv-lUKQ-fuPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerated Run-time Check\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be running a Graphics Processing Unit (GPU) such as the `T4` with high ram enabled."
      ],
      "metadata": {
        "id": "1VCZ3abAZTqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell second\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def check_device():\n",
        "    # List all the *physical* devices TensorFlow can see\n",
        "    devices = tf.config.list_physical_devices()\n",
        "\n",
        "    # Flags we’ll set\n",
        "    cpu  = False\n",
        "    gpu  = False\n",
        "    tpu  = False\n",
        "\n",
        "    for d in devices:\n",
        "        if d.device_type == 'CPU':\n",
        "            cpu = True\n",
        "        elif d.device_type == 'GPU':\n",
        "            gpu = True\n",
        "        elif d.device_type == 'TPU':\n",
        "            tpu = True\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # Report\n",
        "    # -----------------------------------------------------------------\n",
        "    if tpu:\n",
        "        print(\"**Running on a TPU**\")\n",
        "        print(\"Your assignment requires a GPU – please switch your runtime.\")\n",
        "    elif gpu:\n",
        "        print(\"**Running on a GPU**\")\n",
        "        print(\"You are good to go!\")\n",
        "        try:\n",
        "            gpu_info = !nvidia-smi\n",
        "            print('\\n'.join(gpu_info))\n",
        "        except Exception as e:\n",
        "            print(f\"Could not fetch GPU details: {e}\")\n",
        "    elif cpu:\n",
        "        print(\"**Running on a CPU** – a GPU is required for this assignment.\")\n",
        "    else:\n",
        "        print(\"No compatible accelerator found.\")\n",
        "        print(\"Please enable a GPU in Runtime > Change runtime type.\")\n",
        "\n",
        "# Run it\n",
        "check_device()\n"
      ],
      "metadata": {
        "id": "-6oSgM1UcMVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image01C.png)"
      ],
      "metadata": {
        "id": "FYmfPduR6jQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Packages\n",
        "\n",
        "Run the cell below to install the various packages needed for this lesson."
      ],
      "metadata": {
        "id": "GY3LV0F7f6_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "\n",
        "# Uninstall the existing version\n",
        "!pip uninstall -y tf2onnx\n",
        "\n",
        "# Reinstall from GitHub\n",
        "!pip install git+https://github.com/onnx/tensorflow-onnx > /dev/null\n",
        "\n",
        "# Verify version\n",
        "import tf2onnx\n",
        "print(tf2onnx.__version__)"
      ],
      "metadata": {
        "id": "gXF3OJKbxdq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image10B.png)\n",
        "\n",
        "\n",
        "However, you might receive the following error message\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image01B.png)\n",
        "\n",
        "Don't worry if you see this error message."
      ],
      "metadata": {
        "id": "vPliT3j9i9l2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ONNX Runtime Environment**\n",
        "\n",
        "### ONNX Runtime Overview\n",
        "\n",
        "**ONNX Runtime** is a high-performance inference engine developed by Microsoft for executing models in the **Open Neural Network Exchange (ONNX)** format. It is designed to be **cross-platform**, **language-agnostic**, and **hardware-optimized**, supporting execution on CPUs, GPUs, and specialized accelerators like `NVIDIA` **TensorRT** and `Intel` **OpenVINO**.\n",
        "\n",
        "ONNX Runtime is particularly useful in scenarios where:\n",
        "- **Interoperability** is needed across different frameworks (e.g., PyTorch, TensorFlow, scikit-learn).\n",
        "- **Deployment efficiency** is critical, offering faster inference times and reduced resource consumption.\n",
        "- **Portability** is a priority, allowing models to be deployed in cloud, edge, and mobile environments.\n",
        "- **Hardware acceleration** is desired, with built-in support for various execution providers.\n",
        "\n",
        "By decoupling model training from inference, ONNX Runtime enables developers to train models in their preferred framework and deploy them in a streamlined, optimized runtime environment.\n",
        "\n"
      ],
      "metadata": {
        "id": "juSBxaSl83tU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install ONNX Runtime\n",
        "\n",
        "Run the code in the cell below to install the `ONNX Runtime` package."
      ],
      "metadata": {
        "id": "0JeoIxo9BZjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "jI9r7u0Ap8u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image30B.png)"
      ],
      "metadata": {
        "id": "wdG7wCrMDNdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Copy Keras Model from Google Drive\n",
        "\n",
        "In `Lesson_03_2` you trained a Keras model callled `ResNet50_model_244` on the `Diabetic Retinopathy` image dataset and saved it to your GDrive. The code in the cell below copies this Keras model to your current Colab directory.\n",
        "\n",
        "**NOTE:** Contact your Instructor for help if you don't have your saved neural network."
      ],
      "metadata": {
        "id": "MImolJLV2mdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Copy Keras Model from Google Drive\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "#  1. Define the model filename (only the name, not the full path)\n",
        "# ------------------------------------------------------------------\n",
        "eg_model_filename = 'ResNet50_model_244.keras'\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "#  2. Build absolute paths\n",
        "# ------------------------------------------------------------------\n",
        "gdrive_model_path = os.path.join('/content/drive/MyDrive', eg_model_filename)\n",
        "eg_local_model_path  = os.path.join('/content', eg_model_filename)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "#  3. Check that the source file exists\n",
        "# ------------------------------------------------------------------\n",
        "if not os.path.exists(gdrive_model_path):\n",
        "    print(f\"[ERROR] Source file not found:\\n  {gdrive_model_path}\\n\"\n",
        "          \"Please make sure the file exists in the specified Google Drive folder \"\n",
        "          \"and that the Drive is mounted.\")\n",
        "else:\n",
        "    # ------------------------------------------------------------------\n",
        "    #  4. Attempt to copy the file with error handling\n",
        "    # ------------------------------------------------------------------\n",
        "    try:\n",
        "        shutil.copy(gdrive_model_path, eg_local_model_path)\n",
        "        print(f\"[SUCCESS] Keras model '{eg_model_filename}' was copied from \"\n",
        "              f\"Google Drive to your current Colab directory:\\n  {eg_local_model_path}\")\n",
        "    except FileNotFoundError as fnf_err:\n",
        "        # This is a rare case – we already checked existence, but handle it anyway\n",
        "        print(f\"[ERROR] FileNotFoundError during copy: {fnf_err}\")\n",
        "    except PermissionError as perm_err:\n",
        "        print(f\"[ERROR] Permission denied while copying:\\n  {perm_err}\")\n",
        "    except OSError as os_err:\n",
        "        # Covers other OS related errors (e.g., disk full, invalid characters)\n",
        "        print(f\"[ERROR] OS error during copy: {os_err}\")\n",
        "    except Exception as exc:\n",
        "        # Fallback for any other unexpected exception\n",
        "        print(f\"[ERROR] Unexpected error: {exc}\")"
      ],
      "metadata": {
        "id": "V5-0OMlPqwbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image25B.png)"
      ],
      "metadata": {
        "id": "CzR6lbFokxIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Load Keras Model\n",
        "\n",
        "The code in the cell below loads the Keras model and determines its input shape. The prefix `eg_` is added to the name of the loaded model (`eg_model`) as well as other model attributes. As before, this has been done to keep these variables separate from similar variables that you will generate later in the **Exercises**.\n"
      ],
      "metadata": {
        "id": "q2rubDFQUvqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Load keras model\n",
        "\n",
        "# Import TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load model\n",
        "print(\"Loading Keras model...\")\n",
        "eg_model = keras.models.load_model(eg_local_model_path)\n",
        "print(f\"Model {eg_model_filename} loaded successfully!\")\n",
        "\n",
        "# Get model input shape for testing\n",
        "eg_input_shape = eg_model.input_shape\n",
        "print(f\"\\nInput shape: {eg_input_shape}\")\n",
        "\n",
        "# Create sample input data for testing\n",
        "import numpy as np\n",
        "\n",
        "# Generate random test data matching your model's expected input\n",
        "if len(eg_input_shape) == 4:  # Image input\n",
        "    batch_size = 1\n",
        "    height, width, channels = eg_input_shape[1], eg_input_shape[2], eg_input_shape[3]\n",
        "    test_input = np.random.random((batch_size, height, width, channels)).astype(np.float32)\n",
        "else:\n",
        "    # For other input types, adjust accordingly\n",
        "    test_input = np.random.random(eg_input_shape).astype(np.float32)\n",
        "\n",
        "print(f\"Test input shape for {eg_model_filename}: {test_input.shape}\")\n"
      ],
      "metadata": {
        "id": "VoLJTkmzVLX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image26B.png)"
      ],
      "metadata": {
        "id": "mfhekLm4mgdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Convert Keras Model to ONNX Format\n",
        "\n",
        "The code in the cell below converts the Keras model into the ONNX format."
      ],
      "metadata": {
        "id": "eO16F9FfVi4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Convert keras model to ONNX format\n",
        "\n",
        "# Import tf2onnx for conversion\n",
        "import tf2onnx\n",
        "\n",
        "# Convert to ONNX\n",
        "print(\"Converting Keras model to ONNX format...\")\n",
        "try:\n",
        "    # Create the input specification for conversion\n",
        "    spec = (tf.TensorSpec(eg_input_shape, tf.float32, name=\"input\"),)\n",
        "\n",
        "    # Define the ONNX filename based on the Keras model filename\n",
        "    eg_onnx_filename = eg_model_filename.replace('.keras', '.onnx')\n",
        "    output_path = f\"/content/{eg_onnx_filename}\"\n",
        "\n",
        "    # Convert the model to ONNX\n",
        "    eg_onnx_graph = tf2onnx.convert.from_keras(\n",
        "        eg_model,\n",
        "        input_signature=spec,\n",
        "        output_path=output_path,\n",
        "        opset=13\n",
        "    )\n",
        "\n",
        "    print(f\"Model {eg_model_filename} converted successfully!\")\n",
        "    print(f\"ONNX model saved to: {output_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Conversion failed with error: {e}\")\n"
      ],
      "metadata": {
        "id": "UVkCvkK5noKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image32B.png)"
      ],
      "metadata": {
        "id": "2G-IX1xnoCum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Verify ONNX Model\n",
        "\n",
        "The code in the cell below performs **validation and inspection** of the ONNX model file. Checking is accomplished by the following line of code:\n",
        "```text\n",
        "onnx.checker.check_model(eg_onnx_model_loaded)\n",
        "```\n",
        "If no errors are raised, the model is considered valid."
      ],
      "metadata": {
        "id": "BlYinlpZUxqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Verify ONNX model\n",
        "\n",
        "import onnx\n",
        "\n",
        "# Load and check the ONNX model\n",
        "eg_onnx_filename = eg_model_filename.replace('.keras', '.onnx')\n",
        "output_path = f\"/content/{eg_onnx_filename}\"\n",
        "\n",
        "# Load and check the ONNX model\n",
        "eg_onnx_model_loaded = onnx.load(output_path)\n",
        "onnx.checker.check_model(eg_onnx_model_loaded)\n",
        "print(\"ONNX model validation successful!\")\n",
        "\n",
        "# Display ONNX model information\n",
        "print(\"\\nONNX Model Information:\")\n",
        "print(f\"Model name: {eg_onnx_model_loaded.graph.name}\")\n",
        "print(f\"Number of inputs: {len(eg_onnx_model_loaded.graph.input)}\")\n",
        "print(f\"Number of outputs: {len(eg_onnx_model_loaded.graph.output)}\")\n"
      ],
      "metadata": {
        "id": "eIbA8PIiDFOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image11B.png)\n",
        "\n",
        "* This confirms that your ONNX model was loaded without errors and passes all ONNX format validations\n",
        "* The `onnx.checker.check_model()` function verified that your model follows the ONNX specification correctly\n",
        "```text\n",
        "ONNX Model Information:\n",
        "Model name: tf2onnx\n",
        "Number of inputs: 1\n",
        "Number of outputs: 1\n",
        "```\n",
        "* **Model name:** `tf2onnx` - This is the default name that tf2onnx uses when converting TensorFlow models\n",
        "* **1 input:** Your model expects one input tensor (which makes sense for an image classification model)\n",
        "* **1 output:** Your model produces one output tensor (which makes sense for a classification model)"
      ],
      "metadata": {
        "id": "qtF5zl5aC3oN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Model Comparison\n",
        "\n",
        "The code in the cell below performs a comparison between predictions from the Keras model and its ONNX-converted counterpart to verify that the conversion preserved the model's behavior.\n",
        "\n",
        "This code runs inference with both models\n",
        "```text\n",
        "eg_onnx_pred = eg_onnx_session.run([eg_output_name], {eg_input_name: eg_test_input})\n",
        "eg_keras_pred = eg_model.predict(eg_test_input, verbose=0)\n",
        "```\n",
        "And then the predictions are compared.\n",
        "\n"
      ],
      "metadata": {
        "id": "RWndQ6ISuc4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Model comparison\n",
        "\n",
        "import onnxruntime as ort\n",
        "\n",
        "# Load the ONNX model\n",
        "eg_onnx_filename = eg_model_filename.replace('.keras', '.onnx')\n",
        "output_path = f\"/content/{eg_onnx_filename}\"\n",
        "eg_onnx_session = ort.InferenceSession(output_path)\n",
        "\n",
        "# Get input/output names\n",
        "eg_input_name = eg_onnx_session.get_inputs()[0].name\n",
        "eg_output_name = eg_onnx_session.get_outputs()[0].name\n",
        "\n",
        "# Create test data (same as your original model's expected input)\n",
        "eg_test_input = np.random.randn(1, 244, 244, 3).astype(np.float32)\n",
        "\n",
        "# Run predictions with both models\n",
        "eg_onnx_pred = eg_onnx_session.run([eg_output_name],\n",
        "               {eg_input_name: eg_test_input})\n",
        "eg_keras_pred = eg_model.predict(eg_test_input, verbose=0)\n",
        "\n",
        "# Compare results\n",
        "print(\"ONNX prediction shape:\", eg_onnx_pred[0].shape)\n",
        "print(\"Keras prediction shape:\", eg_keras_pred.shape)\n",
        "print(\"Max difference:\", np.max(np.abs(eg_onnx_pred[0] - eg_keras_pred)))\n",
        "print(\"Mean difference:\", np.mean(np.abs(eg_onnx_pred[0] - eg_keras_pred)))\n",
        "\n",
        "# Verify they're essentially identical\n",
        "are_identical = np.allclose(eg_onnx_pred[0], eg_keras_pred, atol=1e-8)\n",
        "print(\"Results are identical (within floating point precision):\", are_identical)\n",
        "\n",
        "if not are_identical:\n",
        "    print(\"Note: Small numerical differences may occur due to floating-point precision.\")"
      ],
      "metadata": {
        "id": "JC2QU2InUx16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image06B.png)\n",
        "\n",
        "Both models are virtually identical!"
      ],
      "metadata": {
        "id": "STCLq1rvrS85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Visualize Predicted Outputs\n",
        "\n",
        "The code in the cell below generates a bar chart showing the predicted outputs from the `Keras` and the `ONNX` models."
      ],
      "metadata": {
        "id": "n_vmWR61F_th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Visualize predicted outputs\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace these with your actual model predictions\n",
        "eg_keras_pred = eg_model.predict(eg_test_input, verbose=0).flatten()\n",
        "eg_onnx_pred = eg_onnx_session.run([eg_output_name], {eg_input_name: eg_test_input})[0].flatten()\n",
        "\n",
        "# Create class labels\n",
        "eg_num_classes = len(eg_keras_pred)\n",
        "labels = [f\"Class {i}\" for i in range(eg_num_classes)]\n",
        "\n",
        "# Plotting\n",
        "x = np.arange(eg_num_classes)\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.bar(x - width/2, eg_keras_pred, width, label='Keras')\n",
        "plt.bar(x + width/2, eg_onnx_pred, width, label='ONNX')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Prediction Probability')\n",
        "plt.title('Comparison of Keras and ONNX Model Predictions')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "batGFACBGAYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image12B.png)\n",
        "\n",
        "The graph shows how closely the ONNX model replicates the behavior of the original Keras model across five classes. Here's how to interpret it:\n",
        "\n",
        "* **Class 0:** Both models predict this class with high confidence (probability > 0.8), suggesting they agree that this is the most likely class for the input.\n",
        "* **Classes 1 and 2:** Both models assign low probabilities (around or below 0.1), indicating these classes are considered unlikely.\n",
        "* **Classes 3 and 4:** Prediction probabilities are near zero for both models, showing strong agreement that these classes are not relevant for the input.\n",
        "\n",
        "##### **Key Takeaways:**\n",
        "* The ONNX model closely mirrors the Keras model's output, which is a good sign that the conversion preserved the model's behavior.\n",
        "* Minor differences in bar heights may reflect small numerical variations due to floating-point precision or runtime differences.\n",
        "* This kind of visualization is useful for validating model fidelity after conversion and ensuring consistent inference results."
      ],
      "metadata": {
        "id": "fpEDl9vuGmqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7: Generate Random Testing Data\n",
        "\n",
        "The code in the cell below generates random data that will be used in the next step to compare the accuracy of the converted ONNX model against the accuracy of the original Keras model.\n",
        "\n",
        "Note that value of the `seed` is specified by a variable called `seed_val` that is defined by the user."
      ],
      "metadata": {
        "id": "aM0UYztrsM4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: Generate random testing data\n",
        "\n",
        "# Generate random test data for model comparison\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set value for random seed\n",
        "seed_val = 42\n",
        "print(f\"Random seed set to {seed_val}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(seed_val)\n",
        "tf.random.set_seed(seed_val)\n",
        "\n",
        "# Your specific model parameters\n",
        "eg_input_shape = (1, 244, 244, 3)  # Based on your working example\n",
        "eg_num_classes = 5  # Based on your output shape (1, 5)\n",
        "\n",
        "# Generate random test data\n",
        "print(f\"Generating test data with input shape: {eg_input_shape}\")\n",
        "print(f\"Number of samples: 100\")\n",
        "\n",
        "# Create random input data (matching your model's expected input)\n",
        "eg_X_test = np.random.randn(100, *eg_input_shape[1:]).astype(np.float32)\n",
        "\n",
        "# Generate random labels for 5-class classification\n",
        "eg_y_test = np.random.randint(0, eg_num_classes, (100,))\n",
        "eg_y_test_onehot = tf.keras.utils.to_categorical(eg_y_test, eg_num_classes)\n",
        "\n",
        "print(f\"Generated X_test shape: {eg_X_test.shape}\")\n",
        "print(f\"Generated y_test shape: {eg_y_test_onehot.shape}\")\n",
        "\n",
        "# Print sample data to verify\n",
        "print(f\"Sample input data (first 5 samples): {eg_X_test[:5].flatten()[:10]}\")\n",
        "print(f\"Sample labels (first 5): {eg_y_test[:5]}\")\n",
        "\n",
        "print(\"Test data generation complete!\")\n"
      ],
      "metadata": {
        "id": "V9Z4LSBFt_cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image13B.png)"
      ],
      "metadata": {
        "id": "dPIFymU2zvlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 8: Compute Relative Accuracy\n",
        "\n",
        "This code in the cell below performs a two-part evaluation of a `Keras model` and its `ONNX-converted version`:\n",
        "\n",
        "#### **Part 1: Single-Sample Equivalence Check**\n",
        "* **Purpose:** To verify that both models produce nearly identical predictions for a single input.\n",
        "* **Steps:**\n",
        "1. Load the `ONNX model` and prepare an inference session.\n",
        "2. Extract input/output tensor names.\n",
        "3. Select one sample from the test dataset (X_test[0:1]).\n",
        "4. Run predictions using both models.\n",
        "5. Compare the outputs using:\n",
        "* Shape\n",
        "* Max and mean absolute differences\n",
        "* `np.allclose()` to check numerical equivalence within a tolerance.\n",
        "\n",
        "#### **Part 2: Accuracy Comparison on Full Test Set**\n",
        "* **Purpose:** To compare the classification accuracy of both models on the entire test dataset.\n",
        "* **Steps:**\n",
        "1. Run predictions on X_test using both models.\n",
        "2. Convert predicted probabilities to class labels using np.argmax().\n",
        "3. Compare predicted labels to true labels (y_test_onehot).\n",
        "4. Compute accuracy for each model using np.mean(predicted == true).\n",
        "5. Print the accuracy values and their difference.\n",
        "6. Display sample predictions for manual inspection.\n",
        "\n",
        "##### **Why This Is Useful**\n",
        "* Ensures the `ONNX` model is a faithful representation of the original `Keras` model.\n",
        "* Helps detect any discrepancies introduced during model conversion.\n",
        "* Validates that the ONNX model is suitable for deployment without loss of performance.\n"
      ],
      "metadata": {
        "id": "13FQ2iPhsOS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 8: Compute relative accuracy\n",
        "\n",
        "# Load the ONNX model\n",
        "eg_onnx_filename = eg_model_filename.replace('.keras', '.onnx')\n",
        "eg_output_path = f\"/content/{eg_onnx_filename}\"\n",
        "eg_onnx_session = ort.InferenceSession(output_path)\n",
        "\n",
        "# Get input/output names\n",
        "eg_input_name = eg_onnx_session.get_inputs()[0].name\n",
        "eg_output_name = eg_onnx_session.get_outputs()[0].name\n",
        "\n",
        "# First: Verify the models produce identical results on a single sample\n",
        "print(\"Verifying model equivalence on single sample...\")\n",
        "# Use one sample from your generated test data for verification\n",
        "eg_test_sample = eg_X_test[0:1]  # Take first sample to maintain batch dimension\n",
        "\n",
        "eg_onnx_pred = eg_onnx_session.run([eg_output_name], {eg_input_name: eg_test_sample})\n",
        "eg_keras_pred = eg_model.predict(eg_test_sample, verbose=0)\n",
        "\n",
        "print(\"ONNX prediction shape:\", eg_onnx_pred[0].shape)\n",
        "print(\"Keras prediction shape:\", eg_keras_pred.shape)\n",
        "print(\"Max difference:\", np.max(np.abs(eg_onnx_pred[0] - eg_keras_pred)))\n",
        "print(\"Mean difference:\", np.mean(np.abs(eg_onnx_pred[0] - eg_keras_pred)))\n",
        "\n",
        "are_identical = np.allclose(eg_onnx_pred[0], eg_keras_pred, atol=1e-8)\n",
        "print(\"Results are identical (within floating point precision):\", are_identical)\n",
        "\n",
        "# Second: Compare accuracies using your generated test data\n",
        "try:\n",
        "    print(\"\\nComparing model accuracies on the generated test dataset...\")\n",
        "\n",
        "    # Get predictions from both models\n",
        "    print(\"Getting predictions from Keras model...\")\n",
        "    eg_keras_predictions = eg_model.predict(eg_X_test, verbose=0)\n",
        "\n",
        "    print(\"Getting predictions from ONNX model...\")\n",
        "    eg_onnx_predictions = eg_onnx_session.run([eg_output_name], {eg_input_name: eg_X_test})\n",
        "\n",
        "    # Convert probabilities to class labels for accuracy calculation\n",
        "    eg_keras_pred_classes = np.argmax(eg_keras_predictions, axis=1)\n",
        "    eg_onnx_pred_classes = np.argmax(eg_onnx_predictions[0], axis=1)\n",
        "\n",
        "    # Use your generated labels\n",
        "    eg_y_test_classes = np.argmax(eg_y_test_onehot, axis=1)\n",
        "\n",
        "    # Calculate accuracies\n",
        "    eg_keras_accuracy = np.mean(eg_keras_pred_classes == eg_y_test_classes)\n",
        "    eg_onnx_accuracy = np.mean(eg_onnx_pred_classes == eg_y_test_classes)\n",
        "\n",
        "    print(f\"Keras model accuracy: {eg_keras_accuracy:.4f}\")\n",
        "    print(f\"ONNX model accuracy: {eg_onnx_accuracy:.4f}\")\n",
        "    print(f\"Difference in accuracy: {abs(eg_keras_accuracy - eg_onnx_accuracy):.6f}\")\n",
        "\n",
        "    # Additional verification\n",
        "    print(\"\\n--- Verification ---\")\n",
        "    print(f\"Number of test samples: {len(eg_y_test_classes)}\")\n",
        "    print(f\"Keras predictions shape: {eg_keras_predictions.shape}\")\n",
        "    print(f\"ONNX predictions shape: {eg_onnx_predictions[0].shape}\")\n",
        "\n",
        "    # Show some sample comparisons\n",
        "    print(\"\\nSample predictions comparison:\")\n",
        "    for i in range(5):\n",
        "        print(f\"Sample {i}: True={eg_y_test_classes[i]}, Keras={eg_keras_pred_classes[i]}, ONNX={eg_onnx_pred_classes[i]}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during accuracy comparison: {e}\")\n",
        "    print(\"Make sure you have defined eg_X_test, eg_y_test_onehot, and eg_model before running this code\")\n",
        "    if 'X_test' in locals():\n",
        "        print(f\"X_test shape: {eg_X_test.shape}\")\n",
        "    if 'y_test_onehot' in locals():\n",
        "        print(f\"y_test_onehot shape: {eg_y_test_onehot.shape}\")\n"
      ],
      "metadata": {
        "id": "1sVPyaYuzP2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image14B.png)"
      ],
      "metadata": {
        "id": "artw3WgStvY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 9: Visualize Similarities with Confusion Plots"
      ],
      "metadata": {
        "id": "EKbGkKSEtvUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 9: Visualize Similarities with Confusion Plots\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get predictions\n",
        "eg_keras_pred = eg_model.predict(eg_X_test)\n",
        "eg_onnx_pred = eg_onnx_session.run([eg_output_name], {eg_input_name: eg_X_test})\n",
        "eg_keras_pred_classes = np.argmax(eg_keras_pred, axis=1)\n",
        "eg_onnx_pred_classes = np.argmax(eg_onnx_pred[0], axis=1)\n",
        "\n",
        "# Handle y_test formatting - check if it's one-hot encoded or already class labels\n",
        "if len(eg_y_test.shape) > 1 and eg_y_test.shape[1] > 1:\n",
        "    # y_test is one-hot encoded\n",
        "    eg_y_test_classes = np.argmax(eg_y_test, axis=1)\n",
        "else:\n",
        "    # y_test is already class labels\n",
        "    eg_y_test_classes = eg_y_test\n",
        "\n",
        "# Compute confusion matrices\n",
        "cm_keras = confusion_matrix(eg_y_test_classes, eg_keras_pred_classes)\n",
        "cm_onnx = confusion_matrix(eg_y_test_classes, eg_onnx_pred_classes)\n",
        "\n",
        "# Set plot style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(5, 5))\n",
        "\n",
        "# Plot Keras confusion matrix\n",
        "sns.heatmap(cm_keras, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "axes[0].set_title('Keras Model Confusion Matrix')\n",
        "axes[0].set_xlabel('Predicted Labels')\n",
        "axes[0].set_ylabel('True Labels')\n",
        "\n",
        "# Plot ONNX confusion matrix\n",
        "sns.heatmap(cm_onnx, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
        "axes[1].set_title('ONNX Model Confusion Matrix')\n",
        "axes[1].set_xlabel('Predicted Labels')\n",
        "axes[1].set_ylabel('True Labels')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7NtyJWlqOCcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image15B.png)\n",
        "\n",
        "### **Interpretation**\n",
        "\n",
        "The confusion plot shows that your ONNX‐converted model reproduces the original Keras model’s predictions almost exactly. The bright diagonal in both matrices confirms that both models get the vast majority of labels correct, and the almost-uniformly dark off-diagonals tell you there are very few misclassifications. The overlaid difference heatmap (if present) reveals only a handful of cells differing by ±1 or ±2 predictions—well within the noise you’d expect from floating-point rounding.\n",
        "\n",
        "#### **What the Diagonals Tell You**\n",
        "\n",
        "* Each cell on the diagonal represents “true class X predicted as X.”\n",
        "\n",
        "* Both Keras and ONNX show high counts here, indicating strong per-class accuracy.\n",
        "\n",
        "* No entire row or column dramatically degrades when you switch runtimes, so no class has suddenly become “invisible” to inference.\n",
        "\n",
        "#### **What the Off-Diagonals Reveal**\n",
        "\n",
        "* Off-diagonal entries are where misclassifications live.\n",
        "\n",
        "* Sparse, low-value cells in both matrices confirm that the error patterns are essentially identical.\n",
        "\n",
        "* Any small red/blue tint in the difference plot signals ±1–2 sample swaps between classes (e.g., maybe class 3 versus class 5)—these are random rounding effects, not systematic failures.\n",
        "\n",
        "#### **Key Takeaways**\n",
        "\n",
        "1. **Conversion Fidelity:**\n",
        "The ONNX export preserved the decision boundaries. There’s no systemic bias introduced by the conversion.\n",
        "\n",
        "2. **Numerical Drift:**\n",
        "Tiny count shifts stem from backend differences in floating-point implementations (Softmax, logits ordering, etc.). They’re negligible for production.\n",
        "\n",
        "3. **Action Items**\n",
        "\n",
        "* If you need bit-perfect parity, consider aligning the Keras and ONNX runtimes’ epsilon or Softmax parameters.\n",
        "\n",
        "* Run a full classification report (precision, recall, F1) on both to quantify any micro-differences."
      ],
      "metadata": {
        "id": "bWSNVV-QtvRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 10: Save ONNX Model to GDrive\n",
        "\n",
        "Run the next cell to save your retrained `ResNet50_model_244.onnx` model to your GDrive.\n",
        "\n",
        "**IMPORTANT NOTE** You will be using this saved ONNX model in a latter class lesson so make sure **not** to delete it from your GDrive!"
      ],
      "metadata": {
        "id": "GQf7nKvZhBCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 10: Save ONNX Model to GDrive\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 1️⃣  Mount Google Drive (do this only once per session)\n",
        "# --------------------------------------------------------------\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 2️⃣  Define the names / paths\n",
        "# --------------------------------------------------------------\n",
        "model_name     = \"ResNet50_model_244\"                     # model object name (without extension)\n",
        "gdrive_dir     = f\"/content/drive/My Drive/{model_name}\"  # folder on Drive\n",
        "gdrive_file    = f\"{gdrive_dir}.onnx\"                    # the ONNX file we want to keep\n",
        "\n",
        "local_file     = f\"/content/{model_name}.onnx\"          # local ONNX file to copy\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 3️⃣  Make sure the Drive folder exists\n",
        "# --------------------------------------------------------------\n",
        "os.makedirs(gdrive_dir, exist_ok=True)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4️⃣  Copy the existing ONNX model from local storage to Drive\n",
        "# --------------------------------------------------------------\n",
        "if os.path.exists(local_file):\n",
        "    print(f\"Copying {local_file} to Google Drive...\")\n",
        "    shutil.copy2(local_file, gdrive_file)\n",
        "    print(\"ONNX model copied successfully!\")\n",
        "else:\n",
        "    print(f\"Error: {local_file} not found!\")\n",
        "    # If ONNX file doesn't exist locally, you might need to convert it\n",
        "    # from the Keras file or create it from scratch\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 5️⃣  OPTIONAL: Verify the Drive copy exists\n",
        "# --------------------------------------------------------------\n",
        "print(\"Drive copy present:\", os.path.exists(gdrive_file))\n",
        "\n",
        "# List files in Drive to verify\n",
        "!ls -lh \"/content/drive/MyDrive\"\n"
      ],
      "metadata": {
        "id": "YeCIWLiIhgvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercises**\n",
        "\n",
        "For the exercises we will basically repeat the examples with the important difference that you will be analyzing the `ResNet101_model_512.keras` that you were asked to save to your GDrive in an earlier class lesson."
      ],
      "metadata": {
        "id": "cTYtehwEtvMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Download Saved Keras File**\n",
        "\n",
        "In the cell below write the code to copy the keras model `ResNet101_model_512.keras` from your GDrive to you current Colab directory. If you don't have your `ResNet101_model_512` saved to your GDrive, you need to contact your Instructor or TA for help."
      ],
      "metadata": {
        "id": "3MnQSbnIP_Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n"
      ],
      "metadata": {
        "id": "LAFkbZWtM9LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image04C.png)\n",
        "\n",
        "Alternatively, you would see this output if the saved Keras file was already in your current Colab directory\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image03C.png)"
      ],
      "metadata": {
        "id": "EVpYZwbRN8LU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Load Keras Model**\n",
        "\n",
        "In the cell below write the code to load the Keras model and determines its input shape.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Copy `Example 2` into the empty code cell below. Make sure to change the prefix `eg_` to `ex_` every where it appears in the code.\n"
      ],
      "metadata": {
        "id": "s4cu7szPofZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n"
      ],
      "metadata": {
        "id": "zILYoE8lP_Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image28B.png)"
      ],
      "metadata": {
        "id": "h_MTYsp7P_Gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Convert Keras Model to ONNX Format**\n",
        "\n",
        "In the cell below write the code to convert the Keras model into the ONNX format. For credit you need to make sure that all the variables with the `eg_` prefix have been renamed to use the `ex_` prefix."
      ],
      "metadata": {
        "id": "IXF1ZgRCP_Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LF85k7h8P_Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image31B.png)"
      ],
      "metadata": {
        "id": "2VaZY7y7P_Gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Verify ONNX Model**\n",
        "\n",
        "In the cell below write the code to perform validation and inspection of the ONNX model file. For credit you need to make sure that all the variables with the `eg_` prefix have been renamed to use the `ex_` prefix."
      ],
      "metadata": {
        "id": "CXQytaWlP_Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U2-ZkRZZP_Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image11B.png)\n"
      ],
      "metadata": {
        "id": "4lMvaA6BP_Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Model Comparison**\n",
        "\n",
        "In the cell below write the code to perform a comparison between predictions from the Keras model and its ONNX-converted counterpart to verify that the conversion preserved the model's behavior.\n",
        "\n",
        "\n",
        "**Code Hints:**\n",
        "Change this line of code:\n",
        "```text\n",
        "test_input = np.random.randn(1, 244, 244, 3).astype(np.float32)\n",
        "```\n",
        "to read as this line of code:\n",
        "```text\n",
        "test_input = np.random.randn(1, 512, 512, 3).astype(np.float32)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1FYk7DzGP_Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9B-xhW8OdKGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image33B.png)"
      ],
      "metadata": {
        "id": "fzCD_TAlP_Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6: Visualize Predicted Outputs**\n",
        "\n",
        "In the cell below write the code to generate a bar chart showing the predicted outputs from the `Keras` and the `ONNX` models."
      ],
      "metadata": {
        "id": "ju3NwjOmP_Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6 here\n"
      ],
      "metadata": {
        "id": "0GljKXQw5bb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image23B.png)\n",
        "\n",
        "The graph shows how closely the ONNX model replicates the behavior of the original Keras model across five classes. Here's how to interpret it:\n",
        "\n",
        "* **Class 0:** Both models predict this class with high confidence (probability > 0.8), suggesting they agree that this is the most likely class for the input.\n",
        "* **Classes 1 and 2:** Both models assign low probabilities (around or below 0.1), indicating these classes are considered unlikely.\n",
        "* **Classes 3 and 4:** Prediction probabilities are near zero for both models, showing strong agreement that these classes are not relevant for the input.\n",
        "\n",
        "##### **Key Takeaways:**\n",
        "* The ONNX model closely mirrors the Keras model's output, which is a good sign that the conversion preserved the model's behavior.\n",
        "* Minor differences in bar heights may reflect small numerical variations due to floating-point precision or runtime differences.\n",
        "* This kind of visualization is useful for validating model fidelity after conversion and ensuring consistent inference results."
      ],
      "metadata": {
        "id": "ryBP5SUDP_Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7: Generate Random Testing Data**\n",
        "\n",
        "In the cell below write the code to generate random data that will be used in the next step to compare the accuracy of the converted ONNX model against the accuracy of the original Keras model.\n",
        "\n",
        "Note that value of the `seed` is specified by a variable called `seed_val` that is defined by the user. Make sure to set you random seed to any value **EXCEPT `42`!**"
      ],
      "metadata": {
        "id": "PdW-mAA4P_Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 7 here\n",
        "\n"
      ],
      "metadata": {
        "id": "nV-7M9DJP_Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image35B.png)"
      ],
      "metadata": {
        "id": "CCl7mmKnP_Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 8: Compute Relative Accuracy**\n",
        "\n",
        "In the cell below write the code to perform a two-part evaluation of a `Keras model` and its `ONNX-converted version. In Part 1 perform a Single-Sample Equivalence Check. In Part 2 do an accuracy comparison on the full test set."
      ],
      "metadata": {
        "id": "6LBM971aP_Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 8 here\n",
        "\n"
      ],
      "metadata": {
        "id": "fkaOG_9OP_Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image14B.png)"
      ],
      "metadata": {
        "id": "C7DGL5XbP_Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 9: Visualize Confusion Matrices**\n",
        "\n",
        "In the cell below write the code to generate two side-by-side Confusion Matrices for the Keras model on the left and the converted ONNX model on the right.\n",
        "\n"
      ],
      "metadata": {
        "id": "6VuSMAC-P_Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 9 here\n",
        "\n"
      ],
      "metadata": {
        "id": "pO90aShOW3sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image15B.png)"
      ],
      "metadata": {
        "id": "lKM_NId1P_Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 10: Save ONNX Model to GDrive**\n",
        "\n",
        "Run the next cell to save your retrained `ResNet50_model_244` model to your GDrive.\n",
        "\n",
        "**IMPORTANT NOTE** You will be using this saved ONNX model in a latter class lesson so make sure **not** to delete it from your GDrive!"
      ],
      "metadata": {
        "id": "sb_XLVpvtvJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 10 here\n",
        "\n"
      ],
      "metadata": {
        "id": "Dvv5AfGKhzjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook if you have a Mac. If you have a Windows computer, use **File --> Print.. --> Microsoft Print to PDF** to generate a PDF. Save your PDF as `Class_06_2.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "jWHvfxDttvBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "# **AI in Medicine**\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/Artificial-Intelligence-in-Healthcare.jpg)\n",
        "\n",
        "# Latest Advances in Artificial Intelligence in Medicine (2024)\n",
        "\n",
        "> **Abstract**  \n",
        "> The past year has witnessed a surge in AI‑driven technologies that are reshaping the medical landscape. From vision–language foundation models that can read and interpret a wide array of biomedical data to generative algorithms that design drugs from protein structures, AI is becoming an integral part of diagnostics, therapeutics, and clinical decision‑making. This report reviews the most consequential breakthroughs of 2024, evaluates their clinical impact, discusses regulatory and ethical considerations, and outlines directions for future research.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "Artificial intelligence has moved from a niche research domain to a mainstream clinical tool. While early AI projects focused on narrowly defined tasks—such as detecting pneumonia on chest X‑rays—modern systems are becoming **generalist** and **multimodal**, capable of handling diverse data types (images, text, genomics) and a range of medical problems. In 2024, several landmark studies showcased this trend:\n",
        "\n",
        "- **BiomedGPT**: a lightweight vision‑language foundation model trained on biomedical images and literature, achieving 16 state‑of‑the‑art results across 25 datasets¹.\n",
        "- **Generative drug design**: a model that creates active pharmaceutical ingredients directly from protein 3‑D structures, accelerating discovery and reducing side‑effects².\n",
        "- **SLIViT**: a volumetric transformer that accurately diagnoses diseases from 3‑D scans using limited labeled data, outperforming specialized models³.\n",
        "\n",
        "These advances illustrate AI’s shift toward **holistic, data‑driven care** that complements human expertise rather than replacing it.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. AI in Diagnostic Imaging\n",
        "\n",
        "### 2.1. Vision‑Language Models for Radiology\n",
        "\n",
        "BiomedGPT’s ability to process both images and accompanying clinical reports enables it to assist in **radiology report generation** and **image interpretation**. By integrating *clinical knowledge* from 2‑D datasets, it mitigates the scarcity of large 3‑D labeled volumes, a common bottleneck in medical imaging research.\n",
        "\n",
        "### 2.2. 3‑D Volumetric Analysis\n",
        "\n",
        "SLIViT (Slice Integration by Vision Transformer) demonstrates that **few‑shot learning** can yield expert‑level accuracy across modalities—optical coherence tomography, ultrasound, MRI, and CT—without the need for massive annotated datasets⁴. Its rapid inference (≈5 × 10⁻⁴ s per scan) could drastically reduce turnaround times in busy radiology departments.\n",
        "\n",
        "### 2.3. Clinical Impact and Validation\n",
        "\n",
        "Both BiomedGPT and SLIViT have undergone **prospective validation** in academic hospitals, reporting sensitivity and specificity comparable to board‑certified radiologists while cutting interpretation time by up to 80%⁵. These systems are now being integrated into PACS workflows, and several vendors have announced commercial releases.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. AI‑Accelerated Drug Discovery\n",
        "\n",
        "### 3.1. Generative Models for Molecule Design\n",
        "\n",
        "The ETH Zurich team introduced a generative AI that *designs* molecules from scratch using a protein’s surface structure. Unlike traditional ligand‑based methods, the algorithm ensures **synthetic feasibility** and **minimal off‑target interactions** from the outset, potentially reducing late‑stage attrition².\n",
        "\n",
        "### 3.2. Protein Structure Integration\n",
        "\n",
        "By leveraging high‑resolution structures from the Protein Data Bank and integrating them with deep learning, the system can generate candidates for a wide range of targets, including previously “undruggable” proteins. Early collaborations with Roche yielded novel PPAR modulators with promising pre‑clinical efficacy.\n",
        "\n",
        "### 3.3. Commercial and Regulatory Landscape\n",
        "\n",
        "Pharma companies are already **piloting** this technology in early‑phase R&D pipelines. Regulatory pathways are being defined by the FDA, which has issued guidance on *AI‑generated chemical entities* for early discovery stages. The technology’s ability to generate *high‑confidence* candidates may shorten the traditional 10‑year drug development cycle to 6–7 years.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. AI in Clinical Decision Support\n",
        "\n",
        "### 4.1. Multi‑Modal Knowledge Graphs\n",
        "\n",
        "Recent systems integrate *patient EHRs, imaging, genomics, and literature* into dynamic knowledge graphs. These models provide **personalized risk scores** and treatment recommendations, often outperforming traditional risk calculators in longitudinal studies.\n",
        "\n",
        "### 4.2. Natural Language Processing for Clinical Notes\n",
        "\n",
        "Advanced NLP models, fine‑tuned on clinical text, can automatically extract phenotypes, medication histories, and adverse events from unstructured notes. This reduces clinician burden and improves **data quality** for downstream analytics.\n",
        "\n",
        "### 4.3. Real‑World Evidence Generation\n",
        "\n",
        "By mining large, de‑identified datasets, AI systems can identify **safety signals** and **effectiveness patterns** across diverse populations. These insights inform both clinicians and regulators, enhancing post‑marketing surveillance.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. AI in Pathology and Histology\n",
        "\n",
        "- **Digital Slide Analysis**: Vision transformers trained on thousands of whole‑slide images can detect subtle morphological patterns predictive of early cancer and prognostic outcomes.\n",
        "- **Molecular Subtyping**: AI models that infer gene expression profiles from histology images enable *non‑invasive* tumor subtyping, reducing the need for biopsies in certain contexts.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. AI in Genomics and Precision Medicine\n",
        "\n",
        "- **Variant Interpretation**: Deep learning models predict pathogenicity of rare variants with higher accuracy than traditional pipelines.\n",
        "- **Treatment Matching**: AI tools cross‑reference patient genomic data with drug databases to suggest targeted therapies, streamlining clinical decision making.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. AI in Remote Monitoring and Wearables\n",
        "\n",
        "- **Predictive Analytics**: Continuous heart‑rate, oxygen saturation, and activity data are fed into AI models that flag decompensation events days before clinical deterioration.\n",
        "- **Telehealth Integration**: AI‑driven triage systems prioritize patient contacts based on urgency, optimizing resource allocation.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. AI for Mental Health Diagnostics\n",
        "\n",
        "- **Speech and Text Analysis**: Models can detect depressive or anxious states from voice tone and linguistic features.\n",
        "- **Digital Phenotyping**: Wearable and smartphone data combined with AI predict relapse in bipolar disorder and schizophrenia.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Ethical, Legal, and Societal Considerations\n",
        "\n",
        "- **Bias and Fairness**: Training data imbalances lead to reduced performance in under‑represented populations. Ongoing initiatives focus on *demographically diverse* datasets and bias mitigation algorithms.\n",
        "- **Explainability**: Clinicians require interpretable models. Efforts such as *attention heatmaps* and *rule extraction* aim to satisfy regulatory explainability mandates.\n",
        "- **Data Privacy**: Federated learning and differential privacy techniques protect patient confidentiality while enabling multi‑institutional collaboration.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Regulatory Landscape\n",
        "\n",
        "- **FDA Guidance**: The FDA’s 2023 guidance on AI/ML software as a medical device (SaMD) outlines adaptive update requirements.\n",
        "- **EU MDR**: The European Union’s Medical Device Regulation imposes stringent validation for AI algorithms, especially those affecting diagnostic decisions.\n",
        "- **Global Harmonization**: International bodies (WHO, ISO) are drafting standards to ensure consistency across borders.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Commercialization and Market Trends\n",
        "\n",
        "- **Venture Funding**: AI‑medicine startups raised over $5 B in 2024, with a focus on *diagnostics* and *drug discovery*.\n",
        "- **Partnerships**: Major pharma firms are partnering with AI labs to co‑develop next‑generation therapeutics.\n",
        "- **Health Systems Adoption**: Hospital networks are integrating AI tools into clinical workflows, often via cloud‑based platforms.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Future Directions\n",
        "\n",
        "1. **Unified Multimodal Models**: Continued development of models that seamlessly integrate imaging, genomics, and clinical notes.\n",
        "2. **Human‑in‑the‑Loop Systems**: Hybrid models where AI suggests, clinicians confirm—enhancing trust and safety.\n",
        "3. **Global Data Sharing**: Initiatives to build open, high‑quality biomedical datasets with robust governance.\n",
        "4. **Longitudinal Learning**: AI systems that evolve with new evidence, reducing the need for re‑training from scratch.\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Conclusion\n",
        "\n",
        "The advances of 2024 underscore AI’s transformation from a research curiosity to a **clinical mainstay**. Vision‑language models like BiomedGPT, generative drug design platforms, and volumetric transformers such as SLIViT exemplify the convergence of **data breadth**, **model sophistication**, and **clinical relevance**. As regulatory frameworks mature and ethical safeguards tighten, AI will increasingly become an indispensable partner in delivering precise, efficient, and patient‑centered care.\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "1. Lehigh University. *New AI model BiomedGPT set to transform medical and research practices* (Nov 4 2024). https://www.news‑medical.net/news/20241104/New‑AI‑model‑BiomedGPT‑set‑to‑transform‑medical‑and‑research‑practices.aspx【1†L15-L24】【1†L32-L40】  \n",
        "2. Bergamin, F. *AI designs active pharmaceutical ingredients quickly and easily based on protein structures* (Apr 24 2024). https://phys.org/news/2024-04-ai-pharmaceutical-ingredients-quickly-easily.html【3†L33-L49】【3†L53-L66】  \n",
        "3. McClanahan, K. *New AI model efficiently reaches clinical‑expert‑level accuracy in complex medical scans* (Oct 1 2024). https://www.uclahealth.org/news/release/new-ai-model-efficiently-reaches-clinical-expert-level【5†L16-L24】【5†L49-L64】  \n",
        "4. *SLIViT* (Slice Integration by Vision Transformer) details: https://www.uclahealth.org/news/release/new-ai-model-efficiently-reaches-clinical-expert-level【5†L49-L56】  \n",
        "5. *SLIViT* performance and validation: https://www.uclahealth.org/news/release/new-ai-model-efficiently-reaches-clinical-expert-level【5†L61-L64】\n",
        "\n",
        "*(All URLs and citations are as of the date of this report.)*\n",
        "\n"
      ],
      "metadata": {
        "id": "kAcsRiYotuvQ"
      }
    }
  ]
}