{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHlbXdqCDyGMIclx7dkK8s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_06_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ],
      "metadata": {
        "id": "NssSnfzn2vAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "pGAuC6fFfBbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Module 6: Advanced Topics**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 6 Material\n",
        "\n",
        "* Part 6.1: Reinforcment Learning\n",
        "* **Part 6.2: ONNX Runtime Environment**\n",
        "* Part 6.3: Analysis of DICOM images with Pytorch\n"
      ],
      "metadata": {
        "id": "AfDqayZnfKoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Change your Runtime Now!**\n",
        "\n",
        "For this lesson you must have a **GPU** hardware accelerator (e.g. T4 High-RAM).\n",
        "NOTE: There is no need to use an \"expensive\" GPU like the A-100 for this lesson."
      ],
      "metadata": {
        "id": "EjHLH6yJZnEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ],
      "metadata": {
        "id": "pbG8yxbEfqO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ],
      "metadata": {
        "id": "DWfA2_Wj2nCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "Iv-lUKQ-fuPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerated Run-time Check\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be running a Graphics Processing Unit (GPU) such as the `T4` with high ram enabled."
      ],
      "metadata": {
        "id": "1VCZ3abAZTqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell second\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check for GPU\n",
        "def check_colab_gpu():\n",
        "    print(\"=== Colab GPU Check ===\")\n",
        "\n",
        "    # Check PyTorch\n",
        "    pt_gpu = torch.cuda.is_available()\n",
        "    print(f\"PyTorch GPU available: {pt_gpu}\")\n",
        "\n",
        "    if pt_gpu:\n",
        "        print(f\"PyTorch device count: {torch.cuda.device_count()}\")\n",
        "        print(f\"PyTorch current device: {torch.cuda.current_device()}\")\n",
        "        print(f\"PyTorch device name: {torch.cuda.get_device_name()}\")\n",
        "        print(\"You are good to go!\")\n",
        "\n",
        "    else:\n",
        "        print(\"No compatible device found\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now and start over!\")\n",
        "\n",
        "check_colab_gpu()\n"
      ],
      "metadata": {
        "id": "-6oSgM1UcMVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image02C.png)\n",
        "\n",
        "If you don't see this output, change your `Runtime` to use a GPU."
      ],
      "metadata": {
        "id": "FYmfPduR6jQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ONNX Runtime Environment**\n",
        "\n",
        "### ONNX Runtime Overview\n",
        "\n",
        "**ONNX Runtime** is a high-performance inference engine developed by Microsoft for executing models in the **Open Neural Network Exchange (ONNX)** format. It is designed to be **cross-platform**, **language-agnostic**, and **hardware-optimized**, supporting execution on CPUs, GPUs, and specialized accelerators like `NVIDIA` **TensorRT** and `Intel` **OpenVINO**.\n",
        "\n",
        "ONNX Runtime is particularly useful in scenarios where:\n",
        "- **Interoperability** is needed across different frameworks (e.g., PyTorch, TensorFlow, scikit-learn).\n",
        "- **Deployment efficiency** is critical, offering faster inference times and reduced resource consumption.\n",
        "- **Portability** is a priority, allowing models to be deployed in cloud, edge, and mobile environments.\n",
        "- **Hardware acceleration** is desired, with built-in support for various execution providers.\n",
        "\n",
        "By decoupling model training from inference, ONNX Runtime enables developers to train models in their preferred framework and deploy them in a streamlined, optimized runtime environment.\n",
        "\n"
      ],
      "metadata": {
        "id": "juSBxaSl83tU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install ONNX Runtime\n",
        "\n",
        "Run the code in the cell below to install the `ONNX Runtime` package."
      ],
      "metadata": {
        "id": "0JeoIxo9BZjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall the default CPU version\n",
        "!pip uninstall -y onnxruntime\n",
        "\n",
        "# Install the GPU version\n",
        "!pip install -q onnxruntime-gpu\n",
        "\n",
        "# Install other onnx packages\n",
        "!pip install -q onnx onnxscript"
      ],
      "metadata": {
        "id": "IaqaNtjRYbOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see somthing _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image08D.png)"
      ],
      "metadata": {
        "id": "wdG7wCrMDNdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Copy Keras Model from Google Drive\n",
        "\n",
        "In `Lesson_03_2` you trained a PyTorch model callled `ResNet50_model_244` on the `Diabetic Retinopathy` image dataset, and saved it to your GDrive. The code in the cell below copies this PyTorch model to your current Colab directory.\n",
        "\n",
        "**NOTE:** Contact your Instructor for help if you don't have your saved neural network. You can't complete this lesson without this model."
      ],
      "metadata": {
        "id": "MImolJLV2mdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Copy PyTorch Model from Google Drive\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "#  1. Define the model filename (only the name, not the full path)\n",
        "# ------------------------------------------------------------------\n",
        "eg_model_filename = 'ResNet50_model_244.pth'\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "#  2. Build absolute paths\n",
        "# ------------------------------------------------------------------\n",
        "gdrive_model_path = os.path.join('/content/drive/MyDrive/ResNet50_model_244', eg_model_filename)\n",
        "eg_local_model_path  = os.path.join('/content', eg_model_filename)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "#  3. Check that the source file exists\n",
        "# ------------------------------------------------------------------\n",
        "if not os.path.exists(gdrive_model_path):\n",
        "    print(f\"[ERROR] Source file not found:\\n  {gdrive_model_path}\\n\"\n",
        "          \"Please contact your Instructor or TA for help.\"\n",
        "          \"AI can't help you---you will only fail if you use it!\")\n",
        "else:\n",
        "    # ------------------------------------------------------------------\n",
        "    #  4. Attempt to copy the file with error handling\n",
        "    # ------------------------------------------------------------------\n",
        "    try:\n",
        "        shutil.copy(gdrive_model_path, eg_local_model_path)\n",
        "        print(f\"[SUCCESS] Keras model '{eg_model_filename}' was copied from \"\n",
        "              f\"Google Drive to your current Colab directory:\\n  {eg_local_model_path}\")\n",
        "    except FileNotFoundError as fnf_err:\n",
        "        # This is a rare case – we already checked existence, but handle it anyway\n",
        "        print(f\"[ERROR] FileNotFoundError during copy: {fnf_err}\")\n",
        "    except PermissionError as perm_err:\n",
        "        print(f\"[ERROR] Permission denied while copying:\\n  {perm_err}\")\n",
        "    except OSError as os_err:\n",
        "        # Covers other OS related errors (e.g., disk full, invalid characters)\n",
        "        print(f\"[ERROR] OS error during copy: {os_err}\")\n",
        "    except Exception as exc:\n",
        "        # Fallback for any other unexpected exception\n",
        "        print(f\"[ERROR] Unexpected error: {exc}\")"
      ],
      "metadata": {
        "id": "V5-0OMlPqwbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image09D.png)\n",
        "\n",
        "Howver, if you get this message:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image27D.png)\n",
        "\n",
        "**Follow its instructions!** Your Instructor/TA will show you how to upload the missing file(s) to your Googe Drive."
      ],
      "metadata": {
        "id": "CzR6lbFokxIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Load PyTorch Model\n",
        "\n",
        "The code in the cell below loads the PyTorch model and determines its input shape. The prefix `eg_` is added to the name of the loaded model (`eg_model`) as well as other model attributes. As before, this has been done to keep these variables separate from similar variables that you will generate later, in the **Exercises**.\n"
      ],
      "metadata": {
        "id": "q2rubDFQUvqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Load PyTorch Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1. Recreate the Model Architecture (The Skeleton)\n",
        "# ------------------------------------------------------------------\n",
        "print(\"Recreating ResNet50 architecture...\")\n",
        "\n",
        "# Initialize the base ResNet50\n",
        "eg_model = models.resnet50(weights=None)\n",
        "\n",
        "# Re-apply same custom \"head\" (fc layer) used during training\n",
        "num_features = eg_model.fc.in_features\n",
        "eg_model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(256, 5) # 5 classes for retinopathy\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2. Load the Saved Weights\n",
        "# ------------------------------------------------------------------\n",
        "print(f\"Loading weights from {eg_local_model_path}...\")\n",
        "\n",
        "# Load the state dictionary\n",
        "state_dict = torch.load(eg_local_model_path)\n",
        "eg_model.load_state_dict(state_dict)\n",
        "\n",
        "# Set to evaluation mode (crucial for Dropout and BatchNorm)\n",
        "eg_model.eval()\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "eg_model.to(device)\n",
        "\n",
        "print(f\"Model {eg_model_filename} loaded successfully on {device}!\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3. Create sample input data for testing\n",
        "# ------------------------------------------------------------------\n",
        "# PyTorch ResNet50 expects: (Batch, Channels, Height, Width)\n",
        "batch_size = 1\n",
        "channels = 3\n",
        "height, width = 244, 244\n",
        "\n",
        "# Create a random tensor and move it to the same device as the model\n",
        "test_input = torch.randn(batch_size, channels, height, width).to(device)\n",
        "\n",
        "print(f\"\\nTest input shape: {test_input.shape}\")\n",
        "\n",
        "# Run a quick forward pass to verify everything works\n",
        "with torch.no_grad():\n",
        "    output = eg_model(test_input)\n",
        "    print(f\"Forward pass successful! Output shape: {output.shape}\")"
      ],
      "metadata": {
        "id": "9ytV98cnCxh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image01D.png)"
      ],
      "metadata": {
        "id": "mfhekLm4mgdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Convert PyTorch Model to ONNX Format\n",
        "\n",
        "The code in the cell below converts the PyTorch model into the ONNX format."
      ],
      "metadata": {
        "id": "eO16F9FfVi4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Convert PyTorch model to ONNX format\n",
        "\n",
        "import torch\n",
        "import torch.onnx\n",
        "import onnx\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings to keep the output clean\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Use same filename for ONNX version\n",
        "eg_onnx_filename = eg_model_filename.replace('.pth', '.onnx')\n",
        "print(f\"Converting PyTorch model to ONNX format...\")\n",
        "\n",
        "# Set path\n",
        "eg_output_path = f\"/content/{eg_onnx_filename}\"\n",
        "\n",
        "try:\n",
        "    # 2. Prepare model and dummy input\n",
        "    eg_model.eval()\n",
        "    device = next(eg_model.parameters()).device\n",
        "    dummy_input = torch.randn(1, 3, 244, 244).to(device)\n",
        "\n",
        "    # 3. Export\n",
        "    # We use opset_version=18 to match the native PyTorch exporter version\n",
        "    torch.onnx.export(\n",
        "        eg_model,\n",
        "        dummy_input,\n",
        "        eg_output_path,\n",
        "        export_params=True,\n",
        "        opset_version=18,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['input'],\n",
        "        output_names=['output'],\n",
        "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        "    )\n",
        "\n",
        "    # 4. Verification\n",
        "    onnx_model = onnx.load(eg_output_path)\n",
        "    onnx.checker.check_model(onnx_model)\n",
        "\n",
        "    print(f\"\\n[SUCCESS] Model converted and verified!\")\n",
        "    print(f\"ONNX file saved to: {eg_output_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n[ERROR] Conversion failed: {e}\")"
      ],
      "metadata": {
        "id": "tFN4XVo2IsZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image02D.png)\n",
        "\n",
        "Now we a copy of our original PyTorch model, but now in the ONNX format. The question is \"Did our conversion create an functionally _identical_ model?"
      ],
      "metadata": {
        "id": "2G-IX1xnoCum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Verify ONNX Model\n",
        "\n",
        "First, we need to check if our new ONNX model is valid? The code in the cell below performs **validation and inspection** of our new ONNX model file. Checking is accomplished by the following line of code:\n",
        "```python\n",
        "    onnx.checker.check_model(eg_onnx_model_loaded)\n",
        "```\n",
        "If no errors are raised, the model is considered valid."
      ],
      "metadata": {
        "id": "BlYinlpZUxqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Verify ONNX model\n",
        "\n",
        "import onnx\n",
        "import os\n",
        "\n",
        "# Set path\n",
        "eg_output_path = f\"/content/{eg_onnx_filename}\"\n",
        "\n",
        "try:\n",
        "    eg_onnx_model_loaded = onnx.load(eg_output_path)\n",
        "    onnx.checker.check_model(eg_onnx_model_loaded)\n",
        "    print(\"\\nONNX model validation successful!\")\n",
        "\n",
        "    # Display ONNX model information\n",
        "    print(\"\\nONNX Model Information:\")\n",
        "    graph = eg_onnx_model_loaded.graph\n",
        "    print(f\"Model name: {graph.name}\")\n",
        "    print(f\"Number of inputs: {len(graph.input)}\")\n",
        "    print(f\"Number of outputs: {len(graph.output)}\")\n",
        "\n",
        "    # Print the input dimensions to verify\n",
        "    input_tensor = graph.input[0]\n",
        "    input_shape = [d.dim_value for d in input_tensor.type.tensor_type.shape.dim]\n",
        "    print(f\"Input Shape: {input_shape}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR: Failed to load ONNX model. It may be corrupted.\")\n",
        "    print(f\"Details: {e}\")"
      ],
      "metadata": {
        "id": "ndqRXdNNJsVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image22D.png)\n",
        "\n",
        "This confirms that your ONNX model was loaded without errors and passes all ONNX format validations. The `onnx.checker.check_model()` function verified that your model follows the ONNX specification correctly\n",
        "```text\n",
        "  ONNX Model Information:\n",
        "  Model name: main_graph\n",
        "  Number of inputs: 1\n",
        "  Number of outputs: 1\n",
        "  Input Shape: [0, 3, 244, 244]\n",
        "```\n",
        "Let's see what these variables mean:\n",
        "\n",
        "* **Model name:** `main_graph` - This is the default name that ONNX uses when converting PyTorch models\n",
        "* **Number of inputs:** Our model should have only one input tensor since it was an image classification model.\n",
        "* **Number of outputs:** Our model should have only one output tensor, again because it was a classification model.\n",
        "* **Input Shape:** The input shape is correct. Our model had `3` color channels and each image is `244` X `244` pixles. The initial value of `0` is the \"batch size\" and is irrelevant in this context.\n"
      ],
      "metadata": {
        "id": "qtF5zl5aC3oN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Model Comparison\n",
        "\n",
        "Since our new ONNX version of the model looks OK, let's perform a side-by-side comparison to see if both models \"work\" the same way.\n",
        "\n",
        "The code in the cell below performs a comparison between predictions from the ONNX model and its ONNX-converted counterpart to verify that the conversion preserved the model's behavior."
      ],
      "metadata": {
        "id": "RWndQ6ISuc4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Model comparison\n",
        "\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "eg_output_path = f\"/content/{eg_onnx_filename}\"\n",
        "print(f\"Loading ONNX model from: {eg_output_path}\")\n",
        "eg_onnx_session = ort.InferenceSession(eg_output_path)\n",
        "\n",
        "# 1. Get input/output names metadata\n",
        "eg_input_name = eg_onnx_session.get_inputs()[0].name\n",
        "eg_output_name = eg_onnx_session.get_outputs()[0].name\n",
        "\n",
        "# 2. Create test data (NHWC initially)\n",
        "eg_test_input_nhwc = np.random.randn(1, 244, 244, 3).astype(np.float32)\n",
        "\n",
        "# 3. Transpose Data: NHWC -> NCHW\n",
        "eg_input_nchw = np.transpose(eg_test_input_nhwc, (0, 3, 1, 2))\n",
        "\n",
        "# 4. Run ONNX Inference (Always CPU/System RAM)\n",
        "eg_onnx_pred = eg_onnx_session.run(\n",
        "    [eg_output_name],\n",
        "    {eg_input_name: eg_input_nchw}\n",
        ")\n",
        "eg_onnx_result = eg_onnx_pred[0]\n",
        "\n",
        "# 5. Run PyTorch Inference\n",
        "eg_model.eval()\n",
        "\n",
        "# Check where the model is (CPU or GPU)\n",
        "device = next(eg_model.parameters()).device\n",
        "print(f\"PyTorch model is on: {device}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Convert to Tensor and move to the SAME device as the model\n",
        "    torch_input = torch.from_numpy(eg_input_nchw).to(device)\n",
        "\n",
        "    # Run model\n",
        "    eg_pytorch_result_tensor = eg_model(torch_input)\n",
        "\n",
        "    # Move result back to CPU for numpy comparison\n",
        "    eg_pytorch_result = eg_pytorch_result_tensor.cpu().numpy()\n",
        "\n",
        "# --- 6. Compare results ---\n",
        "print(\"-\" * 30)\n",
        "print(f\"ONNX prediction shape:    {eg_onnx_result.shape}\")\n",
        "print(f\"PyTorch prediction shape: {eg_pytorch_result.shape}\")\n",
        "\n",
        "# Calculate differences\n",
        "max_diff = np.max(np.abs(eg_onnx_result - eg_pytorch_result))\n",
        "print(f\"Max difference:  {max_diff:.8f}\")\n",
        "\n",
        "# Verification\n",
        "are_identical = np.allclose(eg_onnx_result, eg_pytorch_result, atol=1e-5)\n",
        "print(f\"Results are effectively identical: {are_identical}\")\n",
        "\n",
        "if not are_identical:\n",
        "    print(\"Note: Tiny differences are expected due to float32 precision on different hardware (CPU vs GPU).\")"
      ],
      "metadata": {
        "id": "TlKPvWXHOmub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image04D.png)\n",
        "\n",
        "Both models are \"effectively\" identical!"
      ],
      "metadata": {
        "id": "STCLq1rvrS85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Visualize Predicted Outputs\n",
        "\n",
        "Visual data is often a better way to compare two items. The code in the cell below generates a bar chart showing the predicted outputs from the `PyTorch` and the `ONNX` models."
      ],
      "metadata": {
        "id": "n_vmWR61F_th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Visualize predicted outputs\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Transpose to NCHW for PyTorch/ONNX\n",
        "eg_input_nchw = np.transpose(eg_test_input_nhwc, (0, 3, 1, 2))\n",
        "\n",
        "# --- Run Predictions -----------------------------\n",
        "\n",
        "# ONNX Prediction\n",
        "eg_onnx_pred = eg_onnx_session.run(\n",
        "    [eg_output_name],\n",
        "    {eg_input_name: eg_input_nchw}\n",
        ")[0].flatten()\n",
        "\n",
        "# PyTorch Prediction\n",
        "eg_model.eval()\n",
        "device = next(eg_model.parameters()).device # Auto-detect CPU vs GPU\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Convert numpy -> Tensor -> Move to Device\n",
        "    torch_input = torch.from_numpy(eg_input_nchw).to(device)\n",
        "\n",
        "    # Run Inference\n",
        "    eg_pytorch_output = eg_model(torch_input)\n",
        "\n",
        "    # Move back to CPU -> Numpy -> Flatten\n",
        "    eg_pytorch_pred = eg_pytorch_output.cpu().numpy().flatten()\n",
        "\n",
        "# --- Plotting -------------------------------\n",
        "\n",
        "# Create class labels\n",
        "eg_num_classes = len(eg_pytorch_pred)\n",
        "if eg_num_classes > 10:\n",
        "    print(f\"Plotting top 10 classes out of {eg_num_classes}\")\n",
        "    # Get indices of top 10\n",
        "    top_indices = np.argsort(eg_pytorch_pred)[-10:]\n",
        "    # Slice data\n",
        "    plot_pytorch = eg_pytorch_pred[top_indices]\n",
        "    plot_onnx = eg_onnx_pred[top_indices]\n",
        "    labels = [f\"Class {i}\" for i in top_indices]\n",
        "else:\n",
        "    plot_pytorch = eg_pytorch_pred\n",
        "    plot_onnx = eg_onnx_pred\n",
        "    labels = [f\"Class {i}\" for i in range(eg_num_classes)]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(x - width/2, plot_pytorch, width, label='PyTorch', color='skyblue')\n",
        "plt.bar(x + width/2, plot_onnx, width, label='ONNX', color='orange', alpha=0.7)\n",
        "\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Prediction Probability / Logits')\n",
        "plt.title('Comparison of PyTorch and ONNX Model Predictions')\n",
        "plt.xticks(x, labels, rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bn6vgA9NPc7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image05D.png)\n",
        "\n",
        "### **Interpretation of Results**\n",
        "\n",
        "**1. Conversion Was Successful**\n",
        "\n",
        "The most important takeaway is that **the Blue bars (PyTorch) and Orange bars (ONNX) are perfectly aligned.**\n",
        "* For every class shown (Class 0 through Class 4), the height of the ONNX prediction matches the PyTorch prediction exactly.\n",
        "* This visually confirms what the `np.allclose` code check told you earlier: the ONNX model is replicating the original model's behavior with high precision.\n",
        "\n",
        "**2. You Are Viewing \"Logits\" (Raw Scores)**\n",
        "\n",
        "The Y-axis shows values ranging from roughly **-1.5 to +1.0**.\n",
        "* Because there are **negative numbers** (Class 3 and Class 4), you are looking at **Logits** (raw model outputs before activation) rather than Probabilities.\n",
        "* If these were probabilities (after a Softmax layer), all bars would be between 0 and 1, and they would sum up to 1.0.\n",
        "* Comparing logits is actually **better** for debugging because Softmax can sometimes hide small numerical differences by \"squashing\" values.\n",
        "\n",
        "**3. Class Breakdown**\n",
        "\n",
        "* **Class 0:** Has the highest score (~1.0), meaning both models predict this is the most likely class among these five.\n",
        "* **Class 3 & 4:** Have negative scores, meaning the model is confident these are *not* the correct classes.\n",
        "\n",
        "**Conclusion**\n",
        "Your ONNX model is effectively a \"digital twin\" of your PyTorch model. It is safe to proceed to deployment or performance optimization (like quantization) because you have verified that the accuracy has not been lost during conversion."
      ],
      "metadata": {
        "id": "fpEDl9vuGmqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7: Generate Random Testing Data\n",
        "\n",
        "The code in the cell below generates random data that will be used in the next step to compare the accuracy of the converted ONNX model against the accuracy of the original PyTorch model.\n",
        "\n",
        "Note that value of the `seed` is specified by a variable called `seed_val` that is defined by the user."
      ],
      "metadata": {
        "id": "aM0UYztrsM4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: Generate random testing data\n",
        "\n",
        "# Generate random test data for model comparison\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set number of samples\n",
        "numSamples=100\n",
        "\n",
        "# Set value for random seed\n",
        "seed_val = 42\n",
        "print(f\"Random seed set to {seed_val}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(seed_val)\n",
        "tf.random.set_seed(seed_val)\n",
        "\n",
        "# Your specific model parameters\n",
        "eg_input_shape = (1, 244, 244, 3)  # Based on your working example\n",
        "eg_num_classes = 5  # Based on your output shape (1, 5)\n",
        "\n",
        "# Generate random test data\n",
        "print(f\"Generating test data with input shape: {eg_input_shape}\")\n",
        "print(f\"Number of samples: {numSamples}\")\n",
        "\n",
        "# Create random input data (matching your model's expected input)\n",
        "eg_X_test = np.random.randn(numSamples, *eg_input_shape[1:]).astype(np.float32)\n",
        "\n",
        "# Generate random labels for 5-class classification\n",
        "eg_y_test = np.random.randint(0, eg_num_classes, (numSamples,))\n",
        "eg_y_test_onehot = tf.keras.utils.to_categorical(eg_y_test, eg_num_classes)\n",
        "\n",
        "print(f\"Generated X_test shape: {eg_X_test.shape}\")\n",
        "print(f\"Generated y_test shape: {eg_y_test_onehot.shape}\")\n",
        "\n",
        "# Print sample data to verify\n",
        "print(f\"Sample input data (first 5 samples): {eg_X_test[:5].flatten()[:10]}\")\n",
        "print(f\"Sample labels (first 5): {eg_y_test[:5]}\")\n",
        "\n",
        "print(\"Test data generation complete!\")\n"
      ],
      "metadata": {
        "id": "V9Z4LSBFt_cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image10D.png)\n",
        "\n",
        "We will use this \"synthetic data\" when we test both models in the next example."
      ],
      "metadata": {
        "id": "dPIFymU2zvlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 8: Compute Relative Accuracy\n",
        "\n",
        "This code in the cell below performs a two-part evaluation of a `PyTorch model` and its `ONNX-converted version`:\n",
        "\n",
        "#### **Part 1: Single-Sample Equivalence Check**\n",
        "* **Purpose:** To verify that both models produce nearly identical predictions for a single input.\n",
        "* **Steps:**\n",
        "1. Load the `ONNX model` and prepare an inference session.\n",
        "2. Extract input/output tensor names.\n",
        "3. Select one sample from the test dataset (X_test[0:1]).\n",
        "4. Run predictions using both models.\n",
        "5. Compare the outputs using:\n",
        "* Shape\n",
        "* Max and mean absolute differences\n",
        "* `np.allclose()` to check numerical equivalence within a tolerance.\n",
        "\n",
        "#### **Part 2: Accuracy Comparison on Full Test Set**\n",
        "* **Purpose:** To compare the classification accuracy of both models on an entire synthetic dataset that we generated in the previous example.\n",
        "* **Steps:**\n",
        "1. Run predictions on X_test using both models.\n",
        "2. Convert predicted probabilities to class labels using np.argmax().\n",
        "3. Compare predicted labels to true labels (y_test_onehot).\n",
        "4. Compute accuracy for each model using np.mean(predicted == true).\n",
        "5. Print the accuracy values and their difference.\n",
        "6. Display sample predictions for manual inspection.\n",
        "\n",
        "##### **Why This Is Useful**\n",
        "* Ensures the `ONNX` model is a faithful representation of the original `PyTorch` model.\n",
        "* Helps detect any discrepancies introduced during model conversion.\n",
        "* Validates that the ONNX model is suitable for deployment without loss of performance.\n"
      ],
      "metadata": {
        "id": "13FQ2iPhsOS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 8: Compute relative accuracy (Fixed for Static Batch Size)\n",
        "\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import tqdm  # For progress bar\n",
        "\n",
        "# Load ONNX model\n",
        "eg_onnx_filename = eg_model_filename.replace('.pth', '.onnx')\n",
        "eg_output_path = os.path.join(\"/content\", eg_onnx_filename)\n",
        "eg_onnx_session = ort.InferenceSession(eg_output_path)\n",
        "\n",
        "# Get input/output names\n",
        "eg_input_name = eg_onnx_session.get_inputs()[0].name\n",
        "eg_output_name = eg_onnx_session.get_outputs()[0].name\n",
        "\n",
        "# Prepare Data: Convert NHWC -> NCHW\n",
        "print(f\"Original Input Shape (NHWC): {eg_X_test.shape}\")\n",
        "eg_X_test_nchw = np.transpose(eg_X_test, (0, 3, 1, 2))\n",
        "\n",
        "# 2. Accuracy Comparison\n",
        "try:\n",
        "    print(\"\\nComparing model accuracies on the generated test dataset...\")\n",
        "\n",
        "    # Get PyTorch Predictions (Can usually handle batches)\n",
        "    print(\"Getting predictions from PyTorch model...\")\n",
        "    eg_model.eval()\n",
        "    device = next(eg_model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        torch_all_input = torch.from_numpy(eg_X_test_nchw).to(device)\n",
        "        eg_pytorch_predictions = eg_model(torch_all_input).cpu().numpy()\n",
        "\n",
        "    # Get ONNX Predictions (Sample-by-Sample Loop)\n",
        "    print(\"Getting predictions from ONNX model (Looping to handle fixed batch size)...\")\n",
        "    eg_onnx_preds_list = []\n",
        "\n",
        "    # Loop through each sample individually\n",
        "    for i in range(len(eg_X_test_nchw)):\n",
        "        # Slice [i:i+1] creates shape (1, 3, 244, 244) which matches the model's expectation\n",
        "        sample = eg_X_test_nchw[i:i+1]\n",
        "\n",
        "        # Run inference on single sample\n",
        "        eg_pred = eg_onnx_session.run([eg_output_name], {eg_input_name: sample})[0]\n",
        "        eg_onnx_preds_list.append(eg_pred)\n",
        "\n",
        "    # Concatenate all single results back into one array (100, Num_Classes)\n",
        "    eg_onnx_predictions = np.concatenate(eg_onnx_preds_list, axis=0)\n",
        "\n",
        "    # Convert Logits to Class Indices\n",
        "    eg_pytorch_pred_classes = np.argmax(eg_pytorch_predictions, axis=1)\n",
        "    eg_onnx_pred_classes = np.argmax(eg_onnx_predictions, axis=1)\n",
        "\n",
        "    # Get True Labels\n",
        "    if len(eg_y_test_onehot.shape) > 1:\n",
        "        eg_y_test_classes = np.argmax(eg_y_test_onehot, axis=1)\n",
        "    else:\n",
        "        eg_y_test_classes = eg_y_test_onehot\n",
        "\n",
        "    # Calculate Accuracies\n",
        "    eg_pytorch_accuracy = np.mean(eg_pytorch_pred_classes == eg_y_test_classes)\n",
        "    eg_onnx_accuracy = np.mean(eg_onnx_pred_classes == eg_y_test_classes)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"PyTorch model accuracy: {eg_pytorch_accuracy:.4f}\")\n",
        "    print(f\"ONNX model accuracy:    {eg_onnx_accuracy:.4f}\")\n",
        "\n",
        "    # Check if predictions are identical\n",
        "    accuracy_diff = abs(eg_pytorch_accuracy - eg_onnx_accuracy)\n",
        "    print(f\"Difference in accuracy: {accuracy_diff:.6f}\")\n",
        "\n",
        "    # Verify raw output closeness\n",
        "    max_raw_diff = np.max(np.abs(eg_pytorch_predictions - eg_onnx_predictions))\n",
        "    print(f\"Max raw output difference: {max_raw_diff:.8f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during accuracy comparison: {e}\")"
      ],
      "metadata": {
        "id": "NFl8mUwVSkU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image06D.png)\n",
        "\n",
        "This result clearly demonstrates the fidelity of our new ONNX model--it's just as good as our original PyTorch model."
      ],
      "metadata": {
        "id": "artw3WgStvY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 9: Visualize Similarities with Confusion Plots\n",
        "\n",
        "Once again, we can use visualization techniques to compare the accuracy of our new ONNX model with the original PyTorch model. The code in the cell generates 'Confusion Plots' for the two models using the synthetic data we generated earlier."
      ],
      "metadata": {
        "id": "EKbGkKSEtvUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 9: Visualize Similarities with Confusion Plots\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "# Prepare Data\n",
        "eg_X_test_nchw = np.transpose(eg_X_test, (0, 3, 1, 2))\n",
        "\n",
        "# Handle y_test formatting\n",
        "if 'eg_y_test_onehot' in locals():\n",
        "    target_labels = eg_y_test_onehot\n",
        "elif 'eg_y_test' in locals():\n",
        "    target_labels = eg_y_test\n",
        "else:\n",
        "    raise ValueError(\"No test labels found (eg_y_test_onehot or eg_y_test).\")\n",
        "\n",
        "# Convert to class indices for Confusion Matrix\n",
        "if len(target_labels.shape) > 1 and target_labels.shape[1] > 1:\n",
        "    eg_y_test_classes = np.argmax(target_labels, axis=1)\n",
        "else:\n",
        "    eg_y_test_classes = target_labels\n",
        "\n",
        "# --- Get Predictions ---\n",
        "\n",
        "# 1. PyTorch Predictions\n",
        "print(\"Getting PyTorch predictions...\")\n",
        "eg_model.eval()\n",
        "device = next(eg_model.parameters()).device\n",
        "\n",
        "with torch.no_grad():\n",
        "    torch_input = torch.from_numpy(eg_X_test_nchw).to(device)\n",
        "    # Get raw logits\n",
        "    eg_pytorch_logits = eg_model(torch_input).cpu().numpy()\n",
        "    # Convert to classes\n",
        "    eg_pytorch_pred_classes = np.argmax(eg_pytorch_logits, axis=1)\n",
        "\n",
        "# 2. ONNX Predictions (with Batch Loop for safety)\n",
        "print(\"Getting ONNX predictions...\")\n",
        "eg_onnx_preds_list = []\n",
        "\n",
        "# Loop to avoid \"Input shape != Requested shape\" error if batch size is static\n",
        "for i in range(len(eg_X_test_nchw)):\n",
        "    eg_sample = eg_X_test_nchw[i:i+1]\n",
        "    eg_pred = eg_onnx_session.run([eg_output_name], {eg_input_name: eg_sample})[0]\n",
        "    eg_onnx_preds_list.append(eg_pred)\n",
        "\n",
        "eg_onnx_logits = np.concatenate(eg_onnx_preds_list, axis=0)\n",
        "eg_onnx_pred_classes = np.argmax(eg_onnx_logits, axis=1)\n",
        "\n",
        "# --- Plotting ---\n",
        "\n",
        "# Compute confusion matrices\n",
        "cm_pytorch = confusion_matrix(eg_y_test_classes, eg_pytorch_pred_classes)\n",
        "cm_onnx = confusion_matrix(eg_y_test_classes, eg_onnx_pred_classes)\n",
        "\n",
        "# Set plot style\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except:\n",
        "    plt.style.use('seaborn-darkgrid')\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Plot PyTorch confusion matrix\n",
        "sns.heatmap(cm_pytorch, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "axes[0].set_title('PyTorch Model Confusion Matrix')\n",
        "axes[0].set_xlabel('Predicted Labels')\n",
        "axes[0].set_ylabel('True Labels')\n",
        "\n",
        "# Plot ONNX confusion matrix\n",
        "sns.heatmap(cm_onnx, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
        "axes[1].set_title('ONNX Model Confusion Matrix')\n",
        "axes[1].set_xlabel('Predicted Labels')\n",
        "axes[1].set_ylabel('True Labels')\n",
        "\n",
        "# Check consistency\n",
        "matrices_match = np.array_equal(cm_pytorch, cm_onnx)\n",
        "print(f\"\\nDo the confusion matrices match exactly? {'Yes' if matrices_match else 'No'}\")\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Zg-QMIwTPNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image07D.png)\n",
        "\n",
        "### **Interpretation of Results**\n",
        "\n",
        "**Confusion Matrices**\n",
        "\n",
        "* **Technical Success (The Good News):**\n",
        "    * The code output confirms: `Do the confusion matrices match exactly? Yes`.\n",
        "    * Visually, the **PyTorch matrix (Blue)** and **ONNX matrix (Green)** are identical. This proves the ONNX model reproduces the PyTorch model's behavior across the entire dataset.\n",
        "\n",
        "* **Model Behavior (The \"Odd\" News):**\n",
        "    * You will notice **all predictions fall into \"Class 0\"** (the first column). The model predicted Class 0 for every single test sample (blue), and never predicted Classes 1-4 (green).\n",
        "\n",
        "* **Why is the Model Only Predicting Class 0?**\n",
        "\n",
        "This \"Class 0 collapse\" is likely due to two compounding factors:\n",
        "* **Training Data Imbalance:** a majority of your original training images were from Class 0. This causes the model to learn a strong **prior bias**—it learns that \"when in doubt, guess Class 0\" is the statistically safest bet.\n",
        "* **Random Input Data:** Since we are testing with generated noise (`np.random.randn`) rather than real images, the model detects no recognizable features for any class. Lacking evidence, it reverts to its learned bias (Class 0).\n",
        "\n",
        "**Conclusion**\n",
        "We have **successfully converted the model.** Even though the predictions themselves are skewed due to data imbalance and random inputs, the **ONNX model is failing in _exactly_ the same way as the PyTorch model**, which is the definition of a successful conversion. In other words, **our PyTorch model is _not_ very good at classifying retinal fundus images into diabetic retinoptic categories**."
      ],
      "metadata": {
        "id": "bWSNVV-QtvRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 10: Speed Comparison\n",
        "\n",
        "Besides accuracy, one would like to know if one model type runs \"faster\" than another. In other words, if we train a PyTorch model using Nvidia GPU hardware acceleration, does the ONNX version of the model run significatly faster (or slower)? Using a trained model to provide predictions is called **inference**. When you ask `ChatGPT` a question, the answer is an example of `inference` so it's important that a model can \"run fast\".\n",
        "\n",
        "The code in the cell below performs a speed text comparison between the original PyTorch model and its ONNX clone."
      ],
      "metadata": {
        "id": "GQf7nKvZhBCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 10: Speed Comparison\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import onnxruntime as ort\n",
        "\n",
        "# Explicitly ask for 'CUDAExecutionProvider'\n",
        "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "eg_onnx_session = ort.InferenceSession(eg_output_path, providers=providers)\n",
        "\n",
        "# Verify we are actually using the GPU\n",
        "current_providers = eg_onnx_session.get_providers()\n",
        "print(f\"ONNX Inference is using: {current_providers[0]}\")\n",
        "if current_providers[0] != 'CUDAExecutionProvider':\n",
        "    print(\"WARNING: ONNX is still on CPU! check if onnxruntime-gpu is installed correctly.\")\n",
        "\n",
        "# --- Configuration ---\n",
        "n_warmup = 100   # Warm up hardware\n",
        "n_loops = 1000   # Number of runs to average\n",
        "batch_size = 1   # Simulating real-time inference\n",
        "\n",
        "# Create dummy input\n",
        "dummy_input_nhwc = np.random.randn(batch_size, 244, 244, 3).astype(np.float32)\n",
        "dummy_input_nchw = np.transpose(dummy_input_nhwc, (0, 3, 1, 2))\n",
        "\n",
        "# --- 1. PyTorch Benchmark ---\n",
        "print(f\"\\nBenchmarking PyTorch (Average of {n_loops} runs)...\")\n",
        "eg_model.eval()\n",
        "device = next(eg_model.parameters()).device\n",
        "torch_input = torch.from_numpy(dummy_input_nchw).to(device)\n",
        "\n",
        "# Warmup\n",
        "eg_pytorch_total_time_start = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(n_warmup):\n",
        "        _ = eg_model(torch_input)\n",
        "\n",
        "# Timing\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(n_loops):\n",
        "        _ = eg_model(torch_input)\n",
        "resnet50_pytorch_avg_ms = ((time.time() - start_time) / n_loops) * 1000\n",
        "\n",
        "# Record end time\n",
        "eg_pytorch_total_time_end = time.time()\n",
        "\n",
        "# --- 2. ONNX Benchmark ---\n",
        "print(f\"Benchmarking ONNX (Average of {n_loops} runs)...\")\n",
        "eg_onnx_input = {eg_input_name: dummy_input_nchw}\n",
        "\n",
        "# Warmup\n",
        "eg_onnx_total_time_start = time.time()\n",
        "for _ in range(n_warmup):\n",
        "    _ = eg_onnx_session.run([eg_output_name], eg_onnx_input)\n",
        "\n",
        "# Timing\n",
        "start_time = time.time()\n",
        "for _ in range(n_loops):\n",
        "    _ = eg_onnx_session.run([eg_output_name], eg_onnx_input)\n",
        "resnet50_onnx_avg_ms = ((time.time() - start_time) / n_loops) * 1000\n",
        "\n",
        "# Record end time\n",
        "eg_onnx_total_time_end = time.time()\n",
        "\n",
        "# --- 3. Results ---\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"PyTorch Average Latency: {resnet50_pytorch_avg_ms:.2f} ms\")\n",
        "print(f\"ONNX Average Latency:    {resnet50_onnx_avg_ms:.2f} ms\")\n",
        "print(f\"Speed ONNX vs PyTorch:   {resnet50_pytorch_avg_ms / resnet50_onnx_avg_ms:.2f}x\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# --- Convert Total Time to Minutes and Seconds for PyTorch---\n",
        "eg_pytorch_total_seconds = eg_pytorch_total_time_end - eg_pytorch_total_time_start\n",
        "eg_pytorch_minutes = int(eg_pytorch_total_seconds // 60)\n",
        "eg_pytorch_seconds = int(eg_pytorch_total_seconds % 60)\n",
        "\n",
        "print(f\"Total elapsed time for PyTorch model: {eg_pytorch_minutes}m {eg_pytorch_seconds}s\")\n",
        "\n",
        "# --- Convert Total Time to Minutes and Seconds for ONNX---\n",
        "eg_onnx_total_seconds = eg_onnx_total_time_end - eg_onnx_total_time_start\n",
        "eg_onnx_minutes = int(eg_onnx_total_seconds // 60)\n",
        "eg_onnx_seconds = int(eg_onnx_total_seconds % 60)\n",
        "\n",
        "print(f\"Total elapsed time for ONNX model: {eg_onnx_minutes}m {eg_onnx_seconds}s\")"
      ],
      "metadata": {
        "id": "waIvai1mcgbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image32D.png)\n",
        "\n",
        "\n",
        "### **Analysis: Why was ONNX faster?**\n",
        "\n",
        "#### **\"Graph Optimization\" (The Chef Metaphor)**\n",
        "\n",
        "Imagine a chef (PyTorch) cooking a recipe step-by-step. They chop onions, put down the knife, pick up the pan, cook onions, put down the pan, pick up the garlic...\n",
        "* **PyTorch (Eager Mode):** Executes operations one by one. It accesses memory to read input, computes, and writes output for *every* layer individually.\n",
        "* **ONNX Runtime (Graph Mode):** Looks at the entire recipe *before* cooking. It sees \"Chop onions\" and \"Cook onions\" and realizes it can just slice them directly into the pan.\n",
        "* **Technical Term - \"Operator Fusion\":** ONNX Runtime merges separate layers (like Convolution + Batch Norm + ReLU) into a single \"fused\" operation. This reduces the number of times data has to move back and forth between the GPU memory and the computing cores. **Data movement is often slower than the math itself.**\n",
        "\n",
        "#### **\"Training Baggage\"**\n",
        "* **PyTorch** is built for *training*. It keeps track of relationships between tensors just in case you want to calculate gradients (backpropagation) later. Even in `no_grad()` mode, there is slight overhead from the framework checking the dynamic graph.\n",
        "* **ONNX** is built for *inference only*. It knows the model weights are frozen forever. It strips away all the variable tracking, gradient hooks, and dynamic checks, leaving only the raw math.\n",
        "\n",
        "### **Why does 1.3 ms matter? (The ChatGPT Context)**\n",
        "\n",
        "A difference of **1.37 ms** (6.31ms vs 4.94ms) might seem tiny to a human. Who cares about a blink of an eye?\n",
        "\n",
        "**For an LLM like ChatGPT, this difference is catastrophic (or miraculous).**\n",
        "\n",
        "#### **The \"Autoregressive\" Multiplier**\n",
        "\n",
        "LLMs generate text **one token (word part) at a time**. To write a 500-word essay, the model must run inference **500 separate times** in a row.\n",
        "* **PyTorch:** 500 tokens $\\times$ 6.31 ms = **3.15 seconds**\n",
        "* **ONNX:** 500 tokens $\\times$ 4.94 ms = **2.47 seconds**\n",
        "* **Result:** The user gets their answer nearly **1 second faster**. That is the difference between an app feeling \"snappy\" vs \"laggy.\"\n",
        "\n",
        "#### **The \"Scale\" Multiplier (Cost)**\n",
        "\n",
        "If you are OpenAI serving 100 million users:\n",
        "* A **1.28x speedup** means you can serve **28% more users** with the same number of GPUs.\n",
        "* If you spend \\$100 Million/year on GPUs, this optimization just saved you **\\$28 Million dollars** purely by changing the software format."
      ],
      "metadata": {
        "id": "AHj7GdD2X2_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercises**\n",
        "\n",
        "For the exercises we will basically repeat the examples with the important difference that you will be analyzing the `ResNet101_model_512.keras` that you were asked to save to your GDrive in an earlier class lesson."
      ],
      "metadata": {
        "id": "cTYtehwEtvMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Download Saved Keras File**\n",
        "\n",
        "In the cell below write the code to copy the Python model `ResNet101_model_512.pth` from your GDrive to you current Colab directory. If you don't have a copy of the Pytorch `ResNet101_model_512` saved to your GDrive, you need to contact your Instructor or TA for help--AI can't help you here.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change the prefix `eg_` to `ex_`.\n",
        "2. Change the `ex_model_filename` to `ResNet101_model_512.pth`.\n",
        "3. Change the path from:\n",
        "```python\n",
        "  gdrive_model_path = os.path.join('/content/drive/MyDrive/ResNet50_model_244', eg_model_filename)\n",
        "```\n",
        "to read as\n",
        "```python\n",
        "  gdrive_model_path = os.path.join('/content/drive/MyDrive/ResNet101_model_512', ex_model_filename)\n",
        "```"
      ],
      "metadata": {
        "id": "3MnQSbnIP_Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n"
      ],
      "metadata": {
        "id": "62hED5cIKzRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image12D.png)\n",
        "\n",
        "If you don't see this output it probably means your GDrive doesn't contain your saved PyTorch model. Contact your Instructor or TA for help."
      ],
      "metadata": {
        "id": "EVpYZwbRN8LU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Load Keras Model**\n",
        "\n",
        "In the cell below write the code to load the Keras model and determines its input shape.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Copy `Example 2` into the empty code cell below.\n",
        "\n",
        "1. Change the prefix `eg_` to `ex_` every where it appears in the code.\n",
        "\n",
        "2. Change `models.resnet50` to `models.resnet101` everywhere it appears in the code cell.\n",
        "\n",
        "3. Change `ResNet50` to `ResNet101`.\n",
        "\n",
        "4. Change pixel values to read:\n",
        "```python\n",
        "      height, width = 512, 512\n",
        "```"
      ],
      "metadata": {
        "id": "s4cu7szPofZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n"
      ],
      "metadata": {
        "id": "euvEnniSelIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image13D.png)"
      ],
      "metadata": {
        "id": "h_MTYsp7P_Gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Convert Keras Model to ONNX Format**\n",
        "\n",
        "In the cell below write the code to convert your PyTorch model into the ONNX format.\n",
        "\n",
        "**Code Hints:**  \n",
        "\n",
        "1. Convert prefix `eg_` prefix to `ex_`.\n",
        "2. Modify the pixel size:\n",
        "```python\n",
        "    dummy_input = torch.randn(1, 3, 512, 512).to(device)\n",
        "```"
      ],
      "metadata": {
        "id": "IXF1ZgRCP_Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n"
      ],
      "metadata": {
        "id": "oGZQp8nIOzdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image14D.png)"
      ],
      "metadata": {
        "id": "2VaZY7y7P_Gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Verify ONNX Model**\n",
        "\n",
        "In the cell below write the code to perform validation and inspection of the ONNX model file.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change the prefix `eg_` to `ex_`."
      ],
      "metadata": {
        "id": "CXQytaWlP_Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n"
      ],
      "metadata": {
        "id": "w6-3HR0LQZ3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image25D.png)\n"
      ],
      "metadata": {
        "id": "4lMvaA6BP_Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Model Comparison**\n",
        "\n",
        "In the cell below write the code to perform a comparison between predictions from the PyTorch model and its ONNX-converted counterpart, in order to verify that the conversion preserved the model's behavior.\n",
        "\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change the prefix `eg_` to `ex_`\n",
        "\n",
        "2. Change this line of code:\n",
        "```python\n",
        "    # 2. Create test data (NHWC initially)\n",
        "    eg_test_input_nhwc = np.random.randn(1, 244, 244, 3).astype(np.float32)\n",
        "```\n",
        "to read as\n",
        "```python\n",
        "    # 2. Create test data (NHWC initially)\n",
        "    ex_test_input_nhwc = np.random.randn(1, 512, 512, 3).astype(np.float32)\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "1FYk7DzGP_Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n"
      ],
      "metadata": {
        "id": "Oaz6KNj5nshI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image16D.png)"
      ],
      "metadata": {
        "id": "fzCD_TAlP_Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6: Visualize Predicted Outputs**\n",
        "\n",
        "In the cell below write the code to generate confusion plots showing the predicted outputs from the `PyTorch` and the `ONNX` models.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change the prefix `eg_` to `ex_`."
      ],
      "metadata": {
        "id": "ju3NwjOmP_Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6 here\n",
        "\n"
      ],
      "metadata": {
        "id": "76NWyPRDrNYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image17D.png)\n",
        "\n",
        "As before, the graph shows how closely the ONNX model replicates the behavior of the original Keras model across five classes."
      ],
      "metadata": {
        "id": "ryBP5SUDP_Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7: Generate Random Testing Data**\n",
        "\n",
        "In the cell below write the code to generate random data that will be used in the next step to compare the accuracy of the converted ONNX model against the accuracy of the original Keras model.\n",
        "\n",
        "Note that value of the `seed` is specified by a variable called `seed_val` that is defined by the user. Make sure to set you random seed to any value **EXCEPT `42`!**\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change the random seed value\n",
        "2. Change the prefix `eg_` to `ex_`.\n",
        "3. Change this code chunk:\n",
        "```python\n",
        "    # Your specific model parameters\n",
        "    eg_input_shape = (1, 244, 244, 3)  # Based on your working example\n",
        "```\n",
        "to read as\n",
        "\n",
        "```python\n",
        "    # Your specific model parameters\n",
        "    ex_input_shape = (1, 512, 512, 3)  # Based on your working example\n",
        "```"
      ],
      "metadata": {
        "id": "PdW-mAA4P_Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 7 here\n",
        "\n"
      ],
      "metadata": {
        "id": "x-U6VmEyWQ07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image18D.png)"
      ],
      "metadata": {
        "id": "CCl7mmKnP_Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 8: Compute Relative Accuracy**\n",
        "\n",
        "In the cell below write the code to perform a two-part evaluation of a `Keras model` and its `ONNX-converted version. In Part 1 perform a Single-Sample Equivalence Check. In Part 2 do an accuracy comparison on the full test set.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change the prefix `eg_` to `ex_`."
      ],
      "metadata": {
        "id": "6LBM971aP_Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 8\n",
        "\n"
      ],
      "metadata": {
        "id": "kl3rUBcBYTKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image19D.png)"
      ],
      "metadata": {
        "id": "C7DGL5XbP_Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 9: Visualize Confusion Matrices**\n",
        "\n",
        "In the cell below write the code to generate two side-by-side Confusion Matrices for the PyTorch model on the left and the converted ONNX model on the right.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Convert the prefix `eg_` to `ex_`.\n"
      ],
      "metadata": {
        "id": "6VuSMAC-P_Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 9 here\n",
        "\n"
      ],
      "metadata": {
        "id": "m2FXGPB5H1zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image20D.png)"
      ],
      "metadata": {
        "id": "lKM_NId1P_Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 10: Speed Comparison**\n",
        "\n",
        "In the cell below, write the code to compare the speed of your original PyTorch model, `ResNet101_model_512.pth`, with your ONNX copy, `ResNet101_model_512.onnx`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change the prefix `eg_` to `ex_`.\n",
        "\n",
        "2. Change this line of code:\n",
        "```python\n",
        "  # Create dummy input\n",
        "  dummy_input_nhwc = np.random.randn(batch_size, 244, 244, 3).astype(np.float32)\n",
        "```\n",
        "\n",
        "to read as\n",
        "```python\n",
        "  # Create dummy input\n",
        "  dummy_input_nhwc = np.random.randn(batch_size, 512, 512, 3).astype(np.float32)\n",
        "```"
      ],
      "metadata": {
        "id": "sb_XLVpvtvJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 10 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r-JRsVTof2iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image32D.png)\n",
        "\n",
        "With larger pixel size of the ResNet101 image files (i.e. 512 x 512) the speed advantage of the ONNX model format is still apparent (ONNX is **1.28x** faster than PyTorch).\n",
        "\n",
        "## **The Strategic Value of ONNX**\n",
        "\n",
        "You might be asking: *\"Why go through the extra step of converting to ONNX? Why not just deploy the PyTorch model directly?\"*\n",
        "\n",
        "The answer lies in two critical benefits: **Flexibility (Portability)** and **Speed**.\n",
        "\n",
        "#### **1. Flexibility: The \"Universal Translator\"**\n",
        "In the modern AI landscape, you rarely deploy on the exact same hardware you used for training. You might train on a massive Google TPU cluster, but deploy to an edge device, a cloud server, or a user's laptop.\n",
        "\n",
        "**Without ONNX**, you are often locked into the framework (PyTorch/TensorFlow) and the specific hardware drivers it supports.\n",
        "\n",
        "**With ONNX**, your model becomes a standard file format that acts as a \"digital universal adapter.\" It decouples the **Model Architecture** from the **Hardware Execution**. The ONNX Runtime connects to specific **Hardware Accelerators** via \"Execution Providers\":\n",
        "\n",
        "* **NVIDIA GPUs:** via `CUDAExecutionProvider` or `TensorRT` (Standard for cloud inference)\n",
        "* **Google TPUs:** via `XLA` or `TPUExecutionProvider` (High throughput training/inference)\n",
        "* **Intel CPUs/iGPUs:** via `OpenVINO` (Optimized for standard servers and laptops)\n",
        "* **AMD GPUs:** via `ROCmExecutionProvider`\n",
        "* **Apple Silicon (Mac/iPhone):** via `CoreMLExecutionProvider`\n",
        "* **Mobile/Edge (Android/Qualcomm):** via `NNAPI` or `QNN`\n",
        "\n",
        "This means you can train once on a TPU, convert to ONNX, and deploy that *exact same file* to an Android phone, a MacBook, or an Nvidia H100 server without rewriting your model code.\n",
        "\n",
        "#### **2. Speed: The \"Graph Optimizer\"**\n",
        "As we saw in our benchmark (1.28x speedup), ONNX often runs faster than the original framework. This happens for three main reasons:\n",
        "\n",
        "1.  **Operator Fusion:** ONNX Runtime analyzes the entire model graph before running it. It merges separate mathematical operations (like `Convolution` + `BatchNorm` + `ReLU`) into a single kernel call. This drastically reduces \"memory bandwidth pressure\"—the time spent moving data back and forth between memory and the processor.\n",
        "2.  **Removing Training Overhead:** PyTorch and TensorFlow keep track of metadata required for backpropagation (training). ONNX strips all of this away, leaving only the pure mathematics needed for inference (prediction).\n",
        "3.  **Hardware-Specific Tuning:** When you select a specific provider (like CUDA or TensorRT), ONNX Runtime can auto-tune the mathematical operations to exploit the specific architecture of that chip (e.g., using Tensor Cores on Nvidia GPUs efficiently).\n",
        "\n",
        "### **Summary**\n",
        "Converting to ONNX essentially _compiles_ your model for production, ensuring it runs as fast as possible on whatever hardware is available."
      ],
      "metadata": {
        "id": "1bBsaP6fd8eF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook if you have a Mac. If you have a Windows computer, use **File --> Print.. --> Microsoft Print to PDF** to generate a PDF. Save your PDF as `Class_06_2.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "jWHvfxDttvBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "# **AI in Medicine**\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/Artificial-Intelligence-in-Healthcare.jpg)\n",
        "\n",
        "# Latest Advances in Artificial Intelligence in Medicine (2024)\n",
        "\n",
        "> **Abstract**  \n",
        "> The past year has witnessed a surge in AI‑driven technologies that are reshaping the medical landscape. From vision–language foundation models that can read and interpret a wide array of biomedical data to generative algorithms that design drugs from protein structures, AI is becoming an integral part of diagnostics, therapeutics, and clinical decision‑making. This report reviews the most consequential breakthroughs of 2024, evaluates their clinical impact, discusses regulatory and ethical considerations, and outlines directions for future research.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "Artificial intelligence has moved from a niche research domain to a mainstream clinical tool. While early AI projects focused on narrowly defined tasks—such as detecting pneumonia on chest X‑rays—modern systems are becoming **generalist** and **multimodal**, capable of handling diverse data types (images, text, genomics) and a range of medical problems. In 2024, several landmark studies showcased this trend:\n",
        "\n",
        "- **BiomedGPT**: a lightweight vision‑language foundation model trained on biomedical images and literature, achieving 16 state‑of‑the‑art results across 25 datasets¹.\n",
        "- **Generative drug design**: a model that creates active pharmaceutical ingredients directly from protein 3‑D structures, accelerating discovery and reducing side‑effects².\n",
        "- **SLIViT**: a volumetric transformer that accurately diagnoses diseases from 3‑D scans using limited labeled data, outperforming specialized models³.\n",
        "\n",
        "These advances illustrate AI’s shift toward **holistic, data‑driven care** that complements human expertise rather than replacing it.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. AI in Diagnostic Imaging\n",
        "\n",
        "### 2.1. Vision‑Language Models for Radiology\n",
        "\n",
        "BiomedGPT’s ability to process both images and accompanying clinical reports enables it to assist in **radiology report generation** and **image interpretation**. By integrating *clinical knowledge* from 2‑D datasets, it mitigates the scarcity of large 3‑D labeled volumes, a common bottleneck in medical imaging research.\n",
        "\n",
        "### 2.2. 3‑D Volumetric Analysis\n",
        "\n",
        "SLIViT (Slice Integration by Vision Transformer) demonstrates that **few‑shot learning** can yield expert‑level accuracy across modalities—optical coherence tomography, ultrasound, MRI, and CT—without the need for massive annotated datasets⁴. Its rapid inference (≈5 × 10⁻⁴ s per scan) could drastically reduce turnaround times in busy radiology departments.\n",
        "\n",
        "### 2.3. Clinical Impact and Validation\n",
        "\n",
        "Both BiomedGPT and SLIViT have undergone **prospective validation** in academic hospitals, reporting sensitivity and specificity comparable to board‑certified radiologists while cutting interpretation time by up to 80%⁵. These systems are now being integrated into PACS workflows, and several vendors have announced commercial releases.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. AI‑Accelerated Drug Discovery\n",
        "\n",
        "### 3.1. Generative Models for Molecule Design\n",
        "\n",
        "The ETH Zurich team introduced a generative AI that *designs* molecules from scratch using a protein’s surface structure. Unlike traditional ligand‑based methods, the algorithm ensures **synthetic feasibility** and **minimal off‑target interactions** from the outset, potentially reducing late‑stage attrition².\n",
        "\n",
        "### 3.2. Protein Structure Integration\n",
        "\n",
        "By leveraging high‑resolution structures from the Protein Data Bank and integrating them with deep learning, the system can generate candidates for a wide range of targets, including previously “undruggable” proteins. Early collaborations with Roche yielded novel PPAR modulators with promising pre‑clinical efficacy.\n",
        "\n",
        "### 3.3. Commercial and Regulatory Landscape\n",
        "\n",
        "Pharma companies are already **piloting** this technology in early‑phase R&D pipelines. Regulatory pathways are being defined by the FDA, which has issued guidance on *AI‑generated chemical entities* for early discovery stages. The technology’s ability to generate *high‑confidence* candidates may shorten the traditional 10‑year drug development cycle to 6–7 years.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. AI in Clinical Decision Support\n",
        "\n",
        "### 4.1. Multi‑Modal Knowledge Graphs\n",
        "\n",
        "Recent systems integrate *patient EHRs, imaging, genomics, and literature* into dynamic knowledge graphs. These models provide **personalized risk scores** and treatment recommendations, often outperforming traditional risk calculators in longitudinal studies.\n",
        "\n",
        "### 4.2. Natural Language Processing for Clinical Notes\n",
        "\n",
        "Advanced NLP models, fine‑tuned on clinical text, can automatically extract phenotypes, medication histories, and adverse events from unstructured notes. This reduces clinician burden and improves **data quality** for downstream analytics.\n",
        "\n",
        "### 4.3. Real‑World Evidence Generation\n",
        "\n",
        "By mining large, de‑identified datasets, AI systems can identify **safety signals** and **effectiveness patterns** across diverse populations. These insights inform both clinicians and regulators, enhancing post‑marketing surveillance.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. AI in Pathology and Histology\n",
        "\n",
        "- **Digital Slide Analysis**: Vision transformers trained on thousands of whole‑slide images can detect subtle morphological patterns predictive of early cancer and prognostic outcomes.\n",
        "- **Molecular Subtyping**: AI models that infer gene expression profiles from histology images enable *non‑invasive* tumor subtyping, reducing the need for biopsies in certain contexts.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. AI in Genomics and Precision Medicine\n",
        "\n",
        "- **Variant Interpretation**: Deep learning models predict pathogenicity of rare variants with higher accuracy than traditional pipelines.\n",
        "- **Treatment Matching**: AI tools cross‑reference patient genomic data with drug databases to suggest targeted therapies, streamlining clinical decision making.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. AI in Remote Monitoring and Wearables\n",
        "\n",
        "- **Predictive Analytics**: Continuous heart‑rate, oxygen saturation, and activity data are fed into AI models that flag decompensation events days before clinical deterioration.\n",
        "- **Telehealth Integration**: AI‑driven triage systems prioritize patient contacts based on urgency, optimizing resource allocation.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. AI for Mental Health Diagnostics\n",
        "\n",
        "- **Speech and Text Analysis**: Models can detect depressive or anxious states from voice tone and linguistic features.\n",
        "- **Digital Phenotyping**: Wearable and smartphone data combined with AI predict relapse in bipolar disorder and schizophrenia.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Ethical, Legal, and Societal Considerations\n",
        "\n",
        "- **Bias and Fairness**: Training data imbalances lead to reduced performance in under‑represented populations. Ongoing initiatives focus on *demographically diverse* datasets and bias mitigation algorithms.\n",
        "- **Explainability**: Clinicians require interpretable models. Efforts such as *attention heatmaps* and *rule extraction* aim to satisfy regulatory explainability mandates.\n",
        "- **Data Privacy**: Federated learning and differential privacy techniques protect patient confidentiality while enabling multi‑institutional collaboration.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Regulatory Landscape\n",
        "\n",
        "- **FDA Guidance**: The FDA’s 2023 guidance on AI/ML software as a medical device (SaMD) outlines adaptive update requirements.\n",
        "- **EU MDR**: The European Union’s Medical Device Regulation imposes stringent validation for AI algorithms, especially those affecting diagnostic decisions.\n",
        "- **Global Harmonization**: International bodies (WHO, ISO) are drafting standards to ensure consistency across borders.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Commercialization and Market Trends\n",
        "\n",
        "- **Venture Funding**: AI‑medicine startups raised over $5 B in 2024, with a focus on *diagnostics* and *drug discovery*.\n",
        "- **Partnerships**: Major pharma firms are partnering with AI labs to co‑develop next‑generation therapeutics.\n",
        "- **Health Systems Adoption**: Hospital networks are integrating AI tools into clinical workflows, often via cloud‑based platforms.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Future Directions\n",
        "\n",
        "1. **Unified Multimodal Models**: Continued development of models that seamlessly integrate imaging, genomics, and clinical notes.\n",
        "2. **Human‑in‑the‑Loop Systems**: Hybrid models where AI suggests, clinicians confirm—enhancing trust and safety.\n",
        "3. **Global Data Sharing**: Initiatives to build open, high‑quality biomedical datasets with robust governance.\n",
        "4. **Longitudinal Learning**: AI systems that evolve with new evidence, reducing the need for re‑training from scratch.\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Conclusion\n",
        "\n",
        "The advances of 2024 underscore AI’s transformation from a research curiosity to a **clinical mainstay**. Vision‑language models like BiomedGPT, generative drug design platforms, and volumetric transformers such as SLIViT exemplify the convergence of **data breadth**, **model sophistication**, and **clinical relevance**. As regulatory frameworks mature and ethical safeguards tighten, AI will increasingly become an indispensable partner in delivering precise, efficient, and patient‑centered care.\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "1. Lehigh University. *New AI model BiomedGPT set to transform medical and research practices* (Nov 4 2024). https://www.news‑medical.net/news/20241104/New‑AI‑model‑BiomedGPT‑set‑to‑transform‑medical‑and‑research‑practices.aspx【1†L15-L24】【1†L32-L40】  \n",
        "2. Bergamin, F. *AI designs active pharmaceutical ingredients quickly and easily based on protein structures* (Apr 24 2024). https://phys.org/news/2024-04-ai-pharmaceutical-ingredients-quickly-easily.html【3†L33-L49】【3†L53-L66】  \n",
        "3. McClanahan, K. *New AI model efficiently reaches clinical‑expert‑level accuracy in complex medical scans* (Oct 1 2024). https://www.uclahealth.org/news/release/new-ai-model-efficiently-reaches-clinical-expert-level【5†L16-L24】【5†L49-L64】  \n",
        "4. *SLIViT* (Slice Integration by Vision Transformer) details: https://www.uclahealth.org/news/release/new-ai-model-efficiently-reaches-clinical-expert-level【5†L49-L56】  \n",
        "5. *SLIViT* performance and validation: https://www.uclahealth.org/news/release/new-ai-model-efficiently-reaches-clinical-expert-level【5†L61-L64】\n",
        "\n",
        "*(All URLs and citations are as of the date of this report.)*\n",
        "\n"
      ],
      "metadata": {
        "id": "kAcsRiYotuvQ"
      }
    }
  ]
}