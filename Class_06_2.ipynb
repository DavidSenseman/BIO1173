{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_06_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ki7Qaf8aomp"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZuqN8TMaomp"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "Sb7ypOhxaomp"
      },
      "source": [
        "**Module 6: Convolutional Neural Networks (CNN) for Computer Vision**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 6 Material\n",
        "\n",
        "* Part 6.1: Using Convolutional Neural Networks\n",
        "* **Part 6.2: Using Pretrained Neural Networks with Keras**\n",
        "* Part 6.3: Facial Recognition and Analysis\n",
        "* Part 6.4: Generative AI and Generating Faces with StyleGAN3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGTQ-cciaomp"
      },
      "source": [
        "### Change your Runtime Now\n",
        "\n",
        "For this lesson you should pick the A100 GPU hardware accelerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKQylnEiLDUM"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    import tensorflow as tf\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerated Run-time Check\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be running either a Graphics Processing Unit (GPU) or a Tensor Processing Unit (TPU). This lesson has been validated using the `A100 GPU`."
      ],
      "metadata": {
        "id": "mLts2NxTTR43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell second\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def check_device():\n",
        "    # Check for available devices\n",
        "    devices = tf.config.list_physical_devices()\n",
        "\n",
        "    # Initialize device flags\n",
        "    cpu = False\n",
        "    gpu = False\n",
        "    tpu = False\n",
        "\n",
        "    # Check device types\n",
        "    for device in devices:\n",
        "        if device.device_type == 'CPU':\n",
        "            cpu = True\n",
        "        elif device.device_type == 'GPU':\n",
        "            gpu = True\n",
        "        elif device.device_type == 'TPU':\n",
        "            tpu = True\n",
        "\n",
        "    # Output device status\n",
        "    if tpu:\n",
        "        print(\"Running on TPU\")\n",
        "    elif gpu:\n",
        "        print(\"Running on GPU\")\n",
        "    elif cpu:\n",
        "        print(\"Running on CPU\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU or a TPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now!\")\n",
        "    else:\n",
        "        print(\"No compatible device found\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU or a TPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now!\")\n",
        "\n",
        "# Call the function\n",
        "check_device()"
      ],
      "metadata": {
        "id": "Wm7L1Kk4TbLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6mkraW6aomq"
      },
      "source": [
        "## Define functions\n",
        "\n",
        "The cell below creates the function(s) needed for this lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k_xChO0Fkn4S",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q09yMGGcmp9N"
      },
      "source": [
        "# **Transfer Learning for Computer Vision**\n",
        "\n",
        "Many advanced prebuilt neural networks are available for computer vision, and Keras provides direct access to many networks. **Transfer Learning** is the technique where you use these prebuilt neural networks.\n",
        "\n",
        "There are several different levels of transfer learning.\n",
        "\n",
        "* Use a prebuilt neural network in its entirety\n",
        "* Use a prebuilt neural network's structure\n",
        "* Use a prebuilt neural network's weights\n",
        "\n",
        "We will begin by using the **MobileNet** prebuilt neural network in its entirety. MobileNet will be loaded and allowed to classify simple images. We can already classify 1,000 images through this technique without ever having trained the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB4Im0gKfF0J"
      },
      "source": [
        "We begin by downloading weights for a MobileNet trained for the imagenet dataset, which will take some time to download the first time you train the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUDtlNDxfMBA"
      },
      "outputs": [],
      "source": [
        "# Download Pre-Trained neural network\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "\n",
        "model = MobileNet(weights='imagenet',include_top=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "~~~text\n",
        "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf.h5\n",
        "17225924/17225924 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\n",
        "~~~"
      ],
      "metadata": {
        "id": "RqJkkVnbcAnv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwuvm2W_fRZy"
      },
      "source": [
        "The loaded network is a Keras neural network. However, this is a neural network that a third party engineered on advanced hardware. Merely looking at the structure of an advanced state-of-the-art neural network can be educational."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF5ZFnrqfXA5",
        "scrolled": true,
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Show model structure\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcdKUTOAfbw0"
      },
      "source": [
        "Several clues to neural network architecture become evident when examining the above structure.\n",
        "\n",
        "**MobileNet** is a neural network architecture designed for mobile and embedded applications to be computationally efficient. The key components of the MobileNet architecture include depthwise separable convolutions and pointwise convolutions.\n",
        "* **Depthwise separable convolutions:** MobileNet replaces traditional convolutions with depthwise separable convolutions, which consists of two separate operations: depthwise convolutions and pointwise convolutions. Depthwise convolutions apply a single filter for each input channel, while pointwise convolutions apply 1x1 convolutions to combine the outputs of depthwise convolutions.\n",
        "* **Pointwise convolutions:** Pointwise convolutions are used to increase the depth of the feature maps while keeping the spatial dimensions the same. This allows for efficient learning of complex patterns while reducing the computational cost.\n",
        "* **Inverted residuals with linear bottleneck:** MobileNetV2 introduces inverted residuals with linear bottleneck to improve performance. Inverted residuals use an expansion layer to increase the number of channels followed by a depthwise convolution and a projection layer to reduce the number of channels back to the original dimensions.\n",
        "\n",
        "Overall, the MobileNet architecture is designed to be lightweight and efficient while maintaining high accuracy for a wide range of tasks, making it ideal for deployment on mobile and edge devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTyH6hyyaomr"
      },
      "source": [
        "## Example 1: Use MobileNet to classify images\n",
        "\n",
        "The code in the cell below creates a two functions that we will need to use classify images using MobileNet.\n",
        "\n",
        "* **make_square()** Since MobileNet is designed to classify images with the same number of horizontal and vertical pixels (i.e. a 'square' image), this function uses a combination of padding and cropping to convert any image into a 'square` image.\n",
        "  \n",
        "* **classify_image()** This function does most of the work. It first retrives the image from the HTTPS server and resizes it before processing it by the MobileNet model that we previously downloaded. The actual prediction is made by this line of code:\n",
        "\n",
        "~~~text\n",
        "  # Use MobileNet model to predict image\n",
        "  pred = model.predict(x)\n",
        "~~~\n",
        "\n",
        "We will now use the MobileNet to classify several image URLs below.  You are encourged to to add additional URLs of your own to see how well the MobileNet can classify."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from PIL import Image, ImageFile, UnidentifiedImageError\n",
        "from matplotlib.pyplot import imshow\n",
        "import requests\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from IPython.display import display\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet, decode_predictions\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS = 224, 224, 3\n",
        "\n",
        "model = MobileNet(weights='imagenet')\n",
        "\n",
        "# Define HTTPS image source\n",
        "ROOT = \"https://biologicslab.co/BIO1173/images/class_06_2/\"\n",
        "\n",
        "# Function to make sample image square\n",
        "def make_square(img):\n",
        "    cols, rows = img.size\n",
        "    pad = ((rows - cols) // 2, 0, (rows - cols) // 2, 0) if rows > cols else (0, (cols - rows) // 2, 0, (cols - rows) // 2)\n",
        "    return img.crop(pad)\n",
        "\n",
        "# Function to classify image\n",
        "def classify_image(url):\n",
        "    ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
        "\n",
        "    try:\n",
        "        # Get image from HTTPS server\n",
        "        response = requests.get(url)\n",
        "        img = Image.open(BytesIO(response.content)).resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.LANCZOS)\n",
        "    except UnidentifiedImageError:\n",
        "        print(\"Error: Cannot identify image file. Check the image URL or file format.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return\n",
        "\n",
        "    # Preprocess the image\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x[:, :, :, :3])  # ensure no alpha channel\n",
        "\n",
        "    # Use MobileNet model to predict image\n",
        "    preds = model.predict(x)\n",
        "\n",
        "    # Show the image\n",
        "    display(img)\n",
        "\n",
        "    # Print out MobileNet's first 5 predictions\n",
        "    for pred in decode_predictions(preds, top=5)[0]:\n",
        "        print(pred)\n"
      ],
      "metadata": {
        "id": "k3x1MIVlfsYx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP6UnmqcjAAo"
      },
      "source": [
        "We can now classify an example image.  You can specify the URL of any image you wish to classify."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CE0NTgraomr"
      },
      "source": [
        "### Example 1A: Classify images\n",
        "\n",
        "Our MobileNet model has been trained to recognize a wide range of images. Let's try a few different types of images to get some idea of what MobileNet knows.\n",
        "\n",
        "Let's start with a picture of Abraham Lincoln. Here is the actual image before being processed:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06_2/abraham_lincoln.jpg)\n",
        "\n",
        "Probably most US school children could correctly identify this image. You should notice that this is _not_ a square image. Let's see what happens when we process it and submit the processed image to our model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwfnWNXSo7Vk",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Example 1A: Classify Images\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "\n",
        "classify_image(ROOT+\"abraham_lincoln.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image01.png)"
      ],
      "metadata": {
        "id": "oICNdrBmefqv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEzm0Klfaoms"
      },
      "source": [
        "From visual inspection of the output, MobileNet concluded that there was a 93% chance that the image contained a `bow tie` and less than a 1% chance the image contained a `suit`.\n",
        "\n",
        "Apparently, the creators of MobileNet didn't include images of American presidents in their training set. What MobileNet \"saw\" wasn't the Lincoln's face, but the clothes he was wearing.\n",
        "\n",
        "We can also see what happens when our code converts a rectangular image into a square image. Lincoln's face is still clearly recognizable albeit a bit 'squashed'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB2bFChLaoms"
      },
      "source": [
        "### Example 1B: Classify Images\n",
        "\n",
        "Perhaps MobileNet only recognizes famous people who are currently alive? Let's see how MobileNet does with another President of the US (POTUS)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hH6IpkKPaoms"
      },
      "outputs": [],
      "source": [
        "# Example 1B: Classify Images\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "\n",
        "classify_image(ROOT+\"trump.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image02.png)"
      ],
      "metadata": {
        "id": "qOspbNN_gKwC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkW8SEhDaoms"
      },
      "source": [
        "Once again, MobileNet doesn't seem to know the names of American Presidents. Rather, MobilNet tried to classify the \"objects\" it saw, not the people.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJXZSmXnaoms"
      },
      "source": [
        "### Example 1C: Classify Images\n",
        "\n",
        "Let's try something else. How about a nice Thompson Submachine gun? This particular image is of a [Lancer Tactical Extra 50 Rounds Airsoft Magazine - Airsoft Tommy Thompson Submachine Gun (2X Drum 2X Stick)](https://us.amazon.com/Lancer-Tactical-Rounds-Airsoft-Magazine/dp/B0C24VRLM1). This airgun is sold on Amazon in case you might like to buy it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "PNk_zkAeaoms"
      },
      "outputs": [],
      "source": [
        "# Example 1C: Classify Images\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "\n",
        "classify_image(ROOT+\"submachine_gun.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image03.png)"
      ],
      "metadata": {
        "id": "mQSz42GOhT-W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydJG5nX6aoms"
      },
      "source": [
        "Finally, we have one type of image that MobileNet has been trained on. MobileNet predicted that there was an approximately 54% chance that our image of a Thompson Submachine Gun was an 'assault_rife' and a 49% that the image was some kind of 'rifle'.\n",
        "\n",
        "Technically, the Thompson Submachine Gun is _not_ considered to be an assault rife or even a rifle, but it wasn't a bad guess."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PTCwUCdaoms"
      },
      "source": [
        "### Example 1D: Classify images\n",
        "\n",
        "Let's see how MobileNet does with an image of an American soldier with a _real_ assault rifle, an M16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kljX1r_gaoms"
      },
      "outputs": [],
      "source": [
        "# Example 1D: Classify Images\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "\n",
        "classify_image(ROOT+\"M16.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image04.png)"
      ],
      "metadata": {
        "id": "0arcyhLnh7eU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Eeped0Yaoms"
      },
      "source": [
        "For this image, MobileNet did pretty good. It predicted that there was an 65% chance that the image showed an assault rifle. (The M16 is considered an assult rifle). Again, MobileNet seems to ignore the inclusion of people in an image and focus on inanimate objects in the image including the presence of a `military uniform`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynAikv5taoms"
      },
      "source": [
        "## **Exercise 1: Classify images**\n",
        "\n",
        "For **Exercise 1**, you are present our MobileNet model the following series of 8 animal images.\n",
        "\n",
        "1. puffer_fish.jpg (Exercise 1A)\n",
        "2. king_cobra.jpg (Exercise 1B)\n",
        "3. monarch_butterfly.jpg (Exercise 1C)\n",
        "4. viceroy_butterfly.jpg (Exercise 1D)\n",
        "5. meerkat.jpg (Exercise 1E)\n",
        "6. paramecium.jpg (Exercise 1F)\n",
        "7. prairie_dog.jpg (Exercise 1G)\n",
        "8. great-white-shark.jpg (Exercise 1H)\n",
        "\n",
        "You will need to make a new code cell for each exercise.\n",
        "\n",
        "In COLAB, you can add a new code cell by pointing your cursor at the bottom edge of this code cell and selecting the button that says `+ Code`.\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image02A.png).\n",
        "\n",
        "When you are done with **Exercise 1**, you will have added 8 new code cells, one cell for each image in the list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coCA5NZzaomt"
      },
      "source": [
        "If your code is correct, you should have received the following output (only the the first value is shown):\n",
        "\n",
        "1. `puffer_fish.jpg` ('n02655020', 'puffer', 0.99882895)  Note: 99% accurate!\n",
        "2. `king_cobra.jpg` ('n01748264', 'Indian_cobra', 0.999587) Note: Indian cobras are much smaller than king cobras so technically, MobileNet got this wrong. However, there is no way to know the size of the snake in the image, so we can score this as being 100% accurate.\n",
        "3. `monarch_butterfly.jpg` ('n02279972', 'monarch', 0.99983335) Note: Nailed it!\n",
        "4. `viceroy_butterfly.jpg` ('n02279972', 'monarch', 0.9988518) Note: Oops! A total fail, but completely understandable. MobileNet again predicted that there was 100% probability that the picture was a monarch butterfly (_Danaus plexippus_), when in fact it was a viceroy butterfly, a completely different genus and species (_Limenitis archippus_). Of course these two species look very, very similar. The viceroy and the monarch butterflies are [Müllerian mimics](https://en.wikipedia.org/wiki/M%C3%BCllerian_mimicry) of each other. It is certainly possible to train a neural network to do this for example [MonarchNet](https://ai4earthscience.github.io/neurips-2020-workshop/papers/ai4earth_neurips_2020_57.pdf). As will be shown below you could start with the MobileNet as give it additional training to differentiate images of monarchs and viceroys with a high degree of precision.\n",
        "5. `meerkat.jpg` ('n02138441', 'meerkat', 0.9629027) Note: Not bad! 96% accurate.\n",
        "6. `paramecium.jpg` ('n01930112', 'nematode', 0.8969467) Note: Not too close. A nematode is round worm such as the common human parasite, the pinworm (_Enterobius_). Apparently, MobileNet wasn't extensively programmed for unicelluar organisms.\n",
        "7. `prairie_dog.jpg` ('n02361337', 'marmot', 0.93886554) Note: Another fail, but again understandable. [Marmots](https://en.wikipedia.org/wiki/Marmot) genus _Marmota_ are quite similar in appearance to pairie dogs, so we can give a 'pass' to MobileNet for getting these two species confused.\n",
        "8. `great-white-shark.jpg` ('n01484850', 'great_white_shark', 0.9989716) Note: Nailed it! 100% correct prediction.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D523in0Faomt"
      },
      "source": [
        "## **MobileNet Summary**\n",
        "\n",
        "Overall, our MobileNet neural network did quite well, as long as you picked one of the 1000 image types it supports. For many applications, MobileNet might be entirely acceptable as an image classifier.\n",
        "\n",
        "However, if you need to classify very specialized images, like monarch vs viceroy butterflies, or marmots vs prairie dogs --image types supported by imagenet--it is necessary to use **_transfer learning_**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdhYb-8Daomt"
      },
      "source": [
        "--------------------------------\n",
        "\n",
        "## **ResNet vs MobileNet**\n",
        "\n",
        "MobileNet and ResNet are both popular deep learning architectures, but they have some key differences:\n",
        "\n",
        "* **Architecture:** MobileNet uses depthwise separable convolutions, which consist of depthwise convolutions and pointwise convolutions, to reduce the computational cost and make the model more efficient for mobile and embedded applications. ResNet, on the other hand, uses residual blocks with skip connections to learn residual mappings for easier training of deep networks.\n",
        "* **Model size:** MobileNet is known for its lightweight and compact design, making it easy to deploy on mobile devices with limited computational resources. In contrast, ResNet is a deeper network with more parameters, which can lead to higher accuracy but also requires more computational resources.\n",
        "* **Training complexity:** ResNet can be easier to train compared to MobileNet, especially for deeper architectures, due to the use of skip connections that help with gradient flow and alleviate the vanishing gradient problem.\n",
        "* **Performance:** ResNet is often used for tasks that require high accuracy, such as image classification on large datasets like ImageNet. MobileNet is designed for efficiency and speed, making it suitable for real-time applications on mobile devices.\n",
        "\n",
        "Overall, the choice between MobileNet and ResNet depends on the specific requirements of the application, such as computational resources, accuracy goals, and deployment constraints.\n",
        "\n",
        "----------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example 2: Use ResNet for Regression CNN**\n",
        "\n",
        "For Example 2 we are going to use ResNet as a basis for building a CNN that can count the number of paperclips in an image. We will provide an image and expect the neural network to count items in that image. We will use a dataset created by Jeff Heaton that contains a random number of paperclips.\n",
        "\n",
        "Here are four sample images from the 25,000 images in the paperclips dataset:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_paperclips.png)\n",
        "\n",
        "\n",
        "Our goal will be to create a convolutional neural network (CNN) that can count the number of paperclips in an image. To put in a more ecological or biomedical context, a similar neural network could also be trained to count the number of giant Saguaro cacti (_Carnegiea gigantea_) in an image of the Sonoran Desert, or the number of leucocytes in a blood smear from a patient with symptoms of AML (Acute myeloid leukemia).\n"
      ],
      "metadata": {
        "id": "QFz_GonQgiOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although paperclips are not biomedically relevant items to count, the code shown below could used to count"
      ],
      "metadata": {
        "id": "YN7JybKbixZ2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgVuchh9aomw"
      },
      "source": [
        "### **Example 2 - Step 1: Set ENVIRONMENTAL VARIABLES**\n",
        "\n",
        "The code in the cell below defines a number of ENVIRONMENTAL VARIABLES that are needed for latter code cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpfadrdQcVg8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Example 2 - Step 1: Set ENVIRONMENTAL VARIABLES\n",
        "\n",
        "import os\n",
        "\n",
        "URL = \"https://biologicslab.co/BIO1173/data/\"\n",
        "DOWNLOAD_SOURCE = URL+\"paperclips.zip\"\n",
        "DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfind('/')+1:]\n",
        "print(\"DOWNLOAD_SOURCE=\",DOWNLOAD_SOURCE)\n",
        "print(\"DOWNLOAD_NAME=\",DOWNLOAD_NAME)\n",
        "\n",
        "PATH = \"/content\"\n",
        "EXTRACT_TARGET = os.path.join(PATH,\"clips\")\n",
        "SOURCE = os.path.join(EXTRACT_TARGET, \"paperclips\")\n",
        "\n",
        "print(\"ENVIRONMENTAL VARIABLES were successfully created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuBLdjMiaomw"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "~~~text\n",
        "DOWNLOAD_SOURCE= https://biologicslab.co/BIO1173/data/paperclips.zip\n",
        "DOWNLOAD_NAME= paperclips.zip\n",
        "ENVIRONMENTAL VARIABLES were successfully created.\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQiDl3Z6aomw"
      },
      "source": [
        "### **Example 2 - Step 2: Download and Extract Image Data**\n",
        "\n",
        "The code in the cell below downloads a zip file from the course HTTPS server, `https://biologicslab.co/BIO1173/data`, called `paperclips.zip`. The code then extracts (unzips) data in the zip file into a new folder called `/clips`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Step 2: Download and Extract Data\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Creating necessary directories...\", end='')\n",
        "# Create necessary directories\n",
        "os.makedirs(SOURCE, exist_ok=True)\n",
        "os.makedirs(EXTRACT_TARGET, exist_ok=True)\n",
        "print(\"done.\")\n",
        "\n",
        "print(\"Downloading files...\", end='')\n",
        "# Define paths and URLs\n",
        "download_path = os.path.join(PATH, DOWNLOAD_NAME)\n",
        "extract_path = os.path.join(EXTRACT_TARGET, DOWNLOAD_NAME)\n",
        "\n",
        "# Download the file\n",
        "os.system(f\"wget -O {download_path} {DOWNLOAD_SOURCE}\")\n",
        "print(\"done.\")\n",
        "\n",
        "print(\"Extracting files...\", end='')\n",
        "# Extract the file\n",
        "os.system(f\"unzip -o -d {EXTRACT_TARGET} {download_path} >/dev/null\")\n",
        "print(\"done.\")"
      ],
      "metadata": {
        "id": "xS62QUC7k68H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7yGFymMaomw"
      },
      "source": [
        " If your code is correct, you should see something similar to the following output:\n",
        "\n",
        "~~~text\n",
        "Creating necessary directories...done.\n",
        "Downloading files...done.\n",
        "Extracting files...done.\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX4uUN5DkuYH"
      },
      "source": [
        "### **Example 2 - Step 3: Load the Labels for the Training Set**\n",
        "\n",
        "The labels are contained in a CSV file named **train.csv** for the regression. This file has just two labels, **id** and **clip_count**. The ID specifies the filename; for example, row id 1 corresponds to the file **clips-1.jpg**. The following code loads the labels for the training set and creates a new column, named **filename**, that contains the filename of each image, based on the **id** column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4W4LeGSqYya",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Example 2 - Step 3: Load the Labels for the Training Set\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the label file\n",
        "df = pd.read_csv(\n",
        "        os.path.join(SOURCE,\"train.csv\"),\n",
        "        na_values=['NA', '?'])\n",
        "\n",
        "# Convert\n",
        "df['filename']=\"clips-\"+df[\"id\"].astype(str)+\".jpg\"\n",
        "\n",
        "# Print out df\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGxjITNtz0sC"
      },
      "source": [
        "This results in the following dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6x3Rfnlaomx"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_df.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHLcB3lPaomx"
      },
      "source": [
        "### **Example 2 = Step 4: Split images into training and validation sets**\n",
        "\n",
        "The code in the cell below, splits the paperclips images into a training set and a validation set, with 90% of the images going into the training set. The number images in both sets is printed out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CI4_qNaSqp31",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Split images into training and validation sets\n",
        "\n",
        "# create dataframes to data\n",
        "df_train = pd.read_csv(os.path.join(SOURCE, \"train.csv\"))\n",
        "df_train['filename'] = \"clips-\" + df_train.id.astype(str) + \".jpg\"\n",
        "\n",
        "TRAIN_PCT = 0.9\n",
        "TRAIN_CUT = int(len(df_train) * TRAIN_PCT)\n",
        "\n",
        "df_train_cut = df_train[0:TRAIN_CUT]\n",
        "df_validate_cut = df_train[TRAIN_CUT:]\n",
        "\n",
        "print(f\"Training size: {len(df_train_cut)}\")\n",
        "print(f\"Validate size: {len(df_validate_cut)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dtoDQGgaomx"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "~~~text\n",
        "Training size: 18000\n",
        "Validate size: 2000\n",
        "~~~\n",
        "We want to use early stopping. To do this, we need a validation set. We will break the data into 90% test data and 10% validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdF0KQvFaomx"
      },
      "source": [
        "## **Transfer Learning using `ResNet`**\n",
        "\n",
        "For **Example 2** we will make use of the structure of the **ResNet** neural network. However, there are several significant changes that we will make to ResNet to apply to this task.\n",
        "\n",
        "First, ResNet is a classifier. However, we wish to perform a regression to count. Secondly, we want to change the image resolution that ResNet uses. Since the synaptic weights that comes with ResNet are designed to work with a different image resolution, we can't use these weights, so we won't import them. However, this means we will need to retrain the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj_umT9VgRzI"
      },
      "source": [
        "### Example 2 - Step 5: Create Image Generators\n",
        "\n",
        "Next, we create the generators that will provide the images to the neural network during training.\n",
        "\n",
        "A Keras Image Generator, specifically `ImageDataGenerator`, is a powerful tool for creating batches of image data in real-time with optional data augmentation. This means it can apply random transformations like rotations, shifts, flips, and more, to your images while feeding them to your model, thus enhancing the diversity of your training data and helping prevent overfitting.\n",
        "\n",
        "In short, it automates the flow of image data from directories and applies transformations on-the-fly. It's especially useful for handling large datasets and implementing data augmentation effortlessly.\n",
        "\n",
        "In the cell below, we normalize the images so that the RGB colors between 0-255 become ratios between 0 and 1. We also use the **flow_from_dataframe** generator to connect the Pandas dataframe to the actual image files. We see here a straightforward implementation; you might also wish to use some of the image transformations provided by the data generator.\n",
        "\n",
        "The **HEIGHT** and **WIDTH** constants specify the dimensions to which the image will be scaled (or expanded). It is probably not a good idea to expand the images."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Install Keras Image Generator**\n",
        "\n",
        "The Keras function `ImageDataGenerator` is part of the Keras package `keras_preprocessing` which needs to be installed by running the next code cell."
      ],
      "metadata": {
        "id": "3gE7CIBbozNa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "THpzclwHaomx"
      },
      "outputs": [],
      "source": [
        "# Install Keras package\n",
        "\n",
        "!pip install keras_preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "~~~text\n",
        "Collecting keras_preprocessing\n",
        "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
        "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.26.4)\n",
        "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n",
        "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
        "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 kB 1.8 MB/s eta 0:00:00\n",
        "Installing collected packages: keras_preprocessing\n",
        "Successfully installed keras_preprocessing-1.1.2\n",
        "~~~"
      ],
      "metadata": {
        "id": "030Vcb-fpbYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that correct packages have been loaded, we can create our image generators."
      ],
      "metadata": {
        "id": "KC5Rh9aEpmDo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYiMObEJsoof",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create Image Generator\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "WIDTH = 256\n",
        "HEIGHT = 256\n",
        "BATCH_SIZE_TRAIN = 32\n",
        "BATCH_SIZE_VAL = 256\n",
        "\n",
        "training_datagen = ImageDataGenerator(\n",
        "  rescale = 1./255,\n",
        "  horizontal_flip=True,\n",
        "  #vertical_flip=True,\n",
        "  fill_mode='nearest')\n",
        "\n",
        "train_generator = training_datagen.flow_from_dataframe(\n",
        "        dataframe=df_train_cut,\n",
        "        directory=SOURCE,\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"clip_count\",\n",
        "        target_size=(HEIGHT, WIDTH),\n",
        "        # Keeping the training batch size small\n",
        "        # USUALLY increases performance\n",
        "        batch_size=BATCH_SIZE_TRAIN,\n",
        "        class_mode='raw')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "val_generator = validation_datagen.flow_from_dataframe(\n",
        "        dataframe=df_validate_cut,\n",
        "        directory=SOURCE,\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"clip_count\",\n",
        "        target_size=(HEIGHT, WIDTH),\n",
        "        # Make the validation batch size as large as you\n",
        "        # have memory for\n",
        "        batch_size=BATCH_SIZE_VAL,\n",
        "        class_mode='raw')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYxcfHqTaomy"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "~~~text\n",
        "Found 18000 validated image filenames.\n",
        "Found 2000 validated image filenames.\n",
        "~~~\n",
        "This means that our train and validation generator are working properly and they know where to find the images of paperclips."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rVZJBMIgqJq"
      },
      "source": [
        "# **Transfer Learning with ResNet**\n",
        "\n",
        "We will now use a ResNet neural network as a basis for our neural network. We will redefine both the input shape and output of the ResNet model, so we will not transfer the weights. Since we redefine the input, the weights are of minimal value.\n",
        "\n",
        "We begin by loading, from Keras, the `ResNet50 network`. We specify **include_top** as `False` because we will change the input resolution. We also specify **weights** as `False` because we must retrain the network after changing the top input layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBx-hAl-aomy"
      },
      "source": [
        "### **Example 2 - Step 6: Redefine the input shape**\n",
        "\n",
        "The first modification of the base ResNet model `ResNet50` that we need to make is to redefine the image shape to 256 X 256 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6MJpmLyhtahJ"
      },
      "outputs": [],
      "source": [
        "# Example 2 - Step 6: Redefine the input shape\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# Set variables\n",
        "HEIGHT = 256\n",
        "WIDTH = 256\n",
        "\n",
        "input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
        "\n",
        "base_model = ResNet50(\n",
        "    include_top=False, weights=None, input_tensor=input_tensor,\n",
        "    input_shape=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xF9EKVFhAGl"
      },
      "source": [
        "### **Example 2 - Step 7: Add Layers to Convert Model to Regression**\n",
        "\n",
        "Now we must add a few layers to the end of the neural network so that it becomes a regression model. As you should expect for a regression model, there is only a single neuron in the output layer:\n",
        "~~~text\n",
        "model=Model(inputs=base_model.input,outputs=Dense(1)(x))\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gXO3lTFIs_U1"
      },
      "outputs": [],
      "source": [
        "# Example 2 - Step 7: Add layers to convert model to regression\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "x=base_model.output\n",
        "x=GlobalAveragePooling2D()(x)\n",
        "x=Dense(1024,activation='relu')(x)\n",
        "x=Dense(1024,activation='relu')(x)\n",
        "model=Model(inputs=base_model.input,outputs=Dense(1)(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErCUZTv1hJDx"
      },
      "source": [
        "### **Example 2 - Step 8: Train the Model**\n",
        "\n",
        "In the cell below, we provide **additional** training to the base `ResNet50` model by \"showing it\" 16200 test images with their labels (i.e. how many paperclips are in the image) using a 1800 validation image set to allow us to have EarlyStopping.\n",
        "\n",
        "Training is like before. However, we do not define the entire neural network here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Step 8: Train the Model\n",
        "\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "\n",
        "# Set variables\n",
        "EPOCHS = 100\n",
        "PATIENCE = 20\n",
        "BATCH_SIZE_TRAIN = train_generator.batch_size\n",
        "BATCH_SIZE_VAL = val_generator.batch_size\n",
        "STEP_SIZE_VALID = val_generator.n // BATCH_SIZE_VAL\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam',\n",
        "              metrics=[RootMeanSquaredError(name=\"rmse\")])\n",
        "\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3,\n",
        "                        patience=PATIENCE, verbose=1, mode='auto',\n",
        "                        restore_best_weights=True)\n",
        "\n",
        "print(f\"-- Training is starting for {EPOCHS} epochs, batch size train: {BATCH_SIZE_TRAIN}, batch size val: {BATCH_SIZE_VAL} ----------\")\n",
        "\n",
        "history = model.fit(train_generator, epochs=EPOCHS, steps_per_epoch=250,\n",
        "                    validation_data=val_generator, callbacks=[monitor],\n",
        "                    verbose=1, validation_steps=STEP_SIZE_VALID)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")\n"
      ],
      "metadata": {
        "id": "i3WnGyG0ehW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXHwP_hXwKwZ"
      },
      "source": [
        "If `EarlyStopping` doesn't kick in, training may require a significant amount of time to complete all 100 epochs.\n",
        "\n",
        "Here is the final output for one run using the `A100 GPU`:\n",
        "\n",
        "~~~text\n",
        "Epoch 47/100\n",
        "250/250 ━━━━━━━━━━━━━━━━━━━━ 18s 74ms/step - loss: 4.1940 - rmse: 2.0448 - val_loss: 14.5238 - val_rmse: 3.8110\n",
        "Epoch 47: early stopping\n",
        "Restoring model weights from the end of the best epoch: 27.\n",
        "Elapsed time: 0:13:17.05\n",
        "~~~\n",
        "\n",
        "If we wanted to,  we could zip-up the preprocessed files and store them somewhere for later use if we needed a trained neural neural network to count the number of paperclips on a piece of paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to the GDrive\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.models import save_model\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Save path is the current directory\n",
        "save_path = '/content/drive/My Drive/ResNetModel.keras'\n",
        "\n",
        "# Save the model in the native Keras format\n",
        "model.save(save_path)\n",
        "\n",
        "# Verify the model is saved by listing the directory contents\n",
        "os.listdir('/content/drive/My Drive')\n"
      ],
      "metadata": {
        "id": "Q_v4ko96ksKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following output:\n",
        "\n",
        "~~~text\n",
        "['biologicslab.co.txt',\n",
        " 'Colab Notebooks',\n",
        " 'ResNetModel.keras']\n",
        "~~~"
      ],
      "metadata": {
        "id": "cCPyv3Jqvndl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Delete `ResNetModel` from current directory**\n",
        "\n",
        "Run the next cell to delete `ResNetModel.keras` from your current directory."
      ],
      "metadata": {
        "id": "OGAlscCtl-7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete ResNetModel from current directory\n",
        "\n",
        "import os\n",
        "\n",
        "# Specify the path to the file you want to delete\n",
        "# file_path = '/content/drive/My Drive/ResNetModel.keras'\n",
        "\n",
        "file_path = './ResNetModel.keras'\n",
        "\n",
        "# Check if the file exists before attempting to delete it\n",
        "if os.path.exists(file_path):\n",
        "    os.remove(file_path)\n",
        "    print(f\"File {file_path} has been deleted from your current working directory.\")\n",
        "else:\n",
        "    print(f\"File {file_path} does not exist in your current working directory.\")\n"
      ],
      "metadata": {
        "id": "kXxc10nRmS_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "~~~text\n",
        "File ./ResNetModel.keras has been deleted from your current working directory.\n",
        "~~~"
      ],
      "metadata": {
        "id": "l2-_Wj_ar9Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Copy the trained ResNet model from your GDrive**\n",
        "\n",
        "The cell above deleted the trained model `ResNetModel.keras` from your current Colab directory. This would also happen when you terminate this lesson. When you leave Colab, your `runtime` is deleted which means all of the files in your current working directory will be lost.\n",
        "\n",
        "However, since we previously saved a copy of the trained ResNet model to your GDrive, we can download it to your Colab current working directory by running the next code cell."
      ],
      "metadata": {
        "id": "m4qJgIAnt8xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the trained ResNet model from your GDrive\n",
        "\n",
        "import shutil\n",
        "\n",
        "# Source path in Google Drive\n",
        "source_path = '/content/drive/My Drive/ResNetModel.keras'\n",
        "\n",
        "# Destination path in the current working directory\n",
        "destination_path = './ResNetModel.keras'\n",
        "\n",
        "# Copy the file\n",
        "shutil.copy(source_path, destination_path)\n",
        "\n",
        "# Verify the file has been copied\n",
        "!ls -l\n"
      ],
      "metadata": {
        "id": "PS4Uw6vetas9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following output:\n",
        "\n",
        "~~~text\n",
        "total 473452\n",
        "drwxr-xr-x 4 root root      4096 Jan 26 17:20 clips\n",
        "drwx------ 7 root root      4096 Jan 26 17:17 drive\n",
        "-rw-r--r-- 1 root root 163590691 Apr  8  2024 paperclips.zip\n",
        "-rw------- 1 root root 321208236 Jan 26 17:43 ResNetModel.keras\n",
        "drwxr-xr-x 1 root root      4096 Jan 23 14:24 sample_data\n",
        "~~~"
      ],
      "metadata": {
        "id": "T_G_9sF4slP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example 2 - Step 9: Plot `rmse` and `val_rmse`**\n",
        "\n",
        "The code in the cell below plot's the `rmse` and `val_rmse` recorded during each epoch in the training cycle. As you can see, `rmse` and `val_rmse` decrease during training as model adjusts its synaptic weights to improve its regression precision after each epoch."
      ],
      "metadata": {
        "id": "9WfTTMR8wC39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Step 9: Plot rmse and val_rmse\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.plot(history.history['rmse'])\n",
        "plt.plot(history.history['val_rmse'])\n",
        "plt.legend(['RMSE', 'Val RMSE'], frameon=False)\n",
        "plt.xticks(np.arange(0, 50, 5))\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('RMSE in Every Epoch')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N_wkyphr1Tvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image05.png)"
      ],
      "metadata": {
        "id": "XHynxEjW18wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example - Step 10: Display an Image from the Test Set**\n",
        "\n",
        "The code in the next cell picks a random paperclip image from the training set and prints out with the actual number of paperclips in the image shown in the title.\n"
      ],
      "metadata": {
        "id": "VJsxwYTMwHrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example - Step 10: Display an Image from the Test Set\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get a batch of images and labels from the generator\n",
        "batch = next(train_generator)\n",
        "images, labels = batch\n",
        "\n",
        "# Display the first image in the batch\n",
        "plt.imshow(images[0])\n",
        "plt.title(f\"Actual Number of Paperclips: {labels[0]}\")\n",
        "actual_num_paperclips = labels[0]\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Save the image without the title\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(images[0])\n",
        "ax.axis('off')\n",
        "fig.savefig('saved_image.png', bbox_inches='tight', pad_inches=0)\n",
        "plt.close(fig)\n"
      ],
      "metadata": {
        "id": "oRlRrZXM55ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image06.png)"
      ],
      "metadata": {
        "id": "t4NvpOHK1yZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example 2 - Step 11: Count Paperclips with ResNet Model**\n",
        "\n",
        "The code in the next cell submits the image about (without a title) to our modified ResNet model that we trained in **Example 2 - Step 8** using a custom function called `load_and_preprocess_image()`:\n",
        "\n",
        "~~~text\n",
        "# Function to load and preprocess the image\n",
        "def load_and_preprocess_image(img_path, target_size):\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0  # Rescale if you trained with rescaled images\n",
        "    return img_array\n",
        "~~~\n",
        "\n",
        "Once the image has been processed, it is submitted (without a title) to our modified ResNet model with this code chunk:\n",
        "\n",
        "~~~text\n",
        "# Predict the number of objects\n",
        "prediction = model.predict(img_array)\n",
        "predicted_count = np.round(prediction[0][0])\n",
        "~~~\n",
        "\n",
        "which generates a prediction as to number of paperclips it \"sees\" in the image. The code then prints out the image again, but this time with the number of paperclips the models thinks it sees in the title."
      ],
      "metadata": {
        "id": "spYnMmZIzJtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Step 11: Count Paperclips with ResNet Model\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "# Function to load and preprocess the image\n",
        "def load_and_preprocess_image(img_path, target_size):\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0  # Rescale if you trained with rescaled images\n",
        "    return img_array\n",
        "\n",
        "# Path to your image\n",
        "img_path = 'saved_image.png'\n",
        "\n",
        "# Load and preprocess the image\n",
        "img_array = load_and_preprocess_image(img_path, target_size=(256, 256))\n",
        "\n",
        "# Predict the number of objects\n",
        "prediction = model.predict(img_array)\n",
        "predicted_count = np.round(prediction[0][0])\n",
        "\n",
        "# Display the image and the prediction\n",
        "plt.imshow(image.load_img(img_path))\n",
        "plt.title(f\"Predicted number of paperclips: {predicted_count}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"The actual number of paperclips:{actual_num_paperclips}\")\n"
      ],
      "metadata": {
        "id": "Bm-wdG5iwhJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_image07.png)"
      ],
      "metadata": {
        "id": "vPGWDMJr17Ha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since image selection is randomized, you won't see the same images as shown about. Unless your image has a very low number of paperclips in it, you will see that the number of paperclips predicted by the model is not exactly the same as the actual number. For the image shown above, the actual number of paperclips in the image is `59` which our model predicted the number was `78`. In this paricular instance, the model is off by ~30%.  "
      ],
      "metadata": {
        "id": "lTpqkO3uwgxq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UadJEsfGaomz"
      },
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_06_2.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Poly-A Tail**\n",
        "\n",
        "## **IBM PC/AT**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/6/69/IBM_PC_AT.jpg)\n",
        "\n",
        "\n",
        "The IBM Personal Computer AT (model 5170, abbreviated as IBM AT or PC/AT) was released in 1984 as the fourth model in the IBM Personal Computer line, following the IBM PC/XT and its IBM Portable PC variant. It was designed around the Intel 80286 microprocessor.\n",
        "\n",
        "**Name**\n",
        "\n",
        "IBM did not specify an expanded form of AT on the machine, press releases, brochures or documentation, but some sources expand the term as Advanced Technology, including at least one internal IBM document.\n",
        "\n",
        "**History**\n",
        "\n",
        "IBM's 1984 introduction of the AT was seen as an unusual move for the company, which typically waited for competitors to release new products before producing its own models. At $4,000–6,000, it was only slightly more expensive than considerably slower IBM models. The announcement surprised rival executives, who admitted that matching IBM's prices would be difficult. No major competitor showed a comparable computer at COMDEX Las Vegas that year.\n",
        "\n",
        "The AT is IBM PC compatible, with the most significant difference being a move to the 80286 processor from the 8088 processor of prior models. Like the IBM PC, the AT supported an optional math co-processor chip, the Intel 80287, for faster execution of floating point operations.\n",
        "\n",
        "In addition, it introduced the AT bus, later known as the ISA bus, a 16-bit bus with backward compatibility with 8-bit PC-compatible expansion cards. The bus also offered fifteen IRQs and seven DMA channels, expanded from eight IRQs and four DMA channels for the PC, achieved by adding another 8259A IRQ controller and another 8237A DMA controller. Some IRQ and DMA channels are used by the motherboard and not exposed on the expansion bus. Both dual IRQ and DMA chipsets are cascading which shares the primary pair. In addition to these chipsets, Intel 82284 Clock Driver and Ready Interface and Intel 82288 Bus Controller are to support the microprocessor.\n",
        "\n",
        "PC DOS 3.0 was included with support for the new AT features, including preliminary kernel support for networking (which was fully supported in a later version 3.x release).\n",
        "\n",
        "The motherboard includes a battery-backed real-time clock (RTC) using the Motorola MC146818. This was an improvement from the PC, which required setting the clock manually or installing an RTC expansion card. The RTC also included a 1024 Hz timer (on IRQ 8), a much finer resolution than the 18 Hz timer on the PC.\n",
        "\n",
        "In addition to keeping the time, the RTC includes 50 bytes of CMOS memory which is used to store software-adjustable BIOS parameters. A disk-based BIOS setup program which saved to this memory took the place of the DIP switches used to set system settings on PCs. Most AT clones have the setup program in ROM rather than on disk.\n",
        "\n",
        "**Storage**\n",
        "\n",
        "The standard floppy drive was upgraded to a 1.2 MB 5+1⁄4 inch floppy disk drive (15 sectors of 512 bytes, 80 tracks, two sides), which stored over three times as much data as the 360 KB PC floppy disk, but had compatibility problems with 360k disks (see Problems below). 3+1⁄2 inch floppy drives became available in later ATs.\n",
        "\n",
        "A 20 MB hard disk drive was included as standard. Early drives were manufactured by Computer Memories and were found to be very unreliable.\n",
        "\n",
        "**Peripherals**\n",
        "\n",
        "The AT included the AT keyboard, initially a new 84-key layout (the 84th key being SysRq). The numerical keypad was now clearly separated from the main key group, and indicator LEDs were added for Caps Lock, Scroll Lock and Num Lock. The AT keyboard uses the same 5-pin DIN connector as the PC keyboard, but a different, bidirectional electrical interface with different keyboard scan codes. The bidirectional interface allows the computer to set the LED indicators on the keyboard, reset the keyboard, set the typematic rate, and other features. Later ATs included 101-key keyboards, e.g. the Model M keyboard.\n",
        "\n",
        "The AT is also equipped with a physical lock that prevents access to the computer by disabling the keyboard and holding the system unit's cover in place.\n",
        "\n",
        "ATs could be equipped with CGA, MDA, EGA, or PGA video cards.\n",
        "\n",
        "The 8250 UART from the PC was upgraded to the 16450, but since both chips had single-byte buffers, high-speed serial communication was problematic as with the XT."
      ],
      "metadata": {
        "id": "_DunF4mLubJS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kNZr9pcBuY_O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}