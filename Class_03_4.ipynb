{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEEW13uCtiJ"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/master/Class_03_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIO 1173: Intro Computational Biology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module 3: Introduction to TensorFlow**\n",
    "\n",
    "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
    "\n",
    "\n",
    "### Module 3 Material\n",
    "\n",
    "* Part 3.1: Deep Learning and Neural Network Introduction\n",
    "* Part 3.2: Introduction to Tensorflow and Keras\n",
    "* Part 3.3: Saving and Loading a Keras Neural Network\n",
    "* **Part 3.4: Early Stopping in Keras to Prevent Overfitting**\n",
    "* Part 3.5: Extracting Weights and Manual Calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpVJpj2DCtiM"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wU1AMzx8CtiM",
    "outputId": "24a17c67-4563-471f-9955-dfadd0f171d7"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Setup\n",
    "\n",
    "Run the next code cell to load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You MUST run this code cell first\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "path = '/'\n",
    "memory = shutil.disk_usage(path)\n",
    "dirpath = os.getcwd()\n",
    "print(\"Your current working directory is : \" + dirpath)\n",
    "print(\"Disk\", memory)\n",
    "print(\"Numpy version =\", (np.__version__)) \n",
    "print(\"Tensorflow version =\", (tf.__version__))\n",
    "print(\"Available GPU acceleration =\", tf.test.gpu_device_name())\n",
    "print(\"Current version of h5py =\", (h5py.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.4: Early Stopping in Keras to Prevent Overfitting\n",
    "\n",
    "It can be difficult to determine how many epochs to cycle through to train a neural network. **_Overfitting_** will occur if you train the neural network for too many epochs, and the neural network will not perform well on new data, despite attaining a good accuracy on the training set. Overfitting occurs when a neural network is trained to the point that it begins to memorize rather than generalize, as demonstrated in Figure 3.OVER. \n",
    "\n",
    "**Figure 3.OVER: Training vs. Validation Error for Overfitting**\n",
    "![Training vs. Validation Error for Overfitting](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_3_training_val.png \"Training vs. Validation Error for Overfitting\")\n",
    "\n",
    "It is important to segment the original dataset into several datasets:\n",
    "\n",
    "* **Training Set**\n",
    "* **Validation Set**\n",
    "* **Holdout Set**\n",
    "\n",
    "You can construct these sets in several different ways. The following programs demonstrate some of these.\n",
    "\n",
    "The first method is a training and validation set. We use the training data to train the neural network until the validation set no longer improves. This attempts to stop at a near-optimal training point. This method will only give accurate \"out of sample\" predictions for the validation set; this is usually 20% of the data. The predictions for the training data will be overly optimistic, as these were the data that we used to train the neural network. Figure 3.VAL demonstrates how we divide the dataset.\n",
    "\n",
    "**Figure 3.VAL: Training with a Validation Set**\n",
    "![Training with a Validation Set](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_1_train_val.png \"Training with a Validation Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "We will now see an example of classification training with early stopping. We will train the neural network until the error no longer improves on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Early Stopping with Classification: Iris data\n",
    "\n",
    "The code in the cell below builds and trains a **_classification_** neural network called `irisModel`. The model is trained/fitted to the Iris flower dataset (`iris.csv`) downloaded from the course HTTPS server `corgi.genomelab.utsa.edu` and stored in the DataFrame `irisDF`. The independent variables (attributes) are the values for `sepal_length`, `sepal_width`, `petal_length` and `petal_width` which are stored in the variable `irisX`. The dependent variable (response variable) that the model will be trained to classify is the column `species` which contains the names of the three Iris species in the dataset, _Iris setosa_, _Iris_ versicolor_ and _Iris virginica_. Since the species names are entered in the `species` column as strings, it is necessary to use One-Hot Encoding to convert these strings into the values `0` and `1` using the command `pd.get_dummies()`. The variable holding the dependent values, `irisY`, is created from the `dummies.values` as shown below.\n",
    "\n",
    "In order to implement _Early Stopping_, it is first necessary to split the dataset into 4 separate groups: X train, X test, Y train and Y test using the function `train_test_split()`. The argument `test_size=0.25` tells the function that 75% of the data should be put into the two train sets (i.e. `irisX_train` and `irisY_train`) and the remaining 25% should be put into the two validation sets `irisX_test` and `irisY_test`. Since the separation of data into training and test sets is a random process, the argument `random_state=42` is used for teaching/demonstration purposes to insure that the `split` occurs at the same places when the code is re-run.\n",
    "\n",
    "The model, `irisModel`, is a densely connected sequential neural network with two hidden layers. The 1st layer has 50 neurons, the 2nd hidden layer 25. The activation function for both hidden layers is `relu`. Since this function of this model is classification, the `softmax` activation function is used in the output layer. The model is compiled with the 'categorical_crossentropy` loss function and the `adam` optimizer. \n",
    "\n",
    "The code for implementing early stopping variable `irisMoniter` is shown below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "irisMonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, \n",
    "        verbose=1, mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning/function of the different arguments will be discussed below.\n",
    "\n",
    "Finally, the model is fitted to the Iris data with the number of epochs set to **1000!** Don't worry, you won't have to wait forever for the training to complete--thanks to early stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Early stopping - iris data\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "irisDF = pd.read_csv(\"https://corgi.genomelab.utsa.edu/BIO1173/Datasets/iris.csv\",\n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "irisX = irisDF[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
    "dummies = pd.get_dummies(irisDF['species']) # Classification\n",
    "species = dummies.columns\n",
    "irisY = dummies.values\n",
    "\n",
    "# Split into validation and training sets\n",
    "irisX_train, irisX_test, irisY_train, irisY_test = train_test_split(    \n",
    "    irisX, irisY, test_size=0.25, random_state=42)\n",
    "\n",
    "# Build neural network\n",
    "irisModel = Sequential()\n",
    "irisModel.add(Dense(50, input_dim=irisX.shape[1], activation='relu')) # Hidden 1\n",
    "irisModel.add(Dense(25, activation='relu')) # Hidden 2\n",
    "irisModel.add(Dense(irisY.shape[1],activation='softmax')) # Output\n",
    "irisModel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "irisMonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, \n",
    "        verbose=1, mode='auto', restore_best_weights=True)\n",
    "irisModel.fit(irisX_train,irisY_train,validation_data=(irisX_test,irisY_test),\n",
    "        callbacks=[irisMonitor],verbose=2,epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the number of epochs was set to 1000, the training/fitting should have stopped much earlier. For example, on the machine this assigment is being created, the training stopped after only 102 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of parameters that are specified to the **EarlyStopping** object. \n",
    "\n",
    "* **min_delta** This value should be kept small. It simply means the minimum change in error to be registered as an improvement.  Setting it even smaller will not likely have a great deal of impact.\n",
    "* **patience** How long should the training wait for the validation error to improve?  \n",
    "* **verbose** How much progress information do you want?\n",
    "* **mode** In general, always set this to \"auto\".  This allows you to specify if the error should be minimized or maximized.  Consider accuracy, where higher numbers are desired vs log-loss/RMSE where lower numbers are desired.\n",
    "* **restore_best_weights** This should always be set to true.  This restores the weights to the values they were at when the validation set is the highest.  Unless you are manually tracking the weights yourself (we do not use this technique in this course), you should have Keras perform this step for you.\n",
    "\n",
    "As you can see from above, the entire number of requested epochs were not used.  The neural network training stopped once the validation set no longer improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1: Early Stopping with Classification: Heart Failure data**\n",
    "\n",
    "In the cell below, write the code to read the Heart Failure dataset (\"heart_failure.csv\") from the course HTTPS server (\"corgi.genomelab.utsa.edu\") and store the data in a DataFrame called `hfDF`. As independent variables, **only** use the columns Age, RestingBP, Cholesterol, MaxHR and Oldpeak. You should name the variable holding the independent values `hfX`. \n",
    "\n",
    "Use the column `HeartDisease` as your dependent (response) variable. You will need to One-Hot Encode the `HeartDisease` values. Assign the independent values to the variable `hfY` using the `dummies.values` as was done in Example 1. \n",
    "\n",
    "Use the `train_test_split(hfX, hfY, test_size=0.25, random_state=42)` function to create `hfX_train`, `hfX_test`, `hfY_train` and `hfY_test` datasets. \n",
    "\n",
    "Build a Sequential neural network called `hfModel` with 2 hidden layers with 50 neurons in the first layer and 25 neurons in the second layer. Use `relu` activation for these two hidden layers. The output layer should use `softmax` activation. Compile your model using `categorical_crossentropy` as the loss function with `adam` as the optimizer. \n",
    "\n",
    "After your model has been compiled, create a variable called `hfMonitor` to provide `EarlyStopping()` with the same arguments are shown in Example 1. \n",
    "\n",
    "Finally, train/fit the model for 1000 epochs using the `hfMonitor` to enable early stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 1 here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, your model `hfModel` should have stopped early, before reaching 30 epochs. The training stopped with the `val_loss` value reached a minimum value after waiting 5 epochs (`patience=5`) for the `val_loss` to start going to a lower value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Compute Accuracy Score for `irisModel`\n",
    "\n",
    "Let's see what effect early stopping might have on the accuracy of the `irisModel`?\n",
    "\n",
    "The code below illustrates how to compute the accuracy score for the model `irisModel` created in Example 1 using the Keras `model.predict()` function and the `accuracy_score()` functin from the `scikit-learn` metrics package.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Compute accuracy score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "irisPred = irisModel.predict(irisX_test)\n",
    "irisPredict_classes = np.argmax(irisPred,axis=1)\n",
    "irisExpected_classes = np.argmax(irisY_test,axis=1)\n",
    "irisCorrect = accuracy_score(irisExpected_classes,irisPredict_classes)\n",
    "print(f\"Accuracy: {irisCorrect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2/2 [==============================] - 0s 55ms/step\n",
    "Accuracy: 0.5526315789473685"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2: Compute Accuracy Score for hfModel**\n",
    "\n",
    "In the cell below, compute the accuracy score for your `hfModel` and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 2 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8/8 [==============================] - 0s 2ms/step\n",
    "Accuracy: 0.7260869565217392"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly your `hfModel` is better at predicting heart attacks than the `irisModel` is at predicting the Iris flower species based on the sepal and petal dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping with Regression\n",
    "\n",
    "The following code demonstrates how we can apply early stopping to a regression problem.  The technique is similar to the early stopping for classification code that we just saw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Early Stopping with Regression\n",
    "\n",
    "The code below uses the Apple Quality dataset downloaded from the course HTTPS server (\"corgi.genomelab.utsa.edu\") to create a DataFrame called `apDF`. When we use a neural network for **_classification_**, we design the neural network so there is one output neuron for each \"class\" that we are trying to classify. For example, with the Iris data there were three classes, one for example species (i.e. _Iris setosa_, _Iris versicolor_, and _Iris virginica_). So our neural network model, `irisModel` had just 3 neurons in the output layer -- one for each class (species). \n",
    "\n",
    "Suppose we could build a _prefect_ classification neural network for Iris flowers, with 100% accuracy. If we entered the sepal and petal dimensions of a flower taken from a _Iris setosa_ plant into this perfect neural network, the value in the output neuron reprensenting _Iris setosa_ would have the number `1.0` in it, while the other two output neurons (representing the other two species) would have the number `0` in them. \n",
    "\n",
    "In other words, our goal with a classification neural network is to have only one output neuron with a value close to `1` and the rest of the output neurons to have a value close to `0` when we \"feed\" the neural network the independent values (attributes) from a single subject. \n",
    "\n",
    "The goal of a **_regression_** neural network is somewhat different. In a regression neural network there is only a **single** output neuron. When we use regression neural network to make a prediction, the single output neuron will end up having a floating point number in it. The value of this number is the **_prediction_**.\n",
    "\n",
    "The neural network `apModel`, constructed in the cell below, is designed to predict the `Sweetness` of a particular apple based on its Size, Weight, Crunchiness, Juiciness, Acidity, Ripeness and Quality. According the output from the command `apDF.describe()` (see Appendix), the `Sweetness` of an apple can range from a minimum of -6.894485 to a maximum of 6.374916. Therefore, when we make a prediction with the trained `apModel`, we would expect a value in the output neural between -6.894485 and 6.374916   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 3: Early stopping with regression\n",
    "\n",
    "\n",
    "# Read the datafile\n",
    "apDF = pd.read_csv(\"https://corgi.genomelab.utsa.edu/BIO1173/Datasets/apple_quality.csv\", \n",
    "                    na_values=['NA', '?'])\n",
    "\n",
    "# Define the mapping dictionary\n",
    "mapping = {'bad': 0, 'good': 1}\n",
    "# Map the integer column to strings\n",
    "apDF['Quality'] = apDF['Quality'].map(mapping)\n",
    "\n",
    "# Assign the independent variables (x)\n",
    "apX = apDF[['Size', 'Weight', 'Crunchiness',\n",
    "            'Juiciness', 'Acidity', 'Ripeness', 'Quality']].values\n",
    "\n",
    "# Assign the dependent variables (y)\n",
    "apY = apDF['Sweetness'].values\n",
    "\n",
    "\n",
    "# Specify the model type as sequential\n",
    "apModel = Sequential()\n",
    "\n",
    "# Split into validation and training sets\n",
    "apX_train, apX_test, apY_train, apY_test = train_test_split(    \n",
    "    apX, apY, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "# Build the neural network\n",
    "apModel = Sequential()\n",
    "apModel.add(Dense(25, input_dim=apX.shape[1], activation='relu')) # Hidden 1\n",
    "apModel.add(Dense(10, activation='relu')) # Hidden 2\n",
    "apModel.add(Dense(1)) # Output\n",
    "apModel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "apMonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "        patience=5, verbose=1, mode='auto',\n",
    "        restore_best_weights=True)\n",
    "apModel.fit(apX_train,apY_train,validation_data=(apX_test,apY_test),\n",
    "        callbacks=[apMonitor], verbose=2,epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, training/fitting should have stopped before 100 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3: Early Stopping with Regression**\n",
    "\n",
    "If the cell below, use the same Apple Quality dataset as used in Example 3 to construct a regression neural network that can predict the 'Acidity' of an apple instead of it `Sweetness`. \n",
    "\n",
    "Call your new model `ap2Model`, the independent variables `ap2X`, the dependent variable `ap2Y`, `ap2X_train` and so forth. As in Example 3, implement early stopping.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 3 here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, training/fitting should have stopped around 100 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Compute the RMSE for the neural network `apModel`\n",
    "\n",
    "When working with neural networks that perform a regression analysis, it is customary to use the Root Mean Square Error (RMSE) as a measurement of predictive accuracy. The code in the cell below shows how to compute the RMSE for the `apModel` neural network and then print out the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4 Compute the RMSE\n",
    "\n",
    "# Measure RMSE error. \n",
    "apPred = apModel.predict(apX_test)\n",
    "apScore = np.sqrt(metrics.mean_squared_error(apPred,apY_test))\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Final score (RMSE): {apScore}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "32/32 [==============================] - 0s 2ms/step\n",
    "Final score (RMSE): 0.9086285511469709"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 4: Compute the RMSE for the neural network `ap2Model`**\n",
    "\n",
    "In the cell below, write the code to compute RMSE for your neural network model `ap2Model` and then print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 4 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "32/32 [==============================] - 0s 2ms/step\n",
    "Final score (RMSE): 1.3168628718045605"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: _Ad Hoc_ prediction for neural network `apModel`\n",
    "\n",
    "The code in the cell below uses the `apModel` built and trained in Example 3 above, to predict the sweetness of the first apple in the Apple Quality dataset (`Apple0`). \n",
    "\n",
    "The complete list of **all** the attributes for `Apple0` are as follows:\n",
    "\n",
    "* **Size =** -3.970049\n",
    "* **Weight =** -2.512336\n",
    "* **Sweetness =** 5.346330 \n",
    "* **Crunchiness =** -1.012009\n",
    "* **Juciness =**  1.844900\n",
    "* **Ripeness =** 0.329840\n",
    "* **Acidity =** -0.491590\n",
    "* **Quality =** 1\n",
    "\n",
    "Keep in mind that the attribute `Sweetness` was the dependent (response) variable in Example 3. In other words, the neural network model `apModel` was designed and trained **_only_** to predict the sweetness of an apple given its other attributes.\n",
    "\n",
    "**IMPORTANT:** The _order_ in which the `np.array()` values for Apple0 are entered is critical. To use the model `apModel` to make a prediction, the X values in the prediction must be in the same extact order as the X values were in the training and test datasets. The line of code that was used for assigning the features to variable `apX` is shown in below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Assign the independent variables (x)\n",
    "apX = apDF[['Size', 'Weight', 'Crunchiness',\n",
    "            'Juiciness', 'Acidity', 'Ripeness', 'Quality']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, when creating the `np.array()` for `Apple0` in the cell below, the data values were entered in the following order:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Size, Weight, Crunchiness, Juiciness, Acidity, Ripeness, Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the order in which attributes are placed in the X variable `apX` is not important -- from the standpoint of training the neural network. However, when it comes to making _predictions_ using the trained neural network, the X values passed to the neural network for the ad hoc example **must** be in the same order for the model to work correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZUSWVGnCtiT",
    "outputId": "d98e7cc3-7e10-44f0-a5cb-b858f3c4c850"
   },
   "outputs": [],
   "source": [
    "# Example 5: Ad hoc prediction\n",
    "\n",
    "# x = Size, Weight, Crunchiness, Juciness, Acidity, Ripness, Quality\n",
    "Apple0 = np.array( [[-3.970049, -2.512336, -1.012009,\n",
    "                     1.844900, -0.491590, 0.329840, 1 ]], dtype=float)\n",
    "\n",
    "# Use the neural network to predict which species of Iris\n",
    "apPred = apModel.predict(Apple0)\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Model predicts that the sweetness of Apple0 is: {apPred}\")\n",
    "print(f\"The actual sweetness of Apple0 was: {apDF['Sweetness'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1/1 [==============================] - 0s 20ms/step\n",
    "Model predicts that the sweetness of Apple0 is: [[4.5117397]]\n",
    "The actual sweetness of Apple0 was: 5.346329613"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 5: _Ad Hoc_ prediction for neural network `ap2Model`**\n",
    "\n",
    "In the cell below, use the `ap2Model` that you created in **Exercise 4** to predict the acidity of the same apple (`Apple0`) used in the Example 5 above. Use the values for the attributes of `Apple0`, shown above in Example 5, for creating the `np.array()` for your _ad hoc_ prediction. \n",
    "\n",
    "**BE CAREFUL:** You must make sure to use exactly the same order of these attributes as you used in creating your `ap2Model`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZUSWVGnCtiT",
    "outputId": "d98e7cc3-7e10-44f0-a5cb-b858f3c4c850"
   },
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 5 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1/1 [==============================] - 0s 23ms/step\n",
    "Model predicts that the acidity of Apple0 is: [[-1.8743719]]\n",
    "The actual acidity of Apple0 was: -0.491590483"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY\n",
    "\n",
    "The primary objective of this lesson was to demonstrate how to implement **_Early Stopping_** in the training/fitting of neural networks. Clearly, the ability to stop training when the loss function on the validation training set reaches a minimum can save a significant amount of time. However, perhaps more importantly, Early Stopping can prevent a neural network from **_overtraining_**. Overtraining occurs when the neural network can predict training examples with very high accuracy but cannot generalize to new data. In other words, the neural network starts learning **_specific details_** about the training data. While this improves the model's loss function in the particular training set, it will actually performs worse when presented with new data that it hasn't seen before. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lesson Turn-in**\n",
    "\n",
    "When you have completed all of the code cells, and run them in sequential order (the last code cell should be number 12), use the **File --> Print.. --> Save to PDF** to generate a PDF of your JupyterLab notebook. Save your PDF as `Class_03_4.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "The cells below shows the output from the command `apDF.head()`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " \tA_id \tSize \t  Weight \tSweetness \tCrunchiness  Juiciness \t Ripeness \t Acidity \tQuality\n",
    "0 \t0 \t-3.970049 \t-2.512336 \t5.346330 \t-1.012009 \t 1.844900 \t 0.329840 \t-0.491590 \t   1\n",
    "1 \t1 \t-1.195217 \t-2.839257 \t3.664059 \t1.588232 \t 0.853286 \t 0.867530 \t-0.722809 \t   1\n",
    "2 \t2 \t-0.292024 \t-1.351282 \t-1.738429 \t-0.342616 \t 2.838636 \t-0.038033 \t 2.621636 \t   0\n",
    "3 \t3 \t-0.657196 \t-2.271627 \t1.324874 \t-0.097875 \t 3.637970 \t-3.413761 \t 0.790723 \t   1\n",
    "4 \t4 \t1.364217 \t-1.296612 \t-0.384658 \t-0.553006 \t 3.030874 \t-1.303849 \t 0.501984 \t   1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows an abbreviated version of the output from the command `apDF.describe()`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " \t    A_id \t    Size \t   Weight \t Sweetness \tCrunchiness  Juiciness \t Ripeness \t Acidity\n",
    "count \t4000\t    4000 \t    4000       4000 \t   4000 \t   4000 \t   4000 \t   4000\n",
    "mean \t1999 \t -0.503015 \t -0.989547 \t -0.470479 \t 0.985478 \t 0.512118 \t 0.498277 \t 0.076877\n",
    "std \t1155 \t  1.928059 \t  1.602507 \t  1.943441 \t 1.402757 \t 1.930286 \t 1.874427 \t 2.110270\n",
    "min \t0.00 \t -7.151703 \t -7.149848 \t -6.894485 \t-6.055058 \t-5.961897 \t-5.864599 \t-7.010538\n",
    "max \t3999      6.406367    5.790714 \t  6.374916 \t 7.619852 \t 7.364403 \t 7.237837 \t 7.404736"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
