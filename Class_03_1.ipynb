{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_03_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- BIO1173_CLASS_03_1 -->"
      ],
      "metadata": {
        "id": "TqOk9HJEumKx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYZVwSpdbE3Y"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExN-OzpYbE3Y"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4imk1kbE3Y"
      },
      "source": [
        "##### **Module 3: Convolutional Neural Networks (CNN's)**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 3 Material\n",
        "\n",
        "* **Part 3.1: Using Convolutional Neural Networks**\n",
        "* Part 3.2: Using Pre-Trained Neural Networks with Keras\n",
        "* Part 3.3: Facial Recognition and Analysis\n",
        "* Part 3.4: Introduction to GAN's for Image and Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **You MUST Change Your Runtime Type Now!**\n",
        "\n",
        "#### **Buying a Colab Membership**\n",
        "\n",
        "To run the code in this lesson, you will need to have a paid subscription to Colab. A paid membership is only $9.99 a month and you can cancel at anytime. To purchase a paid membership, check out: [Google Colab Paid Services](https://colab.research.google.com/signup).\n",
        "\n",
        "If you have a paid membership, you should choose the `A100 GPU`. Instead of waiting hours for this lesson to run, training times will usually be less than 10 minutes.\n",
        "\n",
        "Don't forget to change the your runtime if you just bought a paid membership.\n",
        "\n",
        "To run the code in this assignment (Class_03_1) you will need to change your runtime type **BEFORE**  you begin working on this lesson. That's because when you change your runtime type, all your previous work is erased!\n",
        "\n",
        "To change your runtime type, select the downward pointing triangle at the top right of your CoLab notebook as illustrated in the image below.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_1_image67.png)\n",
        "\n",
        "That will give a pop-up window showing the available GPU's and TPU's that you can use with your paid CoLab PRO subscription.\n",
        "\n",
        "The different GPU and TPU selections change from time to time as new accelerators are developed. More importantly, the available choices will also depend upon how many of the CoLab users are currently using a particular accelerator for their coding.\n",
        "\n",
        "In this course, you should always select a **GPU** for your hardware acceleration. For example, at of the time this lesson was created, the **100 GPU** was selected as the hardware accelerator.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_1_image02B.png)\n",
        "\n",
        "Once you have pressed the `Save` button, you can continue with this lesson."
      ],
      "metadata": {
        "id": "Ult76BB_wSzg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_-lPkxLbE3Z"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# @title You MUST Run this Cell First!\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    !curl ipinfo.io\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see something _similar_ to the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "```text\n",
        "Mounted at /content/drive\n",
        "Note: Using Google CoLab\n",
        "{\n",
        "  \"ip\": \"136.110.50.121\",\n",
        "  \"hostname\": \"121.50.110.136.bc.googleusercontent.com\",\n",
        "  \"city\": \"Singapore\",\n",
        "  \"region\": \"Singapore\",\n",
        "  \"country\": \"SG\",\n",
        "  \"loc\": \"1.2897,103.8501\",\n",
        "  \"org\": \"AS396982 Google LLC\",\n",
        "  \"postal\": \"018989\",\n",
        "  \"timezone\": \"Asia/Singapore\",\n",
        "  \"readme\": \"https://ipinfo.io/missingauth\"\n",
        "}studentbio1173@gmail.com\n",
        "```\n",
        "\n",
        "If your GMAIL address does not appear, **Electronic Submission** will not accept your lesson."
      ],
      "metadata": {
        "id": "xG3_sXTDfyjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accelerated Run-time Check\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be using a Runtime evironment that has Graphics Processing Unit (GPU)."
      ],
      "metadata": {
        "id": "LKhQzBV1wu2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Accelerated Run-time Check\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check for GPU\n",
        "def check_colab_gpu():\n",
        "    print(\"=== Colab GPU Check ===\")\n",
        "\n",
        "    # Check PyTorch\n",
        "    pt_gpu = torch.cuda.is_available()\n",
        "    print(f\"PyTorch GPU available: {pt_gpu}\")\n",
        "\n",
        "    if pt_gpu:\n",
        "        print(f\"PyTorch device count: {torch.cuda.device_count()}\")\n",
        "        print(f\"PyTorch current device: {torch.cuda.current_device()}\")\n",
        "        print(f\"PyTorch device name: {torch.cuda.get_device_name()}\")\n",
        "        print(\"You are good to go!\")\n",
        "\n",
        "    else:\n",
        "        print(\"No compatible device found\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now and start over!\")\n",
        "\n",
        "check_colab_gpu()"
      ],
      "metadata": {
        "id": "lO5vwpbNEzgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you current `Runtime` is correct you should see the following output\n",
        "```=== Colab GPU Check ===\n",
        "PyTorch GPU available: True\n",
        "PyTorch device count: 1\n",
        "PyTorch current device: 0\n",
        "PyTorch device name: NVIDIA A100-SXM4-80GB\n",
        "You are good to go!\n",
        "```\n"
      ],
      "metadata": {
        "id": "xow1Nb2-w-Vt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Functions for this Lesson\n",
        "\n",
        "The cell below creates several functions that are needed for this assignment. If you don't run this cell you will receive errors later when you try to run some cells."
      ],
      "metadata": {
        "id": "XBvhiEZ0xYDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Functions for this Lesson\n",
        "\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def check_current_ram():\n",
        "  ram = psutil.virtual_memory()\n",
        "  print(f\"Available RAM: {ram.available / (1024 ** 3):.2f} GB\")\n",
        "\n",
        "def list_files():\n",
        "   files = os.listdir('.')\n",
        "   print(f\"Current files: {files}\")\n",
        "\n",
        "def list_extract():\n",
        "  files = os.listdir(EXTRACT_TARGET)\n",
        "  print(f\"Current files in EXTRACT_TARGET: {files}\")\n",
        "\n",
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "\n",
        "# Set random seed\n",
        "def set_seed():\n",
        "    \"\"\"\n",
        "    Sets the seed for reproducibility across Python, NumPy, and PyTorch.\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    # Set seed value\n",
        "    seed_value=1173\n",
        "\n",
        "    # 1. Base Python\n",
        "    random.seed(seed_value)\n",
        "\n",
        "    # 2. NumPy\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    # 3. PyTorch (CPU and CUDA)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # For multi-GPU setups\n",
        "\n",
        "    # 4. CuDNN Determinism (Crucial for GPU consistency)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # 5. Ensure all operations are deterministic\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "print(f\"✅ All custom functions have been created.\")"
      ],
      "metadata": {
        "id": "tnwowmLuABDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output:\n",
        "```text\n",
        "✅ All custom functions have been created.\n",
        "```"
      ],
      "metadata": {
        "id": "jMgtKqwwYVGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to the `MNIST` Dataset**\n",
        "\n",
        "Run the next cell to see short introduction to the MNIST Dataset. This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "6NcY4RdGYPEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"SrT6QkQUH4Q\"\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\"> </iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "wIQ0-jDHlz1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PyTorch Neural Networks for Medical MNIST**\n",
        "\n",
        "This module will focus on computer vision. There are some important differences and similarities with previous neural networks.\n",
        "\n",
        "* We will usually use classification, though regression is still an option.\n",
        "* The input to the neural network is now 3D (height, width, _and_ color)\n",
        "* Data are not transformed; no more Z-scores or dummy variables.\n",
        "* Processing time is **_much_**  longer.\n",
        "* We now have different layer types. Besides dense layers, we now have _convolution layers_, and _max-pooling layers_.\n",
        "* Data will no longer arrive as tabular data stored in CSV files, but as hundred or even thousands of **_images_**."
      ],
      "metadata": {
        "id": "wCjV86HnxkND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Common Computer Vision Data Sets**\n",
        "\n",
        "There are many data sets for computer vision. Two of the most popular classic datasets are the **MNIST** digits data set and the CIFAR image data sets. We will be using two MNIST data sets in this lesson. It is important to be familiar with both sets, neural network texts often refer to them.\n",
        "\n",
        "### **MNIST Digital Data Set**\n",
        "\n",
        "The [MNIST Digits Data Set](http://yann.lecun.com/exdb/mnist/) is very popular in the neural network research community. You can see a sample of it below.\n",
        "\n",
        "![MNIST Data Set](https://biologicslab.co/BIO1173/images/class_03/class_03_1_image01A.png \"MNIST Data Set\")\n",
        "\n",
        "**MNIST Digital Data Set**\n",
        "\n",
        "The original MNIST Digit Data Set is a large database of handwritten digits that is commonly used for training various image processing systems. It was created by Yan LeCun, Corinna Cortes, and Christopher Burges as a benchmark for evaluating machine learning algorithms in the field of computer vision. The dataset was first released in 1998 and consists of 60,000 training images and 10,000 testing images of handwritten digits from 0 to 9.\n",
        "\n",
        "The MNIST dataset has been widely used in the research community to develop and test classification algorithms, particularly in the field of deep learning. It has become a standard benchmark for evaluating the performance of machine learning models on image recognition tasks. Despite its simplicity, the MNIST dataset remains popular due to its ease of use and ability to quickly assess the effectiveness of new algorithms.\n",
        "\n",
        "Over the years, the MNIST dataset has been used in numerous research studies and competitions, leading to the development of more advanced techniques in computer vision. It continues to be a valuable resource for researchers and practitioners in the field of machine learning.\n",
        "\n",
        "### **MedMNIST Data Set**\n",
        "\n",
        "[MedMINST Data Sets](https://medmnist.com/) are a collection of 18 standardized biomedical datasets produced by a consortium of researchers at Harvard University and colaborators in Germany and China. The image sets cover a variety medical tissues and cell types including Chest X-Rays, Colon Pathology, Breast Ultrasound, Blood Cytology and Abdominal CT scans. The `RetinaMINST` dataset has 1,600 fundus camera samples (1,080 training, 120 validation, 400 test).\n",
        "\n",
        "\n",
        "![RetinaMNIST](https://biologicslab.co/BIO1173/images/class_03/class_03_1_image04A.png  \"RetinaMNIST\")\n",
        "\n",
        "**MedMNIST: RetinaMNIST Data Set**\n",
        "\n",
        "### **CIFAR Data Set**\n",
        "\n",
        "The [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) datasets are also frequently used by the neural network research community.\n",
        "\n",
        "![CIFAR Data Set](https://biologicslab.co/BIO1173/images/class_03/class_03_1_image05A.png  \"CIFAR Data Set\")\n",
        "\n",
        "**CIFAR Data Set**\n",
        "\n",
        "The CIFAR-10 data set contains low-rez images that are divided into 10 classes. The CIFAR-100 data set contains 100 classes in a hierarchy."
      ],
      "metadata": {
        "id": "bJC-LE63xxFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convolutional Neural Networks (CNNs)**\n",
        "\n",
        "The convolutional neural network (CNN) is a neural network technology that has profoundly impacted the area of computer vision (CV). Fukushima  (1980) [[Cite:fukushima1980neocognitron]](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf) introduced the original concept of a convolutional neural network, and   LeCun, Bottou, Bengio & Haffner (1998) [[Cite:lecun1995convolutional]](http://yann.lecun.com/exdb/publis/pdf/lecun-bengio-95a.pdf) greatly improved this work.\n",
        "\n",
        "From this research, Yan LeCun introduced the famous LeNet-5 neural network architecture. This chapter follows the **LeNet-5 style** of convolutional neural network. Although computer vision primarily uses CNNs, this technology has some applications outside of the field. You need to realize that if you want to utilize CNNs on non-visual data, you must find a way to encode your data to mimic the properties of visual data.  \n",
        "\n",
        "The order of the input array elements is _crucial_ to the training. In contrast, most neural networks that are not CNNs, treat their input data as a long vector of values. The order in which you arrange the incoming features in this vector is irrelevant. Importantly, you can't change the order of the data in these vectors for these types of neural networks once your network has been trained.\n",
        "\n",
        "On the other hand, the CNN network arranges the inputs into a **grid**. This arrangement works well with images because the pixels in closer proximity to each other are important to each other. The order of pixels in an image is significant. The human body is a relevant example of this type of order. For the design of the face, we are accustomed to eyes being near to each other.\n",
        "\n",
        "This advance in CNNs is due to years of research on biological eyes. In other words, CNNs utilize overlapping fields of input to simulate features of biological eyes. Until this breakthrough, AI had been unable to reproduce the capabilities of biological vision.\n",
        "\n",
        "Scale, rotation, and noise have presented challenges for AI computer vision research. You can observe the complexity of biological eyes in the example that follows.\n",
        "\n",
        "A friend raises a sheet of paper with a large number written on it. As your friend moves nearer to you, the number is still identifiable. In the same way, you can still identify the number when your friend rotates the paper. Lastly, your friend creates noise by drawing lines on the page, but you can still identify the number.\n",
        "\n",
        "As you can see, these examples demonstrate the high function of the biological eye and allow you to understand better the research breakthrough of CNNs. That is, this neural network can process scale, rotation, and noise in the field of computer vision. You can see this network structure.\n",
        "\n",
        "**LeNET-5 Network (LeCun, 1998)**\n",
        "\n",
        "![A LeNET-5 Network](https://biologicslab.co/BIO1173/images/class_03/class_03_1_image06A.png \"A LeNET-5 Network\")\n",
        "\n",
        "So far, we have only seen one layer type (dense layers). By the end of this course you will also know about:\n",
        "  \n",
        "* **Convolution Layers** - Used to scan across images.\n",
        "* **Max Pooling Layers** - Used to downsample images.\n",
        "* **Dropout Layers** - Used to add regularization.\n",
        "* **LSTM and Transformer Layers** - Used for time series data."
      ],
      "metadata": {
        "id": "Z2qGkCiWx5Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Convolution Layers**\n",
        "\n",
        "The first layer that we will examine is the convolutional layer. We will begin by looking at the hyper-parameters that you must specify for a convolutional layer in most neural network frameworks that support the CNN:\n",
        "\n",
        "* Number of filters\n",
        "* Filter Size\n",
        "* Stride\n",
        "* Padding\n",
        "* Activation Function/Non-Linearity\n",
        "\n",
        "The primary purpose of a convolutional layer is to detect features such as edges, lines, blobs of color, and other visual elements. The filters can detect these features. The more filters we give to a convolutional layer, the more features it can see.\n",
        "\n",
        "A filter is a square-shaped object that scans over the image. A grid can represent the individual pixels of a grid. You can think of the convolutional layer as a smaller grid that sweeps left to right over each image row. There is also a hyperparameter that specifies both the width and height of the square-shaped filter. The following figure shows this configuration in which you see the six convolutional filters sweeping over the image grid:\n",
        "\n",
        "A convolutional layer has weights between it and the previous layer or image grid. Each pixel on each convolutional layer is a weight. Therefore, the number of weights between a convolutional layer and its predecessor layer or image field is the following:\n",
        "\n",
        "```\n",
        "[FilterSize] * [FilterSize] * [# of Filters]\n",
        "```\n",
        "\n",
        "For example, if the filter size were 5 (5x5) for 10 filters, there would be 250 weights.\n",
        "\n",
        "You need to understand how the convolutional filters sweep across the previous layer's output or image grid. Figure 6.CNN illustrates the sweep:\n",
        "\n",
        "**Figure 6.CNN: Convolutional Neural Network**\n",
        "\n",
        "![Convolutional Neural Network](https://biologicslab.co/BIO1173/images/class_03/class_03_1_image07A.png \"Convolutional Neural Network\")\n",
        "\n",
        "The above figure shows a convolutional filter with 4 and a padding size of 1. The **padding size** is responsible for the border of zeros in the area that the filter sweeps. Even though the image is 8x7, the extra padding provides a virtual image size of 9x8 for the filter to sweep across. The **stride** specifies the number of positions the convolutional filters will stop. The convolutional filters move to the right, advancing by the number of cells specified in the stride. Once you reach the far right, the convolutional filter moves back to the far left; then, it moves down by the stride amount and continues to the right again.\n",
        "\n",
        "Some constraints exist concerning the size of the stride. The stride cannot be `0`. The convolutional filter would never move if you set the stride to `0`. Furthermore, neither the stride nor the convolutional filter size can be larger than the previous grid. There are additional constraints on the stride (*s*), padding (*p*), and the filter width (*f*) for an image of width (*w*). Specifically, the convolutional filter must be able to start at the far left or top border, move a certain number of strides, and land on the far right or bottom border. The following equation shows the number of steps a convolutional operator\n",
        "must take to cross the image:\n",
        "\n",
        "$$ steps = \\frac{w - f + 2p}{s}+1 $$\n",
        "\n",
        "The number of steps must be an integer. In other words, it cannot have decimal places. The purpose of the padding (*p*) is to be adjusted to make this equation become an integer value."
      ],
      "metadata": {
        "id": "fkuLAEWpx-pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Max Pooling Layers**\n",
        "\n",
        "Max-pool layers downsample a 3D box to a new one with smaller dimensions. Typically, you can always place a max-pool layer immediately following the convolutional layer. The LENET shows the max-pool layer immediately after layers C1 and C3. These max-pool layers progressively decrease the size of the dimensions of the 3D boxes passing through them. This technique can avoid overfitting (Krizhevsky, Sutskever & Hinton, 2012).\n",
        "\n",
        "A pooling layer has the following hyper-parameters:\n",
        "\n",
        "* Spatial Extent (*f*)\n",
        "* Stride (*s*)\n",
        "\n",
        "Unlike convolutional layers, max-pool layers do not use padding. Additionally, max-pool layers have no weights, so training does not affect them. These layers downsample their 3D box input. The 3D box output by a max-pool layer will have a width equal to this equation:\n",
        "\n",
        "$$ w_2 = \\frac{w_1 - f}{s} + 1 $$\n",
        "\n",
        "The height of the 3D box produced by the max-pool layer is calculated similarly with this equation:\n",
        "\n",
        "$$ h_2 = \\frac{h_1 - f}{s} + 1 $$\n",
        "\n",
        "The depth of the 3D box produced by the max-pool layer is equal to the depth the 3D box received as input. The most common setting for the hyper-parameters of a max-pool layer is f=2 and s=2. The spatial extent (f) specifies that boxes of 2x2 will be scaled down to single pixels. Of these four pixels, the pixel with the maximum value will represent the 2x2 pixel in the new grid. Because squares of size 4 are replaced with size 1, 75% of the pixel information is lost. The following figure shows this transformation as a 6x6 grid becomes a 3x3:\n",
        "\n",
        "**Figure 6.MAXPOOL: Max Pooling Layer**\n",
        "\n",
        "![Max Pooling Layer](https://biologicslab.co/BIO1173/images/class_03/class_03_1_image08A.png \"Max Pooling Layer\")\n",
        "\n",
        "Of course, the above diagram shows each pixel as a single number. A grayscale image would have this characteristic. We usually take the average of the three numbers for an RGB image to determine which pixel has the maximum value."
      ],
      "metadata": {
        "id": "yg8Td47AyFfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------\n",
        "### **Supervised _vs_ Unsupervised Machine Learning**\n",
        "\n",
        "In **_supervised_ machine learning**, the algorithm is trained on a **labeled** dataset, where each training example is paired with the correct output. The goal is to learn a mapping from input features to the corresponding output labels. During training, the algorithm adjusts its parameters to minimize the difference between the predicted output and the true label. Once the model is trained, it can make predictions on new, unseen data by applying the learned mapping. Common supervised learning tasks include classification and regression.\n",
        "\n",
        "On the other hand, **_unsupervised_ machine learning** involves training the algorithm on an _unlabeled_ dataset, where the algorithm must find patterns or relationships in the data without explicit guidance. The goal of unsupervised learning is to discover hidden structures or clusters in the data. This type of learning is often used for tasks such as clustering, anomaly detection, and dimensionality reduction. Unlike supervised learning, there are no explicit output labels to guide the learning process in unsupervised learning.\n",
        "\n",
        "---------------------------------------------"
      ],
      "metadata": {
        "id": "sqOn44uuyN4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Example 1: Classification Convolutional Neural Network**\n",
        "\n",
        "We will now look at an example of a classification neural network. For _supervised_ computer vision, your dataset will need some labels. For classification, this label usually specifies _what_ the image is, e.g., dog, cat, carcinoma, etc.\n",
        "\n",
        "For a classification neural network, we will provide an image and expect the neural network to classify it as being one of several posibilites. In this example, we will use the `bloodmnist_224` dataset from MedMNIST that contains images of different types of blood cells.\n",
        "\n",
        "The blood cell images and consists of 8 different classes. These classes include Neutrophils, Lymphocytes, Monocytes, Eosinophils, Basophils and Platelets.\n",
        "\n",
        "Here are images showing the 8 types of blood cells in the `bloodmnist_224` dataset\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/BloodCellTypes_A.png)\n",
        "\n",
        "Each image is 244 X 244 pixels with 3 color channels (RGB).\n",
        "\n",
        "Our goal will be to create a convolutional neural network (CNN), using PyTorch, that can correctly classify a blood cell image as belonging into one of these 8 blood cell types.\n"
      ],
      "metadata": {
        "id": "G2YE0rOqySxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example - Step 1: Setup Evironmental Variables\n",
        "\n",
        "**Environmental variables** are similar to hidden settings that tell your computer how to behave. They hold information that can affect how programs run on your system, such as paths to files, system directories, or user-specific settings. It's like setting the stage for your computer to know where to find all its props and scripts.\n",
        "\n",
        "The code in the cell below creates environmental variables that are needed to download a specific datafile and then to extract this data into specific folders in your Colab notebook."
      ],
      "metadata": {
        "id": "XyaxI3MgyYl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example - Step 1: Setup Environmental Variables\n",
        "\n",
        "import os\n",
        "\n",
        "# Define datafile location\n",
        "URL = \"https://biologicslab.co/BIO1173/data\"\n",
        "DOWNLOAD_SOURCE = URL+\"/bloodmnist_224.npz\"\n",
        "DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfind('/')+1:]\n",
        "print(f\"DOWNLOAD_SOURCE {DOWNLOAD_SOURCE}\")\n",
        "print(f\"DOWNLOAD_NAME {DOWNLOAD_NAME}\")\n",
        "\n",
        "# Define folder locations to store the data\n",
        "PATH = \"/content\"\n",
        "EXTRACT_TARGET = os.path.join(PATH,\"/bloodmnist_224\")\n",
        "SOURCE = os.path.join(EXTRACT_TARGET)\n",
        "TARGET = SOURCE\n",
        "\n",
        "# Print out environmental variables\n",
        "print(f\"PATH {PATH}\")\n",
        "print(f\"EXTRACT_TARGET {EXTRACT_TARGET}\")\n",
        "print(f\"SOURCE {SOURCE}\")\n",
        "print(f\"TARGET {TARGET}\")"
      ],
      "metadata": {
        "id": "Np7QHbJp7iq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "```text\n",
        "DOWNLOAD_SOURCE https://biologicslab.co/BIO1173/data/bloodmnist_224.npz\n",
        "DOWNLOAD_NAME bloodmnist_224.npz\n",
        "PATH /content\n",
        "EXTRACT_TARGET /bloodmnist_224\n",
        "SOURCE /bloodmnist_224\n",
        "TARGET /bloodmnist_224\n",
        "```\n",
        "\n",
        "We are going to download a datafile called `bloodmnist_224.npz` from the course file server, `https://biologicslab.co`. We will then extract (unzip) its contents into a folder called `/bloodmnist_224`. You should note that the folder has exactly the same name as the datafile, but without the file extension `.npz`."
      ],
      "metadata": {
        "id": "SGIkjihvyl3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Step - 2: Download and Extract Data\n",
        "\n",
        "In Step 1 we defined the file we wanted to download and the folder in which to place the data after we unzipped it. In the next cell we perform both the download and the extraction.\n",
        "\n",
        "#### **Download File**\n",
        "\n",
        "The code in the cell below uses this code chunk to download the datafile.\n",
        "~~~text\n",
        "# Download the file\n",
        "os.system(f\"wget -O {download_path} {DOWNLOAD_SOURCE}\")\n",
        "~~~\n",
        "\n",
        "The program `wget` is a non-interactive network downloader commonly used in Unix-like operating systems. It retrieves files from the web using HTTP, HTTPS, and FTP protocols.\n",
        "\n",
        "#### **Extract File Contents**\n",
        "\n",
        "The datafile is compressed (i.e. a `zip file`), so we need to extract the file contents. The next code chunk unzips it to folder specified by the environmental variables defined above:\n",
        "~~~text\n",
        "# Extract the file\n",
        "os.system(f\"unzip -o -d {EXTRACT_TARGET} {download_path} >/dev/null\")\n",
        "~~~\n",
        "The command `unzip` is a command-line utility used to extract files from a ZIP archive.\n",
        "\n",
        "At the end of the `unzip` command is `>/dev/null`. Normally, the `unzip` command prints out the name of every file that was extracted. Since there are 2,000 images, we don't want to see this print out. Instead the output is sent instead to `> dev/null` which hids the output. `dev/null` a special file that discards all data written to it—like a black hole for unnecessary or unwanted output. If you send a program’s output to /dev/null, it's basically saying, \"I don't need this, just throw it away.\""
      ],
      "metadata": {
        "id": "9Mrt8H0R81h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example Step - 2: Download and Extract Data\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Creating necessary directories...\", end='')\n",
        "# Create necessary directories\n",
        "os.makedirs(SOURCE, exist_ok=True)\n",
        "os.makedirs(EXTRACT_TARGET, exist_ok=True)\n",
        "print(\"done.\")\n",
        "\n",
        "print(\"Downloading files...\", end='')\n",
        "# Define paths and URLs\n",
        "download_path = os.path.join(PATH, DOWNLOAD_NAME)\n",
        "extract_path = os.path.join(EXTRACT_TARGET, DOWNLOAD_NAME)\n",
        "\n",
        "# Download the file\n",
        "os.system(f\"wget -O {download_path} {DOWNLOAD_SOURCE}\")\n",
        "print(\"done.\")\n",
        "\n",
        "print(\"Extracting files...\", end='')\n",
        "# Extract the file\n",
        "os.system(f\"unzip -o -d {EXTRACT_TARGET} {download_path} >/dev/null\")\n",
        "print(\"done.\")\n"
      ],
      "metadata": {
        "id": "ADg2vo8j890E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "```text\n",
        "Creating necessary directories...done.\n",
        "Downloading files...done.\n",
        "Extracting files...done.\n",
        "```\n",
        "\n",
        "Image datafiles used by CNN neural networks need to be relatively large which means they take a fair amount of time to download and extract. They also require a lot of memory and disk space to store and process. This is one of the main reasons for using Google Colab instead of trying to do this on your laptop."
      ],
      "metadata": {
        "id": "Ayg-LmKm9FYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example - Step 3: Load and Shuffle Images and Labels into Numpy arrays\n",
        "\n",
        "Image data in dataset can be stored in different formats. In many cases the image data is stored as individual pictures (frames) in a JPEG format or PNG format.\n",
        "\n",
        "However, in this dataset, the images are stored in a collect of Numpy arrays. The Numpy `.npy` format is a way to save Numpy arrays to disk in a binary file. This format stores the shape, data type, and data of the array efficiently, allowing for fast reading and writing making it perfect for handling large amounts of numerical data in a compact, easy-to-access way.\n",
        "\n",
        "The code in the cell below reads the images and their corresponding labels using the Numpy command `np.load()` to create 6 numpy arrays containing the `training`, `test` and `validation` images and their labels.   \n",
        "\n",
        "In addition to `unpacking` the images, the code also randomly shuffles the data using this code chunk:\n",
        "\n",
        "~~~text\n",
        "combined = list(zip(eg_train_images, eg_train_labels))\n",
        "np.random.shuffle(combined)\n",
        "eg_X_train, eg_Y_train = zip(*combined)\n",
        "~~~\n",
        "\n",
        "Is important to note that when shuffling the images (`eg_train_images`) that their labels (`eg_train_labels`) are shuffled at the same time to keep these arrays \"synchronized\" (i.e. the right label goes with the right image)."
      ],
      "metadata": {
        "id": "yQn3GXQe9OnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example - Step 3: Load and Shuffle Images and Labels into Numpy arrays\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 1️⃣  Unpack and shuffle train images\n",
        "# ------------------------------------------------------------------------\n",
        "eg_train_images = np.load(os.path.join(SOURCE,\"train_images.npy\"),)\n",
        "eg_train_labels = np.load(os.path.join(SOURCE,\"train_labels.npy\"),)\n",
        "combined = list(zip(eg_train_images, eg_train_labels))\n",
        "np.random.shuffle(combined)\n",
        "eg_X_train, eg_y_train = zip(*combined)\n",
        "eg_X_train = np.array(eg_X_train)\n",
        "eg_y_train = np.array(eg_y_train)\n",
        "print(f\"eg_X_train: {eg_X_train.shape}\")\n",
        "print(f\"eg_y_train: {eg_y_train.shape}\")\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 2️⃣  Unpack and shuffle test images\n",
        "# ------------------------------------------------------------------------\n",
        "test_images = np.load(os.path.join(SOURCE,\"test_images.npy\"),)\n",
        "test_labels = np.load(os.path.join(SOURCE,\"test_labels.npy\"),)\n",
        "combined = list(zip(test_images, test_labels))\n",
        "np.random.shuffle(combined)\n",
        "eg_X_test, eg_y_test = zip(*combined)\n",
        "eg_X_test = np.array(eg_X_test)\n",
        "eg_y_test = np.array(eg_y_test)\n",
        "print(f\"eg_X_test: {eg_X_test.shape}\")\n",
        "print(f\"eg_y_test: {eg_y_test.shape}\")\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 3️⃣  Unpack and shuffle validation images\n",
        "# ------------------------------------------------------------------------\n",
        "# Unpack and shuffle val_images\n",
        "val_images = np.load(os.path.join(SOURCE,\"val_images.npy\"),)\n",
        "val_labels = np.load(os.path.join(SOURCE,\"val_labels.npy\"),)\n",
        "combined = list(zip(val_images, val_labels))\n",
        "np.random.shuffle(combined)\n",
        "eg_X_val, eg_y_val = zip(*combined)\n",
        "eg_X_val = np.array(eg_X_val)\n",
        "eg_y_val = np.array(eg_y_val)\n",
        "print(f\"eg_X_val: {eg_X_val.shape}\")\n",
        "print(f\"eg_y_val: {eg_y_val.shape}\")"
      ],
      "metadata": {
        "id": "H1OGthZ09THz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "```text\n",
        "eg_X_train: (11959, 224, 224, 3)\n",
        "eg_y_train: (11959, 1)\n",
        "eg_X_test: (3421, 224, 224, 3)\n",
        "eg_y_test: (3421, 1)\n",
        "eg_X_val: (1712, 224, 224, 3)\n",
        "eg_y_val: (1712, 1)\n",
        "```\n",
        "\n",
        "This output means that after splitting the data, there are `11,959` images of blood cells in the training set, `3,421` images in the test set and `1,712` images in the validation set. Also, these images are rather large, `244 X 244` pixels and they have 3 color channels making them \"RGB\"."
      ],
      "metadata": {
        "id": "6TkU0X_b9WqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------\n",
        "## **Understanding Color Channels in Image Analysis**\n",
        "\n",
        "#### **Definition**\n",
        "\n",
        "A **color channel** refers to a component of a digital image that represents intensity values for a specific color. In the context of image processing and neural networks, color channels are used to separate and process the individual color components of an image.\n",
        "\n",
        "#### **Common Color Channels**\n",
        "\n",
        "Most commonly, images are represented in the **RGB color space**, which includes three channels:\n",
        "\n",
        "- **Red (R)**\n",
        "- **Green (G)**\n",
        "- **Blue (B)**\n",
        "\n",
        "Each channel is a 2D matrix (height × width) that stores intensity values for that specific color. When combined, these channels form a full-color image.\n",
        "\n",
        "#### **Representation in Neural Networks**\n",
        "\n",
        "In neural networks, especially convolutional neural networks (CNNs), images are typically represented as 3D tensors with the shape given in pixels (e.g. `128,128`)\n",
        "\n",
        "----------------------------------------------------\n"
      ],
      "metadata": {
        "id": "wP7Yge269bWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example - Step 4: Add Color Channel(s) and Resize Images\n",
        "\n",
        "When it comes to images in data sets, the number of color channels can vary. Sometimes the images are in color and have 3 color channels (Red, Green, Blue), and sometimes the images only have 1 color channel (Monochrome or \"black-and-white\"). However, in some data sets, there is no color channel information. The code in the cell below examines the number of color channels in the image dataset and if no color channel information is provided, it adds `monochrome` color change information. The code also examines the image size and increases it, or decreases it to `128x128` pixels. In the end all images will be `128 X 128 X 3`."
      ],
      "metadata": {
        "id": "9im2V87dD4B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example - Step 4: Add Color Channels (RGB) and Resize Images\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 1️⃣  Add color channels to make it 3-channel (RGB)\n",
        "# ------------------------------------------------------------------------\n",
        "# Check if images are grayscale (3 dimensions: N, H, W)\n",
        "if eg_X_train.ndim == 3:\n",
        "    print(\"Adding channel dimension...\", end='')\n",
        "    eg_X_train = np.expand_dims(eg_X_train, axis=-1)\n",
        "    eg_X_test = np.expand_dims(eg_X_test, axis=-1)\n",
        "    eg_X_val = np.expand_dims(eg_X_val, axis=-1)\n",
        "    print(\"done\")\n",
        "\n",
        "# Check if images have 1 channel (N, H, W, 1) and need to be 3 (N, H, W, 3)\n",
        "if eg_X_train.shape[-1] == 1:\n",
        "    print(\"Converting 1-channel Grayscale to 3-channel RGB...\", end='')\n",
        "    # Repeat the single channel 3 times along the last axis\n",
        "    eg_X_train = np.repeat(eg_X_train, 3, axis=-1)\n",
        "    eg_X_test = np.repeat(eg_X_test, 3, axis=-1)\n",
        "    eg_X_val = np.repeat(eg_X_val, 3, axis=-1)\n",
        "    print(\"done\")\n",
        "else:\n",
        "    print(\"Images are already 3-channel RGB\")\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 2️⃣  Resize images to standard 128x128 pixels\n",
        "# ------------------------------------------------------------------------\n",
        "TARGET_SIZE = 128\n",
        "\n",
        "if eg_X_train.shape[1] == TARGET_SIZE and eg_X_train.shape[2] == TARGET_SIZE:\n",
        "    print(f\"No need to resize images--already {TARGET_SIZE}x{TARGET_SIZE} pixels.\")\n",
        "    eg_X_train_resized = eg_X_train\n",
        "    eg_X_test_resized = eg_X_test\n",
        "    eg_X_val_resized = eg_X_val\n",
        "else:\n",
        "    print(f\"Resizing images to {TARGET_SIZE}x{TARGET_SIZE} pixels...\", end='')\n",
        "    eg_X_train_resized = np.array([tf.image.resize(img, (TARGET_SIZE, TARGET_SIZE)).numpy() for img in eg_X_train])\n",
        "    eg_X_test_resized = np.array([tf.image.resize(img, (TARGET_SIZE, TARGET_SIZE)).numpy() for img in eg_X_test])\n",
        "    eg_X_val_resized = np.array([tf.image.resize(img, (TARGET_SIZE, TARGET_SIZE)).numpy() for img in eg_X_val])\n",
        "    print(\"done\")\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 3️⃣  Copy back\n",
        "# ------------------------------------------------------------------------\n",
        "eg_X_train = np.copy(eg_X_train_resized)\n",
        "eg_X_test = np.copy(eg_X_test_resized)\n",
        "eg_X_val = np.copy(eg_X_val_resized)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 4️⃣   Check shapes\n",
        "# ------------------------------------------------------------------------\n",
        "print(f\"Train shape: {eg_X_train.shape}\")\n",
        "print(f\"Test shape:  {eg_X_test.shape}\")\n",
        "print(f\"Val shape:   {eg_X_val.shape}\")"
      ],
      "metadata": {
        "id": "fnXgqSpbOd4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "```text\n",
        "Images are already 3-channel RGB\n",
        "Resizing images to 128x128 pixels...done\n",
        "Train shape: (11959, 128, 128, 3)\n",
        "Test shape:  (3421, 128, 128, 3)\n",
        "Val shape:   (1712, 128, 128, 3)\n",
        "```\n",
        "\n",
        "For this particular Medical MNIST dataset, there was no need to add a color channel or to resize the images. However, there are other Medical MNIST datasets in which this is not the case.\n",
        "\n",
        "You should also note the image size is 128x128 pixels and there are 3 color channels (i.e. RGB)."
      ],
      "metadata": {
        "id": "Om3ueI6p9idK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example - Step 5: Check Available Memory\n",
        "\n",
        "The code in the cell below shows how to check the used and the available memory in a Colab session. As will be explained below, knowing the amount of available memory can be useful during data processing. If you perform a function that increases the size of a dataset beyond what can be held in the available memory, your Colab session will _crash!_"
      ],
      "metadata": {
        "id": "7NVtEsrt-xd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example - Step 5: Check available memory\n",
        "\n",
        "import psutil\n",
        "\n",
        "# Get the memory details\n",
        "mem = psutil.virtual_memory()\n",
        "\n",
        "# Print total, available, and used memory\n",
        "print(f\"Total Memory: {mem.total / (1024 ** 3):.2f} GB\")\n",
        "print(f\"Available Memory: {mem.available / (1024 ** 3):.2f} GB\")\n",
        "print(f\"Used Memory: {mem.used / (1024 ** 3):.2f} GB\")"
      ],
      "metadata": {
        "id": "RgkJhlPB_Wl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using the `A100 GPU` accelerator, you should see something similar to the following output:\n",
        "\n",
        "```text\n",
        "Total Memory: 167.05 GB\n",
        "Available Memory: 153.23 GB\n",
        "Used Memory: 12.28 GB\n",
        "```\n",
        "\n",
        "How much total system memory you have depends upon what GPU Runtime you are using. Image datasets can often take up a lot of space in system memory.  \n"
      ],
      "metadata": {
        "id": "uKVJKqIo_fhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example - Step 6: Augment Train Image Set\n",
        "\n",
        "Augmenting an image dataset by flipping images means creating new images by mirroring the original ones either horizontally or vertically.\n",
        "\n",
        "**Why is it useful?**\n",
        "\n",
        "1. **Increased Dataset Size:** Flipping images effectively multiplies your dataset without the need for additional data collection, which can be costly and time-consuming.\n",
        "\n",
        "2. **Enhanced Robustness:** Models trained on augmented datasets learn to recognize objects from different perspectives, making them more robust and better at generalizing to real-world scenarios.\n",
        "\n",
        "3. **Reduced Overfitting:** Augmentation helps reduce overfitting by introducing variability into the training data, ensuring the model doesn't just memorize the training images but learns to generalize from them.\n",
        "\n",
        "It is always a good idea to augment a image dataset. However, there is one important caveat -- you need to have sufficient available memory to hold the augmented dataset.\n",
        "\n",
        "The code in the cell below checks the available memory and compares it to the amount of memory that has already been used with this conditional statement:\n",
        "\n",
        "~~~text\n",
        "if mem.available <= mem.used:\n",
        "~~~\n",
        "\n",
        "If the available memory is less than or equal to the amount of memory that has already been used, no augmentation takes place. However, if there appears to be enough memory, the images in `eg_X_train` are flipped vertically, and then horizontally and added back to `eg_X_train`.\n"
      ],
      "metadata": {
        "id": "OcoPhtQi_kiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example - Step 6: Augment train image set\n",
        "\n",
        "import psutil\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 1️⃣  Get the memory details\n",
        "# ------------------------------------------------------------------------\n",
        "mem = psutil.virtual_memory()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 2️⃣  Augment if available memory is sufficient\n",
        "# ------------------------------------------------------------------------\n",
        "if mem.available <= mem.used:\n",
        "    print(f\"Available memory ({mem.available / (1024 ** 3):.2f} GB) might not be enough to augment eg_X_train\")\n",
        "    print(f\"Number of eg_X_train images: {eg_X_train.shape[0]}\")\n",
        "else:\n",
        "    print(f\"Available memory ({mem.available / (1024 ** 3):.2f} GB) should be enough to augment eg_X_train\")\n",
        "    print(\"Augmenting the number of images in eg_X_train...\", end=' ')\n",
        "\n",
        "    # Initialize lists to store the augmented images and labels\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    # Iterate through each image and its corresponding label\n",
        "    for img, label in zip(eg_X_train, eg_y_train):\n",
        "        # Original image\n",
        "        augmented_images.append(img)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "        # Vertically flipped image\n",
        "        augmented_images.append(np.flipud(img))\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "        # Horizontally flipped image\n",
        "        augmented_images.append(np.fliplr(img))\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "    # Convert lists back to numpy arrays\n",
        "    augmented_images = np.array(augmented_images)\n",
        "    augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "    print(\"done\")\n",
        "    print(f\"Original number of eg_X_train images: {len(eg_X_train)}\")\n",
        "    print(f\"Augmented number of eg_X_train images: {len(augmented_images)}\")\n",
        "\n",
        "    # Copy images back\n",
        "    eg_X_train = np.copy(augmented_images)\n",
        "    eg_y_train = np.copy(augmented_labels)"
      ],
      "metadata": {
        "id": "TbjbyLCGClTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, and you are using the `A100 GPU` accelerator, you should see someting similiar to the following output:\n",
        "```text\n",
        "Available memory (153.20 GB) should be enough to augment eg_X_train\n",
        "Augmenting the number of images in eg_X_train... done\n",
        "Original number of eg_X_train images: 11959\n",
        "Augmented number of eg_X_train images: 35877\n",
        "```\n",
        "In this situation, there was sufficient available memory to go ahead with augmentation. After flipping vertically and horizontally, there are now 3X more images in `eg_X_train`.\n"
      ],
      "metadata": {
        "id": "rl_-kxWhbuhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Step - 7: Construct CNN model\n",
        "\n",
        "The code in the cell below builds a classic **_Convolutional Neural Network (CNN) model_** using PyTorch.\n",
        "\n",
        "#### **Setting the `input_dim`**\n",
        "In a CNN model, `input_dim` refers to the dimensions of the input data that the model will process. It includes the height, width, and number of channels (color depth) of the images.\n",
        "\n",
        "For this particular lesson, we standardized the image size to 128 x 128 pixels with 3 color channels so the input_dim is set to `(128, 128, 3)`. This ensures that the model architecture matches the shape of your data. It’s the initial layer's responsibility to match this input shape, setting the stage for the entire convolutional process.\n",
        "\n",
        "#### **Setting the `learning_rate`**\n",
        "Choosing the optimal learning rate for a CNN involves some experimentation and fine-tuning. You can start with a `learning_rate = 0.001`. If training proceeds smoothly, that's great. However, if you encounter a problem you could increase or decrease the learning rate to see if that resolves the issue.\n",
        "\n",
        "#### **Setting `PATIENCE`**\n",
        "This variable (`PATIENCE = 10`) controls the **Early Stopping** mechanism. It dictates how many epochs the model will continue training without seeing an improvement in validation loss before it automatically stops. This prevents the model from wasting time or overfitting after it has already peaked.\n",
        "\n",
        "#### **Create CNN model**\n",
        "\n",
        "Here is a summary of the `eg_CNNModel` architecture:\n",
        "\n",
        "1. **Conv2d Block 1:** 16 filters, kernel size 3x3, followed by **BatchNorm** and **MaxPool2d** (reduces dimensions by 2).\n",
        "2. **Conv2d Block 2:** 32 filters, kernel size 3x3, followed by **BatchNorm**, **Dropout** (to prevent overfitting), and **MaxPool2d**.\n",
        "3. **Conv2d Block 3:** 64 filters, kernel size 3x3, followed by **BatchNorm**, **Dropout**, and **MaxPool2d**.\n",
        "4. **Conv2d Block 4:** 128 filters, kernel size 3x3, followed by **BatchNorm**, **Dropout**, and **MaxPool2d**.\n",
        "5. **Flatten:** Converts the 3D feature maps (128 channels x 14 x 14) into a 1D vector.\n",
        "6. **Linear (Dense) Layer:** Fully connected layer with 512 neurons, utilizing **BatchNorm** and **Dropout**.\n",
        "7. **Linear (Output) Layer:** Final fully connected layer with neurons equal to `num_classes` (the number of unique categories).\n",
        "\n",
        "#### **Additional Steps**\n",
        "\n",
        "* **optimizer = optim.Adam(learning_rate, weight_decay=0.01):** You're using the `Adam` optimizer to adjust the model's weights. `Adam` is known for being efficient and well-suited for large datasets. Crucially, we added `weight_decay=0.01`, which applies **L2 Regularization**. This penalizes large weights during updates, helping to further reduce overfitting.\n",
        "\n",
        "* **criterion = nn.CrossEntropyLoss():** This is the standard loss function for multi-class classification in PyTorch. It combines a `LogSoftmax` and `NLLLoss` (Negative Log Likelihood) in one single class. It measures the difference between the raw scores (logits) output by the model and the true class indices, guiding the model to assign higher scores to the correct categories.\n",
        "\n",
        "* **EarlyStopping Class:** PyTorch requires us to define our own logic for stopping training. The `EarlyStopping` class defined here tracks the `val_loss`. If the loss improves, it saves a checkpoint of the model. If the loss stops improving for `PATIENCE` (10) epochs, it triggers a stop flag to halt the training loop."
      ],
      "metadata": {
        "id": "UG54IqYioPGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example Step - 7: Construct CNN model\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 0️⃣  Load packages\n",
        "# ------------------------------------------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed\n",
        "set_seed()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 1️⃣  Define variables\n",
        "# ------------------------------------------------------------------------\n",
        "# Define input_dim: UPDATED for 128x128 images\n",
        "input_dim = (128, 128, 3)\n",
        "\n",
        "# Set learning rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Set patience\n",
        "PATIENCE = 10\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 2️⃣  Create CNN model using PyTorch nn.Module\n",
        "# ------------------------------------------------------------------------\n",
        "class eg_CNNModel(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate=0.5):\n",
        "        super(eg_CNNModel, self).__init__()\n",
        "\n",
        "        # 1st Convolution layer (Input 128x128) -> Output 64x64 after Pool\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # 2nd Convolution layer (Input 64x64) -> Output 32x32 after Pool\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # 3rd Convolution layer (Input 32x32) -> Output 16x16 after Pool\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # 4th Convolution layer (Input 16x16) -> Output 8x8 after Pool\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = nn.Dropout2d(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout2d(dropout_rate)\n",
        "        self.dropout3 = nn.Dropout2d(dropout_rate)\n",
        "        self.dropout4 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1st Convolution block\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = self.bn1(x)\n",
        "\n",
        "        # 2nd Convolution block\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # 3rd Convolution block\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = self.bn3(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # 4th Convolution block\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = self.bn4(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)  # Flatten for dense layers\n",
        "\n",
        "        # Dense layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.bn5(x)\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 3️⃣  Determine the number of classes\n",
        "# ------------------------------------------------------------------------\n",
        "# Get unique classes from training labels (assuming they are class indices now)\n",
        "unique_classes = np.unique(eg_y_train)\n",
        "class_count = len(unique_classes)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 4️⃣  Create the model instance\n",
        "# ------------------------------------------------------------------------\n",
        "eg_model = eg_CNNModel(num_classes=class_count)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 5️⃣  Define optimizer and loss function\n",
        "# ------------------------------------------------------------------------\n",
        "# L2 regularization via weight decay in optimizer\n",
        "optimizer = optim.Adam(eg_model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "# Use categorical crossentropy loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Print number of classes to be used\n",
        "print(f\"The number of output classes =\", class_count)\n",
        "\n",
        "# Print model architecture\n",
        "print(\"Model Architecture:\")\n",
        "print(eg_model)\n",
        "\n",
        "# Print model parameters count\n",
        "total_params = sum(p.numel() for p in eg_model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 5️⃣  Create EarlyStopping class\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=1e-3, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif val_loss < self.best_loss - self.min_delta:\n",
        "            # Loss improved significantly\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            # Loss did not improve enough\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.verbose:\n",
        "                    print(f'Early stopping triggered after {self.patience} epochs without improvement')\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decreases'''\n",
        "        if self.verbose and val_loss < self.val_loss_min:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "            # Save the model state (optional, for teaching we just print)\n",
        "            self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "0I5g8TQYXJ2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "```text\n",
        "The number of output classes = 8\n",
        "Model Architecture:\n",
        "eg_CNNModel(\n",
        "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (dropout1): Dropout2d(p=0.5, inplace=False)\n",
        "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
        "  (dropout3): Dropout2d(p=0.5, inplace=False)\n",
        "  (dropout4): Dropout(p=0.5, inplace=False)\n",
        "  (fc1): Linear(in_features=8192, out_features=512, bias=True)\n",
        "  (bn5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (fc2): Linear(in_features=512, out_features=8, bias=True)\n",
        ")\n",
        "Total parameters: 4,297,624\n",
        "```"
      ],
      "metadata": {
        "id": "Wq-QSomoEMuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Step - 8: Train the Neural Network\n",
        "\n",
        "Training takes the greatest amount of time and computer power. Unlike higher-level frameworks that use a single line (like `model.fit`), **PyTorch** requires us to write an explicit loop to iterate through the data. The code in the cell below trains the neural network model on the training data (`eg_train_loader`) and monitors its performance using the validation data (`eg_val_loader`).\n",
        "\n",
        "The two most important variables that you can change are the maximum number of **_epochs_** to train the model and the **_batch size_**. The code in the cell below is set to train the neural network for a maximum of 100 epochs. This is an upper limit since the training includes an _early stopping monitor_ to stop training if overfitting is detected.\n",
        "\n",
        "Here's the summary of the PyTorch training loop:\n",
        "\n",
        "* **The Outer Loop (`for epoch in range(EPOCHS)`):** This loop runs once for every \"Epoch\" (one complete pass through the entire dataset).\n",
        "\n",
        "* **The DataLoaders (`eg_train_loader`):** Instead of calculating `steps_per_epoch` manually, PyTorch uses DataLoaders. These automatically divide your data into batches of size `BATCH_SIZE` and feed them to the model one by one.\n",
        "\n",
        "* **The Training Phase (`eg_model.train()`):**\n",
        "    * **`optimizer.zero_grad()`**: Clears old gradients from the previous step.\n",
        "    * **`loss.backward()`**: Calculates the gradients (how much to change each weight) via backpropagation.\n",
        "    * **`optimizer.step()`**: Updates the model weights to minimize the loss.\n",
        "\n",
        "* **The Validation Phase (`eg_model.eval()`):**\n",
        "    * We use `torch.no_grad()` to turn off gradient calculation. This saves memory and speed since we aren't training the model here, just testing it.\n",
        "    * We calculate the loss on the validation set to check if the model is learning general patterns or just memorizing the training data.\n",
        "\n",
        "* **Early Stopping:**\n",
        "    * At the end of every epoch, we call the `eg_early_stopping` class we created in Step 8.\n",
        "    * It compares the current validation loss to the best loss seen so far. If the model stops improving for `PATIENCE` (10) epochs, it triggers the break command to stop the loop.\n",
        "\n",
        "##### **IMPORTANT NOTICE: Training Time will depend on the GPU**\n",
        "\n",
        "In this lesson we are training a fairly large CNN on a fairly large image dataset. Using a GPU is necessary to shorten train times.\n",
        "\n",
        "The types of GPUs that are available in Colab vary over time. This is necessary for Colab to be able to provide access to these resources free of charge. However, even with a paid subscription, there are limits on how much time you can use any of the GPUs. So you shouldn't automatically change to a runtime with a GPU unless there is a compelling reason to do so. Unfortunately, you can't change from a simple CPU to a GPU runtime right before you train your CNN; you must select it at the start of your session."
      ],
      "metadata": {
        "id": "UkQ5csODYKki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example Step - 8: Train the Neural Network\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set the seed\n",
        "set_seed()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 1️⃣  Set variables\n",
        "# ------------------------------------------------------------------------\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "VERBOSE = 2  # 2 means output during training\n",
        "PATIENCE = 10\n",
        "\n",
        "# Ensure data is in correct format for PyTorch (NCHW)\n",
        "if len(eg_X_train.shape) == 4 and eg_X_train.shape[-1] == 3:\n",
        "    eg_X_train = eg_X_train.transpose(0, 3, 1, 2)\n",
        "    eg_X_val = eg_X_val.transpose(0, 3, 1, 2)\n",
        "\n",
        "# Squeeze the extra dimension from labels\n",
        "eg_y_train_squeezed = np.squeeze(eg_y_train, axis=1)\n",
        "eg_y_val_squeezed = np.squeeze(eg_y_val, axis=1)\n",
        "\n",
        "# Create datasets\n",
        "eg_train_dataset = TensorDataset(\n",
        "    torch.from_numpy(eg_X_train).float(),\n",
        "    torch.from_numpy(eg_y_train_squeezed).long()\n",
        ")\n",
        "eg_val_dataset = TensorDataset(\n",
        "    torch.from_numpy(eg_X_val).float(),\n",
        "    torch.from_numpy(eg_y_val_squeezed).long()\n",
        ")\n",
        "\n",
        "eg_train_loader = DataLoader(eg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "eg_val_loader = DataLoader(eg_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Determine device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move model to device\n",
        "eg_model = eg_model.to(device)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 2️⃣  Train model with Early Stopping\n",
        "# ------------------------------------------------------------------------\n",
        "print(f\"----- Training is starting for {EPOCHS} epochs, batch size: {BATCH_SIZE} --------------\")\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Create early stopping instance\n",
        "eg_early_stopping = EarlyStopping(patience=PATIENCE, min_delta=1e-3, verbose=True)\n",
        "\n",
        "# Initialize lists to store training history\n",
        "eg_train_losses = []\n",
        "eg_val_losses = []\n",
        "eg_train_accs = []\n",
        "eg_val_accs = []\n",
        "\n",
        "# Training loop\n",
        "best_val_acc = 0.0\n",
        "best_epoch = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Training phase\n",
        "    eg_model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(eg_train_loader):\n",
        "        # Move data to device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = eg_model(data)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        train_total += target.size(0)\n",
        "        train_correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    # Validation phase\n",
        "    eg_model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in eg_val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = eg_model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    avg_train_loss = train_loss/ len(eg_train_loader)\n",
        "    avg_val_loss = val_loss / len(eg_val_loader)\n",
        "    train_acc = 100. * train_correct / train_total\n",
        "    val_acc = 100. * val_correct / val_total\n",
        "\n",
        "    # Save history\n",
        "    eg_train_losses.append(avg_train_loss)\n",
        "    eg_val_losses.append(avg_val_loss)\n",
        "    eg_train_accs.append(train_acc)\n",
        "    eg_val_accs.append(val_acc)\n",
        "\n",
        "    # Early stopping check\n",
        "    eg_early_stopping(avg_val_loss, eg_model)\n",
        "\n",
        "    # Print progress\n",
        "    if VERBOSE >= 2:\n",
        "        print(f'Epoch {epoch+1}/{EPOCHS} - '\n",
        "              f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}% - '\n",
        "              f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # Check if this is the best validation accuracy\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_epoch = epoch + 1\n",
        "        best_val_loss = avg_val_loss\n",
        "\n",
        "    # Early stopping condition\n",
        "    if eg_early_stopping.early_stop:\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished.\")\n",
        "print(f\"Best val accuracy: {best_val_acc:.4f} (achieved at epoch {best_epoch})\")\n",
        "print(f\"Best val loss: {best_val_loss:.4f}\")\n",
        "\n",
        "# Record end time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")\n",
        "\n",
        "# Print final training statistics\n",
        "print(f\"Final training completed in {epoch+1} epochs\")\n"
      ],
      "metadata": {
        "id": "0qI2Emn8iF08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code correct, you should see something _similar_ to this output:\n",
        "\n",
        "```text\n",
        "Using device: cuda\n",
        "----- Training is starting for 100 epochs, batch size: 64 --------------\n",
        "Validation loss decreased (inf --> 0.366999).  Saving model ...\n",
        "Epoch 1/100 - Train Loss: 0.4976, Train Acc: 82.61% - Val Loss: 0.3670, Val Acc: 87.97%\n",
        "Validation loss decreased (0.366999 --> 0.214578).  Saving model ...\n",
        "Epoch 2/100 - Train Loss: 0.3562, Train Acc: 88.23% - Val Loss: 0.2146, Val Acc: 93.28%\n",
        "Validation loss decreased (0.214578 --> 0.189801).  Saving model ...\n",
        "Epoch 3/100 - Train Loss: 0.3501, Train Acc: 88.89% - Val Loss: 0.1898, Val Acc: 94.39%\n",
        "Epoch 4/100 - Train Loss: 0.3445, Train Acc: 89.25% - Val Loss: 0.2491, Val Acc: 93.05%\n",
        "Epoch 5/100 - Train Loss: 0.3449, Train Acc: 89.20% - Val Loss: 0.2262, Val Acc: 93.46%\n",
        "Epoch 6/100 - Train Loss: 0.3348, Train Acc: 89.66% - Val Loss: 0.2441, Val Acc: 92.06%\n",
        "Epoch 7/100 - Train Loss: 0.3380, Train Acc: 89.44% - Val Loss: 0.2846, Val Acc: 90.25%\n",
        "Epoch 8/100 - Train Loss: 0.3345, Train Acc: 89.68% - Val Loss: 0.2132, Val Acc: 93.63%\n",
        "Epoch 9/100 - Train Loss: 0.3334, Train Acc: 89.60% - Val Loss: 0.2378, Val Acc: 92.00%\n",
        "Epoch 10/100 - Train Loss: 0.3328, Train Acc: 89.64% - Val Loss: 0.1969, Val Acc: 94.04%\n",
        "Epoch 11/100 - Train Loss: 0.3226, Train Acc: 90.07% - Val Loss: 0.4550, Val Acc: 85.69%\n",
        "Epoch 12/100 - Train Loss: 0.3287, Train Acc: 89.87% - Val Loss: 0.2229, Val Acc: 92.99%\n",
        "Early stopping triggered after 10 epochs without improvement\n",
        "Epoch 13/100 - Train Loss: 0.3235, Train Acc: 90.16% - Val Loss: 0.5977, Val Acc: 76.40%\n",
        "Early stopping at epoch 13\n",
        "\n",
        "Training finished.\n",
        "Best val accuracy: 94.3925 (achieved at epoch 3)\n",
        "Best val loss: 0.1898\n",
        "Elapsed time: 0:01:27.35\n",
        "Final training completed in 13 epochs\n",
        "```\n"
      ],
      "metadata": {
        "id": "XHbiXkwldEUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating Model's Training**\n",
        "\n",
        "Now that we have trained our model, let's look at how it changed during its training."
      ],
      "metadata": {
        "id": "axjJIaLgEiPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example - Step 9: Visualize Training\n",
        "\n",
        "The code in the cell below generates a plot of `Accuracy` on the left showing `train`and`val`accuracy for each epoch and a plot `Loss` on the right showing `train`and`val` loss for each epoch."
      ],
      "metadata": {
        "id": "PAvFP6oeeMgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example - Step 9: Visualize Training\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 1: Loss (Left Graph)    # Save history\n",
        "# ------------------------------------------------------------------\n",
        "ax1.plot(eg_train_losses, label='Train Loss')\n",
        "ax1.plot(eg_val_losses, label='Val Loss')\n",
        "ax1.set_title('Model Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 2: Accuracy (Right Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax2.plot(eg_train_accs, label='Train Acc')\n",
        "ax2.plot(eg_val_accs, label='Val Acc')\n",
        "ax2.set_title('Model Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fADY1-7-dbop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code correct is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_1_image09A.png)\n"
      ],
      "metadata": {
        "id": "IA0DCVixd_xY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------\n",
        "\n",
        "## **Accuracy and Loss**\n",
        "\n",
        "In training a neural network, `train accuracy` and `val_accuracy` represent different performance metrics:\n",
        "\n",
        "* **`train` Accuracy:** This measures how well the model is performing on the training data. It calculates the percentage of correct predictions out of the total predictions made during training.\n",
        "\n",
        "* **`val` Accuracy:** This measures how well the model is performing on the validation data, which is separate from the training data. It calculates the percentage of correct predictions out of the total predictions made on the validation set. This metric helps you understand how well the model generalizes to new, unseen data.\n",
        "\n",
        "In short, `train accuracy` tells you how well the model is learning from the training data, while `val_accuracy` tells you how well the model is expected to perform on new data.\n",
        "\n",
        "Train vs. Validation Loss\n",
        "Train loss – The loss (e.g., cross‑entropy, MSE, etc.) computed on the training data after each epoch.\n",
        "It tells you how well the model is fitting the data it has seen.\n",
        "\n",
        "Val loss – The loss computed on the validation set (a held‑out portion of the data) after each epoch.\n",
        "It indicates how well the model is generalizing to unseen data.\n",
        "\n",
        "Plotting both curves together is a quick way to spot common training problems:"
      ],
      "metadata": {
        "id": "BIKmmSjH2cxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example - Step 10: Plot 4 Frames with Label\n",
        "\n",
        "The code in the cell below generates a 2 X 2 plot showing 4 images from the training dataset along with their labels."
      ],
      "metadata": {
        "id": "EleEfZ9O3hGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example - Step 10: Plot 4 frames with label\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "\n",
        "# Set figure size\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "grid = ImageGrid(\n",
        "        fig, 111,\n",
        "        nrows_ncols=(2,2),\n",
        "        axes_pad=0.5\n",
        ")\n",
        "\n",
        "# Plot 4 images\n",
        "for x in range(0,4):\n",
        "    grid[x].set_title('Blood cell type ' + str(eg_y_test[x]))\n",
        "    # FIX: Cast to uint8 so matplotlib treats them as 0-255 integers\n",
        "    grid[x].imshow(eg_X_test[x].astype(\"uint8\"))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gHGg_gBRaSKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/Class_03_1_image04A.png)\n",
        "\n",
        "The figure shows 4 pictures from the `eg_X_test` with their blood cell type shown in the image title. Since the images were randomly shuffled, you will not see the same 4 blood cells."
      ],
      "metadata": {
        "id": "l-8wrGJf3p-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise: Classification Convolutional Neural Networks**\n",
        "\n",
        "For the **Exercises**, you are to build a CNN classification neural network that can classify 11 different classes of abdominal organs as seen from a saggital view.\n",
        "\n",
        "The classes are:\n",
        "\n",
        "* Liver\n",
        "* Kidney\n",
        "* Pancreas\n",
        "* Spleen\n",
        "* Gallbladder\n",
        "* Stomach\n",
        "* Intestine\n",
        "* Bladder\n",
        "* Colon\n",
        "* Esophagus\n",
        "* Uterus\n",
        "* Prostate\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_1_image10A.png)\n",
        "Each image is 28 X 28 pixels with no color channels.\n",
        "\n",
        "Your goal will be to create a convolutional neural network (CNN) that can classify an abdominal image into one of these 11 organ types.\n"
      ],
      "metadata": {
        "id": "hckUiwwO3vCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise - Step 1: Setup Evironmental Variables**\n",
        "\n",
        "In the cell below create the environmental variables needed to download the datafile called `organsmnist.npz` from the course file server and then extract (unzip) the data into a folder called `organsmnist` in your Colab notebook. This datafile contains a **sagittal** view of different human organs. Specifially, here are the organ respresented by each class type:\n",
        "\n",
        "| Class Label | Organ / Structure |\n",
        "| :--- | :--- |\n",
        "| 0 | Bladder |\n",
        "| 1 | Femur (Left) |\n",
        "| 2 | Femur (Right) |\n",
        "| 3 | Heart |\n",
        "| 4 | Kidney (Left) |\n",
        "| 5 | Kidney (Right) |\n",
        "| 6 | Liver |\n",
        "| 7 | Lung (Left) |\n",
        "| 8 | Lung (Right) |\n",
        "| 9 | Pancreas |\n",
        "| 10 | Spleen |\n"
      ],
      "metadata": {
        "id": "dq2g7Hd2339M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise - Step 1: Setup Evironmental Variables\n",
        "\n"
      ],
      "metadata": {
        "id": "bgu8Y1QC33rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "```text\n",
        "DOWNLOAD_SOURCE https://biologicslab.co/BIO1173/data/organsmnist.npz\n",
        "DOWNLOAD_NAME organsmnist.npz\n",
        "PATH /content\n",
        "EXTRACT_TARGET /organsmnist\n",
        "SOURCE /organsmnist\n",
        "TARGET /organsmnist\n",
        "```\n",
        "Check your output with the expected output above. If there are **_any_** differences, no matter how small, any code you write below will probably not run."
      ],
      "metadata": {
        "id": "i-bFucne32v8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise - Step 2: Download and Extract Data**\n",
        "\n",
        "In the cell below, write the code to download and extract the datafile. You can basically re-use the code in the  `Example Step - 2` but make sure to change the prefix `eg_` to `ex_`."
      ],
      "metadata": {
        "id": "WIPIbLn14EVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise - Step 2: Download and Extract Data\n",
        "\n"
      ],
      "metadata": {
        "id": "cOE-_kPc4EED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "```text\n",
        "Creating necessary directories...done.\n",
        "Downloading files...done.\n",
        "Extracting files...done.\n",
        "```\n",
        "\n",
        "This MedMNIST datafile is much smaller than the one used in Example 1, so downloading and extracting will require significantly less time."
      ],
      "metadata": {
        "id": "oGehrux04D0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise - Step 3: Load and Shuffle Images and Labels into Numpy Arrays**\n",
        "\n",
        "In the cell below use the Numpy command `np.load()` to create 6 numpy arrays containing the `training`, `test` and `validation` images and their labels. Shuffle the data as you \"unpack\" the data into the 6 arrays.  You can basically re-use the code in the  `Example Step - 3` but make sure to change the prefix `eg_` to `ex_`."
      ],
      "metadata": {
        "id": "iPvhRoNk4RDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise - Step 3: Load and Shuffle Images and Labels into Numpy Arrays\n",
        "\n"
      ],
      "metadata": {
        "id": "tE-NE_le4Q0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the following output:\n",
        "```text\n",
        "ex_X_train: (13932, 28, 28)\n",
        "ex_y_train: (13932, 1)\n",
        "ex_X_test: (8827, 28, 28)\n",
        "ex_y_test: (8827, 1)\n",
        "ex_X_val: (2452, 28, 28)\n",
        "ex_y_val: (2452, 1)\n",
        "```"
      ],
      "metadata": {
        "id": "zWRsQJ2NgwsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise - Step 4: Add Color Channel and Resize Images**\n",
        "\n",
        "Write the code need add a color channel (if necessary) and then resize the images (if necessary). You can basically re-use the code in the  `Example Step - 4` but make sure to change the prefix `eg_` to `ex_`."
      ],
      "metadata": {
        "id": "tzwJrO-R4cES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise - Step 4: Add Color Channel and Resize Images\n",
        "\n"
      ],
      "metadata": {
        "id": "YVJ94xqBakof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output\n",
        "```text\n",
        "Adding channel dimension...done\n",
        "Converting 1-channel Grayscale to 3-channel RGB...done\n",
        "Resizing images to 128x128 pixels...done\n",
        "Train shape: (13932, 128, 128, 3)\n",
        "Test shape:  (8827, 128, 128, 3)\n",
        "Val shape:   (2452, 128, 128, 3)\n",
        "```\n",
        "Since this datafile did not have a color channel, the code in the cell above added 3 color channels.\n",
        "\n",
        "Also, your images were rather small, only 28 X 28 pixels, when they were downloaded. The code in the cell above resized the images in the training, test and validation sets to 128 x 128 pixels."
      ],
      "metadata": {
        "id": "M6m0ypgxiRx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise - Step 5: Check Available Memory**\n",
        "\n",
        "Write the code need to check the available system memory."
      ],
      "metadata": {
        "id": "zgcWpW36g9o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise - Step 5: Check Available Memory\n",
        "\n"
      ],
      "metadata": {
        "id": "k5UrgYWU4bhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct and you are using the `A100 GPU`, you should see something _similar_ to the following output:\n",
        "```text\n",
        "Total Memory: 167.05 GB\n",
        "Available Memory: 130.32 GB\n",
        "Used Memory: 35.18 GB\n",
        "```\n",
        "If you are using a different GPU you will see different values.\n"
      ],
      "metadata": {
        "id": "cx7Nm2jB4h2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise - Step 6: Augment Train Image Set**\n",
        "\n",
        "In the cell below, write the code needed to augment the train image set (`ex_X_train`) if there is sufficient memory available.\n",
        "\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Make sure to change the prefix `eg_` to `ex_` everywhere in your code block."
      ],
      "metadata": {
        "id": "SxjANfuB4mOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise - Step 6: Augment Train Image Set\n",
        "\n"
      ],
      "metadata": {
        "id": "rFQ1mioz4l8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "```text\n",
        "Available memory (130.30 GB) should be enough to augment ex_X_train\n",
        "Augmenting the number of images in ex_X_train... done\n",
        "Original number of ex_X_train images: 13932\n",
        "Augmented number of ex_X_train images: 41796\n",
        "```\n",
        "\n",
        "If you are not using a `A100 GPU` you might see different values and you may not have enough memory to augment the training data."
      ],
      "metadata": {
        "id": "XAoMAnYE4lpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise - Step 7: Create CNN Neural Network Model**\n",
        "\n",
        "In the cell below, build a convolutional Neural Network (CNN) model to classify the 11 different abdominal organs as seen in a saggital view. You can basically re-use the code in the  `Example Step - 7` but make sure to change the prefix `eg_` to `ex_`."
      ],
      "metadata": {
        "id": "Qp-lWtBu5PBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise - Step 7: Create CNN Neural Network Model\n",
        "\n"
      ],
      "metadata": {
        "id": "J55i0zAKdkhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "```text\n",
        "The number of output classes = 11\n",
        "Model Architecture:\n",
        "ex_CNNModel(\n",
        "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (dropout1): Dropout2d(p=0.5, inplace=False)\n",
        "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
        "  (dropout3): Dropout2d(p=0.5, inplace=False)\n",
        "  (dropout4): Dropout(p=0.5, inplace=False)\n",
        "  (fc1): Linear(in_features=8192, out_features=512, bias=True)\n",
        "  (bn5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (fc2): Linear(in_features=512, out_features=11, bias=True)\n",
        ")\n",
        "Total parameters: 4,297,624\n",
        "```"
      ],
      "metadata": {
        "id": "72aIHhi15Ocn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise - Step 8: Train the Neural Network**\n",
        "\n",
        "In the cell below, train your neural network. Make sure to set the number of epochs to `100`, batch size to `64`, Verbose to `2` and Patience to `10`."
      ],
      "metadata": {
        "id": "9QowgMvz5Xvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise - Step 8: Train the Neural Network\n",
        "\n"
      ],
      "metadata": {
        "id": "BwoZw6MSfTzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code correct is correct, the output should look something _similar_ to the following:\n",
        "\n",
        "```text\n",
        "Using device: cuda\n",
        "----- Training is starting for 100 epochs, batch size: 64 --------------\n",
        "Validation loss decreased (inf --> 0.802630).  Saving model ...\n",
        "Epoch 1/100 - Train Loss: 1.1754, Train Acc: 57.24% - Val Loss: 0.8026, Val Acc: 73.61%\n",
        "Validation loss decreased (0.802630 --> 0.707356).  Saving model ...\n",
        "Epoch 2/100 - Train Loss: 0.9715, Train Acc: 64.24% - Val Loss: 0.7074, Val Acc: 73.94%\n",
        "Validation loss decreased (0.707356 --> 0.603373).  Saving model ...\n",
        "Epoch 3/100 - Train Loss: 0.9570, Train Acc: 64.98% - Val Loss: 0.6034, Val Acc: 76.43%\n",
        "Epoch 4/100 - Train Loss: 0.9387, Train Acc: 65.67% - Val Loss: 0.7219, Val Acc: 74.92%\n",
        "Validation loss decreased (0.603373 --> 0.564615).  Saving model ...\n",
        "Epoch 5/100 - Train Loss: 0.9381, Train Acc: 65.80% - Val Loss: 0.5646, Val Acc: 79.36%\n",
        "Epoch 6/100 - Train Loss: 0.9336, Train Acc: 65.65% - Val Loss: 0.6790, Val Acc: 71.29%\n",
        "Epoch 7/100 - Train Loss: 0.9235, Train Acc: 66.04% - Val Loss: 0.6446, Val Acc: 77.85%\n",
        "Epoch 8/100 - Train Loss: 0.9259, Train Acc: 66.06% - Val Loss: 0.5922, Val Acc: 76.79%\n",
        "Validation loss decreased (0.564615 --> 0.544318).  Saving model ...\n",
        "Epoch 9/100 - Train Loss: 0.9200, Train Acc: 66.40% - Val Loss: 0.5443, Val Acc: 80.67%\n",
        "Epoch 10/100 - Train Loss: 0.9205, Train Acc: 66.44% - Val Loss: 0.6056, Val Acc: 79.53%\n",
        "Epoch 11/100 - Train Loss: 0.9334, Train Acc: 65.77% - Val Loss: 0.6627, Val Acc: 75.37%\n",
        "Epoch 12/100 - Train Loss: 0.9164, Train Acc: 66.56% - Val Loss: 0.5720, Val Acc: 80.34%\n",
        "Validation loss decreased (0.544318 --> 0.542354).  Saving model ...\n",
        "Epoch 13/100 - Train Loss: 0.9109, Train Acc: 66.67% - Val Loss: 0.5424, Val Acc: 80.06%\n",
        "Epoch 14/100 - Train Loss: 0.9137, Train Acc: 66.48% - Val Loss: 0.7059, Val Acc: 72.10%\n",
        "Epoch 15/100 - Train Loss: 0.9117, Train Acc: 66.86% - Val Loss: 0.6117, Val Acc: 76.75%\n",
        "Epoch 16/100 - Train Loss: 0.9075, Train Acc: 66.84% - Val Loss: 0.5421, Val Acc: 81.20%\n",
        "Epoch 17/100 - Train Loss: 0.9121, Train Acc: 66.83% - Val Loss: 0.5680, Val Acc: 80.91%\n",
        "Validation loss decreased (0.542354 --> 0.529622).  Saving model ...\n",
        "Epoch 18/100 - Train Loss: 0.9130, Train Acc: 66.72% - Val Loss: 0.5296, Val Acc: 82.01%\n",
        "Epoch 19/100 - Train Loss: 0.9116, Train Acc: 66.57% - Val Loss: 0.7753, Val Acc: 70.43%\n",
        "Epoch 20/100 - Train Loss: 0.9048, Train Acc: 66.79% - Val Loss: 0.8214, Val Acc: 68.19%\n",
        "Epoch 21/100 - Train Loss: 0.9182, Train Acc: 66.63% - Val Loss: 0.7397, Val Acc: 75.00%\n",
        "Epoch 22/100 - Train Loss: 0.9039, Train Acc: 67.03% - Val Loss: 0.6888, Val Acc: 72.68%\n",
        "Epoch 23/100 - Train Loss: 0.9027, Train Acc: 67.10% - Val Loss: 0.5633, Val Acc: 82.63%\n",
        "Epoch 24/100 - Train Loss: 0.9080, Train Acc: 67.08% - Val Loss: 0.6010, Val Acc: 78.83%\n",
        "Epoch 25/100 - Train Loss: 0.9080, Train Acc: 66.92% - Val Loss: 0.6746, Val Acc: 76.31%\n",
        "Epoch 26/100 - Train Loss: 0.8980, Train Acc: 67.15% - Val Loss: 0.8806, Val Acc: 66.31%\n",
        "Epoch 27/100 - Train Loss: 0.9026, Train Acc: 67.05% - Val Loss: 0.5499, Val Acc: 79.53%\n",
        "Early stopping triggered after 10 epochs without improvement\n",
        "Epoch 28/100 - Train Loss: 0.9161, Train Acc: 66.43% - Val Loss: 0.6115, Val Acc: 77.94%\n",
        "Early stopping at epoch 28\n",
        "\n",
        "Training finished.\n",
        "Best val accuracy: 82.6264 (achieved at epoch 23)\n",
        "Best val loss: 0.5633\n",
        "Elapsed time: 0:03:35.58\n",
        "Final training completed in 28 epochs\n",
        "```\n"
      ],
      "metadata": {
        "id": "l4_4cK6y5ZrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating Model's Performance**\n",
        "\n",
        "Now that you have trained your model, let's look at its performance."
      ],
      "metadata": {
        "id": "PQcEgyf85uWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise- Step 9: Visualize Training**\n",
        "\n",
        "In the cell below, write the code to generates a plot of `Accuracy` on the left showing `train`and`val`accuracy for each epoch and a plot `Loss` on the right showing `train`and`val` loss for each epoch."
      ],
      "metadata": {
        "id": "a2g9E4K_K79K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise- Step 9: Visualize Training\n",
        "\n"
      ],
      "metadata": {
        "id": "yLPDIDqIgUpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code correct is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_1_image11A.png)\n"
      ],
      "metadata": {
        "id": "boiHESV3K79K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise - Step 10: Plot 4 Frames with Label**\n",
        "\n",
        "In the cell below. generate a 2 X 2 plot showing 4 images from the training dataset along with their labels. Don't forget to correctly label your frames. In other words, what are you looking at in these images?"
      ],
      "metadata": {
        "id": "RLQrOxXsK79M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise - Step 10: Plot 4 Frames with Label\n",
        "\n"
      ],
      "metadata": {
        "id": "HzZ6zVVnK79M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_1_image12A.png)\n"
      ],
      "metadata": {
        "id": "ITqIr92jK79N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Other Resources\n",
        "\n",
        "* [Imagenet:Large Scale Visual Recognition Challenge 2014](http://image-net.org/challenges/LSVRC/2014/index)\n",
        "* [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/) - PhD student/instructor at Stanford.\n",
        "* [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/) - Stanford course on computer vision/CNN's.\n",
        "* [CS231n - GitHub](http://cs231n.github.io/)\n",
        "* [ConvNetJS](http://cs.stanford.edu/people/karpathy/convnetjs/) - JavaScript library for deep learning."
      ],
      "metadata": {
        "id": "_wcvMXf26jaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Electronic Submission**\n",
        "\n",
        "When you run the code in the cell below, it will grade your Colab notebook and tell you your pending grade as it currently stands. You will be given the choice to either submit your notebook for final grading or the option to continue your work on one (or more) Exercises. You no longer have the option to upload a PDF of your Colab notebook to Canvas for grading. Grant Access to your Colab Secrets if you are asked to do so.\n",
        "\n",
        "**NOTE:** You grade on this Colab notebook will be based solely on the code in your **Exercises**. Failure to run one (or more) Examples will not affect your grade."
      ],
      "metadata": {
        "id": "_C4z-cN2u2op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Electronic Submission\n",
        "\n",
        "import urllib.request\n",
        "import ssl\n",
        "import time\n",
        "\n",
        "url = \"https://biologicslab.co/BIO1173/backend_code/validate.py?v=\" + str(time.time())\n",
        "\n",
        "ctx = ssl.create_default_context()\n",
        "ctx.check_hostname = False\n",
        "ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "req = urllib.request.Request(\n",
        "    url,\n",
        "    headers={\n",
        "        \"Cache-Control\": \"no-cache, no-store, must-revalidate\",\n",
        "        \"Pragma\": \"no-cache\",\n",
        "        \"Expires\": \"0\"\n",
        "    }\n",
        ")\n",
        "\n",
        "with urllib.request.urlopen(req, context=ctx) as r:\n",
        "    exec(r.read().decode(\"utf-8\"))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "2n17PuDUu3Gp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}