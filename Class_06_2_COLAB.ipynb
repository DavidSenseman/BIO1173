{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEEW13uCtiJ",
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/master/Class_06_2_COLAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIO 1173: Intro Computational Biology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Module 6: Convolutional Neural Networks (CNN) for Computer Vision**\n",
    "\n",
    "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
    "\n",
    "### Module 6 Material\n",
    "\n",
    "* Part 6.1: Using Convolutional Neural Networks \n",
    "* **Part 6.2: Using Pretrained Neural Networks with Keras** \n",
    "* Part 6.3: Looking at Keras Generators and Image Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google COLAB\n",
    "\n",
    "In order to run this lesson on Google COLAB, you will already need to have a [Google Drive account](https://support.google.com/a/users/answer/13022292?hl=en).\n",
    "\n",
    "When you open this lesson in COLAB, you must immediately change the `RUNTIME` environment to take advantage of GPU **_before_** you start your lesson. While you can change your `RUNTIME` environment after you start, you will to restart from the beginning. \n",
    "\n",
    "When you open up this lesson in COLAB, you should select `RUNTIME -> Change runtime type` \n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_COLAB1.png)\n",
    "\n",
    "\n",
    "\n",
    "This will open the following popup menu:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_COLAB2.png)\n",
    "\n",
    "\n",
    "Select `T4 GPU` and then `Save`. \n",
    "\n",
    "Make sure to save a copy of your lesson to your Google Drive. And don't forget, if you stop working on your lesson, COLAB will \"kick you off\" after some period of inactivity.\n",
    "\n",
    "So if you decide you want to run this lesson on Google COLAB, click on this button:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKQylnEiLDUM"
   },
   "source": [
    "### Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n",
    "  Running the following code will map your GDrive to ```/content/drive```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seXFCYH4LDUM",
    "outputId": "c05015aa-871e-4779-9265-5ad07e8bf617",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Setup\n",
    "\n",
    "Run the next code cell to load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You MUST run this code cell first\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "path = '/'\n",
    "memory = shutil.disk_usage(path)\n",
    "LESSON_DIRECTORY = os.getcwd()\n",
    "print(\"Your LESSON_DIRECTORY is: \" + LESSON_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions\n",
    "\n",
    "The cell below creates the function(s) needed for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_xChO0Fkn4S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple function to print out elasped time\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q09yMGGcmp9N"
   },
   "source": [
    "# Part 6.2: Transfer Learning for Computer Vision\n",
    "\n",
    "Many advanced prebuilt neural networks are available for computer vision, and Keras provides direct access to many networks. **_Transfer Learning_** is the technique where you use these prebuilt neural networks. \n",
    "\n",
    "There are several different levels of transfer learning.\n",
    "\n",
    "* Use a prebuilt neural network in its entirety\n",
    "* Use a prebuilt neural network's structure\n",
    "* Use a prebuilt neural network's weights\n",
    "\n",
    "We will begin by using the **_MobileNet_** prebuilt neural network in its entirety. MobileNet will be loaded and allowed to classify simple images. We can already classify 1,000 images through this technique without ever having trained the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpN-M-BScz5K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vB4Im0gKfF0J"
   },
   "source": [
    "We begin by downloading weights for a MobileNet trained for the imagenet dataset, which will take some time to download the first time you train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUDtlNDxfMBA",
    "outputId": "0c9597ac-9385-4fda-f7cb-de18f82de4a9"
   },
   "outputs": [],
   "source": [
    "# Download Pre-Trained neural network\n",
    "\n",
    "model = MobileNet(weights='imagenet',include_top=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwuvm2W_fRZy"
   },
   "source": [
    "The loaded network is a Keras neural network. However, this is a neural network that a third party engineered on advanced hardware. Merely looking at the structure of an advanced state-of-the-art neural network can be educational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YF5ZFnrqfXA5",
    "outputId": "7747c94b-0dc2-4dc0-a1bc-60313059a6de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcdKUTOAfbw0"
   },
   "source": [
    "Several clues to neural network architecture become evident when examining the above structure.\n",
    "\n",
    "**_MobileNet_** is a neural network architecture designed for mobile and embedded applications to be computationally efficient. The key components of the MobileNet architecture include depthwise separable convolutions and pointwise convolutions.\n",
    "* **Depthwise separable convolutions:** MobileNet replaces traditional convolutions with depthwise separable convolutions, which consists of two separate operations: depthwise convolutions and pointwise convolutions. Depthwise convolutions apply a single filter for each input channel, while pointwise convolutions apply 1x1 convolutions to combine the outputs of depthwise convolutions.\n",
    "* **Pointwise convolutions:** Pointwise convolutions are used to increase the depth of the feature maps while keeping the spatial dimensions the same. This allows for efficient learning of complex patterns while reducing the computational cost.\n",
    "* **Inverted residuals with linear bottleneck:** MobileNetV2 introduces inverted residuals with linear bottleneck to improve performance. Inverted residuals use an expansion layer to increase the number of channels followed by a depthwise convolution and a projection layer to reduce the number of channels back to the original dimensions.\n",
    "\n",
    "Overall, the MobileNet architecture is designed to be lightweight and efficient while maintaining high accuracy for a wide range of tasks, making it ideal for deployment on mobile and edge devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Use MobileNet to classify images\n",
    "\n",
    "The code in the cell below creates a two functions that we will need to use classify images using MobileNet. \n",
    "\n",
    "* **make_square()** Since MobileNet is designed to classify images with the same number of horizontal and vertical pixels (i.e. a 'square' image), this function uses a combination of padding and cropping to convert any image into a 'square` image.\n",
    "  \n",
    "* **classify_image()** This function does most of the work. It first retrives the image from the HTTPS server and resizes it before processing it by the MobileNet model that we previously downloaded. The actual prediction is made by this line of code:\n",
    "\n",
    "~~~text\n",
    "  # Use MobileNet model to predict image\n",
    "  pred = model.predict(x)\n",
    "~~~\n",
    "\n",
    "We will now use the MobileNet to classify several image URLs below.  You are encourged to to add additional URLs of your own to see how well the MobileNet can classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDX_FwNDfvlX"
   },
   "outputs": [],
   "source": [
    "# Example 1: Use MobileNet to classify images\n",
    "\n",
    "%matplotlib inline\n",
    "from PIL import Image, ImageFile\n",
    "from matplotlib.pyplot import imshow\n",
    "import requests\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from IPython.display import display, HTML\n",
    "from tensorflow.keras.applications.mobilenet import decode_predictions\n",
    "\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "# Define HTTPS image source \n",
    "ROOT = \"https://biologicslab.co/BIO1173/images/class_06_2/\"\n",
    "\n",
    "# Function to make sample image square\n",
    "def make_square(img):\n",
    "    cols,rows = img.size\n",
    "    \n",
    "    if rows>cols:\n",
    "        pad = (rows-cols)/2\n",
    "        img = img.crop((pad,0,cols,cols))\n",
    "    else:\n",
    "        pad = (cols-rows)/2\n",
    "        img = img.crop((0,pad,rows,rows))\n",
    "    \n",
    "    return img\n",
    "        \n",
    "# Function to classify image\n",
    "def classify_image(url):\n",
    "  x = []\n",
    "  ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
    "\n",
    "  # Get image from HTTPS server\n",
    "  response = requests.get(url)\n",
    "\n",
    "  # Resize the image to 224 x 224 x 3\n",
    "  img = Image.open(BytesIO(response.content))\n",
    "  img.load()\n",
    "  img = img.resize((IMAGE_WIDTH,IMAGE_HEIGHT),Image.LANCZOS)\n",
    "\n",
    "  # Additional image processing\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  x = preprocess_input(x)\n",
    "  x = x[:,:,:,:3] # maybe an alpha channel\n",
    "  \n",
    "  # Use MobileNet model to predict image\n",
    "  pred = model.predict(x)\n",
    "\n",
    "  # Show the image in Jupyterlab/COLAB\n",
    "  display(img)\n",
    "\n",
    "  # Print out the image number in MobileNet \n",
    "  print(np.argmax(pred,axis=1))\n",
    "\n",
    "  # Print out MobileNet's first 5 predictions\n",
    "  lst = decode_predictions(pred, top=5)\n",
    "  for itm in lst[0]:\n",
    "      print(itm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP6UnmqcjAAo"
   },
   "source": [
    "We can now classify an example image.  You can specify the URL of any image you wish to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1A: Classify images\n",
    "\n",
    "Our MobileNet model has been trained to recognize a wide range of images. Let's try a few different types of images to get some idea of what MobileNet knows.\n",
    "\n",
    "Let's start with a picture of Abraham Lincoln. Here is the actual image before being processed:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_06_2/abraham_lincoln.jpg)\n",
    "\n",
    "Probably most US school children could correctly identify this image. You should notice that this is _not_ a square image. It is actually 1024 X 576 pixels.\n",
    "\n",
    "Let's see what happens when we process it and submit the processed image to our model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "FwfnWNXSo7Vk",
    "outputId": "f286ab26-7c13-489f-ccf4-c9c0c60b19c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1A: \n",
    "classify_image(ROOT+\"abraham_lincoln.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the creators of MobileNet didn't include images of American presidents in their training set. What MobileNet \"saw\" wasn't the Lincoln's face, but the clothes he was wearing. MobileNet 'nailed' Lincoln's bow tie with a 95% probability!\n",
    "\n",
    "We can also see what happens when our code converts a rectangular image (1024 X 576 pixels) into a square image. Lincoln's face is still clearly recognizable albeit a bit 'squashed'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1B: Classify Images\n",
    "\n",
    "Perhaps MobileNet only recognizes famous people who are currently alive? Let's see how MobileNet does with another former President of the US (POTUS)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "FwfnWNXSo7Vk",
    "outputId": "f286ab26-7c13-489f-ccf4-c9c0c60b19c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1B: \n",
    "\n",
    "classify_image(ROOT+\"trump.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, MobileNet doesn't seem to know the names of American Presidents, but seems to focus more on what the person in the image is wearing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1C: Classify Images\n",
    "\n",
    "Let's try something else. How about a nice Thompson Submachine gun? This particular image is of a [Lancer Tactical Extra 50 Rounds Airsoft Magazine - Airsoft Tommy Thompson Submachine Gun (2X Drum 2X Stick)](https://us.amazon.com/Lancer-Tactical-Rounds-Airsoft-Magazine/dp/B0C24VRLM1) sold on Amazon in case you might want to buy it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "FwfnWNXSo7Vk",
    "outputId": "f286ab26-7c13-489f-ccf4-c9c0c60b19c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1C:\n",
    "\n",
    "classify_image(ROOT+\"submachine_gun.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have one type of image that MobileNet has been trained on. MobileNet predicted that there was an approximately 54% chance that our image of a Thompson Submachine Gun was an 'assault_rife' and a 49% that the image was some kind of 'rifle'. Technically, Thompson Submachine Guns are _not_ considered to be an assault rife or even a rifle, but it wasn't a bad guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1D: Classify images\n",
    "\n",
    "Let's see how MobileNet does with an image of an American soldier with a _real_ assault rifle, an M16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "FwfnWNXSo7Vk",
    "outputId": "f286ab26-7c13-489f-ccf4-c9c0c60b19c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1D: \n",
    "\n",
    "classify_image(ROOT+\"M16.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are definitely better with a 65% prediction that the image showed an assault rifle, in this case an M16. Again, MobileNet seems to ignore the inclusion of people in an image and focus on inanimate objects in the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 1: Classify images**\n",
    "\n",
    "For **Exercise 1**, you are present our MobileNet model the following series of 8 animal images. \n",
    "\n",
    "1. puffer_fish.jpg (Exercise 1A)\n",
    "2. king_cobra.jpg (Exercise 1B)\n",
    "3. monarch_butterfly.jpg (Exercise 1C)\n",
    "4. viceroy_butterfly.jpg (Exercise 1D)\n",
    "5. meerkat.jpg (Exercise 1E)\n",
    "6. paramecium.jpg (Exercise 1F)\n",
    "7. prairie_dog.jpg (Exercise 1G)\n",
    "8. great-white-shark.jpg (Exercise 1H)\n",
    "\n",
    "You will need to make a new code cell for each exercise.\n",
    "\n",
    "If you are using COLAB, you can add a new code cell by pointing your cursor at the bottom edge of the code cell and pressing the button below: \n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_06_2_add_code.png). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should have received the following output (only the the first value is shown):\n",
    "\n",
    "1. `puffer_fish.jpg` ('n02655020', 'puffer', 0.99882895)  Note: 99% accurate!\n",
    "2. `king_cobra.jpg` ('n01748264', 'Indian_cobra', 0.999587) Note: Indian cobras are much smaller than king cobras so technically, MobileNet got this wrong. However, there is no way to know the size of the snake in the image, so we can score this as being 100% accurate.\n",
    "3. `monarch_butterfly.jpg` ('n02279972', 'monarch', 0.99983335) Note: Nailed it!\n",
    "4. `viceroy_butterfly.jpg` ('n02279972', 'monarch', 0.9988518) Note: Oops! A total fail, but completely understandable. MobileNet again predicted that there was 100% probability that the picture was a monarch butterfly (_Danaus plexippus_), when in fact it was a viceroy butterfly, a completely different genus and species (_Limenitis archippus_). Of course these two species look very, very similar. The viceroy and the monarch butterflies are [MÃ¼llerian mimics](https://en.wikipedia.org/wiki/M%C3%BCllerian_mimicry) of each other. It is certainly possible to train a neural network to do this for example [MonarchNet](https://ai4earthscience.github.io/neurips-2020-workshop/papers/ai4earth_neurips_2020_57.pdf). As will be shown below you could start with the MobileNet as give it additional training to differentiate images of monarchs and viceroys with a high degree of precision.\n",
    "5. `meerkat.jpg` ('n02138441', 'meerkat', 0.9629027) Note: Not bad! 96% accurate.\n",
    "6. `paramecium.jpg` ('n01930112', 'nematode', 0.8969467) Note: Not too close. A nematode is round worm such as the common human parasite, the pinworm (_Enterobius_). Apparently, MobileNet wasn't extensively programmed for unicelluar organisms.\n",
    "7. `prairie_dog.jpg` ('n02361337', 'marmot', 0.93886554) Note: Another fail, but again understandable. [Marmots](https://en.wikipedia.org/wiki/Marmot) genus _Marmota_ are quite similar in appearance to pairie dogs, so we can give a 'pass' to MobileNet for getting these two species confused.\n",
    "8. `great-white-shark.jpg` ('n01484850', 'great_white_shark', 0.9989716) Note: Nailed it! 100% correct prediction.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet Summary\n",
    "\n",
    "Overall, our MobileNet neural network did quite well, as long as you picked one of the 1000 image types it supports. For many applications, MobileNet might be entirely acceptable as an image classifier. \n",
    "\n",
    "However, if you need to classify very specialized images, like monarch vs viceroy butterflies, or marmots vs prairie dogs --image types supported by imagenet--it is necessary to use **_transfer learning_**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "\n",
    "## **ResNet vs MobileNet**\n",
    "\n",
    "MobileNet and ResNet are both popular deep learning architectures, but they have some key differences:\n",
    "\n",
    "* **Architecture:** MobileNet uses depthwise separable convolutions, which consist of depthwise convolutions and pointwise convolutions, to reduce the computational cost and make the model more efficient for mobile and embedded applications. ResNet, on the other hand, uses residual blocks with skip connections to learn residual mappings for easier training of deep networks.\n",
    "* **Model size:** MobileNet is known for its lightweight and compact design, making it easy to deploy on mobile devices with limited computational resources. In contrast, ResNet is a deeper network with more parameters, which can lead to higher accuracy but also requires more computational resources.\n",
    "* **Training complexity:** ResNet can be easier to train compared to MobileNet, especially for deeper architectures, due to the use of skip connections that help with gradient flow and alleviate the vanishing gradient problem.\n",
    "* **Performance:** ResNet is often used for tasks that require high accuracy, such as image classification on large datasets like ImageNet. MobileNet is designed for efficiency and speed, making it suitable for real-time applications on mobile devices.\n",
    "\n",
    "Overall, the choice between MobileNet and ResNet depends on the specific requirements of the application, such as computational resources, accuracy goals, and deployment constraints.\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP7lQFZ6f3VM"
   },
   "source": [
    "## Preparing your computer/lap\n",
    "\n",
    "In this section, we will train a neural network to count the number of paper clips in images. If you have already completed Class_06_1 using Jupyter Lab (not COLAB) on the computer/laptop that you are using now, you should have already all of the paper clips images stored in a temporary folder, `temp`. \n",
    "\n",
    "In class you didn't complete Class_06_1 on the computer/laptop you are using now, or you have deleted these images, the following code will create the necessary folders and download the image data. If these folders have already been created and the image data already downloaded and extracted, you will be informed of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set ENVIRONMENTAL VARIABLES\n",
    "\n",
    "The code in the cell below defines a number of ENVIRONMENTAL VARIABLES that are needed for latter code cells. If you are having any problem with this code, uncomment the print statements are re-run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpfadrdQcVg8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set ENVIRONMENTAL VARIABLES\n",
    "\n",
    "URL = \"https://biologicslab.co/BIO1173/data/\"\n",
    "DOWNLOAD_SOURCE = URL+\"paperclips.zip\"\n",
    "DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfind('/')+1:]\n",
    "print(\"DOWNLOAD_SOURCE=\",DOWNLOAD_SOURCE)\n",
    "print(\"DOWNLOAD_NAME=\",DOWNLOAD_NAME)\n",
    "\n",
    "PATH = \"/content\"\n",
    "EXTRACT_TARGET = os.path.join(PATH,\"clips\")\n",
    "SOURCE = os.path.join(EXTRACT_TARGET, \"paperclips\")\n",
    "\n",
    "print(\"ENVIRONMENTAL VARIABLES were successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "~~~text\n",
    "ENVIROMENTAL VARIABLES were sucessfully created.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Extract Image Data\n",
    "\n",
    "The code in the cell below downloads a zip file from the course HTTPS server, `https://biologicslab.co/BIO1173/data`, called `paperclips.zip` and stores this file to your `/temp` folder. The code then extracts (unzips) data in the zip file into a new folder called `/clips`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CExT2Z6gpAhz",
    "outputId": "44069f67-c041-45da-a4ab-67f5c135c1df"
   },
   "outputs": [],
   "source": [
    "# Download and extract the image data\n",
    "!wget -O {os.path.join(PATH,DOWNLOAD_NAME)} {DOWNLOAD_SOURCE}\n",
    "!mkdir -p {SOURCE}\n",
    "!mkdir -p {TARGET}\n",
    "!mkdir -p {EXTRACT_TARGET}\n",
    "!unzip -o -j -d {SOURCE} {os.path.join(PATH, DOWNLOAD_NAME)} >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PX4uUN5DkuYH"
   },
   "source": [
    "### Load the Labels for the Training Set\n",
    "\n",
    "The labels are contained in a CSV file named **train.csv** for the regression. This file has just two labels, **id** and **clip_count**. The ID specifies the filename; for example, row id 1 corresponds to the file **clips-1.jpg**. The following code loads the labels for the training set and creates a new column, named **filename**, that contains the filename of each image, based on the **id** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4W4LeGSqYya",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the labels for the training set\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "        os.path.join(SOURCE,\"train.csv\"), \n",
    "        na_values=['NA', '?'])\n",
    "    \n",
    "df['filename']=\"clips-\"+df[\"id\"].astype(str)+\".jpg\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGxjITNtz0sC"
   },
   "source": [
    "This results in the following dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "rJpaMpmfswIp",
    "outputId": "c6462003-f016-4f0c-b8a8-81f276b9b3f4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following table:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_06/class_06_2_df.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split images into training and validation sets\n",
    "\n",
    "The code in the cell below, splits the paperclips images into training set and a validation set, with 90% of the images going into the training set. The number images in both sets is printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CI4_qNaSqp31",
    "outputId": "abe450db-ce00-44ac-cbbd-166912687df4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split images into training and validation sets\n",
    "\n",
    "# create dataframes to data\n",
    "df_train = pd.read_csv(os.path.join(SOURCE, \"train.csv\"))\n",
    "df_train['filename'] = \"clips-\" + df_train.id.astype(str) + \".jpg\"\n",
    "\n",
    "TRAIN_PCT = 0.9\n",
    "TRAIN_CUT = int(len(df) * TRAIN_PCT)\n",
    "\n",
    "df_train = df[0:TRAIN_CUT]\n",
    "df_validate = df[TRAIN_CUT:]\n",
    "\n",
    "print(f\"Training size: {len(df_train)}\")\n",
    "print(f\"Validate size: {len(df_validate)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "~~~text\n",
    "Training size: 18000\n",
    "Validate size: 2000\n",
    "~~~\n",
    "We want to use early stopping. To do this, we need a validation set. We will break the data into 80 percent test data and 20 validation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP7lQFZ6f3VM"
   },
   "source": [
    "## Transfer Learning using `ResNet`\n",
    "We will make use of the structure of the **_ResNet_** neural network. There are several significant changes that we will make to ResNet to apply to this task. First, ResNet is a classifier; we wish to perform a regression to count. Secondly, we want to change the image resolution that ResNet uses. We will not use the weights from ResNet; changing this resolution invalidates the current weights. Thus, it will be necessary to retrain the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj_umT9VgRzI"
   },
   "source": [
    "Next, we create the generators that will provide the images to the neural network during training. We normalize the images so that the RGB colors between 0-255 become ratios between 0 and 1. We also use the **flow_from_dataframe** generator to connect the Pandas dataframe to the actual image files. We see here a straightforward implementation; you might also wish to use some of the image transformations provided by the data generator.\n",
    "\n",
    "The **HEIGHT** and **WIDTH** constants specify the dimensions to which the image will be scaled (or expanded). It is probably not a good idea to expand the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Keras package\n",
    "\n",
    "!pip install keras_preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYiMObEJsoof",
    "outputId": "d2043497-d895-441e-9fd4-19374f01135b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "  rescale = 1./255,\n",
    "  horizontal_flip=True,\n",
    "  #vertical_flip=True,\n",
    "  fill_mode='nearest')\n",
    "\n",
    "train_generator = training_datagen.flow_from_dataframe(\n",
    "        dataframe=df_train_cut,\n",
    "        directory=SOURCE,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"clip_count\",\n",
    "        target_size=(HEIGHT, WIDTH),\n",
    "        # Keeping the training batch size small \n",
    "        # USUALLY increases performance\n",
    "        batch_size=32, \n",
    "        class_mode='raw')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "val_generator = validation_datagen.flow_from_dataframe(\n",
    "        dataframe=df_validate_cut,\n",
    "        directory=SOURCE,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"clip_count\",\n",
    "        target_size=(HEIGHT, WIDTH),\n",
    "        # Make the validation batch size as large as you \n",
    "        # have memory for\n",
    "        batch_size=256, \n",
    "        class_mode='raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "~~~text\n",
    "Found 18000 validated image filenames.\n",
    "Found 2000 validated image filenames.\n",
    "~~~\n",
    "This means that our train and validation generator are working properly and know where to find where in your filesystem your images of paperclips are stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rVZJBMIgqJq"
   },
   "source": [
    "### Example 2: Transfer Learning with ResNet\n",
    "\n",
    "We will now use a ResNet neural network as a basis for our neural network. We will redefine both the input shape and output of the ResNet model, so we will not transfer the weights. Since we redefine the input, the weights are of minimal value. We begin by loading, from Keras, the ResNet50 network. We specify **include_top** as False because we will change the input resolution. We also specify **weights** as false because we must retrain the network after changing the top input layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Step 1 - Redefine the input shape\n",
    "\n",
    "The first modification of the base ResNet model `ResNet50` that we need to make is to redefine the image shape to 256 X 256 pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MJpmLyhtahJ"
   },
   "outputs": [],
   "source": [
    "# Example 2: Step 1 - Redefine the input shape\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# Set variables\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "\n",
    "input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "\n",
    "base_model = ResNet50(\n",
    "    include_top=False, weights=None, input_tensor=input_tensor,\n",
    "    input_shape=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xF9EKVFhAGl"
   },
   "source": [
    "### Example 2: Step 2 - Add layers to convert model to regression\n",
    "\n",
    "Now we must add a few layers to the end of the neural network so that it becomes a regression model. As you should expect for a regression model, there is only a single neuron in the output layer:\n",
    "~~~text\n",
    "model=Model(inputs=base_model.input,outputs=Dense(1)(x))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXO3lTFIs_U1"
   },
   "outputs": [],
   "source": [
    "# Example 2: Step 2 - Add layers to convert model to regression\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x) \n",
    "x=Dense(1024,activation='relu')(x) \n",
    "model=Model(inputs=base_model.input,outputs=Dense(1)(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErCUZTv1hJDx"
   },
   "source": [
    "### Example 2: Step 3 - Train the model\n",
    "\n",
    "In the cell below, we provide **_additional_** training to the base `ResNet50` model by \"showing it\" 16200 test images with their labels (i.e. how many paperclips are in the image) using a 1800 validation image set to allow us to have EarlyStopping. \n",
    "\n",
    "Training is like before. However, we do not define the entire neural network here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nonHOozTtpD3",
    "outputId": "2a54ad35-0394-4730-85dc-d6fcf7778a4a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 2: Step 3 - Train the model\n",
    "import time\n",
    "\n",
    "# Set variables\n",
    "EPOCHS=1 #00\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "# Important, calculate a valid step size for the validation dataset\n",
    "STEP_SIZE_VALID=val_generator.n//val_generator.batch_size\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer='adam', \n",
    "              metrics=[RootMeanSquaredError(name=\"rmse\")])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "        patience=50, verbose=1, mode='auto',\n",
    "        restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_generator, epochs=EPOCHS, steps_per_epoch=250, \n",
    "                    validation_data = val_generator, callbacks=[monitor],\n",
    "                    verbose = 1, validation_steps=STEP_SIZE_VALID)\n",
    "\n",
    "# Print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXHwP_hXwKwZ"
   },
   "source": [
    "Training will require a significant amount of time. Even with T4 GPU acceleration, it make require more than 1 hour to complete all 100 epochs if `EarlyStopping` doesn't kick in. \n",
    "\n",
    "If we wanted to,  we could zip-up the preprocessed files and store them somewhere for later use if we needed a trained neural neural network to count the number of paperclips on a piece of paper. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lesson Turn-in**\n",
    "\n",
    "When you have completed all of the code cells, and run them in sequential order (the last code cell should be number 32) use the **File --> Print.. --> Save to PDF** to generate a PDF of your JupyterLab notebook. Save your PDF as `Class_06_2.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
