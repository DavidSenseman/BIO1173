{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEEW13uCtiJ"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/master/Class_05_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BIO 1173: Intro Computational Biology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module 5: Regularization and Dropout**\n",
    "\n",
    "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
    "\n",
    "### Module 5 Material\n",
    "\n",
    "* Part 5.1: Part 5.1: Introduction to Regularization: Ridge and Lasso\n",
    "* Part 5.2: Using K-Fold Cross Validation with Keras\n",
    "* Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting\n",
    "* Part 5.4: Drop Out for Keras to Decrease Overfitting\n",
    "* **Part 5.5: Benchmarking Keras Deep Learning Regularization Techniques**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Setup\n",
    "\n",
    "Run the next code cell to load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You MUST run this code cell first\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "path = '/'\n",
    "memory = shutil.disk_usage(path)\n",
    "dirpath = os.getcwd()\n",
    "print(\"Your current working directory is : \" + dirpath)\n",
    "print(\"Disk\", memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seXFCYH4LDUM",
    "outputId": "c05015aa-871e-4779-9265-5ad07e8bf617",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You must run this cell second\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "    import requests\n",
    "    gcloud_token = !gcloud auth print-access-token\n",
    "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
    "    print(gcloud_tokeninfo['email'])\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcplr2pVkn4S"
   },
   "source": [
    "# Part 5.5: Benchmarking Regularization Techniques\n",
    "\n",
    "Quite a few hyperparameters have been introduced so far.  Tweaking each of these values can have an effect on the score obtained by your neural networks.  Some of the hyperparameters seen so far include:\n",
    "\n",
    "* Number of layers in the neural network\n",
    "* How many neurons in each layer\n",
    "* What activation functions to use on each layer\n",
    "* Dropout percent on each layer\n",
    "* L1 and L2 values on each layer\n",
    "\n",
    "To try out each of these hyperparameters you will need to run train neural networks with multiple settings for each hyperparameter.  However, you may have noticed that neural networks often produce somewhat different results when trained multiple times.  This is because the neural networks start with random weights.  Because of this it is necessary to fit and evaluate a neural network several times to ensure that one set of hyperparameters are actually better than another.  **_Bootstrapping_** can be an effective means of benchmarking (comparing) two sets of hyperparameters.  \n",
    "\n",
    "Bootstrapping is similar to cross-validation.  Both go through a number of cycles/folds providing validation and training sets.  However, bootstrapping can have an unlimited number of cycles.  Bootstrapping chooses a new train and validation split each cycle, with **_replacement_**.  The fact that each cycle is chosen with replacement means that, unlike cross validation, there will often be repeated rows selected between cycles.  If you run the bootstrap for enough cycles, there will be duplicate cycles.\n",
    "\n",
    "In this part we will use bootstrapping for **_hyperparameter benchmarking_**.  We will train a neural network for a specified number of splits (denoted by the SPLITS constant).  For these examples we use 100.  We will compare the average score at the end of the 100th epoch.  By the end of the cycles the mean score will have converged somewhat.  This ending score will be a much better basis of comparison than a single cross-validation.  Additionally, the average number of epochs will be tracked to give an idea of a possible optimal value.  Because the early stopping validation set is also used to evaluate the the neural network as well, it might be slightly inflated.  This is because we are both stopping and evaluating on the same sample.  However, we are using the scores only as relative measures to determine the superiority of one set of hyperparameters to another, so this slight inflation should not present too much of a problem.\n",
    "\n",
    "Because we are benchmarking, we will display the amount of time taken for each cycle.  The following function can be used to nicely format a time span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for this lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_xChO0Fkn4S"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOBKcIUzkn4T"
   },
   "source": [
    "## Bootstrapping for Regression\n",
    "\n",
    "Regression bootstrapping uses the **ShuffleSplit** object to perform the splits.  This technique is similar to **KFold** for cross-validation; no balancing occurs.  We will attempt to predict the `age` column for the [Body Performance Dataset](https://www.kaggle.com/datasets/kukuroo3/body-performance-data) that we have used in previous lessons (e.g. Class_05_2, Class_05_3). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Body Performance Dataset\n",
    "\n",
    "The dataset used for this lesson is the [Body Performance Dataset](https://www.kaggle.com/datasets/kukuroo3/body-performance-data) that we have used in previous lessons (e.g. Class_05_2, Class_05_3). \n",
    "\n",
    "This dataset has the following 12 categories:\n",
    "\n",
    "* **age:** 20 ~64\n",
    "* **gender:** M,F\n",
    "* **height_cm:** (If you want to convert to feet, divide by 30.48)\n",
    "* **weight_kg:**\n",
    "* **body fat_%:**\n",
    "* **diastolic:** diastolic blood pressure (min)\n",
    "* **systolic:** systolic blood pressure (min)\n",
    "* **gripForce:**\n",
    "* **sit and bend forward_cm:**\n",
    "* **sit-ups counts:**\n",
    "* **broad jump_cm:**\n",
    "* **class:** A,B,C,D ( A: best) / stratified\n",
    "\n",
    "There are only two columns that are non-numeric (i.e. contain string values): `gender` and `class`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1A: Create feature vector\n",
    "\n",
    "The code in the cell below reads the Body Performance dataset, `bodyPerformance.csv` from the course HTTPS server and creates a new DataFrame called `dfBig`. To speedup training, only 15% of `dfBig` is used (about 2,000 subjects) to create the DataFrame,`df`. \n",
    "\n",
    "In Example 1, the objective is to create a **_regression_** neural network that can predict the `age` of a subject (i.e., the values in the column `age` are the y-values).  \n",
    "\n",
    "To create our feature vection, we need to convert any string values to numbers. The column `gender` is mapped (`M`=`0`,`F`=`1`) while the column `classes` is One-Hot encoded. \n",
    "\n",
    "In this lesson only the data in the columns `height_cm`, `weight_kg`, `diastolic`, `systolic` and `gripForce` are standardized. \n",
    "\n",
    "All of the columns, except `age`, are used for creating the x-value variable. The y-values are generated directly from the values in the column `age`. Both the x-values and the y-values must be converted to type `float32` to avoid errors during training.\n",
    "\n",
    "As a final check, the `ages` of the first 10 subjects are printed out using the \"star\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWKzWCRskn4T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1A: Create feature vector\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Read the data set\n",
    "dfBig = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/bodyPerformance.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Only use 15% for neural network\n",
    "df=dfBig.sample(frac=0.15, random_state=2)\n",
    "\n",
    "# Map Gender\n",
    "mapping =  {'M': 0,\n",
    "            'F': 1}\n",
    "df['gender'] = df['gender'].map(mapping)\n",
    "\n",
    "# Generate dummies for class\n",
    "df = pd.concat([df,pd.get_dummies(df['class'],prefix=\"class\")],axis=1)\n",
    "df.drop('class', axis=1, inplace=True)\n",
    "\n",
    "# Standardize ranges\n",
    "df['height_cm'] = zscore(df['height_cm'])\n",
    "df['weight_kg'] = zscore(df['weight_kg'])\n",
    "df['diastolic'] = zscore(df['diastolic'])\n",
    "df['systolic'] = zscore(df['systolic'])\n",
    "df['gripForce'] = zscore(df['gripForce'])\n",
    "\n",
    "# Generate list of columns for x\n",
    "x_columns = df.columns.drop('age')  # `age` is the y-value \n",
    "\n",
    "# Generate x-values using x-columns list\n",
    "x = df[x_columns].values\n",
    "# Convert x-values to float 32\n",
    "x = np.asarray(x).astype('float32')\n",
    "\n",
    "# Generate y-values\n",
    "y = df['age'].values\n",
    "# Convert y-values to float 32\n",
    "y = np.asarray(y).astype('float32')\n",
    "\n",
    "# Print y categorical names\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:\n",
    "\n",
    "~~~text\n",
    "[28. 23. 60. 27. 25. 51. 37. 34. 43. 24.]\n",
    "~~~\n",
    "\n",
    "These are the ages of the first 10 subjects in the y-value variable, `y`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mHk4XHLkn4T"
   },
   "source": [
    "### Example 1B: Bootstrapping for Regression\n",
    "\n",
    "The following code performs the bootstrap. The architecture of the neural network can be adjusted to compare many different configurations. The code below uses a very basic architecture with only 2 hidden layers, and no dropout layers. \n",
    "\n",
    "The number of times the code \"loops\" is defined by the variable `SPLITS`. In the example below, `SPLITS=5`. During each loop, the neural network will train for a maximal value specified by the variable `EPOCHS`. In this example, EPOCHS is set to `1000`. However, since the code also implements `EarlyStopping` the number of epochs actually run will usually be less than `100`. \n",
    "\n",
    "As mentioned above, the uses the `ShuffleSplit()` object to perform the splits for the bootstrapping. This technique is similar to _K_-Fold for cross-validation in the sense that no balancing occurs. \n",
    "\n",
    "The specific code that generates the `bootstrapping` is provided by the code chunk:\n",
    "\n",
    "~~~text\n",
    "# Bootstrap\n",
    "boot = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=42)\n",
    "~~~\n",
    "\n",
    "Keep in mind that bootstrapping chooses a **_new_** train and validation split each cycle, with **_replacement_**.  The fact that each cycle is chosen with replacement means that, unlike cross validation, there will often be **_repeated rows_** selected between cycles.  If you run the bootstrap for enough cycles, there will be duplicate cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WbhxDX1rkn4U",
    "outputId": "45488bc1-c828-42ec-ebb5-f5b569fa96d0"
   },
   "outputs": [],
   "source": [
    "# Example 1B: Bootstrapping for regression\n",
    "\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Set variables\n",
    "SPLITS = 5\n",
    "EPOCHS = 1000\n",
    "\n",
    "# Record start time\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Bootstrap\n",
    "boot = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=42)\n",
    "\n",
    "# Track progress\n",
    "mean_benchmark = []\n",
    "epochs_needed = []\n",
    "num = 0\n",
    "\n",
    "print(f\"STARTING {SPLITS} BOOTSTRAP CYCLES...\")\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "# Loop through samples------------------------------------------#\n",
    "\n",
    "for train, test in boot.split(x): # loops = number of splits\n",
    "    start_time = time.time()\n",
    "    num+=1\n",
    "    \n",
    "    # Split train and test \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "\n",
    "    # Construct and compile new neural network each loop\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=x_train.shape[1], \n",
    "                    activation='relu')) # Hidden 1\n",
    "    model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "    model.add(Dense(1)) # Output only 1 neuron for regression\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Create monitor for early stopping\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "        patience=15, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "    # Train on the current bootstrap sample\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
    "              callbacks=[monitor],verbose=0,epochs=EPOCHS)\n",
    "    epochs = monitor.stopped_epoch\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "    # Predict on the out of boot (validation)\n",
    "    pred = model.predict(x_test)\n",
    "  \n",
    "    # Measure this bootstrap's log loss\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    mean_benchmark.append(score)\n",
    "    m1 = statistics.mean(mean_benchmark)\n",
    "    m2 = statistics.mean(epochs_needed)\n",
    "    print(f\"mean_benchmark={mean_benchmark}\")\n",
    "    #mdev = statistics.pstdev(mean_benchmark)\n",
    "    \n",
    "    # Record this iteration\n",
    "    time_took = time.time() - start_time\n",
    "    print(f\"#{num}: score={score:.6f}, mean score={m1:.6f},\"\n",
    "          f\" epochs={epochs}, mean epochs={int(m2)}\", \n",
    "          f\" time={hms_string(time_took)}\")\n",
    "\n",
    "# End Loop-----------------------------------------------------------------#\n",
    "\n",
    "# Print Final elapsed time\n",
    "elapsed_time = time.time() - total_start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "~~~text\n",
    "STARTING 5 BOOTSTRAP CYCLES...\n",
    "----------------------------------------\n",
    "7/7 [==============================] - 0s 1ms/step\n",
    "mean_benchmark=[8.469069]\n",
    "#1: score=8.469069, mean score=8.469069, epochs=250, mean epochs=250  time=0:00:56.85\n",
    "7/7 [==============================] - 0s 1ms/step\n",
    "mean_benchmark=[8.469069, 9.062882]\n",
    "#2: score=9.062882, mean score=8.765976, epochs=290, mean epochs=270  time=0:01:06.42\n",
    "7/7 [==============================] - 0s 1ms/step\n",
    "mean_benchmark=[8.469069, 9.062882, 7.268235]\n",
    "#3: score=7.268235, mean score=8.266728, epochs=299, mean epochs=279  time=0:01:08.90\n",
    "7/7 [==============================] - 0s 1ms/step\n",
    "mean_benchmark=[8.469069, 9.062882, 7.268235, 8.557194]\n",
    "#4: score=8.557194, mean score=8.339345, epochs=391, mean epochs=307  time=0:01:30.51\n",
    "7/7 [==============================] - 0s 1ms/step\n",
    "mean_benchmark=[8.469069, 9.062882, 7.268235, 8.557194, 9.046312]\n",
    "#5: score=9.046312, mean score=8.480739, epochs=397, mean epochs=325  time=0:01:31.29\n",
    "Elapsed time: 0:06:13.98\n",
    "~~~\n",
    "\n",
    "You should notice that there are a **_variety_** of `epochs`, `mean epochs` and `times` reported in the output above. While your output will be different, it should also show some degree of variety. \n",
    "\n",
    "This variety is an integral part of the `boostrapping` process which **_randomly_** splits the x and y data into _n_ different \"chunks\" where _n_=number of splits. As you should expect, in splits where EarlyStopping caused the training to terminate after a lower number of epochs, the `time` for that split will also be lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIFZhOuykn4U"
   },
   "source": [
    "## Bootstrapping for Classification\n",
    "\n",
    "While the code for Bootstrapping for Classification is similar the code above used for regression, we need to use different code for creating the boostrap object. As we saw in _K_-fold cross-validation, it is important, when the goal is classification, to **_balance the classes_** when generating the splits. \n",
    "\n",
    "\"Balancing\" isn't necessary when performing regression. As long as the data is shuffled, any data sample should have roughtly the same distribution of high, low and medium values. But this is not necessarily true when it come to classification, especially if the number of items/subjects/patients in each class is significantly different. Even if the data is shuffled, there is some possibility that the random selection might inadvertly include too many samples from one class and not enough samples from a another class. \n",
    "\n",
    "For example, imagine a dataset that contains the blood types of subjects who are `white`, `black` or `Asian`. And suppose that the number of `Asian` subjects is significantly smaller that the number of `white` subjects. If you randomly selected a subset of this dataset (what is called **Simple Random Sampling** in statistics), you might end up with a sample with very few or even no `Asian` subjects, and a disproportionate high number of `white` subjects. Training your neural network on this unbalanced subset sample would give you very bad results when tested later on a more general population sample. \n",
    "\n",
    "In statistics, the way to avoid this problem is called **_Stratified Random Sampling_**. The population is divided into groups (strata), and members are randomly chosen from each group. This approach ensures really equal representation from every group (e.g., blood types of `white`, `black` or `Asian` subjects). \n",
    "\n",
    "Our regression bootstrapping, in Example 1, just used the **ShuffleSplit()** class to perform the splits. But in buiding our classification neural network below, we will need to use instead, the **StratifiedShuffleSplit** class to perform the splits.  This class is similar to **StratifiedKFold** for cross-validation, as the classes are balanced so that the sampling does not affect proportions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1A: Create feature vector**\n",
    "\n",
    "In the cell below, create a feature vector from the Body Performance dataset that can be used in a classification neural network. For the most part, the code for **Exercise 1A** should be **_exactly_** the same as that used in Example 1A, with the few exceptions noted below. \n",
    "\n",
    "The main difference in preparing your feature vector will be the column you use to create your y-values. In Example 1, the y-values were `age`. Using 'age' for a regression neural network but not for a classification neural network. \n",
    "\n",
    "For **Exercise 1** you will the y-values that are in the column `class`, that assigns a different performance level to each subject. In other words, you are building a neural network that can classify a subject's athletic \"classe\", `A`, `B`, `C` and `D`, based on their performance.  \n",
    "\n",
    "You should use the same code that was shown in Example 1A, but you will need to make the following changes: \n",
    "\n",
    "1. When you create your x column list use this code:\n",
    "~~~text\n",
    "# Generate list of columns for x\n",
    "x_columns = df.columns.drop('class')  # `class` is y-value \n",
    "~~~\n",
    "Since `class` is your y-value, you don't want it included with the dependent (x) variables.\n",
    "\n",
    "2. Do **not** use this code to create dummies for the column `class`:\n",
    "~~~text\n",
    "# Generate dummies for class\n",
    "df = pd.concat([df,pd.get_dummies(df['class'],prefix=\"class\")],axis=1)\n",
    "df.drop('class', axis=1, inplace=True)\n",
    "~~~\n",
    "\n",
    "Instead, use this code to One-Hot encode the column `class`:\n",
    "~~~text\n",
    "# One-Hot encode y-values for classification \n",
    "dummies = pd.get_dummies(df['class']) # Classification\n",
    "categories = dummies.columns\n",
    "y = dummies.values\n",
    "y = np.asarray(y).astype('float32')\n",
    "~~~\n",
    "\n",
    "3. At the end of the cell, print out the y-categories using this code chunk:\n",
    "~~~text\n",
    "# Print y categorical names\n",
    "print(*categories)\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWKzWCRskn4T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 1A here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output.\n",
    "~~~text\n",
    "A B C D\n",
    "~~~\n",
    "\n",
    "These are the four \"classes\" of performance that your neural network will predict based on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mHk4XHLkn4T"
   },
   "source": [
    "### **Exercise 1B: Bootstrapping for Classification**\n",
    "\n",
    "In the cell below, write the code needed to create a classification neural network that uses bootstrapping. As wth Part A above, your code should be **_exactly_** the same as that used in Example 1B, except with the following exceptions:\n",
    "\n",
    "Specific code differences:\n",
    "\n",
    "1. Do **not** use this code to define the bootstrap:\n",
    "~~~text\n",
    "# Bootstrap\n",
    "boot = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=42)\n",
    "~~~\n",
    "\n",
    "Instead use this code to define the bootstrap:\n",
    "~~~text\n",
    "# Bootstrap\n",
    "boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1, \n",
    "                                random_state=42)\n",
    "~~~\n",
    "Remember, you need to use the stratified version to make sure each class is equally represented. \n",
    "\n",
    "2. Do **not** use this code to contruct your neural network:\n",
    "~~~text\n",
    "    # Construct and compile new neural network each loop\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=x_train.shape[1], \n",
    "                    activation='relu')) # Hidden 1\n",
    "    model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "    model.add(Dense(1)) # Output only 1 neuron for regression\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "~~~\n",
    "\n",
    "Instead use this code to construct your neural network:\n",
    "~~~text\n",
    "    # Construct neural network for this loop\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "    model.add(Dense(25, activation='relu')) # Hidden 2\n",
    "    model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "~~~\n",
    "\n",
    "\n",
    "3. In the section called `# Measure this bootstrap's log loss`, uncomment the line:\n",
    "~~~text\n",
    "#mdev = statistics.pstdev(mean_benchmark)\n",
    "~~~\n",
    "\n",
    "It should now read:\n",
    "~~~text\n",
    "mdev = statistics.pstdev(mean_benchmark)\n",
    "~~~\n",
    "\n",
    "4. Do **not** use the code in the section called `# Record this iteration`\n",
    "~~~text\n",
    "    # Record this iteration\n",
    "    time_took = time.time() - start_time\n",
    "    print(f\"#{num}: score={score:.6f}, mean score={m1:.6f},\"\n",
    "          f\" epochs={epochs}, mean epochs={int(m2)}\", \n",
    "          f\" time={hms_string(time_took)}\")\n",
    "~~~\n",
    "\n",
    "Instead, use this code section:\n",
    "~~~text\n",
    "    # Record this iteration\n",
    "    time_took = time.time() - start_time\n",
    "    print(f\"#{num}: score={score:.6f}, mean score={m1:.6f},\" +\\\n",
    "          f\"stdev={mdev:.6f}, epochs={epochs}, mean epochs={int(m2)},\" +\\\n",
    "          f\" time={hms_string(time_took)}\")\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQSViKNJkn4U"
   },
   "source": [
    "We now run this data through a number of splits specified by the SPLITS variable. We track the average error through each of these splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzu6bqDukn4U",
    "outputId": "583d468c-5d89-45e1-9353-3cfcfc795ae7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 1B here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:\n",
    "\n",
    "~~~text\n",
    "STARTING 5 BOOTSTRAP CYCLES...\n",
    "----------------------------------------\n",
    "7/7 [==============================] - 0s 2ms/step\n",
    "#1: score=0.780601, mean score=0.780601,stdev=0.000000, epochs=95, mean epochs=95, time=0:00:22.10\n",
    "7/7 [==============================] - 0s 1ms/step\n",
    "#2: score=0.949405, mean score=0.865003,stdev=0.084402, epochs=71, mean epochs=83, time=0:00:16.87\n",
    "7/7 [==============================] - 0s 1ms/step\n",
    "#3: score=0.984252, mean score=0.904753,stdev=0.088933, epochs=76, mean epochs=80, time=0:00:17.98\n",
    "7/7 [==============================] - 0s 1ms/step\n",
    "#4: score=0.909016, mean score=0.905819,stdev=0.077041, epochs=39, mean epochs=70, time=0:00:09.49\n",
    "7/7 [==============================] - 0s 1ms/step\n",
    "#5: score=0.953406, mean score=0.915336,stdev=0.071488, epochs=106, mean epochs=77, time=0:00:24.14\n",
    "Elapsed time: 0:01:30.60\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHxvxKl0kn4V"
   },
   "source": [
    "## **Exercise/Example 2: Benchmarking**\n",
    "\n",
    "Now that we've seen how to bootstrap with both classification and regression, we can start to try to **_optimize_** the hyperparameters for the **Body Performance** data.  For this example, we will encode for classification of the `class` column.  Evaluation will be in log loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2: Create feature vector**\n",
    "\n",
    "Here we are going to change things around a little bit. Instead of giving you an example followed by an exercise, here you will be given **Exercise 2** first followed by Example 2. The \"catch\" is that Example 2 won't run correctly unless your **Execise 2** runs correctly!\n",
    "\n",
    "For Exercise 2, you are to create a feature vector from the Body Performance dataset that can be used in a classification neural network. The code for **Exercise 2A** should be **_exactly_** the same as that used in **Exercise 1A** above. As in **Exercise 1A**, the y-values will be in the column `class`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWKzWCRskn4T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 2 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "~~~text\n",
    "A B C D\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Optimized Hyperparameters\n",
    "\n",
    "We end this lesson with the following example in which some optimization has been performed on the code in the cell below. The code has the best settings that could be manually determined. Later in this course, we will see how we can use an automatic process to optimize the hyperparameters.\n",
    "\n",
    "The architecture of this semi-optimized neural network has 3 hidden layers with 2 dropout layers, one after the first and one after the second hidden layer. Since the objective is classification, the bootstrap object is created using `StratifiedShuffleSplit()` to insure balanced classes.  \n",
    "\n",
    "L2 regularizers are also used to reduce the effects of overfitting by adding a penalty to the synaptic weigths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nk4w1p42kn4V",
    "outputId": "9b6c41e5-58e2-482f-c69f-bdfaf4e9172a"
   },
   "outputs": [],
   "source": [
    "# Example 2: Optimized Hyperparametes\n",
    "\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tensorflow.keras.layers import LeakyReLU,PReLU\n",
    "\n",
    "# Set variables\n",
    "SPLITS = 5\n",
    "EPOCHS = 1000\n",
    "\n",
    "# Record start time\n",
    "total_start_time = time.time()\n",
    "\n",
    "print(f\"STARTING {SPLITS} BOOTSTRAP CYCLES...\")\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "# Bootstrap\n",
    "boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1)\n",
    "\n",
    "# Track progress\n",
    "mean_benchmark = []\n",
    "epochs_needed = []\n",
    "num = 0\n",
    "\n",
    "# Loop through samples--------------------------------------------------#\n",
    "\n",
    "for train, test in boot.split(x,df['class']):\n",
    "    start_time = time.time()\n",
    "    num+=1\n",
    "\n",
    "    # Split train and test\n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "\n",
    "    # Construct neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=x.shape[1], activation=PReLU(), \\\n",
    "        kernel_regularizer=regularizers.l2(1e-4))) # Hidden 1\n",
    "    model.add(Dropout(0.5)) # Add dropout after Hidden 1\n",
    "    model.add(Dense(100, activation=PReLU(), \\\n",
    "        activity_regularizer=regularizers.l2(1e-4))) # Hidden 2\n",
    "    model.add(Dropout(0.5)) # Add dropout after Hidden 2\n",
    "    model.add(Dense(100, activation=PReLU(), \\\n",
    "        activity_regularizer=regularizers.l2(1e-4))) # Hidden 3\n",
    "    model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "\n",
    "    # Compile model using categorical cross entropy for loss\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    # Create Early Stopping monitor\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "        patience=100, verbose=0, mode='auto', restore_best_weights=True)\n",
    "\n",
    "    # Train on the bootstrap sample\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test), \\\n",
    "              callbacks=[monitor],verbose=0,epochs=EPOCHS)\n",
    "    \n",
    "    epochs = monitor.stopped_epoch\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "    # Predict on the out of boot (validation)\n",
    "    pred = model.predict(x_test)\n",
    "  \n",
    "    # Measure this bootstrap's log loss\n",
    "    y_compare = np.argmax(y_test,axis=1) # For log loss calculation\n",
    "    score = metrics.log_loss(y_compare, pred)\n",
    "    mean_benchmark.append(score)\n",
    "    m1 = statistics.mean(mean_benchmark)\n",
    "    m2 = statistics.mean(epochs_needed)\n",
    "    mdev = statistics.pstdev(mean_benchmark)\n",
    "    \n",
    "    # Record this iteration\n",
    "    time_took = time.time() - start_time\n",
    "    print(f\"#{num}: score={score:.6f}, mean score={m1:.6f},\"\n",
    "          f\"stdev={mdev:.6f}, epochs={epochs},\"\n",
    "          f\"mean epochs={int(m2)}, time={hms_string(time_took)}\")\n",
    "\n",
    "# End Loop------------------------------------------------------------------#\n",
    "\n",
    "# Record end time\n",
    "total_end_time = time.time()\n",
    "print(f\"Total Elapsed time = {hms_string(total_end_time - total_start_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "If the code is correct you should see something similar to the following output:\n",
    "\n",
    "~~~text\n",
    "STARTING 5 BOOTSTRAP CYCLES...\n",
    "----------------------------------------\n",
    "7/7 [==============================] - 0s 2ms/step\n",
    "#1: score=0.803877, mean score=0.803877,stdev=0.000000, epochs=550,mean epochs=550, time=0:03:04.93\n",
    "7/7 [==============================] - 0s 2ms/step\n",
    "#2: score=0.889108, mean score=0.846493,stdev=0.042615, epochs=252,mean epochs=401, time=0:01:47.02\n",
    "7/7 [==============================] - 0s 2ms/step\n",
    "#3: score=0.881665, mean score=0.858217,stdev=0.038544, epochs=462,mean epochs=421, time=0:03:13.05\n",
    "7/7 [==============================] - 0s 2ms/step\n",
    "#4: score=0.849627, mean score=0.856070,stdev=0.033587, epochs=353,mean epochs=404, time=0:02:01.76\n",
    "7/7 [==============================] - 0s 2ms/step\n",
    "#5: score=0.891693, mean score=0.863194,stdev=0.033249, epochs=286,mean epochs=380, time=0:01:59.55\n",
    "Total Elapsed time = 0:12:06.31\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lesson Turn-in**\n",
    "\n",
    "When you have completed all of the code cells, and run them in sequential order (the last code cell should be number 9) use the **File --> Print.. --> Save to PDF** to generate a PDF of your JupyterLab notebook. Save your PDF as `Class_05_5.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tf-sum24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
