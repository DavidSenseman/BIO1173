{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/SP25_Assigment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdRFfd32xmjj"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_CU7EF2xmjk"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sRFBydyxmjk"
      },
      "source": [
        "**Assignment 1: Neural Networks for Analysis of Tabular Data**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Change your Runtime Now**\n",
        "\n",
        "Before you can begin, you will need to change your Runtime from CPU to an accelerated **GPU Runtime**. This assignment will **not** run on a TPU, only with GPU acceleration.\n",
        "\n",
        "Assignment_02 has been validated to run on the `A100`, `L4` and the `T4` GPUs."
      ],
      "metadata": {
        "id": "q5CiYa2y1sNh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfyIfxOVjw-4"
      },
      "source": [
        "# **READ CAREFULLY**\n",
        "\n",
        "The **_second_**  digit in your myUTSA ID (e.g. \"abc123\") will determine which dataset you are to analyze for this assignment. For example, if your myUTSA ID was **vue682**, then your second digit is the number `8`.\n",
        "\n",
        "**---WARNING------WARNING------WARNING------WARNING------WARNING------WARNING---**\n",
        "\n",
        "You are **not** free to choose any dataset for this assignment. If analyze the wrong dataset, your assignment will **NOT BE GRADED** and you will receive a `0`.\n",
        "\n",
        "If you are uncertain which dataset you should be working on, contact your Instructor for help. Remember, your score in this assignment will have a large impact on your course grade so please be careful.\n",
        "\n",
        "\n",
        "| **2nd Digit myUTSA ID** | **Dataset to Analyze**  | **Datafile Name**\n",
        "--------------------------|-------------------------|-----------------\n",
        "0                         | DermaMNIST              | dermamnist_64.npz\n",
        "1                         | OCTMNIST                | octmnist.npz\n",
        "2                         | PneumoniaMNIST          | pneumoniamnist_64.npz\n",
        "3                         | BreastMNIST             | breastmnist_224.npz\n",
        "4                         | PathMNIST               | pathmnist_128.npz\n",
        "5                         | RetinaMNIST             | retinamnist_128.npz\n",
        "6                         | TissueMNIST             | tissuemnist.npz\n",
        "7                         | OrganMNIST - A          | organamnist.npz\n",
        "8                         | OrganMNIST - C          | organcmnist.npz\n",
        "9                         | OrganMNIST - S          | organsmnist.npz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfs64u4eDRpD"
      },
      "source": [
        "# **The Purpose of Assignments**\n",
        "\n",
        "In this course, **_Assignments_** are designed to help me (and you) assess your ability to transfer knowledge gained in completing class coding exercises to solving more realistic problems.\n",
        "\n",
        "Assignments play a pivotal role in reinforcing your learning, as they require you to apply theoretical concepts to practical scenarios. This helps solidify your understanding and enhances your problem-solving skills. By tackling these assignments independently, you develop critical thinking and the ability to synthesize information from various sources. Moreover, assignments encourage you to explore topics more deeply, fostering intellectual curiosity and promoting a deeper engagement with the subject matter. Ultimately, these assignments are not just a measure of your learning, but a means to equip you with the skills needed for real-world applications and future challenges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nR0VGFVPF--"
      },
      "source": [
        "## **MAKE A COPY OF THIS NOTBOOK!**\n",
        "\n",
        "For your assignment to be graded, you **must** make a copy of this Colab notebook in your GDrive and you **must** use this copy as your worksheet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKQylnEiLDUM"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM"
      },
      "outputs": [],
      "source": [
        "# YOU MUST RUN THIS CELL FIRST\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this assignment.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEMDiVKsMzpY"
      },
      "source": [
        "Your GMAIL address **must** appear in the output in order for your work to be graded."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Accelerated Run-time Check**\n",
        "\n",
        "You MUST run the following code cell to get credit for this assignment. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be running either a Graphics Processing Unit (GPU) or a Tensor Processing Unit (TPU). This lesson has been validated using the A100 GPU but other hardware assisted environments should also work."
      ],
      "metadata": {
        "id": "R9Jt7dQI2QV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell second\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def check_device():\n",
        "    # Check for available devices\n",
        "    devices = tf.config.list_physical_devices()\n",
        "\n",
        "    # Initialize device flags\n",
        "    cpu = False\n",
        "    gpu = False\n",
        "    tpu = False\n",
        "\n",
        "    # Check device types\n",
        "    for device in devices:\n",
        "        if device.device_type == 'CPU':\n",
        "            cpu = True\n",
        "        elif device.device_type == 'GPU':\n",
        "            gpu = True\n",
        "        elif device.device_type == 'TPU':\n",
        "            tpu = True\n",
        "\n",
        "    # Output device status\n",
        "    if tpu:\n",
        "        print(\"Running on TPU\")\n",
        "        print(\"This assignment will **not** run on a TPU\")\n",
        "        print(\"You must change your Runtime to a GPU\")\n",
        "    elif gpu:\n",
        "        print(\"Running on GPU\")\n",
        "        print(\"This an appropiate Runtime environment for this assignment\")\n",
        "    elif cpu:\n",
        "        print(\"Running on CPU\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU or a TPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now!\")\n",
        "    else:\n",
        "        print(\"No compatible device found\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU or a TPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now!\")\n",
        "\n",
        "# Call the function\n",
        "check_device()"
      ],
      "metadata": {
        "id": "ULzCHQeK2QEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order for your assignment to be graded, the output from the last code cell must say:\n",
        "\n",
        "~~~text\n",
        "Running on GPU\n",
        "This an appropiate Runtime environment for this assignment\n",
        "~~~\n"
      ],
      "metadata": {
        "id": "rIkTDIVM27yN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf_otSJdmp8k"
      },
      "source": [
        "# **Assignment_02: Convolutional Neural Networks (CNN) for Computer Vision**\n",
        "\n",
        "**Assignment_02** is specifically designed to assess your ability to write the Python/Tensorflow/Keras code necessary to build neural networks that can analyze medical images using Convultional Neural Networks (CNNs).\n",
        "\n",
        "Like **Assignment_01**, this assignment is broken down into a series of steps to make your coding task easier. For guidance in writing your code, look at your `Class_06_1`.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbcorfFql3Sx"
      },
      "source": [
        "# **Descriptions of Data Sets for Assignment_02**\n",
        "\n",
        "This section describes the various Medical MNIST datasets that will be used in this assignment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3gTLi7al93Q"
      },
      "source": [
        "-----------------------------\n",
        "\n",
        "\n",
        "## **DermaMNIST - 2nd myUTSA Digit = 0**\n",
        "\n",
        "#### **Filename:** `dermamnist_64.npz`\n",
        "\n",
        "**DermaMNIST** is a medical imaging dataset containing 7,000 images of skin lesions, each with a resolution of 64x64 pixels. It is designed for dermatological image classification and is based on the HAM10000 dataset, which includes multi-source dermatoscopic images of common pigmented skin lesions. The dataset is structured as a multi-class classification task, categorizing images into seven different skin conditions, including melanoma, basal cell carcinoma, and benign keratosis-like lesions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSvdzxeOn6Rq"
      },
      "source": [
        "\n",
        "------------------------------------------\n",
        "\n",
        "## **OCTMNIST Dataset - 2nd myUTSA Digit = 1**\n",
        "\n",
        "#### **Filename:** `octmnist.npz`\n",
        "\n",
        "**OCTMNIST** is a medical imaging dataset containing **109,309 optical coherence tomography (OCT) images**, designed for retinal disease classification. The dataset consists of four diagnosis categories, making it a multi-class classification task. The images are grayscale and originally vary in size between (384–1,536)x(277–512) pixels, but they have now been center-cropped and resized to 28x28 pixels for standardization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXOLnLAg0FM2"
      },
      "source": [
        "-----------------------------------------\n",
        "\n",
        "## **PneumoniaMNIST Dataset - 2nd myUTSA Digit = 2**\n",
        "\n",
        "#### **Filename:** `pneumoniamnist_64.npz`\n",
        "\n",
        "**PneumoniaMNIST** is a medical imaging dataset containing **5,856 pediatric chest X-ray images**, designed for binary classification of pneumonia versus normal cases. The dataset is derived from a prior collection of chest X-rays and has been preprocessed to standardize image sizes. The original images vary in resolution between (384–2,916)×(127–2,713) pixels, but they are now center-cropped and resized to 28×28 pixels for consistency\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIjrGHTCuqB8"
      },
      "source": [
        "-----------------------------\n",
        "\n",
        "\n",
        "## **BreastMNIST - 2nd myUTSA Digit = 3**\n",
        "\n",
        "#### **Filename:** `breastmnist_224.npz`\n",
        "\n",
        "**BreastMNIST** is a medical imaging dataset that is part of the MedMNIST collection, designed for breast cancer classification using **10,239 mammography images**. The dataset consists of two classes: benign and malignant, making it a binary classification task. The images are preprocessed and standardized to a resolution of 224x224 pixels, ensuring consistency for deep learning applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_48gnbFMuqB8"
      },
      "source": [
        "\n",
        "------------------------------------------\n",
        "\n",
        "## **PathMNIST Dataset - 2nd myUTSA Digit = 4**\n",
        "\n",
        "#### **Filename:** `pathmnist_128.npz`\n",
        "\n",
        "**PathMNIST** is a medical imaging dataset derived from **colorectal cancer histology slides**. It consists of 100,000 non-overlapping image patches from hematoxylin & eosin-stained histological images, with an additional 7,180 image patches from a separate clinical center for testing. The dataset is structured as a multi-class classification task, categorizing images into nine different tissue types, including adipose, lymphocytes, mucus, smooth muscle, normal colon mucosa, cancer-associated stroma, and colorectal adenocarcinoma epithelium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4jCplkYuqB9"
      },
      "source": [
        "-----------------------------------------\n",
        "\n",
        "## **RetinalMNIST Dataset - 2nd myUTSA Digit = 5**\n",
        "\n",
        "#### **Filename:** `retinamnist_128.npz`\n",
        "\n",
        "**RetinaMNIST** is a medical imaging dataset designed for machine learning tasks related to **diabetic retinopathy detection**. It originates from the DeepDRiD challenge, which provides 1,600 retina fundus images. The dataset is structured for ordinal regression, where images are graded on a five-level severity scale for diabetic retinopathy. The images, originally 3x1,736x1,824, are center-cropped and resized to 3x128x128 for compatibility with deep learning models\n",
        "\n",
        "-----------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQQ67iQ4wuvO"
      },
      "source": [
        "-----------------------------\n",
        "\n",
        "\n",
        "## **TissueMNIST - 2nd myUTSA Digit = 6**\n",
        "\n",
        "#### **Filename:** `tissuemnist.npz`\n",
        "\n",
        "**TissueMNIST** is a medical imaging dataset derived from BBBC051, a collection of **human kidney cortex cell images** from the Broad Bioimage Benchmark Collection. It contains 236,386 grayscale images of segmented kidney cells, categorized into eight different tissue types, including collecting ducts, glomerular endothelial cells, leukocytes, and proximal tubule segments.\n",
        "\n",
        "Each image originally has 32x32x7 pixels, where 7 represents different slices of the same tissue sample. To standardize the dataset, the maximum pixel values across slices are taken, and the images are resized to 28x28 pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGgro3rywuvO"
      },
      "source": [
        "\n",
        "------------------------------------------\n",
        "\n",
        "## **OrganMNIST-A Dataset - 2nd myUTSA Digit = 7**\n",
        "\n",
        "#### **Filename:** `organamnist.npz`\n",
        "\n",
        "**OrganMNIST-A**, the \"A\" stands for \"Axial\", referring to the axial view of medical imaging scans. OrganMNIST-A is derived from **full-body CT scans**, and it specifically contains axial slices of different organs, making it a structured dataset for organ classification tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhFQP5nmyCZ6"
      },
      "source": [
        "\n",
        "------------------------------------------\n",
        "\n",
        "## **OrganMNIST-C Dataset - 2nd myUTSA Digit = 8**\n",
        "\n",
        "#### **Filename:** `organcmnist.npz`\n",
        "\n",
        "**OrganMNIST-C**, the \"C\" stands for \"Coronal\", referring to the coronal view of medical imaging scans. OrganMNIST-C is derived from **full-body CT scans**, and it specifically contains coronal slices of different organs, making it a structured dataset for organ classification tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbftWFG9yCiC"
      },
      "source": [
        "\n",
        "------------------------------------------\n",
        "\n",
        "## **OrganMNIST-S Dataset - 2nd myUTSA Digit = 9**\n",
        "\n",
        "#### **Filename:** `organsmnist.npz`\n",
        "\n",
        "**OrganAMNIST**, the \"S\" stands for \"Sagittal\", referring to the saggital view of medical imaging scans. OrganAMNIST is derived from **full-body CT scans**, and it specifically contains sagittal slices of different organs, making it a structured dataset for organ classification tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4vNNCrnnn5A"
      },
      "source": [
        "\n",
        "-----------------------------------------------------\n",
        "-----------------------------------------------------\n",
        "\n",
        "# **General Instructions**\n",
        "\n",
        "To make the assignment more manageable, you will given a number of specific steps to perform. These steps correspond to the steps used in Example 1 (and Execise 1) in `Class_06_1`.\n",
        "\n",
        "## **Can I Use AI?**\n",
        "\n",
        "You are free to use AI (e.g. Microsoft Co-Pilot) to help you complete your assignment---but you need need to be *very* careful. Since AI doesn't really understand what your code is trying to do, it often starts adding additional code and variables that generate an incorrect output. AI may suggest code that so incorrect the you might fail this assignment.\n",
        "\n",
        "If your aren't sure what you are doing, it's much, much safer to get help with any of your coding problems by coming to class and asking for help from your course instructor and/or one of the course TA's."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Functions for this Assignment\n",
        "\n",
        "In order to complete this assignment, you will need to create several functions by running the next code cell."
      ],
      "metadata": {
        "id": "TQAS0bHK3Sas"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWG5pvkXPnff"
      },
      "outputs": [],
      "source": [
        "# Create functions for this lesson\n",
        "\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def check_current_ram():\n",
        "  ram = psutil.virtual_memory()\n",
        "  print(f\"Available RAM: {ram.available / (1024 ** 3):.2f} GB\")\n",
        "\n",
        "def list_files():\n",
        "   files = os.listdir('.')\n",
        "   print(f\"Current files: {files}\")\n",
        "\n",
        "def list_extract():\n",
        "  files = os.listdir(EXTRACT_TARGET)\n",
        "  print(f\"Current files in EXTRACT_TARGET: {files}\")\n",
        "\n",
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "\n",
        "# List files in current directory\n",
        "list_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TxbZZQayNCu"
      },
      "source": [
        "## **Step - 1: Setup Evironmental Variables**\n",
        "\n",
        "In the cell below, create environmental variables so you can download your specific MedMNIST dataset that has been assigned to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6_nGGnFcnK2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 1 here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_inb_cq2jWb"
      },
      "source": [
        "## **Step - 2: Download and Extract Data**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ4tcdRW23-6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 2 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEEeh78z1m-Z"
      },
      "source": [
        "### **Step - 3: Load and Shuffle Images and Labels into Numpy arrays**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKRWqOVp1-6X"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 3 here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r7YL7KE3NiU"
      },
      "source": [
        "### **Step 4 - Add Color Channel and Resize Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2ibFXD73m8L"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 4 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWUAeTIz4QJB"
      },
      "source": [
        "## **Step - 5: Check Available Memory**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiP9n4mB4X5o"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 5 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DAvfkkK8n4M"
      },
      "source": [
        "## **Step - 6: Augment Training Image Set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coiEBPXz8oxr"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 6 here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBhIX2Wj9FMq"
      },
      "source": [
        "### **Step - 7: One-Hot Encode Labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nFInexU9E4C"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 7 here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGyzXBigeFpq"
      },
      "source": [
        "### **Step - 8: Create and Compile CNN neural network model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "195gnTAFeY8a"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 8 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcWklesN1kjO"
      },
      "source": [
        "### **Step - 9: Train the Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_2pzz9Z1h6H"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 9 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------\n",
        "------------------------------------------------\n",
        "\n",
        "## **Evaluating Model's Training**\n"
      ],
      "metadata": {
        "id": "xrHpRqD28gte"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXtCEnW7BsR6"
      },
      "source": [
        "### **Step - 10: Plot `accuracy` and `val_accuracy`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KOEkmU5Bhc7"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 10 here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyabL3THtdIP"
      },
      "source": [
        "### **Step - 11: Compute Accuracy Score with Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze3HxEjAZsEY"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 11 here\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaZy2UeUprey"
      },
      "source": [
        "### **Step - 12: Plot Image with _Correct_ Label**\n",
        "\n",
        "Make sure you use a meaningful label, **not** the label used in Class_06_1, Step 12. For example, if your datafile was `pathmnist_128`, you label might say `Organ type` or `Pathology type`. Do not use `Blood Cell type` unless you analyzing the bloodcell datafile, `bloodmnist_224`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CKXmI-VpeiY"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 12 here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMF_1asAp9_n"
      },
      "source": [
        "### **Step - 13: Plot 4 Frames with _Correct_ Labels**\n",
        "\n",
        "Again, make sure you use a meaningful label, **not** the label used in Class_06_1, Step 12. For example, if your datafile was `pathmnist_128`, you label might say `Organ type` or `Pathology type`. Do not use `Blood Cell type` unless you analyzing the bloodcell datafile, `bloodmnist_224`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arpKI32lqOXO"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Step 13 here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atkFq1eGxmjw"
      },
      "source": [
        "## **Assignment Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Assignment_02.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNtya3vqufwZ"
      },
      "source": [
        "## **Poly-A Tail**\n",
        "\n",
        "\n",
        "## **Convolutional neural network**\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/Assignment_02_image01.png)\n",
        "\n",
        "A **Convolutional Neural Network (CNN)** is a type of feedforward neural network that learns features via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently been replaced—in some cases—by newer deep learning architectures such as the transformer.\n",
        "\n",
        "**Vanishing gradients** and exploding gradients, seen during **backpropagation** in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections. For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 x 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels, only 25 weights for each convolutional layer are required to process 5x5-sized tiles. Higher-layer features are extracted from wider context windows, compared to lower-layer features.\n",
        "\n",
        "Some applications of CNNs include:\n",
        "\n",
        "* image and video recognition,\n",
        "* recommender systems,\n",
        "* image classification,\n",
        "* image segmentation,\n",
        "* **medical image analysis**\n",
        "* natural language processing,\n",
        "* brain–computer interfaces, and\n",
        "* financial time series.\n",
        "\n",
        "CNNs are also known as **shift invariant** or **space invariant artificial neural networks**, based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.\n",
        "\n",
        "**Feedforward neural networks** are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks makes them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.\n",
        "\n",
        "Convolutional networks were inspired by **biological processes** in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\n",
        "\n",
        "CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This simplifies and automates the process, enhancing efficiency and scalability overcoming human-intervention bottlenecks.\n",
        "\n",
        "**Architecture**\n",
        "\n",
        "A convolutional neural network consists of an input layer, hidden layers and an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a **dot product** of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers. Here it should be noted how close a convolutional neural network is to a matched filter.\n",
        "\n",
        "**Convolutional layers**\n",
        "In a CNN, the input is a tensor with shape:\n",
        "\n",
        "(number of inputs) x (input height) x (input width) × (input channels)\n",
        "\n",
        "After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape:\n",
        "\n",
        "(number of inputs) x (feature map height) x (feature map width) x (feature map channels).\n",
        "\n",
        "Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field.\n",
        "\n",
        "Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 × 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper. For example, using a 5 × 5 tiling region, each with the same shared weights, requires only 25 neurons. Using shared weights means there are many fewer parameters, which helps avoid the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.\n",
        "\n",
        "To speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers, which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of\n",
        "\n",
        "**Pooling layers**\n",
        "\n",
        "Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 × 2 are commonly used. Global pooling acts on all the neurons of the feature map. There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map, while average pooling takes the average value.\n",
        "\n",
        "**Fully connected layers**\n",
        "\n",
        "Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.\n",
        "\n",
        "**Receptive field**\n",
        "\n",
        "In neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.\n",
        "\n",
        "To manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer. For example, atrous or dilated convolution expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios,thus having a variable receptive field size.\n",
        "\n",
        "**Weights**\n",
        "\n",
        "Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights.\n",
        "\n",
        "The vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.\n",
        "\n",
        "**Deconvolutional**\n",
        "\n",
        "A deconvolutional neural network is essentially the reverse of a CNN. It consists of deconvolutional layers and unpooling layers.\n",
        "\n",
        "A deconvolutional layer is the transpose of a convolutional layer. Specifically, a convolutional layer can be written as a multiplication with a matrix, and a deconvolutional layer is multiplication with the transpose of that matrix.\n",
        "\n",
        "Deconvolution layers are used in image generators. By default, it creates periodic checkerboard artifact, which can be fixed by upscale-then-convolve.\n",
        "\n",
        "### **History**\n",
        "\n",
        "CNN are often compared to the way the brain achieves vision processing in living organisms.\n",
        "\n",
        "**Receptive fields in the visual cortex**\n",
        "\n",
        "Work by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.\n",
        "\n",
        "Their 1968 paper identified two basic visual cell types in the brain:\n",
        "\n",
        "* **simple cells**, whose output is maximized by straight edges having particular orientations within their receptive field\n",
        "\n",
        "* **complex cells**, which have larger receptive fields, whose output is insensitive to the exact position of the edges in the field.\n",
        "Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\n",
        "\n",
        "**Fukushima's analog threshold elements in a vision model**\n",
        "\n",
        "In 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\" This is the essential core of a convolutional network, but the weights were not trained. In the same paper, Fukushima also introduced the ReLU (rectified linear unit) activation function.\n",
        "\n",
        "**Neocognitron, origin of the trainable CNN architecture**\n",
        "\n",
        "The \"neocognitron\" was introduced by Fukushima in 1980. The neocognitron introduced the two basic types of layers:\n",
        "\n",
        "* **\"S-layer\":** a shared-weights receptive-field layer, later known as a convolutional layer, which contains units whose receptive fields cover a patch of the previous layer. A shared-weights receptive-field group (a \"plane\" in neocognitron terminology) is often called a filter, and a layer typically has several such filters.\n",
        "\n",
        "* **\"C-layer\":** a downsampling layer that contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes a weighted average of the activations of the units in its patch, and applies inhibition (divisive normalization) pooled from a somewhat larger patch and across different filters in a layer, and applies a saturating activation function. The patch weights are nonnegative and are not trainable in the original neocognitron. The downsampling and competitive inhibition help to classify features and objects in visual scenes even when the objects are shifted.\n",
        "\n",
        "Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today, however, the CNN architecture is usually trained through backpropagation.\n",
        "\n",
        "Fukushima's ReLU activation function was not used in his neocognitron since all the weights were nonnegative; lateral inhibition was used instead. The rectifier has become a very popular activation function for CNNs and deep neural networks in general.\n",
        "\n",
        "**Convolution in time**\n",
        "\n",
        "The term \"convolution\" first appears in neural networks in a paper by Toshiteru Homma, Les Atlas, and Robert Marks II at the first Conference on Neural Information Processing Systems in 1987. Their paper replaced multiplication with convolution in time, inherently providing shift invariance, motivated by and connecting more directly to the signal-processing concept of a filter, and demonstrated it on a speech recognition task. They also pointed out that as a data-trainable system, convolution is essentially equivalent to correlation since reversal of the weights does not affect the final learned function (\"For convenience, we denote * as correlation instead of convolution. Note that convolving a(t) with b(t) is equivalent to correlating a(-t) with b(t).\"). Modern CNN implementations typically do correlation and call it convolution, for convenience, as they did here.\n",
        "\n",
        "**Time delay neural networks**\n",
        "\n",
        "The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was an early convolutional network exhibiting shift-invariance. A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.\n",
        "\n",
        "TDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution. Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts, as with images processed by a neocognitron.\n",
        "\n",
        "TDNNs improved the performance of far-distance speech recognition.\n",
        "\n",
        "**Image recognition with CNNs trained by gradient descent**\n",
        "\n",
        "Denker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers. However, the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.\n",
        "\n",
        "Following the advances in the training of 1-D CNNs by Waibel et al. (1987), Yann LeCun et al.(1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types. Wei Zhang et al. (1988) used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called shift-invariant pattern recognition neural network before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991) and breast cancer detection in mammograms (1994).\n",
        "\n",
        "This approach became a foundation of modern computer vision.\n",
        "\n",
        "**Max pooling**\n",
        "\n",
        "In 1990 Yamaguchi et al. introduced the concept of max pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system. In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.\n",
        "\n",
        "In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging with inhibition and saturation, J. Weng et al. in 1993 used max pooling, where a downsampling unit computes the maximum of the activations of the units in its patch, introducing this method into the vision field.\n",
        "\n",
        "Max pooling is often used in modern CNNs.\n",
        "\n",
        "\n",
        "**Shift-invariant neural network**\n",
        "\n",
        "A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer. The model was trained with back-propagation. The training algorithm was further improved in 1991 to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991) and automatic detection of breast cancer in mammograms (1994).\n",
        "\n",
        "A different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.\n",
        "\n",
        "**GPU implementations**\n",
        "\n",
        "Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).\n",
        "\n",
        "In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU.In 2005, another paper also emphasised the value of GPGPU for machine learning.\n",
        "\n",
        "The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU. In the same period, GPUs were also used for unsupervised training of deep belief networks.\n",
        "\n",
        "In 2010, Dan Ciresan et al. at IDSIA trained deep feedforward networks on GPUs. In 2011, they extended this to CNNs, accelerating by 60 compared to training CPU. In 2011, the network won an image recognition contest where they achieved superhuman performance for the first time. Then they won more competitions and achieved state of the art on several benchmarks.\n",
        "\n",
        "Subsequently, AlexNet, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012. It was an early catalytic event for the AI boom.\n",
        "\n",
        "Compared to the training of CNNs using GPUs, not much attention was given to CPU. (Viebke et al 2019) parallelizes CNN by thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.\n",
        "\n",
        "**Distinguishing features**\n",
        "\n",
        "In the past, traditional multilayer perceptron (MLP) models were used for image recognition. However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher-resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights per fully-connected neuron, which is too high to feasibly process efficiently at scale.\n",
        "\n",
        "CNN layers arranged in 3 dimensions\n",
        "For example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\n",
        "\n",
        "Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.\n",
        "\n",
        "Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\n",
        "\n",
        "3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth. Where each neuron inside a convolutional layer is connected to only a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture.\n",
        "Local connectivity: following the concept of receptive fields, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learned \"filters\" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to nonlinear filters that become increasingly global (i.e. responsive to a larger region of pixel space) so that the network first creates representations of small parts of the input, then from them assembles representations of larger areas.\n",
        "\n",
        "* **Shared weights:** In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field. Replicating units in this way allows for the resulting activation map to be equivariant under shifts of the locations of input features in the visual field, i.e. they grant translational equivariance—given that the layer has a stride of one.\n",
        "\n",
        "* **Pooling:** In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value. In addition to reducing the sizes of feature maps, the pooling operation grants a degree of local translational invariance to the features contained therein, allowing the CNN to be more robust to variations in their positions.\n",
        "\n",
        "Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.\n",
        "\n",
        "**Building blocks**\n",
        "\n",
        "A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used.\n",
        "\n",
        "The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the filter entries and the input, producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.\n",
        "\n",
        "Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input. Each entry in an activation map use the same set of parameters that define the filter.\n",
        "\n",
        "Self-supervised learning has been adapted for use in convolutional layers by using sparse patches with a high-mask ratio and a global response normalization layer.\n",
        "\n",
        "**Typical CNN architecture**\n",
        "\n",
        "When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.\n",
        "\n",
        "The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learned filters produce the strongest response to a spatially local input pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJjKbpyWovaz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}