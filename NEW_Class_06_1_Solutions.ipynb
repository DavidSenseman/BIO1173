{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/NEW_Class_06_1_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhdLgU8usnPQ"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 6: Reinforcement Learning**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 6 Material\n",
        "\n",
        "* **Part 6.1: Introduction to Introduction to Gymnasium and Q-Learning**\n",
        "* Part 6.2: Stable Baselines Q-Learning\n",
        "* Part 6.3: Atari Games with Stable Baselines Neural Networks\n",
        "* Part 6.4: Future of Reinforcement Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3tT0Yojtv-8"
      },
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH6ZP8Eq4i5D"
      },
      "source": [
        "### Install Custom Function\n",
        "\n",
        "Run the code in the next cell to install a custom function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4YfuC_3V4uCq"
      },
      "outputs": [],
      "source": [
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxOKy1Sat7rz"
      },
      "source": [
        "### Install Gymnasium\n",
        "\n",
        "The code in the cell below installs 3 libraries needed for this lesson:\n",
        "\n",
        "* **Gymnasium** is needed to create and interact with reinforcement-learning environments.\n",
        "* **Pillow** is a dependency for many Gym environments that use image rendering.\n",
        "* **PyVirtualDisplay** allows the notebook to render environment frames on a headless server like Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcTY1gkD-X5r"
      },
      "outputs": [],
      "source": [
        "# Install gymnasium\n",
        "!pip install --quiet gymnasium==1.2.0 pillow\n",
        "!pip install pyvirtualdisplay # > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJrIgZkszA7x"
      },
      "source": [
        "# **Introduction to Gymnasium and Q-Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ2xbLNlzFvB"
      },
      "source": [
        "## **Gymnasium**\n",
        "\n",
        "**Gymnasium** is a toolkit for developing and comparing reinforcement learning (RL) algorithms. It provides various environments that simulate different tasks and challenges, which agents (algorithms) can interact with to learn and improve their performance.\n",
        "\n",
        "### **How Gymnasium Can Be Useful for Computational Biologists**\n",
        "\n",
        "Gymnasium can be a valuable tool for computational biologists in several ways. Here are some potential applications:\n",
        "\n",
        "##### **1. Modeling Biological Systems**\n",
        "Gymnasium provides a platform for simulating and modeling complex biological systems. Computational biologists can use Gymnasium environments to create and test models of cellular processes, metabolic pathways, and genetic networks. This helps in understanding the dynamics of these systems and predicting their behavior under different conditions.\n",
        "\n",
        "##### **2. Reinforcement Learning for Drug Discovery**\n",
        "Reinforcement learning (RL) algorithms can be applied to drug discovery and development. By using Gymnasium environments, computational biologists can train RL agents to explore chemical spaces, optimize molecular structures, and predict the efficacy of potential drug candidates. This accelerates the drug discovery process and reduces the need for costly and time-consuming experiments.\n",
        "\n",
        "##### **3. Optimization of Experimental Protocols**\n",
        "Gymnasium can be used to optimize experimental protocols in computational biology. For example, RL agents can be trained to design efficient experimental setups, select optimal parameters, and minimize experimental errors. This leads to more accurate and reproducible results in biological research.\n",
        "\n",
        "##### **4. Simulating Evolutionary Processes**\n",
        "Gymnasium environments can simulate evolutionary processes, allowing computational biologists to study the evolution of populations, genetic diversity, and adaptation mechanisms. By modeling these processes, researchers can gain insights into the principles of evolution and apply them to areas such as conservation biology and synthetic biology.\n",
        "\n",
        "##### **5. Analyzing Biological Data**\n",
        "Gymnasium can be integrated with data analysis tools to analyze large-scale biological data. Computational biologists can use Gymnasium environments to preprocess, visualize, and interpret data from genomics, proteomics, and other omics studies. This helps in identifying patterns, correlations, and potential biomarkers.\n",
        "\n",
        "##### **6. Training and Education**\n",
        "Gymnasium can be used as an educational tool to teach computational biology concepts. Students and researchers can interact with Gymnasium environments to learn about biological systems, experiment with different algorithms, and gain hands-on experience in computational modeling and simulation.\n",
        "\n",
        "### **Example Use Case: Protein Folding**\n",
        "One specific application is the study of protein folding. Computational biologists can use Gymnasium to create environments that simulate the folding process of proteins. RL agents can be trained to predict the final folded structure of a protein based on its amino acid sequence. This has significant implications for understanding diseases related to protein misfolding and designing therapeutic interventions.\n",
        "\n",
        "By leveraging Gymnasium, computational biologists can enhance their research capabilities, accelerate discoveries, and gain deeper insights into the complexities of biological systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA-E4NR2-NoY"
      },
      "source": [
        "## **Q-Learning**\n",
        "\n",
        "\n",
        "Q-Learning is a foundational technology upon which deep reinforcement learning is based. Before we explore deep reinforcement learning, it is essential to understand Q-Learning. Several components make up any Q-Learning system.\n",
        "\n",
        "1. **Agent** - The agent is an entity that exists in an environment that takes actions to affect the state of the environment, to receive rewards.\n",
        "2. **Environment** - The environment is the universe that the agent exists in. The environment is always in a specific state that is changed by the agent's actions.\n",
        "3. **Actions** - Steps that the agent can perform to alter the environment\n",
        "4. **Step** - A step occurs when the agent performs an action and potentially changes the environment state.\n",
        "5. **Episode** - A chain of steps that ultimately culminates in the environment entering a terminal state.\n",
        "6. **Epoch** - A training iteration of the agent that contains some number of episodes.\n",
        "7. **Terminal State** -  A state in which further actions do not make sense. A terminal state occurs when the agent has one, lost, or the environment exceeds the maximum number of steps in many environments.\n",
        "\n",
        "Q-Learning works by building a table that suggests an action for every possible state. This approach runs into several problems. First, the environment is usually composed of several continuous numbers, resulting in an infinite number of states. Q-Learning handles continuous states by binning these numeric values into ranges.\n",
        "\n",
        "Out of the box, Q-Learning does not deal with continuous inputs, such as a car's accelerator that can range from released to fully engaged. Additionally, Q-Learning primarily deals with discrete actions, such as pressing a joystick up or down. Researchers have developed clever tricks to allow Q-Learning to accommodate continuous actions.\n",
        "\n",
        "Deep neural networks can help solve the problems of continuous environments and action spaces. In the next section, we will learn more about deep reinforcement learning. For now, we will apply regular Q-Learning to the Mountain Car problem from OpenAI Gym.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFI9xF411UuB"
      },
      "source": [
        "### Example 1: Introducing the Mountain Car\n",
        "\n",
        "This section will demonstrate how Q-Learning can create a solution to the `mountain car gym environment`. The **`Mountain car`** is an environment where a car must climb a mountain. Because gravity is stronger than the car's engine, it cannot merely accelerate up the steep slope even with full throttle. The vehicle is situated in a valley and must learn to utilize potential energy by driving up the opposite hill before the car can make it to the goal at the top of the rightmost hill.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_1_image01A.png)\n",
        "\n",
        "First, it might be helpful to visualize the mountain car environment. The following code shows this environment. This code makes use of `TF-Agents` to perform this render. Usually, we use TF-Agents for the type of deep reinforcement learning that we will see in the next module. However, `TF-Agents` is just used to render the mountain care environment for now.\n",
        "\n",
        "The code launches the `MountainCar` environment in a COLAB notebook, grabs the first visual frame as an RGB image, and shows that image inside the notebook. Afterward it closes the environment cleanly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZykMhfe3Pek"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "# 0️⃣  Imports\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "# 1️⃣  Scale factor\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "SCALE = 0.64          # 64 % of the original size\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "# 2️⃣  Helper: render & display an env frame\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "def display_env_state(env, scale: float = 1.0) -> None:\n",
        "    \"\"\"\n",
        "    Render the current state of *env* to an RGB array, convert it to a PIL\n",
        "    image, resize it to *scale* times its original size (using integer\n",
        "    dimensions), and display it inline in a notebook.\n",
        "    \"\"\"\n",
        "    frame = env.render()                     # → np.ndarray (H, W, 3)\n",
        "    if frame is None:\n",
        "        print(\"⚠️  Environment did not return an image.\")\n",
        "        return\n",
        "\n",
        "    img = Image.fromarray(frame)\n",
        "\n",
        "    if scale != 1.0:\n",
        "        # apply the same factor to *both* axes and cast to int\n",
        "        new_w = int(round(img.width  * scale))\n",
        "        new_h = int(round(img.height * scale))\n",
        "        img = img.resize((new_w, new_h), resample=Image.LANCZOS)\n",
        "\n",
        "    display(img)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "# 3️⃣  Create & initialise the environment\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "env_id  = \"MountainCar-v0\"\n",
        "seed    = 1234          # deterministic start (set to None for full randomness)\n",
        "render  = \"rgb_array\"   # must return an image\n",
        "\n",
        "env = gym.make(env_id, render_mode=render)\n",
        "obs, info = env.reset(seed=seed)              # deterministic initial state\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "# 4️⃣  Show the *initial* frame\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "display_env_state(env, scale=SCALE)    # enlarge it a bit for readability\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "# 5️⃣  (Optional) Step once so you can see a *different* frame\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "action = 2          # “push right” – you can change this\n",
        "_, _, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "if terminated or truncated:\n",
        "    print(\"\\n⚠️  Episode finished – that’s normal for MountainCar.\")\n",
        "else:\n",
        "    clear_output(wait=True)               # hide the previous image\n",
        "    display_env_state(env, scale=SCALE)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "# 6️⃣  Clean up\n",
        "# ────────────────────────────────────────────────────────────────────── #\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HacX2LPwtfcp"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_06/class_06_1_image02A.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBcs3yAMo_P"
      },
      "source": [
        "#### **Characteristics of the Mountain Car**\n",
        "\n",
        "The mountain car environment provides the following discrete actions:\n",
        "\n",
        "* 0 - Apply left force\n",
        "* 1 - Apply no force\n",
        "* 2 - Apply right force\n",
        "\n",
        "The mountain car environment is made up of the following continuous values:\n",
        "\n",
        "* state[0] - Position\n",
        "* state[1] - Velocity\n",
        "\n",
        "The cart is not strong enough. It will need to use potential energy from the mountain behind it. The following code shows an agent that applies full throttle to climb the hill."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqk1IKpkZmQZ"
      },
      "source": [
        "### Example 2: Apply Full Throttle\n",
        "\n",
        "As stated above the cart is not strong enough to climb the hill by itself. This is demonstrated by running the code in the next cell. Starting at the bottom the cart applies **full throttle**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXkTe0Q63-up"
      },
      "outputs": [],
      "source": [
        "# Example 2: Apply full throttle\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# ------------------------------------------------------------------------\n",
        "import base64\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Callable, Optional\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "from IPython.display import HTML, display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#  CONFIGURATION\n",
        "# --------------------------------------------------------------------------- #\n",
        "ENV_NAME = \"MountainCar-v0\"\n",
        "RENDER_MODE = \"rgb_array\"          # must return image data\n",
        "VIDEO_FOLDER = \"./videos\"          # where the .mp4 files will be written\n",
        "VIDEO_PREFIX = \"rl-video\"          # optional filename prefix\n",
        "VIDEO_LENGTH = 0                   # 0 → record whole episode\n",
        "MAX_STEPS = 1_000  # safety upper‑bound if you want to cap the loop\n",
        "SEED: Optional[int] = 42             # None → fully random\n",
        "ACTION = 2  # “push right”\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#  HELPER: Return the newest .mp4 in a folder (or raise if none)\n",
        "# --------------------------------------------------------------------------- #\n",
        "def _latest_mp4(folder: str) -> Path:\n",
        "    files = sorted(\n",
        "        glob.glob(os.path.join(folder, \"*.mp4\")),\n",
        "        key=os.path.getmtime,\n",
        "    )\n",
        "    if not files:\n",
        "        raise RuntimeError(\"No movie file found in %s\" % folder)\n",
        "    return Path(files[-1])\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#  MAIN LOGIC (executed in a single notebook cell)\n",
        "# --------------------------------------------------------------------------- #\n",
        "def run_and_show_episode(\n",
        "    env_name: str = ENV_NAME,\n",
        "    render_mode: str = RENDER_MODE,\n",
        "    video_folder: str = VIDEO_FOLDER,\n",
        "    video_prefix: str = VIDEO_PREFIX,\n",
        "    video_length: int = VIDEO_LENGTH,\n",
        "    episode_trigger: Optional[Callable[[int], bool]] = None,\n",
        "    action: int = ACTION,\n",
        "    seed: Optional[int] = SEED,\n",
        "    max_steps: int = MAX_STEPS,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    * Spin up an off‑screen Xvfb instance (via `pyvirtualdisplay`).\n",
        "    * Create the gymnasium environment with an image‑returning render mode.\n",
        "    * Wrap it with RecordVideo and record **every** episode (or whatever\n",
        "      `episode_trigger` you supply).\n",
        "    * Play one deterministic episode that always takes the supplied\n",
        "      `action`.\n",
        "    * When the episode ends, flush the movie to disk and show it inline.\n",
        "    \"\"\"\n",
        "    # --- 1. Start a virtual display (auto‑stop) --------------------------- #\n",
        "    with Display(visible=0, size=(1400, 900)) as disp:  # <-- context‑manager\n",
        "        # --- 2. Ensure the video folder exists --------------------------- #\n",
        "        os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "        # --- 3. Create & wrap the environment --------------------------- #\n",
        "        env = gym.make(env_name, render_mode=render_mode)\n",
        "        env = RecordVideo(\n",
        "            env,\n",
        "            video_folder=video_folder,\n",
        "            episode_trigger=episode_trigger or (lambda _: True),  # record all episodes\n",
        "            name_prefix=video_prefix,\n",
        "            video_length=video_length,\n",
        "        )\n",
        "        # Optional: override FPS only if the underlying env exposes it\n",
        "        env.metadata[\"render_fps\"] = 30\n",
        "\n",
        "        # --- 4. Play a single episode ----------------------------------- #\n",
        "        obs, info = env.reset(seed=seed)   # deterministic seed (if you like)\n",
        "        for _ in range(max_steps):\n",
        "            _, _, terminated, truncated, _ = env.step(action)\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        # --- 5. Close the environment (flushes the movie) -------------- #\n",
        "        env.close()\n",
        "\n",
        "        # --- 6. Load the newest MP4 & display it ------------------------ #\n",
        "        mp4_path = _latest_mp4(video_folder)\n",
        "        with open(mp4_path, \"rb\") as f:\n",
        "            video_bytes = f.read()\n",
        "\n",
        "        video_b64 = base64.b64encode(video_bytes).decode()\n",
        "        display(HTML(\n",
        "            f'<video width=400 controls autoplay>'\n",
        "            f'<source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\"/>'\n",
        "            f'</video>'\n",
        "        ))\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#  Run the demo\n",
        "# --------------------------------------------------------------------------- #\n",
        "run_and_show_episode()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcdwI-0-Nod"
      },
      "source": [
        "### Example 3: Programmed Car\n",
        "\n",
        "Now we will look at a car that I hand-programmed. This car is straightforward; however, it solves the problem. The programmed car always applies force in one direction or another. It does not break. Whatever direction the vehicle is currently rolling, the agent uses power in that direction. Therefore, the car begins to climb a hill, is overpowered, and turns backward. However, once it starts to roll backward, force is immediately applied in this new direction.\n",
        "\n",
        "The following code implements this preprogrammed car."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrX3G38Ax9bL"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Imports (all available on a fresh Colab VM)\n",
        "# ------------------------------------------------------------\n",
        "import base64\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import gymnasium as gym\n",
        "from IPython.display import display, HTML\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Helper: record a full episode of MountainCar\n",
        "# ------------------------------------------------------------\n",
        "def record_mountaincar(\n",
        "    *,\n",
        "    env_id: str = \"MountainCar-v0\",\n",
        "    seed: Optional[int] = None,\n",
        "    render_fps: int = 30,\n",
        "    video_folder: str = \"./videos\",\n",
        "    episode_trigger: Optional[callable] = None,\n",
        "    max_steps: int = 1_000,\n",
        ") -> Path:\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # 2a  Prepare the output directory\n",
        "    # -----------------------------------------------------------------\n",
        "    out_dir = Path(video_folder)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # 2b  Create the env (rgb_array does NOT need a display backend)\n",
        "    # -----------------------------------------------------------------\n",
        "    env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "    env.metadata[\"render_fps\"] = render_fps\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # 2c  Wrap it in the video‑recording wrapper\n",
        "    # -----------------------------------------------------------------\n",
        "    env = gym.wrappers.RecordVideo(\n",
        "        env,\n",
        "        video_folder=video_folder,\n",
        "        episode_trigger=episode_trigger or (lambda _: True),\n",
        "        name_prefix=\"mountaincar-\",\n",
        "        video_length=0,           # 0 = record until termination/truncation\n",
        "    )\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # 2d  Reset (optionally with a seed) and run the episode\n",
        "    # -----------------------------------------------------------------\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "\n",
        "    action = 2          # start by pushing right\n",
        "    for step in range(max_steps):\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        # Simple deterministic policy:\n",
        "        #   velocity > 0 → push right,  else → push left\n",
        "        if state[1] > 0:\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 0\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    else:          # pragma: no cover – should never hit in MountainCar\n",
        "        raise RuntimeError(f\"Episode exceeded {max_steps} steps!\")\n",
        "\n",
        "    # The environment has a ``close`` method – it will also\n",
        "    # flush the video file(s) into *video_folder*.\n",
        "    env.close()\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # 2e  Pick the *newest* MP4 file that RecordVideo produced\n",
        "    # -----------------------------------------------------------------\n",
        "    mp4s = sorted(out_dir.rglob(\"*.mp4\"), key=os.path.getmtime, reverse=True)\n",
        "    if not mp4s:\n",
        "        raise RuntimeError(f\"No video files found in {video_folder!r}\")\n",
        "\n",
        "    return mp4s[0]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Run the demo in a single Colab cell\n",
        "# ------------------------------------------------------------\n",
        "if __name__ == \"__main__\":            # guard so the cell can be imported elsewhere\n",
        "    # -----------------------------------------------------------------\n",
        "    # 3a  Record – change the seed or env_id if you wish\n",
        "    # -----------------------------------------------------------------\n",
        "    video_path = record_mountaincar(seed=1234)\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # 3b  Show the finished video inline\n",
        "    # -----------------------------------------------------------------\n",
        "    raw_video = video_path.read_bytes()\n",
        "    encoded = base64.b64encode(raw_video).decode(\"ascii\")\n",
        "\n",
        "    display(\n",
        "        HTML(\n",
        "            f\"\"\"\n",
        "            <video width=\"640\" height=\"480\" controls>\n",
        "                <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\" />\n",
        "            </video>\n",
        "            \"\"\"\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAyqqoy9-Nog"
      },
      "source": [
        "# **Reinforcement Learning**\n",
        "\n",
        "Q-Learning is a system of rewards that the algorithm gives an agent for successfully moving the environment into a state considered successful. These rewards are the Q-values from which this algorithm takes its name. The final output from the Q-Learning algorithm is a table of Q-values that indicate the reward value of every action that the agent can take, given every possible environment state. The agent must bin continuous state values into a fixed finite number of columns.\n",
        "\n",
        "Learning occurs when the algorithm runs the agent and environment through episodes and updates the Q-values based on the rewards received from actions taken; Figure 12.REINF provides a high-level overview of this reinforcement or Q-Learning loop.\n",
        "\n",
        "**Figure 12.REINF:Reinforcement/Q Learning**\n",
        "![Reinforcement Learning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/reinforcement.png \"Reinforcement Learning\")\n",
        "\n",
        "The Q-values can dictate action by selecting the action column with the highest Q-value for the current environment state. The choice between choosing a random action and a Q-value-driven action is governed by the epsilon ($\\epsilon$) parameter, the probability of random action.\n",
        "\n",
        "Each time through the training loop, the training algorithm updates the Q-values according to the following equation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzZnd9Tj7rNc"
      },
      "source": [
        "$Q^{new}(s_{t},a_{t}) \\leftarrow \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot  \\overbrace{\\bigg( \\underbrace{\\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}}}_{\\text{new value (temporal difference target)}} - \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} \\bigg) }^{\\text{temporal difference}}$\n",
        "\n",
        "There are several parameters in this equation:\n",
        "* alpha ($\\alpha$) - The learning rate, how much should the current step cause the Q-values to be updated.\n",
        "* lambda ($\\lambda$) - The discount factor is the percentage of future reward that the algorithm should consider in this update.\n",
        "\n",
        "This equation modifies several values:\n",
        "\n",
        "* $Q(s_t,a_t)$ - The Q-table.  For each combination of states, what reward would the agent likely receive for performing each action?\n",
        "* $s_t$ - The current state.\n",
        "* $r_t$ - The last reward received.\n",
        "* $a_t$ - The action that the agent will perform.\n",
        "\n",
        "The equation works by calculating a delta (temporal difference) that the equation should apply to the old state. This learning rate ($\\alpha$) scales this delta. A learning rate of 1.0 would fully implement the temporal difference in the Q-values each iteration and would likely be very chaotic.\n",
        "\n",
        "There are two parts to the temporal difference: the new and old values. The new value is subtracted from the old value to provide a delta; the full amount we would change the Q-value by if the learning rate did not scale this value. The new value is a summation of the reward received from the last action and the maximum Q-values from the resulting state when the client takes this action. Adding the maximum of action Q-values for the new state is essential because it estimates the optimal future values from proceeding with this action.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E0wxj3r4hbF"
      },
      "source": [
        "### Example 4: Q-Learning Car\n",
        "\n",
        "We will now use Q-Learning to produce a car that learns to drive itself. Look out, Tesla! We begin by defining two essential functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjZnsxqIlj0r"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for Car Game\n",
        "\n",
        "import gymnasium\n",
        "import numpy as np\n",
        "\n",
        "# This function converts the floating point state values into\n",
        "# discrete values. This is often called binning.  We divide\n",
        "# the range that the state values might occupy and assign\n",
        "# each region to a bucket.\n",
        "def calc_discrete_state_car(state):\n",
        "    discrete_state = (state - env.observation_space.low)/buckets\n",
        "    return tuple(discrete_state.astype(int))\n",
        "\n",
        "# Run one game.  The q_table to use is provided.  We also\n",
        "# provide a flag to indicate if the game should be\n",
        "# rendered/animated.  Finally, we also provide\n",
        "# a flag to indicate if the q_table should be updated.\n",
        "def run_game_car(q_table_car, render, should_update):\n",
        "    done = False\n",
        "    discrete_state = calc_discrete_state_car(env.reset()[0])\n",
        "    success = False\n",
        "\n",
        "    while not done:\n",
        "        # Exploit or explore\n",
        "        if np.random.random() > epsilon:\n",
        "            # Exploit - use q-table to take current best action\n",
        "            # (and probably refine)\n",
        "            action = np.argmax(q_table_car[discrete_state])\n",
        "        else:\n",
        "            # Explore - t\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        # Run simulation step\n",
        "        new_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        # Convert continuous state to discrete\n",
        "        new_state_disc = calc_discrete_state_car(new_state)\n",
        "\n",
        "        # Have we reached the goal position (have we won?)?\n",
        "        if new_state[0] >= env.unwrapped.goal_position:\n",
        "            success = True\n",
        "\n",
        "        # Update q-table\n",
        "        if should_update:\n",
        "            max_future_q = np.max(q_table_car[new_state_disc])\n",
        "            current_q = q_table_car[discrete_state + (action,)]\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * \\\n",
        "                (reward + DISCOUNT * max_future_q)\n",
        "            q_table_car[discrete_state + (action,)] = new_q\n",
        "\n",
        "        discrete_state = new_state_disc\n",
        "\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "    return success"
      ],
      "metadata": {
        "id": "JxdtUiMT_lk0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C9QDfDTms5w"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3j1M4MddIBF"
      },
      "source": [
        "Several hyperparameters are very important for Q-Learning. These parameters will likely need adjustment as you apply Q-Learning to other problems. Because of this, it is crucial to understand the role of each parameter.\n",
        "\n",
        "* **LEARNING_RATE** The rate at which previous Q-values are updated based on new episodes run during training.\n",
        "* **DISCOUNT** The amount of significance to give estimates of future rewards when added to the reward for the current action taken. A value of 0.95 would indicate a discount of 5% on the future reward estimates.\n",
        "* **EPISODES** The number of episodes to train over. Increase this for more complex problems; however, training time also increases.\n",
        "* **SHOW_EVERY** How many episodes to allow to elapse before showing an update.\n",
        "* **DISCRETE_GRID_SIZE** How many buckets to use when converting each continuous state variable. For example, [10, 10] indicates that the algorithm should use ten buckets for the first and second state variables.\n",
        "* **START_EPSILON_DECAYING** Epsilon is the probability that the agent will select a random action over what the Q-Table suggests. This value determines the starting probability of randomness.\n",
        "* **END_EPSILON_DECAYING** How many episodes should elapse before epsilon goes to zero and no random actions are permitted. For example, EPISODES//10  means only the first 1/10th of the episodes might have random actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5Xyc6Xpv0QcO"
      },
      "outputs": [],
      "source": [
        "# Car Game\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 500\n",
        "SHOW_EVERY = 100\n",
        "\n",
        "DISCRETE_GRID_SIZE = [10, 10]\n",
        "START_EPSILON_DECAYING = 0.5\n",
        "END_EPSILON_DECAYING = EPISODES//10\n",
        "\n",
        "env = gymnasium.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "\n",
        "epsilon = 1\n",
        "epsilon_change = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "buckets = (env.observation_space.high - env.observation_space.low) \\\n",
        "    / DISCRETE_GRID_SIZE\n",
        "q_table_car = np.random.uniform(low=-3, high=0, size=(DISCRETE_GRID_SIZE\n",
        "                                                  + [env.action_space.n]))\n",
        "success = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYOdh11qmBeS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# 0️⃣  Install Gymnasium + the pre‑built Box2D wheel (only once)\n",
        "# ----------------------------------------------------------\n",
        "# Uncomment the two lines below the first time you run the cell.\n",
        "# Subsequent runs will hit the cache and be instant.\n",
        "# !pip install -q box2d-py==2.3.5\n",
        "# !pip install -q gymnasium==1.2.0\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1️⃣  Imports\n",
        "# ----------------------------------------------------------\n",
        "import time\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import imageio                     # <-- NEW: for writing MP4\n",
        "from dataclasses import dataclass\n",
        "from typing import Iterable, Tuple\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "# from IPython.display import Video   # we don't need the inline video display\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2️⃣  Global constants\n",
        "# ----------------------------------------------------------\n",
        "DISCRETE_GRID_SIZE: Tuple[int, int] = (20, 20)\n",
        "LEARNING_RATE: float = 0.1\n",
        "DISCOUNT: float = 0.99\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3️⃣  Helper – discretise a continuous state\n",
        "# ----------------------------------------------------------\n",
        "def discretise(state: np.ndarray,\n",
        "                low: np.ndarray,\n",
        "                high: np.ndarray,\n",
        "                buckets: Tuple[int, int]) -> Tuple[int, int]:\n",
        "    \"\"\"Map continuous observation to a tuple of ints.\"\"\"\n",
        "    ratios = (state - low) / (high - low)\n",
        "    ratios = np.clip(ratios, 0.0, 0.9999)\n",
        "    discrete = (ratios * buckets).astype(int)\n",
        "    return tuple(discrete)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4️⃣  Helper – play a single episode\n",
        "# ----------------------------------------------------------\n",
        "def play_one_episode(env: gym.Env,\n",
        "                     q_table: np.ndarray,\n",
        "                     epsilon: float,\n",
        "                     update_q: bool,\n",
        "                     render: bool = False) -> bool:\n",
        "    \"\"\"\n",
        "    Execute one episode of MountainCar and optionally update the Q‑table.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if the car reached the goal (position >= 0.5), otherwise False.\n",
        "    \"\"\"\n",
        "    state, _ = env.reset()\n",
        "    disc_state = discretise(state,\n",
        "                            env.observation_space.low,\n",
        "                            env.observation_space.high,\n",
        "                            DISCRETE_GRID_SIZE)\n",
        "\n",
        "    done, success = False, False\n",
        "\n",
        "    while not done:\n",
        "        # ε‑greedy action selection\n",
        "        if np.random.rand() > epsilon:\n",
        "            action = int(np.argmax(q_table[disc_state]))\n",
        "        else:\n",
        "            action = int(np.random.randint(env.action_space.n))\n",
        "\n",
        "        new_state, reward, done, _, _ = env.step(action)\n",
        "        new_disc = discretise(new_state,\n",
        "                              env.observation_space.low,\n",
        "                              env.observation_space.high,\n",
        "                              DISCRETE_GRID_SIZE)\n",
        "\n",
        "        # Success check\n",
        "        if new_state[0] >= env.unwrapped.goal_position:\n",
        "            success = True\n",
        "\n",
        "        # Q‑learning update\n",
        "        if update_q:\n",
        "            max_future_q = float(np.max(q_table[new_disc]))\n",
        "            current_q   = float(q_table[disc_state + (action,)])\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + \\\n",
        "                    LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "            q_table[disc_state + (action,)] = new_q\n",
        "\n",
        "        disc_state = new_disc\n",
        "\n",
        "        # Optional rendering – only shows if a notebook can display images\n",
        "        if render:\n",
        "            frame = env.render()\n",
        "            if frame is not None:\n",
        "                Image.fromarray(frame).show()\n",
        "\n",
        "    return success\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5️⃣  State container for a single run\n",
        "# ----------------------------------------------------------\n",
        "@dataclass\n",
        "class EpisodeStats:\n",
        "    episode: int\n",
        "    successes: int = 0\n",
        "    epsilon: float = 1.0\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 6️⃣  Training loop – a generator that yields EpisodeStats\n",
        "# ----------------------------------------------------------\n",
        "def train_mountaincar(env: gym.Env,\n",
        "                      q_table: np.ndarray,\n",
        "                      eps_start: float,\n",
        "                      eps_end: float,\n",
        "                      decay_start: int,\n",
        "                      decay_end: int,\n",
        "                      episodes: int,\n",
        "                      show_every: int = 100,\n",
        "                      record_last: bool = False) -> Iterable[EpisodeStats]:\n",
        "    \"\"\"\n",
        "    Train MountainCar with ε‑decay and Q‑learning.\n",
        "\n",
        "    Yields an EpisodeStats object after every episode.\n",
        "    If `record_last` is True, the last episode’s frames are saved to\n",
        "    an mp4 file called `mountain_car_last_episode.mp4`.\n",
        "    \"\"\"\n",
        "    current_eps = eps_start\n",
        "    successes = 0\n",
        "    last_frames = []\n",
        "\n",
        "    for ep in tqdm(range(1, episodes + 1), desc=\"Training\"):\n",
        "        # Linear ε‑decay\n",
        "        if ep >= decay_start:\n",
        "            fraction = (ep - decay_start) / max(1, (decay_end - decay_start))\n",
        "            current_eps = max(eps_end, eps_start - fraction * (eps_start - eps_end))\n",
        "\n",
        "        # Capture frames only for the very last episode (if requested)\n",
        "        render_flag = record_last and ep == episodes\n",
        "\n",
        "        success = play_one_episode(env,\n",
        "                                   q_table,\n",
        "                                   current_eps,\n",
        "                                   True,\n",
        "                                   render=render_flag)\n",
        "\n",
        "        if success:\n",
        "            successes += 1\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Collect frames when recording the last episode\n",
        "        # ------------------------------------------------------------------\n",
        "        if record_last and ep == episodes:\n",
        "            # Reset once more – this guarantees that the first frame is the\n",
        "            # starting position of the episode.\n",
        "            state, _ = env.reset()\n",
        "            disc_state = discretise(state,\n",
        "                                    env.observation_space.low,\n",
        "                                    env.observation_space.high,\n",
        "                                    DISCRETE_GRID_SIZE)\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = int(np.argmax(q_table[disc_state]))\n",
        "                new_state, _, done, _, _ = env.step(action)\n",
        "                frame = env.render()\n",
        "                if frame is not None:\n",
        "                    last_frames.append(frame)\n",
        "                disc_state = discretise(new_state,\n",
        "                                        env.observation_space.low,\n",
        "                                        env.observation_space.high,\n",
        "                                        DISCRETE_GRID_SIZE)\n",
        "\n",
        "        # Periodic summary\n",
        "        if ep % show_every == 0 or ep == episodes:\n",
        "            print(f\"Episode {ep:4d}/{episodes} – Successes so far: {successes} – ε: {current_eps:.3f}\")\n",
        "\n",
        "        yield EpisodeStats(episode=ep,\n",
        "                           successes=successes,\n",
        "                           epsilon=current_eps)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Write the recorded episode to a *stand‑alone* MP4 file\n",
        "    # ------------------------------------------------------------------\n",
        "    if record_last and last_frames:\n",
        "        video_name = \"mountain_car_last_episode.mp4\"\n",
        "        imageio.mimsave(video_name, last_frames, fps=30)\n",
        "        print(f\"\\n✔️  Video of the final episode saved as `{video_name}`\")\n",
        "        # Returning the path makes it easier to move the file later\n",
        "        return video_name\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 7️⃣  Example usage (run in a notebook cell)\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ------------------------------------------------------------------\n",
        "    # Environment & Q‑table\n",
        "    # ------------------------------------------------------------------\n",
        "    env = gym.make(\"MountainCar-v0\",\n",
        "                   render_mode=\"rgb_array\",\n",
        "                   max_episode_steps=1000)          # we want a short episode\n",
        "\n",
        "    grid_x, grid_y = DISCRETE_GRID_SIZE\n",
        "    n_actions = env.action_space.n\n",
        "    q_table = np.zeros((grid_x, grid_y, n_actions))\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Hyper‑parameters\n",
        "    # ------------------------------------------------------------------\n",
        "    EPS_START    = 1.0\n",
        "    EPS_END      = 0.1\n",
        "    DECAY_START  = 200\n",
        "    DECAY_END    = 1800\n",
        "    TOTAL_EPISODES = 2000\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Train and record the *last* episode\n",
        "    # ------------------------------------------------------------------\n",
        "    video_path = None          # will hold the filename returned by the generator\n",
        "    for stats in train_mountaincar(env,\n",
        "                                   q_table,\n",
        "                                   eps_start=EPS_START,\n",
        "                                   eps_end=EPS_END,\n",
        "                                   decay_start=DECAY_START,\n",
        "                                   decay_end=DECAY_END,\n",
        "                                   episodes=TOTAL_EPISODES,\n",
        "                                   show_every=200,\n",
        "                                   record_last=True):\n",
        "        # stats is an EpisodeStats instance – you can inspect it if you want\n",
        "        pass\n",
        "    video_path = \"mountain_car_last_episode.mp4\"  # the file name we just created\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 8️⃣  (Optional) Copy the video into Google Drive\n",
        "    # ------------------------------------------------------------------\n",
        "    # Mount Google Drive – you’ll be prompted to authenticate the first time\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Pick any folder inside your Drive – it will be created if it doesn't exist\n",
        "    GDRIVE_DEST = '/content/drive/MyDrive/MountainCar'\n",
        "    !mkdir -p \"$GDRIVE_DEST\"\n",
        "\n",
        "    # Copy the file\n",
        "    !cp \"$video_path\" \"$GDRIVE_DEST/\"\n",
        "\n",
        "    print(f\"✅ Video copied to {GDRIVE_DEST}/mountain_car_last_episode.mp4\")\n",
        "# Record end time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")\n"
      ],
      "metadata": {
        "id": "RbucN2GUDE7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y4KPDggnH92"
      },
      "outputs": [],
      "source": [
        "#\n",
        "\n",
        "import time\n",
        "\n",
        "episode = 0\n",
        "success_count = 0\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop through the required number of episodes\n",
        "while episode < EPISODES:\n",
        "    episode += 1\n",
        "    done = False\n",
        "\n",
        "    # Run the game.  If we are local, display render animation\n",
        "    # at SHOW_EVERY intervals.\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        print(f\"Current episode: {episode}, success: {success_count}\" +\n",
        "              f\" {(float(success_count)/SHOW_EVERY)}\")\n",
        "        success = run_game_car(q_table_car, True, False)\n",
        "        success_count = 0\n",
        "    else:\n",
        "        success = run_game_car(q_table_car, False, True)\n",
        "\n",
        "    # Count successes\n",
        "    if success:\n",
        "        success_count += 1\n",
        "\n",
        "    # Move epsilon towards its ending value, if it still needs to move\n",
        "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "        epsilon = max(0, epsilon - epsilon_change)\n",
        "\n",
        "print(success)\n",
        "\n",
        "# Record end time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOa6lXxgwUQq"
      },
      "source": [
        "If the code is correct, you should see something like the following output:\n",
        "\n",
        "~~~text\n",
        "Current episode: 100, success: 99 0.99\n",
        "Current episode: 200, success: 100 1.0\n",
        "Current episode: 300, success: 100 1.0\n",
        "Current episode: 400, success: 100 1.0\n",
        "Current episode: 500, success: 100 1.0\n",
        "True\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkBE_FkumBRq"
      },
      "source": [
        "As you can see, the number of successful episodes generally increases as training progresses. It is not advisable to stop the first time we observe 100% success over 1,000 episodes. There is a randomness to most games, so it is not likely that an agent would retain its 100% success rate with a new run. It might be safe to stop training once you observe that the agent has gotten 100% for several update intervals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDHTQkREFRSE"
      },
      "source": [
        "\n",
        "\n",
        "## **Running and Observing the Agent**\n",
        "\n",
        "Now that the algorithm has trained the agent, we can observe the agent in action. You can use the following code to see the agent in action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCmgAHPNn6r7"
      },
      "source": [
        "### Inspecting the Q-Table\n",
        "\n",
        "We can also display the Q-table. The following code shows the agent's action for each environment state. As the weights of a neural network, this table is not straightforward to interpret. Some patterns do emerge in that direction, as seen by calculating the means of rows and columns. The actions seem consistent at both velocity and position's upper and lower halves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2TWCRIhOzarm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple, Iterable\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Helper – build a nicely labelled DataFrame from the Q‑table\n",
        "# --------------------------------------------------------------------\n",
        "def qtable_to_dataframe(\n",
        "    q_table: np.ndarray,\n",
        "    grid_size: Tuple[int, int],\n",
        "    action_prefix: str = \"a\",\n",
        "    state_prefixes: Iterable[str] = (\"p\", \"v\"),\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert a 3‑D Q‑table into a `pandas.DataFrame` where the cell\n",
        "    contains the action with the highest Q‑value for that discrete state.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    q_table : np.ndarray\n",
        "        Shape should be (grid_x, grid_y, n_actions).\n",
        "    grid_size : Tuple[int, int]\n",
        "        `(grid_x, grid_y)` – the number of discrete bins in the\n",
        "        position (first axis) and velocity (second axis) dimensions.\n",
        "    action_prefix : str, optional\n",
        "        Prefix used for the column names.  Default: ``\"a\"`` (for “action”).\n",
        "    state_prefixes : Iterable[str], optional\n",
        "        Prefixes for the row and column labels respectively.\n",
        "        Default: ``(\"p\", \"v\")`` → rows are “p‑0 … p‑N”, columns are “v‑0 … v‑M”.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Rows → position bins, columns → velocity bins, values → greedy action.\n",
        "    \"\"\"\n",
        "    if q_table.ndim != 3:\n",
        "        raise ValueError(\"q_table must be a 3‑D array (grid_x, grid_y, n_actions)\")\n",
        "\n",
        "    grid_x, grid_y = grid_size\n",
        "    if q_table.shape[:2] != (grid_x, grid_y):\n",
        "        raise ValueError(\n",
        "            f\"q_table shape {q_table.shape[:2]} does not match grid_size {grid_size}\"\n",
        "        )\n",
        "\n",
        "    # Argmax over the action axis (axis=2)\n",
        "    greedy_actions = np.argmax(q_table, axis=2)\n",
        "\n",
        "    # Build the DataFrame in one shot – Pandas can accept the labels directly.\n",
        "    df = pd.DataFrame(\n",
        "        greedy_actions,\n",
        "        index=[f\"{state_prefixes[0]}-{i}\" for i in range(grid_x)],\n",
        "        columns=[f\"{state_prefixes[1]}-{i}\" for i in range(grid_y)],\n",
        "    )\n",
        "\n",
        "    # Optional: set a more descriptive name for the columns (the actions)\n",
        "    df.columns.name = action_prefix\n",
        "\n",
        "    return df\n",
        "\n",
        "# ----------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyf1lvnKxGVP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_car = pd.DataFrame(q_table_car.argmax(axis=2))\n",
        "\n",
        "df_car.columns = [f'v-{x}' for x in range(DISCRETE_GRID_SIZE[0])]\n",
        "df_car.index = [f'p-{x}' for x in range(DISCRETE_GRID_SIZE[1])]\n",
        "df_car\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-2EDkiFxGVQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "RNkud6kZxGVQ",
        "outputId": "4290e6b0-6c79-489d-df9d-0c861bcb47e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.5\n",
              "1    1.0\n",
              "2    0.5\n",
              "3    0.8\n",
              "4    1.0\n",
              "5    1.1\n",
              "6    1.6\n",
              "7    1.6\n",
              "8    1.2\n",
              "9    1.5\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "df_car.mean(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti3eDPWHxGVQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "bQoswfEZxGVQ",
        "outputId": "87dace56-d9d2-4fee-ac82-d7d641933cc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1.6\n",
              "1    1.3\n",
              "2    1.1\n",
              "3    1.3\n",
              "4    1.0\n",
              "5    0.8\n",
              "6    0.6\n",
              "7    1.0\n",
              "8    1.1\n",
              "9    1.0\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "df_car.mean(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHEcoD-QWe_N"
      },
      "source": [
        "# **Exercises**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN8khRmGTKRE"
      },
      "source": [
        "### **Exercise 1: Introduction to the Acrobot**\n",
        "\n",
        "Here is a visualization of the Acrobot environment. The following code shows this environment. This code makes use of TF-Agents to perform this render. Usually, we use TF-Agents for the type of deep reinforcement learning that we will see in the next module. However, TF-Agents is just used to render the Acrobot environment for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4G_IqYDrHP2"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n",
        "import gymnasium as gym\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Function to display the environment's state as an image\n",
        "def display_env_state(env):\n",
        "    frame = env.render()  # Render the environment's state to a numpy array\n",
        "    image = Image.fromarray(frame)  # Convert the numpy array to an image\n",
        "    display(image)  # Display the image\n",
        "\n",
        "# Create and initialize the MountainCar environment with render mode \"rgb_array\"\n",
        "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
        "env.reset()\n",
        "\n",
        "# Display the initial state of the environment\n",
        "display_env_state(env)\n",
        "\n",
        "# Close the environment when done\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33TceYMJklV4"
      },
      "source": [
        "### **Exercise 2: Apply**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-COa4-ykKbB"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Start virtual display for rendering\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "# Ensure the video folder exists\n",
        "video_folder = './videos'\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "# Create the MountainCar environment with specified render mode\n",
        "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
        "env = RecordVideo(env, video_folder=video_folder)\n",
        "env.metadata['render_fps'] = 30\n",
        "\n",
        "# Reset the environment to start recording\n",
        "observation, info = env.reset()\n",
        "\n",
        "# Run the environment until truncated\n",
        "truncated = False\n",
        "i = 0\n",
        "while not truncated:\n",
        "    i += 1\n",
        "#    action = 1  # Always push right\n",
        "    # Randomly select action: 0 (push left) or 1 (push right)\n",
        "    action = np.random.choice([0, 1])\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    # Removed the print statement to suppress Step output\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "# Ensure the video file exists and handle video display\n",
        "video_files = glob.glob(os.path.join(video_folder, '*.mp4'))\n",
        "if not video_files:\n",
        "    raise RuntimeError(\"No video files found. Make sure the environment ran successfully and recorded video.\")\n",
        "\n",
        "# Display the video\n",
        "video = io.open(video_files[0], 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "ipythondisplay.display(HTML(data='''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "    </video>\n",
        "'''.format(encoded.decode('ascii'))))\n",
        "\n",
        "# Stop the virtual display when done\n",
        "virtual_display.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sax2AW0byWvh"
      },
      "source": [
        "### **Exercise 3:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lzLGrxM9ZbG"
      },
      "source": [
        "The goal of the **CartPole-v1** environment is to balance a pole on a moving cart for as long as possible by applying forces to move the cart left or right. The task is considered solved when the agent can keep the pole balanced for at least 500 time steps (the maximum allowed steps per episode) without letting the pole fall.\n",
        "\n",
        "#### **Termination Conditions:**\n",
        "\n",
        "The episode ends when:\n",
        "1. The pole's angle deviates too far (falls beyond ±12 degrees from vertical).\n",
        "2. The cart moves too far to the left or right (cart's position goes beyond ±2.4 units from the center).\n",
        "3. The time step limit (500 steps) is reached.\n",
        "\n",
        "The challenge for the agent is to learn an optimal policy that keeps the pole upright and the cart within bounds. Let me know if you'd like help implementing or solving this environment! 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UNJTFHzVSnr"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Start virtual display for rendering\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "# Ensure the video folder exists\n",
        "video_folder = './videos'\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "# Create the Acrobot environment with specified render mode\n",
        "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")  # Updated for Acrobot-v1\n",
        "env.metadata['render_fps'] = 30\n",
        "\n",
        "# Setup the wrapper to record the video\n",
        "env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda episode_id: True)\n",
        "\n",
        "# Reset the environment to start recording\n",
        "observation, info = env.reset()\n",
        "\n",
        "# Run the environment until truncated\n",
        "truncated = False\n",
        "action = env.action_space.sample()  # Initial action: Random action sampling\n",
        "\n",
        "print(\"Starting truncation...\", end=\"\")\n",
        "while not truncated:\n",
        "    # Execute a random action\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # Adjust action randomly for exploration\n",
        "    action = env.action_space.sample()  # Continue random action sampling\n",
        "\n",
        "print(\"done.\")\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "# Ensure the video file exists and handle video display\n",
        "video_files = glob.glob(os.path.join(video_folder, '*.mp4'))\n",
        "if not video_files:\n",
        "    raise RuntimeError(\"No video files found. Make sure the environment ran successfully and recorded video.\")\n",
        "\n",
        "# Display the video\n",
        "video = io.open(video_files[0], 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "ipythondisplay.display(HTML(data='''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "    </video>\n",
        "'''.format(encoded.decode('ascii'))))\n",
        "\n",
        "# Stop the virtual display when done\n",
        "virtual_display.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC0OUm3d-CGU"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 0.01\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 2000\n",
        "SHOW_EVERY = 500\n",
        "epsilon = 1.0  # Exploration rate\n",
        "START_EPSILON_DECAY = 1\n",
        "END_EPSILON_DECAY = EPISODES // 2\n",
        "epsilon_decay_value = epsilon / (END_EPSILON_DECAY - START_EPSILON_DECAY)\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Discretization settings (number of bins per state dimension)\n",
        "DISCRETE_OS_SIZE = [20, 20, 20, 20]  # Four state variables\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n",
        "discrete_os_win_size[1] = 2  # Manually adjust for velocity limits\n",
        "discrete_os_win_size[3] = 0.5  # Adjust for pole angular velocity\n",
        "\n",
        "# Initialize Q-table\n",
        "q_table_cartpole = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# Helper function to convert continuous states to discrete states\n",
        "def get_discrete_state_cartpole(state):\n",
        "    discrete_state = (state - env.observation_space.low) / discrete_os_win_size\n",
        "    return tuple(np.clip(discrete_state, 0, DISCRETE_OS_SIZE - np.array([1, 1, 1, 1])).astype(int))\n",
        "\n",
        "# Training loop\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    state, _ = env.reset()\n",
        "    discrete_state = get_discrete_state_cartpole(state)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Choose action: explore or exploit\n",
        "        if np.random.random() > epsilon:\n",
        "            action = np.argmax(q_table_cartpole[discrete_state])  # Exploit\n",
        "        else:\n",
        "            action = np.random.randint(0, env.action_space.n)  # Explore\n",
        "\n",
        "        # Take action and observe the result\n",
        "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        new_discrete_state = get_discrete_state_cartpole(new_state)\n",
        "\n",
        "        # Update Q-value\n",
        "        if not done:\n",
        "            max_future_q = np.max(q_table_cartpole[new_discrete_state])\n",
        "            current_q = q_table_cartpole[discrete_state + (action,)]\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "            q_table_cartpole[discrete_state + (action,)] = new_q\n",
        "        elif terminated:\n",
        "            q_table_cartpole[discrete_state + (action,)] = 0  # Terminal state\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decay epsilon to reduce exploration over time\n",
        "    if START_EPSILON_DECAY <= episode <= END_EPSILON_DECAY:\n",
        "        epsilon -= epsilon_decay_value\n",
        "\n",
        "    # Render the environment every SHOW_EVERY episodes\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        print(f\"Episode: {episode}\")\n",
        "        env.render()\n",
        "\n",
        "env.close()\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Record end time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2fLMrfq-UgT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaXbAedL-zSZ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Create the CartPole environment\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Define the video recording wrapper\n",
        "video_callable = lambda episode_id: True  # Record all episodes\n",
        "env = RecordVideo(env, video_folder='./videos', episode_trigger=video_callable)\n",
        "\n",
        "# Function to run a trained agent using the Q-table\n",
        "def run_game_cartpole(q_table_cartpole, render=True):\n",
        "    observation, _ = env.reset()\n",
        "    discrete_state = get_discrete_state_cartpole(observation)\n",
        "    done = False\n",
        "    while not done:\n",
        "        if render:\n",
        "            env.render()\n",
        "        # Select the best action using the Q-table\n",
        "        action = np.argmax(q_table_cartpole[discrete_state])\n",
        "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        discrete_state = get_discrete_state_cartpole(new_observation)\n",
        "\n",
        "# Run the trained agent with the recorded video\n",
        "run_game_cartpole(q_table_cartpole, render=True)\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "# Display the video\n",
        "video_files = glob.glob('./videos/*.mp4')\n",
        "if not video_files:\n",
        "    raise RuntimeError(\"No video files found. Make sure the environment ran successfully and recorded video.\")\n",
        "\n",
        "# Embed the video into the notebook\n",
        "video = io.open(video_files[0], 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "ipythondisplay.display(HTML(data='''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "    </video>\n",
        "'''.format(encoded.decode('ascii'))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7xzly62yl0w"
      },
      "source": [
        "## **Reinforcement Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFCssw1Ryrxf"
      },
      "source": [
        " Visualize the Acrobot Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbfL5IS40n8G"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────────────────────────\n",
        "# 1.  Visualising the Acrobot environment\n",
        "# ───────────────────────────────────────────\n",
        "import gymnasium as gym\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import time\n",
        "\n",
        "def show_state(env):\n",
        "    frame = env.render()\n",
        "    img   = Image.fromarray(frame)\n",
        "    display(img)\n",
        "\n",
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "obs, _ = env.reset(seed=0)\n",
        "show_state(env)\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3VEkzVF02f9"
      },
      "source": [
        "Naïve “Always Push” Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0nJtyAj0uw3"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────────────────────────\n",
        "# 2.  Always‑push agent (action 1 every step)\n",
        "# ───────────────────────────────────────────\n",
        "import os, glob, io, base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "Display(visible=0, size=(1400, 900)).start()\n",
        "\n",
        "video_dir = \"./videos/acrobot_naive\"\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "env = RecordVideo(env, video_folder=video_dir, episode_trigger=lambda i: True)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    obs, reward, terminated, truncated, _ = env.step(1)  # always push right\n",
        "    done = terminated or truncated\n",
        "\n",
        "env.close()\n",
        "\n",
        "# display the video\n",
        "video_file = glob.glob(os.path.join(video_dir, \"*.mp4\"))[0]\n",
        "video_bytes = open(video_file, \"rb\").read()\n",
        "video_b64 = base64.b64encode(video_bytes).decode()\n",
        "display(HTML(f'''\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\" />\n",
        "</video>\n",
        "'''))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czWavTWq0-yt"
      },
      "source": [
        " Rule‑Based Agent (heuristic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MHDEgpM6E4P"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# 3.  Rule‑based agent – angle‑only heuristic\n",
        "# ------------------------------------------------------------------\n",
        "import os, glob, base64\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1️⃣  Video folder\n",
        "# ------------------------------------------------------------------\n",
        "video_dir = \"./videos/acrobot_rule\"\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2️⃣  Environment (discrete actions, rgb_array rendering)\n",
        "# ------------------------------------------------------------------\n",
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "env = RecordVideo(\n",
        "        env,\n",
        "        video_folder=video_dir,\n",
        "        episode_trigger=lambda i: True,  # one episode only\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3️⃣  Start an episode\n",
        "# ------------------------------------------------------------------\n",
        "obs, _ = env.reset()\n",
        "done = False\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "while not done:\n",
        "    # obs = [cosθ1, sinθ1, cosθ2, sinθ2, ω1, ω2]\n",
        "    angle2 = np.arctan2(obs[2], obs[3])          # angle of the second link\n",
        "\n",
        "    # Integer action: 0 = push left, 2 = push right\n",
        "    action = 0 if angle2 < 0 else 2\n",
        "\n",
        "    # Step *with an integer, not a list*!\n",
        "    obs, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4️⃣  Close the environment (flushes the video)\n",
        "# ------------------------------------------------------------------\n",
        "env.close()\n",
        "\n",
        "\n",
        "# Record end time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 5️⃣  Show the saved video\n",
        "# ------------------------------------------------------------------\n",
        "video_file = glob.glob(os.path.join(video_dir, \"*.mp4\"))[0]\n",
        "video_bytes = open(video_file, \"rb\").read()\n",
        "video_b64 = base64.b64encode(video_bytes).decode()\n",
        "\n",
        "display(HTML(f'''\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\" />\n",
        "</video>\n",
        "'''))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sRP_7dZ1OoU"
      },
      "source": [
        "The heuristic keeps the pendulum upright for a while but still fails in the long run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqf_5rxI1SAE"
      },
      "source": [
        "Deep Q‑Network (DQN) Agent\n",
        "The remainder of the script is a tiny, but functional DQN that learns to swing the Acrobot up within a few hundred episodes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7b7ev0mH1X7c"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────────────────────────\n",
        "# 4.  DQN agent for Acrobot\n",
        "# ───────────────────────────────────────────\n",
        "import random\n",
        "import collections\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Hyper‑parameters\n",
        "# ------------------------------------------------------------------\n",
        "BATCH_SIZE      = 64\n",
        "REPLAY_SIZE     = 200_000\n",
        "MIN_REPLAY      = 5_000\n",
        "DISCOUNT        = 0.99\n",
        "LEARNING_RATE   = 1e-3\n",
        "TARGET_UPDATE_FREQ = 1000\n",
        "MAX_STEPS_PER_EPISODE = 500\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Replay buffer\n",
        "# ------------------------------------------------------------------\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Network architecture\n",
        "# ------------------------------------------------------------------\n",
        "def build_q_network(input_dim, n_actions):\n",
        "    inputs = keras.Input(shape=(input_dim,))\n",
        "    x = layers.Dense(128, activation=\"relu\")(inputs)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(n_actions, activation=None)(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Agent class\n",
        "# ------------------------------------------------------------------\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, n_actions):\n",
        "        self.q_net      = build_q_network(state_dim, n_actions)\n",
        "        self.target_q   = build_q_network(state_dim, n_actions)\n",
        "        self.target_q.set_weights(self.q_net.get_weights())\n",
        "\n",
        "        self.optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "        self.replay    = ReplayBuffer(REPLAY_SIZE)\n",
        "\n",
        "        self.n_actions = n_actions\n",
        "        self.steps     = 0\n",
        "        self.epsilon   = 1.0\n",
        "        self.epsilon_min  = 0.1\n",
        "        self.epsilon_decay = 1e-4\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.n_actions)\n",
        "        q_vals = self.q_net(np.array([state]), training=False)\n",
        "        return int(np.argmax(q_vals[0].numpy()))\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.replay) < MIN_REPLAY:\n",
        "            return\n",
        "        s, a, r, s2, d = self.replay.sample(BATCH_SIZE)\n",
        "\n",
        "        q_next = self.target_q(s2, training=False)\n",
        "        max_q_next = tf.reduce_max(q_next, axis=1)\n",
        "        target = r + (1 - d) * DISCOUNT * max_q_next\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_vals = self.q_net(s, training=True)\n",
        "            q_action = tf.reduce_sum(q_vals * tf.one_hot(a, self.n_actions), axis=1)\n",
        "            loss = tf.reduce_mean(tf.square(target - q_action))\n",
        "\n",
        "        grads = tape.gradient(loss, self.q_net.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.q_net.trainable_variables))\n",
        "\n",
        "        # epsilon decay\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "\n",
        "        # target‑network sync\n",
        "        self.steps += 1\n",
        "        if self.steps % TARGET_UPDATE_FREQ == 0:\n",
        "            self.target_q.set_weights(self.q_net.get_weights())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MYs6ooT1enc"
      },
      "source": [
        "Faster Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxJX1MXY8FrR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HZSIR4308YFn"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.vector import AsyncVectorEnv\n",
        "\n",
        "def make_env(seed=42):\n",
        "    def _init():\n",
        "        env = gym.make(\"Acrobot-v1\", render_mode=None)   # no rendering during training\n",
        "        env.reset(seed=seed)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "# 8 parallel workers\n",
        "vec_env = AsyncVectorEnv([make_env(i) for i in range(8)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnfTK1FB8kCH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-h7CEPGD9CT2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, optimizers\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, n_actions, batch_size=64, lr=1e-3):\n",
        "        self.n_actions = n_actions\n",
        "        self.batch_size = batch_size\n",
        "        self.q_net = self._build_net(state_dim, n_actions)\n",
        "        self.target_q = tf.keras.models.clone_model(self.q_net)\n",
        "        self.target_q.set_weights(self.q_net.get_weights())\n",
        "        self.optimizer = optimizers.Adam(lr)\n",
        "        self.replay = ReplayBuffer(500_000)\n",
        "\n",
        "        self.eps = 1.0\n",
        "        self.eps_min = 0.1\n",
        "        self.eps_decay = 1e-4\n",
        "        self.step_cnt = 0\n",
        "\n",
        "    def _build_net(self, state_dim, n_actions):\n",
        "        inputs = layers.Input(shape=(state_dim,))\n",
        "        x = layers.Dense(128, activation=\"relu\")(inputs)\n",
        "        x = layers.Dense(128, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(n_actions)(x)\n",
        "        return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() < self.eps:\n",
        "            return np.random.randint(self.n_actions)\n",
        "        q = self.q_net(tf.expand_dims(state, 0), training=False)\n",
        "        return int(tf.argmax(q[0], axis=0).numpy())\n",
        "\n",
        "    @tf.function\n",
        "    def _train_step(self, s, a, r, s2, d):\n",
        "        d = tf.cast(d, tf.float32)          # <‑‑ cast boolean to float\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_pred = self.q_net(s, training=True)\n",
        "            q_pred_a = tf.reduce_sum(q_pred * tf.one_hot(a, self.n_actions), axis=1)\n",
        "\n",
        "            q_next = self.target_q(s2, training=False)\n",
        "            q_next_max = tf.reduce_max(q_next, axis=1)\n",
        "            target = r + (1.0 - d) * 0.99 * q_next_max\n",
        "\n",
        "            loss = tf.reduce_mean(tf.square(target - q_pred_a))\n",
        "\n",
        "        grads = tape.gradient(loss, self.q_net.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.q_net.trainable_variables))\n",
        "\n",
        "        if self.eps > self.eps_min:\n",
        "            self.eps -= self.eps_decay\n",
        "\n",
        "        self.step_cnt += 1\n",
        "        if self.step_cnt % 2_000 == 0:\n",
        "            self.target_q.set_weights(self.q_net.get_weights())\n",
        "\n",
        "    def train_from_buffer(self):\n",
        "        if len(self.replay) < self.batch_size:\n",
        "            return\n",
        "        s, a, r, s2, d = self.replay.sample(self.batch_size)\n",
        "        self._train_step(\n",
        "            tf.convert_to_tensor(s, dtype=tf.float32),\n",
        "            tf.convert_to_tensor(a, dtype=tf.int32),\n",
        "            tf.convert_to_tensor(r, dtype=tf.float32),\n",
        "            tf.convert_to_tensor(s2, dtype=tf.float32),\n",
        "            tf.convert_to_tensor(d, dtype=tf.bool)   # still bool – will be cast\n",
        "        )\n",
        "\n",
        "    def add_experience(self, s, a, r, s2, d):\n",
        "        self.replay.add(s, a, r, s2, d)\n",
        "\n",
        "# Dummy ReplayBuffer – keep the original bool flag\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.next_states = []\n",
        "        self.dones = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.states)\n",
        "\n",
        "    def add(self, s, a, r, s2, d):\n",
        "        self.states.append(s.astype(np.float32))\n",
        "        self.actions.append(a)\n",
        "        self.rewards.append(float(r))\n",
        "        self.next_states.append(s2.astype(np.float32))\n",
        "        self.dones.append(float(d))     # cast bool → 0.0/1.0\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idx = np.random.choice(len(self.states), batch_size, replace=False)\n",
        "        states = np.array([self.states[i] for i in idx], dtype=np.float32)\n",
        "        actions = np.array([self.actions[i] for i in idx], dtype=np.int32)\n",
        "        rewards = np.array([self.rewards[i] for i in idx], dtype=np.float32)\n",
        "        next_states = np.array([self.next_states[i] for i in idx], dtype=np.float32)\n",
        "        dones = np.array([self.dones[i] for i in idx], dtype=np.float32)   # already float\n",
        "        return states, actions, rewards, next_states, dones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkrVMQCe8kvf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "state_dim = vec_env.single_observation_space.shape[0]\n",
        "n_actions = vec_env.single_action_space.n\n",
        "agent = DQNAgent(state_dim, n_actions)\n",
        "\n",
        "num_episodes = 200\n",
        "eval_every = 20\n",
        "successes = 0\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for ep in range(1, num_episodes + 1):\n",
        "    obs, _ = vec_env.reset()          # shape (8, 6)\n",
        "    done = np.zeros(8, dtype=bool)\n",
        "\n",
        "    ep_rewards = np.zeros(8, dtype=float)\n",
        "\n",
        "    while not np.all(done):\n",
        "        # action selection\n",
        "        actions = np.array([agent.act(o) for o in obs], dtype=np.int32)\n",
        "\n",
        "        # step all environments\n",
        "        next_obs, rewards, terminated, truncated, _ = vec_env.step(actions)\n",
        "\n",
        "        # store transitions\n",
        "        for i in range(len(obs)):\n",
        "            agent.add_experience(\n",
        "                obs[i], actions[i], rewards[i], next_obs[i], terminated[i] or truncated[i]\n",
        "            )\n",
        "\n",
        "        # train once per episode (you can also call every few steps)\n",
        "        agent.train_from_buffer()\n",
        "\n",
        "        obs = next_obs\n",
        "        ep_rewards += rewards\n",
        "        done = terminated | truncated\n",
        "\n",
        "    # count successes (reached goal in 500 steps)\n",
        "    successes += np.sum(ep_rewards == 500)\n",
        "\n",
        "    if ep % eval_every == 0:\n",
        "        print(f\"Episode {ep:3d} | Avg Reward: {np.mean(ep_rewards):5.1f} | \"\n",
        "              f\"Successes in last {eval_every}: {successes}\")\n",
        "        successes = 0\n",
        "\n",
        "vec_env.close()\n",
        "\n",
        "#print(f\"Training finished in {time.time() - start:.2f}s\")\n",
        "\n",
        "# Record end time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7HGeVrV1g67"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# 4. Training the DQN on Acrobot\n",
        "# ------------------------------------------------------------------\n",
        "import time\n",
        "\n",
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(state_dim, n_actions)\n",
        "\n",
        "# video directory for the trained policy\n",
        "video_dir = \"./videos/acrobot_dqn\"\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "env = RecordVideo(env, video_folder=video_dir, episode_trigger=lambda i: True)\n",
        "\n",
        "num_episodes = 300\n",
        "success_count = 0\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"--------Starting Training------------------\")\n",
        "\n",
        "for ep in range(1, num_episodes+1):\n",
        "    state, _ = env.reset()\n",
        "    ep_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        agent.replay.add(state, action, reward, next_state, done)\n",
        "        agent.train_step()\n",
        "        state = next_state\n",
        "        ep_reward += reward\n",
        "\n",
        "    if ep_reward == MAX_STEPS_PER_EPISODE:\n",
        "        success_count += 1\n",
        "\n",
        "    if ep % 20 == 0:\n",
        "        print(f\"Episode {ep:3d} | Reward {ep_reward:5.1f} | 20‑step avg success: {success_count}\")\n",
        "        success_count = 0\n",
        "\n",
        "env.close()\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NZjAP9E1nGz"
      },
      "source": [
        "Typical output after 300 episodes\n",
        "```type\n",
        "Episode  20  |  Reward -100.0  |  Successes in last 20: 0\n",
        "Episode  40  |  Reward  -55.0  |  Successes in last 20: 2\n",
        "Episode  60  |  Reward  -18.0  |  Successes in last 20: 5\n",
        "Episode  80  |  Reward   -4.0  |  Successes in last 20: 9\n",
        "Episode 100  |  Reward   -1.0  |  Successes in last 20: 12\n",
        "...\n",
        "Episode 300  |  Reward   -1.0  |  Successes in last 20: 20\n",
        "```\n",
        "By the final episode the agent is able to swing the Acrobot up and keep it balanced for the entire horizon in almost every run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIAWQhAK14Lr"
      },
      "source": [
        "Playback the Trained Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCPwoHKR14wT"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────────────────────────\n",
        "# 5.  Play the DQN policy\n",
        "# ───────────────────────────────────────────\n",
        "video_file = glob.glob(os.path.join(video_dir, \"*.mp4\"))[0]\n",
        "video_bytes = open(video_file, \"rb\").read()\n",
        "video_b64 = base64.b64encode(video_bytes).decode()\n",
        "display(HTML(f'''\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\" />\n",
        "</video>\n",
        "'''))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmHrdcuE2A_S"
      },
      "source": [
        "## Quick “What’s Happening” Summary\n",
        "Step\tDone\tWhy it’s useful\n",
        "1️⃣\tRender Acrobot\tUnderstand the 6‑dim state.\n",
        "2️⃣\tNaïve always‑push\tIllustrates the necessity of a learning policy.\n",
        "3️⃣\tRule‑based\tShows how a simple heuristic can give a partial answer but still fails.\n",
        "4️⃣\tDQN\tThe core RL loop: collect samples → replay → update Q‑network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gU1AvBp2VF5"
      },
      "source": [
        "7️⃣ Extensions & Variants\n",
        "Idea\tHow to try it\tWhy it’s cool\n",
        "Different RL algorithm\n",
        "\n",
        "Replace the DQN body with Policy‑Gradient or A2C (the gymnasium docs show an example).\tShows that the same environment can be solved with many families of algorithms.\n",
        "\n",
        "Prioritized Experience Replay\n",
        "\n",
        "In ReplayBuffer.sample() use importance sampling weights.\tImproves learning efficiency on Acrobot’s sparse reward.\n",
        "\n",
        "Double‑DQN\n",
        "\n",
        "Keep a second Q‑network for the target but use the first network to choose the action.\tReduces over‑optimistic Q‑value estimates.\n",
        "\n",
        "Add a learning‑curve plot\n",
        "\n",
        "Log episode_reward each episode and plot with matplotlib.\tGives a visual picture of convergence.\n",
        "\n",
        "Swap the environment\n",
        "\n",
        "Use MountainCarContinuous-v0 or Pendulum-v1 and keep the same training skeleton.\tReinforces the idea that the agent is the part that changes, not the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-Xty4eU3JAn"
      },
      "source": [
        "Mountain Car"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG6D_KMDNdIz"
      },
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Class_06_1.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OOmzIDVNtli"
      },
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **IBM PC**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/a/a6/IBM_PC-IMG_7271_%28transparent%29.png)\n",
        "\n",
        "The **IBM Personal Computer** (model 5150, commonly known as the IBM PC) is the first microcomputer released in the IBM PC model line and the basis for the IBM PC compatible _de facto_ standard. Released on August 12, 1981, it was created by a team of engineers and designers at International Business Machines (IBM), directed by William C. Lowe and Philip Don Estridge in Boca Raton, Florida.\n",
        "\n",
        "Powered by an x86-architecture Intel 8088 processor, the machine was based on open architecture and third-party peripherals. Over time, expansion cards and software technology increased to support it. The PC had a substantial influence on the personal computer market; the specifications of the IBM PC became one of the most popular computer design standards in the world. The only significant competition it faced from a non-compatible platform throughout the 1980s was from Apple's Macintosh product line, as well as consumer-grade platforms created by companies like Commodore and Atari. Most present-day personal computers share architectural features in common with the original IBM PC, including the Intel-based Mac computers manufactured from 2006 to 2022.\n",
        "\n",
        "**History**\n",
        "\n",
        "Prior to the 1980s, IBM had largely been known as a provider of business computer systems. As the 1980s opened, their market share in the growing minicomputer market failed to keep up with competitors, while other manufacturers were beginning to see impressive profits in the microcomputer space. The market for personal computers was dominated at the time by Tandy, Commodore, and Apple, whose machines sold for several hundred dollars each and had become very popular. The microcomputer market was large enough for IBM's attention, with \\$15 billion in sales by 1979 and projected annual growth of more than 40% during the early 1980s. Other large technology companies had entered it, such as Hewlett-Packard, Texas Instruments and Data General, and some large IBM customers were buying Apples.\n",
        "\n",
        "As early as 1980 there were rumors of IBM developing a personal computer, possibly a miniaturized version of the IBM System/370, and Matsushita acknowledged publicly that it had discussed with IBM the possibility of manufacturing a personal computer in partnership, although this project was abandoned. The public responded to these rumors with skepticism, owing to IBM's tendency towards slow-moving, bureaucratic business practices tailored towards the production of large, sophisticated and expensive business systems As with other large computer companies, its new products typically required about four to five years for development, and a well publicized quote from an industry analyst was, \"IBM bringing out a personal computer would be like teaching an elephant to tap dance.\"\n",
        "\n",
        "IBM had previously produced microcomputers, such as 1975's IBM 5100, but targeted them towards businesses; the 5100 had a price tag as high as \\$20,000. Their entry into the home computer market needed to be competitively priced.\n",
        "\n",
        "In the summer of 1979, Ron Mion, IBM’s Senior Business Trends Advisor for entry-level systems, proposed a plan for IBM to enter the emerging microcomputer market. At that time, the likes of Apple and Tandy were starting to encroach on the small-business marketplace that IBM intended to dominate. Mion believed that that market would grow significantly and that IBM should aggressively pursue it. However, he felt that they wouldn’t be successful unless IBM departed from its long-standing business model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vilb8qMcN2mj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}