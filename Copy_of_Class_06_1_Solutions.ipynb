{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Copy_of_Class_06_1_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 6: Reinforcement Learning**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 6 Material\n",
        "\n",
        "* **Part 6.1: Introduction to Introduction to Gymnasium and Q-Learning**\n",
        "* Part 6.2: Stable Baselines Q-Learning\n",
        "* Part 6.3: Atari Games with Stable Baselines Neural Networks\n",
        "* Part 6.4: Future of Reinforcement Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERfMETL_-NoS",
        "outputId": "fd68c5c6-00ac-4ff1-88e5-afb0ea025335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: Using Google CoLab\n",
            "david.senseman@gmail.com\n"
          ]
        }
      ],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Gymnasium\n",
        "\n",
        "Before we can beging, we need to install Hugging Face datasets by running the code in the next cell."
      ],
      "metadata": {
        "id": "dxOKy1Sat7rz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcTY1gkD-X5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "448aa185-81d4-44d1-a11b-70f3b07ee24a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "# Install gymnasium\n",
        "!pip install --quiet gymnasium==1.2.0 pillow\n",
        "!pip install pyvirtualdisplay # > /dev/null\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# -------------------------------------------------\n",
        "# 1ï¸âƒ£  Imports & global helpers\n",
        "# -------------------------------------------------\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Optional, Iterable, Callable\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "from IPython.display import display, HTML\n",
        "from PIL import Image\n",
        "from pyvirtualdisplay import Display as VirtualDisplay\n",
        "from tqdm.auto import tqdm\n"
      ],
      "metadata": {
        "id": "uEEnGW_ey7Ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "# 1ï¸âƒ£  Imports & global helpers\n",
        "# -------------------------------------------------\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Optional, Iterable, Callable\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "from IPython.display import display, HTML\n",
        "from PIL import Image\n",
        "from pyvirtualdisplay import Display as VirtualDisplay\n",
        "from tqdm.auto import tqdm\n"
      ],
      "metadata": {
        "id": "eE6uDqG5y-17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "# 2ï¸âƒ£  Context manager for a single virtual display\n",
        "# -------------------------------------------------\n",
        "class VirtualDisplay:\n",
        "    \"\"\"A context manager that starts a headless X server and tears it\n",
        "    down automatically, even if the block raises an exception.\"\"\"\n",
        "    def __init__(self, visible=0, size=(1400, 900)):\n",
        "        self.visible = visible\n",
        "        self.size = size\n",
        "        self.display = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.display = VirtualDisplay(visible=self.visible, size=self.size)\n",
        "        self.display.start()\n",
        "        return self.display\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        if self.display:\n",
        "            self.display.stop()\n"
      ],
      "metadata": {
        "id": "XJe-KASpzRcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(\n",
        "    name: str,\n",
        "    render_mode: str = \"rgb_array\",\n",
        "    record: bool = False,\n",
        "    video_folder: Optional[str] = None,\n",
        "    seed: int = 42,\n",
        ") -> gym.Env:\n",
        "    \"\"\"\n",
        "    Create a Gymnasium environment with optional recording.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    name : str\n",
        "        Environment id (e.g. `\"MountainCar-v0\"`, `\"CartPole-v1\"`, `\"Acrobot-v1\"`).\n",
        "    render_mode : str\n",
        "        `\"rgb_array\"` for frameâ€‘byâ€‘frame rendering.\n",
        "    record : bool\n",
        "        If True, wrap the env with `RecordVideo`.\n",
        "    video_folder : Optional[str]\n",
        "        Folder where the video will be saved (required if `record=True`).\n",
        "    seed : int\n",
        "        Random seed for reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    gym.Env\n",
        "    \"\"\"\n",
        "    env = gym.make(name, render_mode=render_mode)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    if record:\n",
        "        assert video_folder is not None, \"video_folder must be supplied if record=True\"\n",
        "        os.makedirs(video_folder, exist_ok=True)\n",
        "        env.metadata[\"render_fps\"] = 30  # Keeps a consistent FPS\n",
        "        env = RecordVideo(env, video_folder=video_folder,\n",
        "                          episode_trigger=lambda x: True)  # record every episode\n",
        "    return env\n"
      ],
      "metadata": {
        "id": "OC5ZL2yizV16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C2Q57wnPy7B7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bQsJMAYxy6-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nEovIVVYy65T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Gymnasium and Q-Learning**"
      ],
      "metadata": {
        "id": "ZJrIgZkszA7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gymnasium**\n",
        "\n",
        "**Gymnasium** is a toolkit for developing and comparing reinforcement learning (RL) algorithms. It provides various environments that simulate different tasks and challenges, which agents (algorithms) can interact with to learn and improve their performance.\n",
        "\n",
        "### **How Gymnasium Can Be Useful for Computational Biologists**\n",
        "\n",
        "Gymnasium can be a valuable tool for computational biologists in several ways. Here are some potential applications:\n",
        "\n",
        "##### **1. Modeling Biological Systems**\n",
        "Gymnasium provides a platform for simulating and modeling complex biological systems. Computational biologists can use Gymnasium environments to create and test models of cellular processes, metabolic pathways, and genetic networks. This helps in understanding the dynamics of these systems and predicting their behavior under different conditions.\n",
        "\n",
        "##### **2. Reinforcement Learning for Drug Discovery**\n",
        "Reinforcement learning (RL) algorithms can be applied to drug discovery and development. By using Gymnasium environments, computational biologists can train RL agents to explore chemical spaces, optimize molecular structures, and predict the efficacy of potential drug candidates. This accelerates the drug discovery process and reduces the need for costly and time-consuming experiments.\n",
        "\n",
        "##### **3. Optimization of Experimental Protocols**\n",
        "Gymnasium can be used to optimize experimental protocols in computational biology. For example, RL agents can be trained to design efficient experimental setups, select optimal parameters, and minimize experimental errors. This leads to more accurate and reproducible results in biological research.\n",
        "\n",
        "##### **4. Simulating Evolutionary Processes**\n",
        "Gymnasium environments can simulate evolutionary processes, allowing computational biologists to study the evolution of populations, genetic diversity, and adaptation mechanisms. By modeling these processes, researchers can gain insights into the principles of evolution and apply them to areas such as conservation biology and synthetic biology.\n",
        "\n",
        "##### **5. Analyzing Biological Data**\n",
        "Gymnasium can be integrated with data analysis tools to analyze large-scale biological data. Computational biologists can use Gymnasium environments to preprocess, visualize, and interpret data from genomics, proteomics, and other omics studies. This helps in identifying patterns, correlations, and potential biomarkers.\n",
        "\n",
        "##### **6. Training and Education**\n",
        "Gymnasium can be used as an educational tool to teach computational biology concepts. Students and researchers can interact with Gymnasium environments to learn about biological systems, experiment with different algorithms, and gain hands-on experience in computational modeling and simulation.\n",
        "\n",
        "### **Example Use Case: Protein Folding**\n",
        "One specific application is the study of protein folding. Computational biologists can use Gymnasium to create environments that simulate the folding process of proteins. RL agents can be trained to predict the final folded structure of a protein based on its amino acid sequence. This has significant implications for understanding diseases related to protein misfolding and designing therapeutic interventions.\n",
        "\n",
        "By leveraging Gymnasium, computational biologists can enhance their research capabilities, accelerate discoveries, and gain deeper insights into the complexities of biological systems."
      ],
      "metadata": {
        "id": "FQ2xbLNlzFvB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA-E4NR2-NoY"
      },
      "source": [
        "## **Q-Learning**\n",
        "\n",
        "\n",
        "Q-Learning is a foundational technology upon which deep reinforcement learning is based. Before we explore deep reinforcement learning, it is essential to understand Q-Learning. Several components make up any Q-Learning system.\n",
        "\n",
        "1. **Agent** - The agent is an entity that exists in an environment that takes actions to affect the state of the environment, to receive rewards.\n",
        "2. **Environment** - The environment is the universe that the agent exists in. The environment is always in a specific state that is changed by the agent's actions.\n",
        "3. **Actions** - Steps that the agent can perform to alter the environment\n",
        "4. **Step** - A step occurs when the agent performs an action and potentially changes the environment state.\n",
        "5. **Episode** - A chain of steps that ultimately culminates in the environment entering a terminal state.\n",
        "6. **Epoch** - A training iteration of the agent that contains some number of episodes.\n",
        "7. **Terminal State** -  A state in which further actions do not make sense. A terminal state occurs when the agent has one, lost, or the environment exceeds the maximum number of steps in many environments.\n",
        "\n",
        "Q-Learning works by building a table that suggests an action for every possible state. This approach runs into several problems. First, the environment is usually composed of several continuous numbers, resulting in an infinite number of states. Q-Learning handles continuous states by binning these numeric values into ranges.\n",
        "\n",
        "Out of the box, Q-Learning does not deal with continuous inputs, such as a car's accelerator that can range from released to fully engaged. Additionally, Q-Learning primarily deals with discrete actions, such as pressing a joystick up or down. Researchers have developed clever tricks to allow Q-Learning to accommodate continuous actions.\n",
        "\n",
        "Deep neural networks can help solve the problems of continuous environments and action spaces. In the next section, we will learn more about deep reinforcement learning. For now, we will apply regular Q-Learning to the Mountain Car problem from OpenAI Gym.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Introducing the Mountain Car\n",
        "\n",
        "This section will demonstrate how Q-Learning can create a solution to the mountain car gym environment. The Mountain car is an environment where a car must climb a mountain. Because gravity is stronger than the car's engine, it cannot merely accelerate up the steep slope even with full throttle. The vehicle is situated in a valley and must learn to utilize potential energy by driving up the opposite hill before the car can make it to the goal at the top of the rightmost hill.\n",
        "\n",
        "First, it might be helpful to visualize the mountain car environment. The following code shows this environment. This code makes use of TF-Agents to perform this render. Usually, we use TF-Agents for the type of deep reinforcement learning that we will see in the next module. However, TF-Agents is just used to render the mountain care environment for now."
      ],
      "metadata": {
        "id": "GFI9xF411UuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Introduce Mountain Car\n",
        "\n",
        "import gymnasium as gym\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Function to display the environment's state as an image\n",
        "def display_env_state(env):\n",
        "    frame = env.render()  # Render the environment's state to a numpy array\n",
        "    image = Image.fromarray(frame)  # Convert the numpy array to an image\n",
        "    display(image)  # Display the image\n",
        "\n",
        "# Create and initialize the MountainCar environment with render mode \"rgb_array\"\n",
        "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
        "env.reset()\n",
        "\n",
        "# Display the initial state of the environment\n",
        "display_env_state(env)\n",
        "\n",
        "# Close the environment when done\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "VPuxwG6U3mPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBcs3yAMo_P"
      },
      "source": [
        "The mountain car environment provides the following discrete actions:\n",
        "\n",
        "* 0 - Apply left force\n",
        "* 1 - Apply no force\n",
        "* 2 - Apply right force\n",
        "\n",
        "The mountain car environment is made up of the following continuous values:\n",
        "\n",
        "* state[0] - Position\n",
        "* state[1] - Velocity\n",
        "\n",
        "The cart is not strong enough. It will need to use potential energy from the mountain behind it. The following code shows an agent that applies full throttle to climb the hill."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Apply Full Throttle\n",
        "\n"
      ],
      "metadata": {
        "id": "Qqk1IKpkZmQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "import os\n",
        "\n",
        "# Start virtual display for rendering\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "# Ensure the video folder exists\n",
        "video_folder = './videos'\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "# Create the MountainCar environment with specified render mode\n",
        "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
        "env = RecordVideo(env, video_folder=video_folder)\n",
        "env.metadata['render_fps'] = 30\n",
        "\n",
        "# Reset the environment to start recording\n",
        "observation, info = env.reset()\n",
        "\n",
        "# Run the environment until truncated\n",
        "truncated = False\n",
        "i = 0\n",
        "while not truncated:\n",
        "    i += 1\n",
        "    action = 2  # Always push right\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    # Removed the print statement to suppress Step output\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "# Ensure the video file exists and handle video display\n",
        "video_files = glob.glob(os.path.join(video_folder, '*.mp4'))\n",
        "if not video_files:\n",
        "    raise RuntimeError(\"No video files found. Make sure the environment ran successfully and recorded video.\")\n",
        "\n",
        "# Display the video\n",
        "video = io.open(video_files[0], 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "ipythondisplay.display(HTML(data='''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "    </video>\n",
        "'''.format(encoded.decode('ascii'))))\n",
        "\n",
        "# Stop the virtual display when done\n",
        "virtual_display.stop()\n"
      ],
      "metadata": {
        "id": "LiqJoVJVkmDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcdwI-0-Nod"
      },
      "source": [
        "### Example 3: Programmed Car\n",
        "\n",
        "Now we will look at a car that I hand-programmed. This car is straightforward; however, it solves the problem. The programmed car always applies force in one direction or another. It does not break. Whatever direction the vehicle is currently rolling, the agent uses power in that direction. Therefore, the car begins to climb a hill, is overpowered, and turns backward. However, once it starts to roll backward, force is immediately applied in this new direction.\n",
        "\n",
        "The following code implements this preprogrammed car."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Programmed Car\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Start virtual display for rendering\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "# Ensure the video folder exists\n",
        "video_folder = './videos'\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "# Create the MountainCar environment with specified render mode\n",
        "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
        "env.metadata['render_fps'] = 30\n",
        "\n",
        "# Setup the wrapper to record the video\n",
        "env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda episode_id: True)\n",
        "\n",
        "# Reset the environment to start recording\n",
        "observation, info = env.reset()\n",
        "\n",
        "# Run the environment until truncated\n",
        "truncated = False\n",
        "action = 2  # Initial action: Always push right\n",
        "\n",
        "print(\"Starting truncation...\", end=\"\")\n",
        "while not truncated:\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # Adjust action based on the state\n",
        "    if state[1] > 0:\n",
        "        action = 2  # Push right\n",
        "    else:\n",
        "        action = 0  # Push left\n",
        "\n",
        "print(\"done.\")\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "# Ensure the video file exists and handle video display\n",
        "video_files = glob.glob(os.path.join(video_folder, '*.mp4'))\n",
        "if not video_files:\n",
        "    raise RuntimeError(\"No video files found. Make sure the environment ran successfully and recorded video.\")\n",
        "\n",
        "# Display the video\n",
        "video = io.open(video_files[0], 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "ipythondisplay.display(HTML(data='''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "    </video>\n",
        "'''.format(encoded.decode('ascii'))))\n",
        "\n",
        "# Stop the virtual display when done\n",
        "virtual_display.stop()\n"
      ],
      "metadata": {
        "id": "U959dxVFfavV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAyqqoy9-Nog"
      },
      "source": [
        "# **Reinforcement Learning**\n",
        "\n",
        "Q-Learning is a system of rewards that the algorithm gives an agent for successfully moving the environment into a state considered successful. These rewards are the Q-values from which this algorithm takes its name. The final output from the Q-Learning algorithm is a table of Q-values that indicate the reward value of every action that the agent can take, given every possible environment state. The agent must bin continuous state values into a fixed finite number of columns.\n",
        "\n",
        "Learning occurs when the algorithm runs the agent and environment through episodes and updates the Q-values based on the rewards received from actions taken; Figure 12.REINF provides a high-level overview of this reinforcement or Q-Learning loop.\n",
        "\n",
        "**Figure 12.REINF:Reinforcement/Q Learning**\n",
        "![Reinforcement Learning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/reinforcement.png \"Reinforcement Learning\")\n",
        "\n",
        "The Q-values can dictate action by selecting the action column with the highest Q-value for the current environment state. The choice between choosing a random action and a Q-value-driven action is governed by the epsilon ($\\epsilon$) parameter, the probability of random action.\n",
        "\n",
        "Each time through the training loop, the training algorithm updates the Q-values according to the following equation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzZnd9Tj7rNc"
      },
      "source": [
        "$Q^{new}(s_{t},a_{t}) \\leftarrow \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot  \\overbrace{\\bigg( \\underbrace{\\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}}}_{\\text{new value (temporal difference target)}} - \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} \\bigg) }^{\\text{temporal difference}}$\n",
        "\n",
        "There are several parameters in this equation:\n",
        "* alpha ($\\alpha$) - The learning rate, how much should the current step cause the Q-values to be updated.\n",
        "* lambda ($\\lambda$) - The discount factor is the percentage of future reward that the algorithm should consider in this update.\n",
        "\n",
        "This equation modifies several values:\n",
        "\n",
        "* $Q(s_t,a_t)$ - The Q-table.  For each combination of states, what reward would the agent likely receive for performing each action?\n",
        "* $s_t$ - The current state.\n",
        "* $r_t$ - The last reward received.\n",
        "* $a_t$ - The action that the agent will perform.\n",
        "\n",
        "The equation works by calculating a delta (temporal difference) that the equation should apply to the old state. This learning rate ($\\alpha$) scales this delta. A learning rate of 1.0 would fully implement the temporal difference in the Q-values each iteration and would likely be very chaotic.\n",
        "\n",
        "There are two parts to the temporal difference: the new and old values. The new value is subtracted from the old value to provide a delta; the full amount we would change the Q-value by if the learning rate did not scale this value. The new value is a summation of the reward received from the last action and the maximum Q-values from the resulting state when the client takes this action. Adding the maximum of action Q-values for the new state is essential because it estimates the optimal future values from proceeding with this action.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Q-Learning Car\n",
        "\n",
        "We will now use Q-Learning to produce a car that learns to drive itself. Look out, Tesla! We begin by defining two essential functions."
      ],
      "metadata": {
        "id": "8E0wxj3r4hbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FjZnsxqIlj0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for Car Game\n",
        "\n",
        "import gymnasium\n",
        "import numpy as np\n",
        "\n",
        "# This function converts the floating point state values into\n",
        "# discrete values. This is often called binning.  We divide\n",
        "# the range that the state values might occupy and assign\n",
        "# each region to a bucket.\n",
        "def calc_discrete_state_car(state):\n",
        "    discrete_state = (state - env.observation_space.low)/buckets\n",
        "    return tuple(discrete_state.astype(int))\n",
        "\n",
        "# Run one game.  The q_table to use is provided.  We also\n",
        "# provide a flag to indicate if the game should be\n",
        "# rendered/animated.  Finally, we also provide\n",
        "# a flag to indicate if the q_table should be updated.\n",
        "def run_game_car(q_table_car, render, should_update):\n",
        "    done = False\n",
        "    discrete_state = calc_discrete_state_car(env.reset()[0])\n",
        "    success = False\n",
        "\n",
        "    while not done:\n",
        "        # Exploit or explore\n",
        "        if np.random.random() > epsilon:\n",
        "            # Exploit - use q-table to take current best action\n",
        "            # (and probably refine)\n",
        "            action = np.argmax(q_table_car[discrete_state])\n",
        "        else:\n",
        "            # Explore - t\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        # Run simulation step\n",
        "        new_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        # Convert continuous state to discrete\n",
        "        new_state_disc = calc_discrete_state_car(new_state)\n",
        "\n",
        "        # Have we reached the goal position (have we won?)?\n",
        "        if new_state[0] >= env.unwrapped.goal_position:\n",
        "            success = True\n",
        "\n",
        "        # Update q-table\n",
        "        if should_update:\n",
        "            max_future_q = np.max(q_table_car[new_state_disc])\n",
        "            current_q = q_table_car[discrete_state + (action,)]\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * \\\n",
        "                (reward + DISCOUNT * max_future_q)\n",
        "            q_table_car[discrete_state + (action,)] = new_q\n",
        "\n",
        "        discrete_state = new_state_disc\n",
        "\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "    return success"
      ],
      "metadata": {
        "id": "23drChSYm-Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1C9QDfDTms5w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3j1M4MddIBF"
      },
      "source": [
        "Several hyperparameters are very important for Q-Learning. These parameters will likely need adjustment as you apply Q-Learning to other problems. Because of this, it is crucial to understand the role of each parameter.\n",
        "\n",
        "* **LEARNING_RATE** The rate at which previous Q-values are updated based on new episodes run during training.\n",
        "* **DISCOUNT** The amount of significance to give estimates of future rewards when added to the reward for the current action taken. A value of 0.95 would indicate a discount of 5% on the future reward estimates.\n",
        "* **EPISODES** The number of episodes to train over. Increase this for more complex problems; however, training time also increases.\n",
        "* **SHOW_EVERY** How many episodes to allow to elapse before showing an update.\n",
        "* **DISCRETE_GRID_SIZE** How many buckets to use when converting each continuous state variable. For example, [10, 10] indicates that the algorithm should use ten buckets for the first and second state variables.\n",
        "* **START_EPSILON_DECAYING** Epsilon is the probability that the agent will select a random action over what the Q-Table suggests. This value determines the starting probability of randomness.\n",
        "* **END_EPSILON_DECAYING** How many episodes should elapse before epsilon goes to zero and no random actions are permitted. For example, EPISODES//10  means only the first 1/10th of the episodes might have random actions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Car Game\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 500\n",
        "SHOW_EVERY = 100\n",
        "\n",
        "DISCRETE_GRID_SIZE = [10, 10]\n",
        "START_EPSILON_DECAYING = 0.5\n",
        "END_EPSILON_DECAYING = EPISODES//10\n",
        "\n",
        "env = gymnasium.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "\n",
        "epsilon = 1\n",
        "epsilon_change = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "buckets = (env.observation_space.high - env.observation_space.low) \\\n",
        "    / DISCRETE_GRID_SIZE\n",
        "q_table_car = np.random.uniform(low=-3, high=0, size=(DISCRETE_GRID_SIZE\n",
        "                                                  + [env.action_space.n]))\n",
        "success = False\n"
      ],
      "metadata": {
        "id": "5Xyc6Xpv0QcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YYOdh11qmBeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode = 0\n",
        "success_count = 0\n",
        "\n",
        "# Loop through the required number of episodes\n",
        "while episode < EPISODES:\n",
        "    episode += 1\n",
        "    done = False\n",
        "\n",
        "    # Run the game.  If we are local, display render animation\n",
        "    # at SHOW_EVERY intervals.\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        print(f\"Current episode: {episode}, success: {success_count}\" +\n",
        "              f\" {(float(success_count)/SHOW_EVERY)}\")\n",
        "        success = run_game_car(q_table_car, True, False)\n",
        "        success_count = 0\n",
        "    else:\n",
        "        success = run_game_car(q_table_car, False, True)\n",
        "\n",
        "    # Count successes\n",
        "    if success:\n",
        "        success_count += 1\n",
        "\n",
        "    # Move epsilon towards its ending value, if it still needs to move\n",
        "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "        epsilon = max(0, epsilon - epsilon_change)\n",
        "\n",
        "print(success)"
      ],
      "metadata": {
        "id": "9Y4KPDggnH92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something like the following output:\n",
        "\n",
        "~~~text\n",
        "Current episode: 100, success: 99 0.99\n",
        "Current episode: 200, success: 100 1.0\n",
        "Current episode: 300, success: 100 1.0\n",
        "Current episode: 400, success: 100 1.0\n",
        "Current episode: 500, success: 100 1.0\n",
        "True\n",
        "~~~"
      ],
      "metadata": {
        "id": "KOa6lXxgwUQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the number of successful episodes generally increases as training progresses. It is not advisable to stop the first time we observe 100% success over 1,000 episodes. There is a randomness to most games, so it is not likely that an agent would retain its 100% success rate with a new run. It might be safe to stop training once you observe that the agent has gotten 100% for several update intervals."
      ],
      "metadata": {
        "id": "AkBE_FkumBRq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDHTQkREFRSE"
      },
      "source": [
        "\n",
        "\n",
        "## **Running and Observing the Agent**\n",
        "\n",
        "Now that the algorithm has trained the agent, we can observe the agent in action. You can use the following code to see the agent in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_mf3A0h-Nox"
      },
      "outputs": [],
      "source": [
        "# Game 1\n",
        "\n",
        "# Setup the wrapper to record the video\n",
        "env = gymnasium.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "video_callable=lambda episode_id: True\n",
        "env = RecordVideo(env, video_folder='./videos', episode_trigger=video_callable)\n",
        "\n",
        "run_game_car(q_table_car, True, False)\n",
        "\n",
        "# Display the video\n",
        "video = io.open(glob.glob('videos/*.mp4')[0], 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "ipythondisplay.display(HTML(data='''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "    </video>\n",
        "'''.format(encoded.decode('ascii'))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting the Q-Table\n",
        "\n",
        "We can also display the Q-table. The following code shows the agent's action for each environment state. As the weights of a neural network, this table is not straightforward to interpret. Some patterns do emerge in that direction, as seen by calculating the means of rows and columns. The actions seem consistent at both velocity and position's upper and lower halves."
      ],
      "metadata": {
        "id": "WCmgAHPNn6r7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyf1lvnKxGVP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_car = pd.DataFrame(q_table_car.argmax(axis=2))\n",
        "\n",
        "df_car.columns = [f'v-{x}' for x in range(DISCRETE_GRID_SIZE[0])]\n",
        "df_car.index = [f'p-{x}' for x in range(DISCRETE_GRID_SIZE[1])]\n",
        "df_car\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l-2EDkiFxGVQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNkud6kZxGVQ"
      },
      "outputs": [],
      "source": [
        "df_car.mean(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ti3eDPWHxGVQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQoswfEZxGVQ"
      },
      "outputs": [],
      "source": [
        "df_car.mean(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercises**\n"
      ],
      "metadata": {
        "id": "nHEcoD-QWe_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Introduction to the Acrobot**\n",
        "\n",
        "Here is a visualization of the Acrobot environment. The following code shows this environment. This code makes use of TF-Agents to perform this render. Usually, we use TF-Agents for the type of deep reinforcement learning that we will see in the next module. However, TF-Agents is just used to render the Acrobot environment for now."
      ],
      "metadata": {
        "id": "KN8khRmGTKRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n",
        "import gymnasium as gym\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Function to display the environment's state as an image\n",
        "def display_env_state(env):\n",
        "    frame = env.render()  # Render the environment's state to a numpy array\n",
        "    image = Image.fromarray(frame)  # Convert the numpy array to an image\n",
        "    display(image)  # Display the image\n",
        "\n",
        "# Create and initialize the MountainCar environment with render mode \"rgb_array\"\n",
        "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
        "env.reset()\n",
        "\n",
        "# Display the initial state of the environment\n",
        "display_env_state(env)\n",
        "\n",
        "# Close the environment when done\n",
        "env.close()"
      ],
      "metadata": {
        "id": "G4G_IqYDrHP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Apply**"
      ],
      "metadata": {
        "id": "33TceYMJklV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Start virtual display for rendering\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "# Ensure the video folder exists\n",
        "video_folder = './videos'\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "# Create the MountainCar environment with specified render mode\n",
        "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
        "env = RecordVideo(env, video_folder=video_folder)\n",
        "env.metadata['render_fps'] = 30\n",
        "\n",
        "# Reset the environment to start recording\n",
        "observation, info = env.reset()\n",
        "\n",
        "# Run the environment until truncated\n",
        "truncated = False\n",
        "i = 0\n",
        "while not truncated:\n",
        "    i += 1\n",
        "#    action = 1  # Always push right\n",
        "    # Randomly select action: 0 (push left) or 1 (push right)\n",
        "    action = np.random.choice([0, 1])\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    # Removed the print statement to suppress Step output\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "# Ensure the video file exists and handle video display\n",
        "video_files = glob.glob(os.path.join(video_folder, '*.mp4'))\n",
        "if not video_files:\n",
        "    raise RuntimeError(\"No video files found. Make sure the environment ran successfully and recorded video.\")\n",
        "\n",
        "# Display the video\n",
        "video = io.open(video_files[0], 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "ipythondisplay.display(HTML(data='''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "    </video>\n",
        "'''.format(encoded.decode('ascii'))))\n",
        "\n",
        "# Stop the virtual display when done\n",
        "virtual_display.stop()\n"
      ],
      "metadata": {
        "id": "t-COa4-ykKbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3:**"
      ],
      "metadata": {
        "id": "Sax2AW0byWvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of the **CartPole-v1** environment is to balance a pole on a moving cart for as long as possible by applying forces to move the cart left or right. The task is considered solved when the agent can keep the pole balanced for at least 500 time steps (the maximum allowed steps per episode) without letting the pole fall.\n",
        "\n",
        "#### **Termination Conditions:**\n",
        "\n",
        "The episode ends when:\n",
        "1. The pole's angle deviates too far (falls beyond Â±12 degrees from vertical).\n",
        "2. The cart moves too far to the left or right (cart's position goes beyond Â±2.4 units from the center).\n",
        "3. The time step limit (500 steps) is reached.\n",
        "\n",
        "The challenge for the agent is to learn an optimal policy that keeps the pole upright and the cart within bounds. Let me know if you'd like help implementing or solving this environment! ðŸš€"
      ],
      "metadata": {
        "id": "7lzLGrxM9ZbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Start virtual display for rendering\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "# Ensure the video folder exists\n",
        "video_folder = './videos'\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "# Create the Acrobot environment with specified render mode\n",
        "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")  # Updated for Acrobot-v1\n",
        "env.metadata['render_fps'] = 30\n",
        "\n",
        "# Setup the wrapper to record the video\n",
        "env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda episode_id: True)\n",
        "\n",
        "# Reset the environment to start recording\n",
        "observation, info = env.reset()\n",
        "\n",
        "# Run the environment until truncated\n",
        "truncated = False\n",
        "action = env.action_space.sample()  # Initial action: Random action sampling\n",
        "\n",
        "print(\"Starting truncation...\", end=\"\")\n",
        "while not truncated:\n",
        "    # Execute a random action\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # Adjust action randomly for exploration\n",
        "    action = env.action_space.sample()  # Continue random action sampling\n",
        "\n",
        "print(\"done.\")\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "# Ensure the video file exists and handle video display\n",
        "video_files = glob.glob(os.path.join(video_folder, '*.mp4'))\n",
        "if not video_files:\n",
        "    raise RuntimeError(\"No video files found. Make sure the environment ran successfully and recorded video.\")\n",
        "\n",
        "# Display the video\n",
        "video = io.open(video_files[0], 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "ipythondisplay.display(HTML(data='''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "    </video>\n",
        "'''.format(encoded.decode('ascii'))))\n",
        "\n",
        "# Stop the virtual display when done\n",
        "virtual_display.stop()\n"
      ],
      "metadata": {
        "id": "6UNJTFHzVSnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 0.01\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 2000\n",
        "SHOW_EVERY = 500\n",
        "epsilon = 1.0  # Exploration rate\n",
        "START_EPSILON_DECAY = 1\n",
        "END_EPSILON_DECAY = EPISODES // 2\n",
        "epsilon_decay_value = epsilon / (END_EPSILON_DECAY - START_EPSILON_DECAY)\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Discretization settings (number of bins per state dimension)\n",
        "DISCRETE_OS_SIZE = [20, 20, 20, 20]  # Four state variables\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n",
        "discrete_os_win_size[1] = 2  # Manually adjust for velocity limits\n",
        "discrete_os_win_size[3] = 0.5  # Adjust for pole angular velocity\n",
        "\n",
        "# Initialize Q-table\n",
        "q_table_cartpole = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "# Helper function to convert continuous states to discrete states\n",
        "def get_discrete_state_cartpole(state):\n",
        "    discrete_state = (state - env.observation_space.low) / discrete_os_win_size\n",
        "    return tuple(np.clip(discrete_state, 0, DISCRETE_OS_SIZE - np.array([1, 1, 1, 1])).astype(int))\n",
        "\n",
        "# Training loop\n",
        "for episode in range(EPISODES):\n",
        "    state, _ = env.reset()\n",
        "    discrete_state = get_discrete_state_cartpole(state)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Choose action: explore or exploit\n",
        "        if np.random.random() > epsilon:\n",
        "            action = np.argmax(q_table_cartpole[discrete_state])  # Exploit\n",
        "        else:\n",
        "            action = np.random.randint(0, env.action_space.n)  # Explore\n",
        "\n",
        "        # Take action and observe the result\n",
        "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        new_discrete_state = get_discrete_state_cartpole(new_state)\n",
        "\n",
        "        # Update Q-value\n",
        "        if not done:\n",
        "            max_future_q = np.max(q_table_cartpole[new_discrete_state])\n",
        "            current_q = q_table_cartpole[discrete_state + (action,)]\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "            q_table_cartpole[discrete_state + (action,)] = new_q\n",
        "        elif terminated:\n",
        "            q_table_cartpole[discrete_state + (action,)] = 0  # Terminal state\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decay epsilon to reduce exploration over time\n",
        "    if START_EPSILON_DECAY <= episode <= END_EPSILON_DECAY:\n",
        "        epsilon -= epsilon_decay_value\n",
        "\n",
        "    # Render the environment every SHOW_EVERY episodes\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        print(f\"Episode: {episode}\")\n",
        "        env.render()\n",
        "\n",
        "env.close()\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "id": "GC0OUm3d-CGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s2fLMrfq-UgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Create the CartPole environment\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Define the video recording wrapper\n",
        "video_callable = lambda episode_id: True  # Record all episodes\n",
        "env = RecordVideo(env, video_folder='./videos', episode_trigger=video_callable)\n",
        "\n",
        "# Function to run a trained agent using the Q-table\n",
        "def run_game_cartpole(q_table_cartpole, render=True):\n",
        "    observation, _ = env.reset()\n",
        "    discrete_state = get_discrete_state_cartpole(observation)\n",
        "    done = False\n",
        "    while not done:\n",
        "        if render:\n",
        "            env.render()\n",
        "        # Select the best action using the Q-table\n",
        "        action = np.argmax(q_table_cartpole[discrete_state])\n",
        "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        discrete_state = get_discrete_state_cartpole(new_observation)\n",
        "\n",
        "# Run the trained agent with the recorded video\n",
        "run_game_cartpole(q_table_cartpole, render=True)\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "# Display the video\n",
        "video_files = glob.glob('./videos/*.mp4')\n",
        "if not video_files:\n",
        "    raise RuntimeError(\"No video files found. Make sure the environment ran successfully and recorded video.\")\n",
        "\n",
        "# Embed the video into the notebook\n",
        "video = io.open(video_files[0], 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "ipythondisplay.display(HTML(data='''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "    </video>\n",
        "'''.format(encoded.decode('ascii'))))\n"
      ],
      "metadata": {
        "id": "MaXbAedL-zSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reinforcement Learning**"
      ],
      "metadata": {
        "id": "e7xzly62yl0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XFCssw1Ryrxf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeQzNFSosdys"
      },
      "source": [
        "## **Inspecting the Q-Table**\n",
        "\n",
        "We can also display the Q-table. The following code shows the agent's action for each environment state. As the weights of a neural network, this table is not straightforward to interpret. Some patterns do emerge in that direction, as seen by calculating the means of rows and columns. The actions seem consistent at both velocity and position's upper and lower halves."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(q_table_cartpole)"
      ],
      "metadata": {
        "id": "j9lTD6TDBwpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cartpole_1 = pd.DataFrame(q_table_cartpole.argmax(axis=3).flatten(), columns=[\"Best Action\"])\n",
        "\n",
        "df_cartpole_1"
      ],
      "metadata": {
        "id": "k4HanLoTC9UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Class_06_1.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "iG6D_KMDNdIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **IBM PC**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/a/a6/IBM_PC-IMG_7271_%28transparent%29.png)\n",
        "\n",
        "The **IBM Personal Computer** (model 5150, commonly known as the IBM PC) is the first microcomputer released in the IBM PC model line and the basis for the IBM PC compatible _de facto_ standard. Released on August 12, 1981, it was created by a team of engineers and designers at International Business Machines (IBM), directed by William C. Lowe and Philip Don Estridge in Boca Raton, Florida.\n",
        "\n",
        "Powered by an x86-architecture Intel 8088 processor, the machine was based on open architecture and third-party peripherals. Over time, expansion cards and software technology increased to support it. The PC had a substantial influence on the personal computer market; the specifications of the IBM PC became one of the most popular computer design standards in the world. The only significant competition it faced from a non-compatible platform throughout the 1980s was from Apple's Macintosh product line, as well as consumer-grade platforms created by companies like Commodore and Atari. Most present-day personal computers share architectural features in common with the original IBM PC, including the Intel-based Mac computers manufactured from 2006 to 2022.\n",
        "\n",
        "**History**\n",
        "\n",
        "Prior to the 1980s, IBM had largely been known as a provider of business computer systems. As the 1980s opened, their market share in the growing minicomputer market failed to keep up with competitors, while other manufacturers were beginning to see impressive profits in the microcomputer space. The market for personal computers was dominated at the time by Tandy, Commodore, and Apple, whose machines sold for several hundred dollars each and had become very popular. The microcomputer market was large enough for IBM's attention, with \\$15 billion in sales by 1979 and projected annual growth of more than 40% during the early 1980s. Other large technology companies had entered it, such as Hewlett-Packard, Texas Instruments and Data General, and some large IBM customers were buying Apples.\n",
        "\n",
        "As early as 1980 there were rumors of IBM developing a personal computer, possibly a miniaturized version of the IBM System/370, and Matsushita acknowledged publicly that it had discussed with IBM the possibility of manufacturing a personal computer in partnership, although this project was abandoned. The public responded to these rumors with skepticism, owing to IBM's tendency towards slow-moving, bureaucratic business practices tailored towards the production of large, sophisticated and expensive business systems As with other large computer companies, its new products typically required about four to five years for development, and a well publicized quote from an industry analyst was, \"IBM bringing out a personal computer would be like teaching an elephant to tap dance.\"\n",
        "\n",
        "IBM had previously produced microcomputers, such as 1975's IBM 5100, but targeted them towards businesses; the 5100 had a price tag as high as \\$20,000. Their entry into the home computer market needed to be competitively priced.\n",
        "\n",
        "In the summer of 1979, Ron Mion, IBMâ€™s Senior Business Trends Advisor for entry-level systems, proposed a plan for IBM to enter the emerging microcomputer market. At that time, the likes of Apple and Tandy were starting to encroach on the small-business marketplace that IBM intended to dominate. Mion believed that that market would grow significantly and that IBM should aggressively pursue it. However, he felt that they wouldnâ€™t be successful unless IBM departed from its long-standing business model."
      ],
      "metadata": {
        "id": "9OOmzIDVNtli"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vilb8qMcN2mj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}