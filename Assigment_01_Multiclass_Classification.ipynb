{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Assigment_01_Multiclass_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdRFfd32xmjj"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_CU7EF2xmjk"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sRFBydyxmjk"
      },
      "source": [
        "**Assignment 1:  Neural Networks for Analysis of Tabular Data**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **READ CAREFULLY**\n",
        "\n",
        "The **_first_**  digit in your myUTSA ID (e.g. \"abc123\") will determine which dataset you are to analyze for this assignment and which type of neural network (i.e. classification or regression) you will need to construct. For example, if your myUTSA ID was **vue682**, then your first digit is the number `6`.\n",
        "\n",
        "**---WARNING------WARNING------WARNING------WARNING------WARNING------WARNING---**\n",
        "\n",
        "You are **not** free to choose any dataset for this assignment. If analyze the wrong dataset, your assignment will **NOT BE GRADED**. If you are uncertain which dataset you should be working on, contact your Instructor for help. Remember, your score in this assignment will have a large impact on your course grade so please be careful.\n",
        "\n",
        "\n",
        "| First Digit myUTSA ID    | Dataset to Analyze      | Neural Network Type\n",
        "--------------------------|-------------------------|-----------------\n",
        "0                         | Hepatitis               | Binary Classification\n",
        "1                         | Coimbra Breast Cancer   | Binary Classification\n",
        "2                         | Parkinson Speech        | Binary Classification\n",
        "3                         | Indian Liver            | Binary Classification\n",
        "4                         | Thyroid Replacement     | Multiclass Classification\n",
        "5                         | Wine Quality            | Multiclass Classification\n",
        "6                         | Liver Disease           | Multiclass Classification\n",
        "7                         | Bone Marrow Transplant  | Regression\n",
        "8                         | German Breast Cancer    | Regression\n",
        "9                         | Diabetes Progression    | Regression\n",
        "\n",
        "#### **NOTE: You can only use this Colab notebook if the first digit of your _myUTSA_ ID  is between `4` and `6`.**"
      ],
      "metadata": {
        "id": "41MLUvDGEs2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Purpose of Assignments**\n",
        "\n",
        "In this course, **_Assignments_** are designed to help me (and you) assess your ability to transfer knowledge gained in completing class coding exercises to solving more realistic problems.\n",
        "\n",
        "Assignments play a pivotal role in reinforcing your learning, as they require you to apply theoretical concepts to practical scenarios. This helps solidify your understanding and enhances your problem-solving skills. By tackling these assignments independently, you develop critical thinking and the ability to synthesize information from various sources. Moreover, assignments encourage you to explore topics more deeply, fostering intellectual curiosity and promoting a deeper engagement with the subject matter. Ultimately, these assignments are not just a measure of your learning, but a means to equip you with the skills needed for real-world applications and future challenges."
      ],
      "metadata": {
        "id": "Qfs64u4eDRpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MAKE A COPY OF THIS NOTBOOK!**\n",
        "\n",
        "For your assignment to be graded, you **must** make a copy of this Colab notebook in your GDrive and you this copy as your worksheet."
      ],
      "metadata": {
        "id": "1nR0VGFVPF--"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKQylnEiLDUM"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM"
      },
      "outputs": [],
      "source": [
        "# YOU MUST RUN THIS CELL FIRST\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this assignment.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your GMAIL address **must** appear in the output in order for your work to be graded."
      ],
      "metadata": {
        "id": "vEMDiVKsMzpY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf_otSJdmp8k"
      },
      "source": [
        "# **Assigment 1: Multiclass Classification**\n",
        "\n",
        "**Assignment_01** is specifically designed to assess your ability to write the Python/Tensorflow/Keras code necessary to build neural networks that can analyze tabular data stored in a Pandas DataFrame. These analyzes include: (1) binary classification, (2) multiclass classification and (3) regression.\n",
        "\n",
        "You will use this Colab notebook **only** if the first digit in your myUTSA ID is between `4` and `6`. If that is correct, you have been assigned to perform **multiclass classification**.\n",
        "\n",
        "Unlike your class lessons, you will **not** be given examples that you can use to simply copy-and-paste code. Rather, you will be given a problem to solve and it will be up to you to use code snippets that you have been given previously to solve different aspects of this assignment. And unlike your class lessons, your will **not** be given the correct output. In other words, this assignment is basically how you would solve an actual biomedical problem."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multiclass Classification by Neural Networks**\n",
        "**Multiclass classification** of tabular data is a type of supervised learning task where the goal is to categorize observations into one of three or more possible classes based on their attributes.\n",
        "\n",
        "#### **Multiclass Classification:**\n",
        "Multiclass classification deals with problems where there are multiple possible outcomes. Examples include:\n",
        "1. **Disease Diagnosis:**\n",
        "   - What type of diabetes does the patient have? (Type 1/Type 2/Gestational)\n",
        "   - What stage is this breast tumor in? (Stage 0/Stage 1/Stage 2/Stage 3/Stage 4)\n",
        "   - What type of hypertension does the patient have? (Primary/Secondary)\n",
        "   - What variant of COVID-19 is the patient infected with? (Alpha/Beta/Delta/Omicron)\n",
        "2. **Medical Outcomes:**\n",
        "   - What is the prognosis for the patient after a heart attack? (Good/Fair/Poor)\n",
        "   - What type of response will the patient have to a specific treatment? (Positive/Negative/Neutral)\n",
        "   - What risk level does the patient have for developing heart disease? (Low/Medium/High)\n",
        "3. **Medical Conditions and Symptoms:**\n",
        "   - What type of depression is the patient experiencing? (Mild/Moderate/Severe)\n",
        "   - What severity of sleep apnea does the patient have? (Mild/Moderate/Severe)\n",
        "   - What is the patient's risk level for osteoporosis? (Low/Medium/High)\n",
        "   - What genetic condition does the patient have a predisposition for? (Condition A/Condition B/Condition C)\n",
        "4. **Medical Procedures:**\n",
        "   - What type of surgery is recommended for this patient? (Surgery A/Surgery B/Surgery C)\n",
        "   - What blood transfusion type does the patient need? (Type A/Type B/Type AB/Type O)\n",
        "   - What category does the patient fall into for a particular clinical trial? (Group 1/Group 2/Group 3)\n",
        "\n",
        "Here’s a step-by-step guide on how to perform multiclass classification using neural networks:\n",
        "#### **Data Preparation:**\n",
        "- **Collect Data:** Obtain a dataset with numerical features and a categorical target variable with multiple classes.\n",
        "- **Clean Data:** Handle missing values, outliers, and erroneous entries.\n",
        "- **Data Normalization:** Normalize your data (e.g. convert to Z-scores) to help the neural network learn more efficiently.\n",
        "- **Data Pre-Processing:** Create X- and Y-feature vectors and encode the target variable into numerical format (e.g., one-hot encoding).\n",
        "- **Split Data:** Divide your data into training and test sets.\n",
        "#### **Neural Network Model**\n",
        "- **Build the Neural Network Model:** Use TensorFlow and Keras to define the neural network architecture suitable for multiclass classification (e.g., using softmax activation in the output layer).\n",
        "- **Train the Model:** Fit the model to your training data, using the validation set to monitor performance.\n",
        "- **Evaluate the Model:** Assess the model’s performance on the test set using metrics like accuracy, precision, recall, F1-score, and Confusion Matrix:\n"
      ],
      "metadata": {
        "id": "mJhkTYiyFt3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Descriptions of Data Sets for Multiclass Classification**\n",
        "\n",
        "This section describes the various datasets, information for downloading them, and what variable(s) your network should predict. Remember, you do **not** earn and credit if you analyze the wrong dataset. Pay particular attention to the **output** variable for Multiclass Classification because this column will contain the `Y- values` for your assigned dataset. You will need to know the name of the output feature when you are constructing yor `X-` and `Y-feature vectors`."
      ],
      "metadata": {
        "id": "QD8bsNrPF-VN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Thyroid Replacement Dataset - 1st myUTSA Digit = 4**\n",
        "\n",
        "#### **Filename:** `thyroid_replacement.csv`\n",
        "#### **Output Variable (Y values):** `class`\n",
        "\n",
        "\n",
        "### **Thyroid Replacement Dataset**\n",
        "\n",
        "The Thyroid Replacement dataset contains patient records and clinical features used to predict thyroid hormone replacement therapy. The dataset includes demographic information and various thyroid-related clinical test results.\n",
        "\n",
        "#### Features:\n",
        "1. **age**: Age of the patient.\n",
        "2. **sex**: Gender of the patient (male/female).\n",
        "3. **t_med**: Use of thyroxine (Yes/No).\n",
        "5. **ant-med**: Use of antithyroid medication (Yes/No).\n",
        "6. **sick**: Whether the patient is sick (Yes/No).\n",
        "7. **pregnant**: Whether the patient is pregnant (Yes/No).\n",
        "8. **t_surgery**: Whether the patient had thyroid surgery (Yes/No).\n",
        "9. **I131_treat**: Whether the patient had radioactive iodine treatment (Yes/No).\n",
        "12. **lithium**: Whether the patient is on lithium treatment (Yes/No).\n",
        "13. **goiter**: Presence of goiter (Yes/No).\n",
        "14. **turmor**: Presence of tumor (Yes/No).\n",
        "15. **hypopituitary**: Whether the patient is hypopituitary (Yes/No).\n",
        "16. **psych**: Whether the patient has psychiatric disorders (Yes/No).\n",
        "17. **TSH**: Thyroid Stimulating Hormone level (mU/L).\n",
        "18. **T3**: Triiodothyronine level (ng/dL).\n",
        "19. **TT4**: Total Thyroxine level (μg/dL).\n",
        "20. **FTI**: Free Thyroxine Index (unitless).\n",
        "21. **TBG**: Free Thyroxine Index (unitless).\n",
        "\n",
        "\n",
        "#### Output (Y variable):\n",
        "- **class**: Indicates the thyroid hormone replacement therapy category (1: Class 1, 2: Class 2, 3: Class 3, 4: Class 4, 5: Class 5).\n"
      ],
      "metadata": {
        "id": "ncvXN0xZHEBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Wine Quality Dataset - 1st myUTSA Digit = 5**\n",
        "\n",
        "#### **Filename:** `wine_quality.csv`\n",
        "#### **Output Variable (Y values):** `quality`\n",
        "\n",
        "\n",
        "### **Wine Quality Dataset**\n",
        "\n",
        "The Wine Quality dataset contains physicochemical properties of wines, which are used to predict the quality of the wine. The dataset includes two sets of wines: red and white vinho verde wines from the north of Portugal. Each wine sample is described by 11 features and a quality score.\n",
        "\n",
        "#### Features:\n",
        "1. **color**: Wine color, red or white.\n",
        "2. **fixed acidity**: Fixed acids that remain in the wine during fermentation (e.g., tartaric acid).\n",
        "3. **volatile acidity**: Acetic acid content, which can give wine an undesirable vinegar flavor.\n",
        "4. **citric acid**: Provides freshness to wines and can contribute to flavor.\n",
        "5. **residual sugar**: The amount of sugar remaining after fermentation.\n",
        "6. **chlorides**: The amount of salt in the wine.\n",
        "7. **free sulfur dioxide**: SO₂ that is not bound to other molecules and is available to act as an antimicrobial agent.\n",
        "8. **total sulfur dioxide**: The sum of free and bound forms of SO₂.\n",
        "9. **density**: The density of the wine, closely related to alcohol and sugar content.\n",
        "10. **pH**: Measures the acidity or alkalinity of wine.\n",
        "11. **sulphates**: A wine preservative and antioxidant.\n",
        "12. **alcohol**: Alcohol content of the wine.\n",
        "\n",
        "#### Output (Y variable):\n",
        "- **quality**: Quality score of the wine, likely on a scale from 0 to 10, based on sensory data from wine experts.\n"
      ],
      "metadata": {
        "id": "mA6-PLvZHjX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Liver Disease Dataset - 1st myUTSA Digit = 6**\n",
        "\n",
        "#### **Filename:** `liver_disease.csv`\n",
        "#### **Output Variable (Y values):** `drinks`\n",
        "\n",
        "### **Liver Disease Dataset**\n",
        "\n",
        "The Liver Disease dataset contains patient records and clinical features used to predict the presence of liver disease. The dataset includes various biochemical markers and a classification label indicating liver disease status.\n",
        "\n",
        "#### Features:\n",
        "1. **class**: Indicates the presence of liver disease (`Diseased`, `No_Disease`)\n",
        "2. **mcv**: Mean Corpuscular Volume (fl)\n",
        "3. **alkphos**: Alkaline Phosphatase levels (IU/L)\n",
        "4. **sgpt**: Serum Glutamic-Pyruvic Transaminase (ALT) levels (IU/L)\n",
        "5. **sgoit**: Serum Glutamic-Oxaloacetic Transaminase (AST) levels (IU/L)\n",
        "6. **gammagt**: Gamma-Glutamyl Transferase levels (IU/L)\n",
        "\n",
        "#### Output (Y variable):\n",
        "- **drinks**: Number of alcoholic drinks consumed per day-\n"
      ],
      "metadata": {
        "id": "GbE2mekIHU2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Instructions**\n",
        "\n",
        "To make the assignment more manageable, you will given a number of specific steps to perform. To help guide you in writing your code, you will be given a specific example in a particular class lesson that you can use for a reference. For example, in **Step 1: Download and Extract Data** you are given **REF: Class_01_6 (Example 1)**. That means Example 1 in Class_01_6 provides similar code that you could use to complete that step of this assignment.\n",
        "\n",
        "### **Variable Names**\n",
        "\n",
        "In writing your code for this assignment, you are free to give your variables any name that makes sense to you. This includes the name of the DataFrame that holds your data. When you `copy-and-paste` code from earlier Class assignments, you always have to edit the name of the DataFrame to match the name you select for this assignment in **Step 1**.\n",
        "\n",
        "When it has been necessary to give an example that includes a DataFrame name, the DataFrame has been called `dataFrameDF`. You will need to edit the name `dataFrameDF` to match the actual name you have given to your DataFrame in **Step 1**.\n",
        "\n",
        "### **Can I Use AI?**\n",
        "\n",
        "You are free to use AI (e.g. Microsoft Co-Pilot) to help you complete your assignment---but you need to be very careful.\n",
        "\n",
        "While AI can be very helpful in correcting coding errors, but it can also give you code that is totally incorrect for this assignment. A small number of students in previous classes have flunked their assignment by using AI code that did not generate the correct output. Useless you give the AI a well-constructed prompt, the answer you get back might lead you in the wrong direction.\n",
        "\n",
        "If your aren't sure what you are doing, it's much, much safer to get help with any of your coding problems from your course instructor and/or course TA's."
      ],
      "metadata": {
        "id": "w4vNNCrnnn5A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho0r__E9xmjn"
      },
      "source": [
        "### Define functions\n",
        "\n",
        "The cell below creates several functions that are needed for this assignment. If you don't run this cell, you will receive errors later when you try to run some cells."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create functions for this lesson\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "def list_float_columns(dataframe):\n",
        "    \"\"\"\n",
        "    Create a list of all columns in a DataFrame that contain float values.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (pd.DataFrame): The DataFrame to check.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of column names that contain float values.\n",
        "    \"\"\"\n",
        "    float_columns = [col for col in dataframe.columns if dataframe[col].dtype == 'float64']\n",
        "    return float_columns"
      ],
      "metadata": {
        "id": "EWG5pvkXPnff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TxbZZQayNCu"
      },
      "source": [
        "## **STEP 1: Download and Extract Data**\n",
        "\n",
        "**REF: Class_01_6 (Example 1)**\n",
        "\n",
        "As usual, a coding project starts with downloading a dataset. Since your dataset is in tabular form, you should use Pandas to read the datafile and store the information in a DataFrame. You are free to choose the name for all or your variables in this assignment, including the name of your DataFrame.\n",
        "\n",
        "In the cell below, write the code to download your datafile from the course server and create a Pandas DataFrame to store your data. Use the function `display()` to show the data in 8 rows. For full credit, you need to show **all** of the columns in your DataFrame. You can do this by using the following code:\n",
        "\n",
        "~~~text\n",
        "# Set max columns and max rows\n",
        "pd.set_option('display.max_columns', dataFrameDF.shape[1])\n",
        "pd.set_option('display.max_rows', 8)\n",
        "~~~\n",
        "\n",
        "where `dataFrameDF` is the name of your DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download and Extract Data\n",
        "\n"
      ],
      "metadata": {
        "id": "H6_nGGnFcnK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see a table with a relatively large number of columns that may extend beyond the right edge of your notebook."
      ],
      "metadata": {
        "id": "aJbPZbaqshHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 2: Describe DataFrame**\n",
        "\n",
        "**REF: Class_01_6 (Example 3)**\n",
        "\n",
        "The `df.describe()` command in Python is used with pandas DataFrames. It provides a summary of statistics for each column in the DataFrame. By default, it will return the count, mean, standard deviation, min, 25th percentile (Q1), median (50th percentile), 75th percentile (Q3), and max values for numerical columns. It can be a handy tool for getting a quick overview of your dataset!\n",
        "\n",
        "Use the `df.describe()` command to summaries the data in your DataFrame. Make sure to replace the `df` with the actual name of your DataFrame.\n",
        "\n",
        "Again use these commands to set the display options:\n",
        "\n",
        "~~~text\n",
        "# Set max columns and max rows\n",
        "pd.set_option('display.max_columns', dataFrameDF.shape[1])\n",
        "pd.set_option('display.max_rows', 8)\n",
        "~~~\n"
      ],
      "metadata": {
        "id": "Q_inb_cq2jWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Describe DataFrame\n",
        "\n"
      ],
      "metadata": {
        "id": "XQ4tcdRW23-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see a table with a relatively large number of columns that may extend beyond the right edge of your notebook and 8 rows countaining the summary statistics for each column."
      ],
      "metadata": {
        "id": "co5hZV6yuUu2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ-JOutqfmgw"
      },
      "source": [
        "## **STEP 3: Find Missing Values**\n",
        "\n",
        "**REF: Class_01_6 (Example 4)**\n",
        "\n",
        "In **Biostatistics**, finding and replacing missing values is crucial for several reasons:\n",
        "\n",
        "1. **Preserving Data Integrity**: Missing values can distort the analysis, leading to biased results. By addressing missing values, you ensure that your conclusions are based on complete and accurate data.\n",
        "\n",
        "2. **Statistical Validity**: Many statistical tests and models require complete data. Missing values can reduce the statistical power of these tests, making it difficult to detect real effects.\n",
        "\n",
        "3. **Avoiding Data Loss**: Simply discarding rows or columns with missing values can lead to a significant loss of data, especially if the dataset is already small. Imputing missing values helps retain as much data as possible.\n",
        "\n",
        "4. **Model Accuracy**: Machine learning models can be sensitive to missing data. Handling missing values appropriately can improve the performance and accuracy of predictive models.\n",
        "\n",
        "5. **Consistency**: Different columns in a dataset may have varying levels of missing data. Addressing these inconsistencies helps in creating a more uniform dataset, which is easier to analyze and interpret.\n",
        "\n",
        "The `df.isnull()` command in pandas is used to detect missing values in a DataFrame. It returns a DataFrame of the same shape as the original, but with Boolean values: `True` where the value is missing (`NaN`) and `False` where the value is not missing.\n",
        "\n",
        "In the cell below, use the command `df.isnull()` to find and print out any missing values in your DataFrame.\n",
        "\n",
        "To make sure you see all of the values, use this code to set your display output:\n",
        "\n",
        "~~~text\n",
        "# Set max columns and max rows\n",
        "pd.set_option('display.max_columns', dataFrameDF.shape[1])\n",
        "pd.set_option('display.max_rows', dataFrameDF.shape[0])\n",
        "\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Find Missing Values\n",
        "\n"
      ],
      "metadata": {
        "id": "ak3eLdlyfmgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the your output and see if one (or more) columns have the word `True` after them. These columns contain one (or more) missing values. Make note of the column name since your will need to handle these missing values in the next step.  "
      ],
      "metadata": {
        "id": "reAsY9rh1t-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 4: Replace Missing Values**\n",
        "\n",
        "**REF: Class_01_6 (Example 6)**\n",
        "\n",
        "One common strategy for replacing missing values is to use the `median` of the column to replace the missing value. The `median` is used instead of the column `mean` since the `median` is a robust measure of central tendency and is less affected by outliers compared to the `mean`.\n",
        "\n",
        "To do this:\n",
        "1. Calculate the median of the column: Use the `median()` function to find the median value of the column.\n",
        "\n",
        "2. Replace the missing values: Use the `fillna()` function to replace the missing values with the median.\n",
        "\n",
        "If your DataFrame had one (or more) columns with missing values, you will need write the Python code to replace these missing values with the column's `median` value in the cell below.\n",
        "\n",
        "After you have replaced the missing values, again use the `df.isnull()` command to print out the names of columns with missing values to make sure all of this missing values have been taken care of. (Just use the same code you wrote in **Step 3**.)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tkgw7gJk3cTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Replace Missing Values\n",
        "\n"
      ],
      "metadata": {
        "id": "7UMnNd3_5ecr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see the same output as in **Step 3**, but this time all of the column names should be followed by the word `False`."
      ],
      "metadata": {
        "id": "KFgGZy4N3xN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 5: Display Non-numeric Categories**\n",
        "\n",
        "**REF: Class_02_2 (Example 3 - Step 1)**\n",
        "\n",
        "When building neural networks it is especially important to know which columns contain non-numeric data (\"strings\").\n",
        "\n",
        "In the cell below, write the code to print out a list of columns in your DataFrame that contain non-numeric data."
      ],
      "metadata": {
        "id": "lXmFdpAkOd_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Display non-numeric categories\n",
        "\n"
      ],
      "metadata": {
        "id": "rhOyTc5kO2Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 6: Print Names in a Categorical Column**\n",
        "\n",
        "**REF: Class_02_2 (Example 3 - Step 3)**\n",
        "\n",
        "In the cell below, write the code to print out a list of categorical values (strings) that are in the non-numeric column found in **Step 5**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8kuMgUcZO-bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Print names in a categorical column\n",
        "\n"
      ],
      "metadata": {
        "id": "nZ4SqrJpO-G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output above gives the string names that you need to map to integers in next step.\n"
      ],
      "metadata": {
        "id": "HXRf1YAzPN79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 7: Map Strings to Integer Values**\n",
        "\n",
        "**REF: Class_02_2 (Example 3 - Step 2)**\n",
        "\n",
        "In the cell below, write the code to map each string shown in the output above to either the integer `0` or `1`.\n",
        "\n",
        "To make sure your mapping worked as intended, use the `display(df)` function to display your updated DataFrame."
      ],
      "metadata": {
        "id": "B7m-F9jdPRVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Map Strings to Integer Values\n",
        "\n"
      ],
      "metadata": {
        "id": "KElaK5n3PRCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct your output should be the same as **Step 1** except that the strings in the non-numeric column should now be integers."
      ],
      "metadata": {
        "id": "GZcdggqnP1qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 8: Shuffle and Reindex your DataFrame**\n",
        "\n",
        "**REF: Class_02_3 (Example 2)**\n",
        "\n",
        "**Shuffling and reindexing** the data are important steps when building neural networks for several reasons:\n",
        "\n",
        "1. **Preventing Overfitting**: When the data is in a specific order, the model might learn patterns that are a result of the order rather than the actual data. Shuffling helps to prevent the model from overfitting to these spurious patterns.\n",
        "\n",
        "2. **Improving Generalization**: By shuffling the data, you ensure that the model is exposed to a wide variety of examples during training, which helps it to generalize better to new, unseen data.\n",
        "\n",
        "3. **Reducing Bias**: Shuffling ensures that the distribution of data is more uniform across the training batches. This reduces the risk of introducing bias, especially if the data has some inherent order that might otherwise affect the model's performance.\n",
        "\n",
        "4. **Enhancing Convergence**: Neural networks often train faster and more reliably when the data is shuffled. This is because the model updates its parameters in a more representative and balanced manner.\n",
        "\n",
        "5. **Ensuring Robustness**: By reindexing the data, you avoid potential issues that could arise from relying on the original indices, which might have some underlying structure or grouping that could affect the training process.\n",
        "\n",
        "\n",
        "In the cell below, write the code to shuffle and reindex your DataFrame. Then display your shuffled DataFrame using the following display settings:\n",
        "\n",
        "~~~text\n",
        "# Set max columns and max rows\n",
        "pd.set_option('display.max_columns', dataFrameDF.shape[1])\n",
        "pd.set_option('display.max_rows', 8)\n",
        "~~~"
      ],
      "metadata": {
        "id": "dGyzXBigeFpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Shuffle and Reindex your DataFrame\n",
        "\n"
      ],
      "metadata": {
        "id": "195gnTAFeY8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, the output generated above by **Step 8** should look almost identical to the output you got after running **Step 7** except that this index numbers shown in the leftmore column should be in a random order."
      ],
      "metadata": {
        "id": "KL3JgX7F0UY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 9: Normalize Data to the Z-values**\n",
        "\n",
        "**REF: Class_02_2 (Example 2)**\n",
        "\n",
        "When building neural networks, it is important to normalize the data using Z-score normalization for several reasons:\n",
        "\n",
        "1. **Standardization**: Z-score normalization standardizes the features to have a mean of 0 and a standard deviation of 1. This ensures that all features contribute equally to the model and prevents features with larger scales from dominating the learning process.\n",
        "\n",
        "2. **Improved Convergence**: Neural networks often use gradient-based optimization algorithms. Normalized data can lead to more stable and faster convergence of these algorithms because the gradients are more balanced and less likely to explode or vanish.\n",
        "\n",
        "3. **Enhanced Performance**: Normalized data often leads to better model performance. By bringing all features to a similar scale, the model can learn more effectively and produce more accurate predictions.\n",
        "\n",
        "4. **Handling Outliers**: Z-score normalization reduces the impact of outliers by transforming the data to a standard scale. Outliers will have high positive or negative Z-scores, but their influence will be mitigated compared to unnormalized data.\n",
        "\n",
        "5. **Consistency**: Normalization ensures that different features are on the same scale, which is particularly important when combining multiple datasets or using features with different units of measurement.\n",
        "\n",
        "To make things easier, only normalize the `float` data in your dataset by converting these numbers into their Z-values.\n",
        "\n",
        "In the cell below, use the function `list_float_columns()` to find the names of all of the columns in your DataFrame that contain float number. (NOTE: This is custom function that was created near the beginning of the assignment). Once you know the column names, use can use the `zscore()` function to normalize the values only in the columns containing floats. Print out the names of the columns containing floats, and the first five Z-scores in each of these columns."
      ],
      "metadata": {
        "id": "mcWklesN1kjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Normalize Data to the Z-values\n",
        "\n"
      ],
      "metadata": {
        "id": "b_2pzz9Z1h6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HINT:** If you see this error:\n",
        "\n",
        "~~~text\n",
        "---------------------------------------------------------------------------\n",
        "NameError                                 Traceback (most recent call last)\n",
        "<ipython-input-8-9b058d8fbfea> in <cell line: 0>()\n",
        "      4 from scipy.stats import zscore\n",
        "      5\n",
        "----> 6 float_columns = list_float_columns(dataFrameDF)\n",
        "      7 print(f\"Columns with float values: {float_columns}\")\n",
        "      8\n",
        "\n",
        "NameError: name 'list_float_columns' is not defined\n",
        "~~~~\n",
        "\n",
        "it means you didn't run the cell called `Define functions` at the beginning of this assignment.\n"
      ],
      "metadata": {
        "id": "QNX_KVk_8GSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 10: Pre-process Data for Neural Network Training**\n",
        "\n",
        "**REF: Class_04_3 (Example 9)**\n",
        "\n",
        "In the cell below, write the code to preprocess the data in your DataFrame to make it ready to feed into your neural network.\n",
        "\n",
        "**NOTES: Please follow these directions carefully:**\n",
        "\n",
        "1. Since you have already converted your float values into their Z-scores, you should **not** normalize any data during your pre-processing. In other words, converting Z-scores into Z-scores, a second time, is not a good thing.\n",
        "\n",
        "2. Basically all you need to do is write the code to generate your `X-feature vector` and your `Y-feature vector`. Your `Y-values` will be in the column that is the output variable. The name of the output variable (`Y` values) for your particular dataset was specified in the dataset description at the start of this assignment.\n",
        "\n",
        "3. When generating your `X-feature vector`, you should use _all_ of the columns in your DataFrame **EXCEPT** for the column containing the `Y-values`.\n",
        "\n",
        "4. You do **not** need to use the code in the snippet: `Standardize ranges in numeric column`. You have already converted the numeric columns (in this case `floats`) to their Z-scores.\n",
        "\n",
        "5. Since you will be building a **Multiclass Classification** neural network, you **must** one-hot encode the Y-values when generating your `Y-feature vector`.\n",
        "\n",
        "6. Do **not** split your data into training and test set yet. You will do the split later.\n",
        "\n",
        "7. When you are done, generating both your `X-` and `Y-` feature vectors, print out the first 4 values in each vector."
      ],
      "metadata": {
        "id": "cXtCEnW7BsR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10 - Preprocess Data for Neural Network Training\n",
        "\n"
      ],
      "metadata": {
        "id": "4KOEkmU5Bhc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 11: Construct and Compile Neural Network**\n",
        "\n",
        "**REF: Class_04_2 (Example 10)**\n",
        "\n",
        "In the cell below, use the Keras/Tensorflow libraries to split your data into `test` and `train` splits, making the test size = 0.25, and set the random state to `42`.\n",
        "\n",
        "Then construct and compile a binary classification neural network with 3 hidden layers but do **not** start your training in this step.\n",
        "\n",
        "To construt your model, you can use the code chunk in Example 10 in the section that starts with:\n",
        "\n",
        "~~~text\n",
        "# Construct model-----------------------------------------------------\n",
        "~~~\n",
        "Since this neural network will perform multiclass classification, there should be same number of neurons in the output layer as their are Y classes.\n",
        "\n",
        "After you construct it,  complie your model. To compile your model you can use the code chunk in Example 3 in the section that starts with:\n",
        "\n",
        "~~~text\n",
        "# Compile model------------------------------------------------------------------\n",
        "~~~\n",
        "\n",
        "Do **not** start training your model yet. This will be done in a separate step."
      ],
      "metadata": {
        "id": "1M0WWgmKS_NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Construct and compile neural network\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ov0wv7EETuRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should **not** see any output after running the previous cell."
      ],
      "metadata": {
        "id": "uyosSEtjprAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 12: Print Summary of Your Model**\n",
        "\n",
        "**REF: Class_04_2**\n",
        "\n",
        "The `model.summary()` command in deep learning frameworks like Keras and TensorFlow provides a detailed summary of the neural network model. This summary includes useful information about the model's architecture, including:\n",
        "\n",
        "1. **Layer Names and Types:** The name and type (e.g., Dense, Conv2D, LSTM) of each layer in the model.\n",
        "\n",
        "2. **Output Shape:** The shape of the output produced by each layer.\n",
        "\n",
        "3. **Number of Parameters:** The total number of trainable and non-trainable parameters in each layer. This includes both the weights and biases.\n",
        "\n",
        "4. **Model Parameters Summary:** A total count of all trainable and non-trainable parameters in the model.\n",
        "\n",
        "In the cell below, use the `model.summary()` command to print out the information about your neural network.\n",
        "\n"
      ],
      "metadata": {
        "id": "aaZy2UeUprey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Print Summary of Your Model\n",
        "\n"
      ],
      "metadata": {
        "id": "1CKXmI-VpeiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 13: Create Early Stopping Monitor**\n",
        "\n",
        "**REF: Class_04_2 (Example 9)**\n",
        "\n",
        "An **Early Stopping Monitor** is a technique used during the training of neural networks to prevent overfitting and improve the model's generalization to new, unseen data. It works by monitoring the performance of the model on a validation dataset and stopping the training process when the performance starts to degrade.\n",
        "\n",
        "#### Here’s how it works:\n",
        "\n",
        "1. **Monitoring Performance**: Early stopping keeps track of a specific metric, such as validation loss or validation accuracy, during each epoch of training.\n",
        "\n",
        "2. **Patience**: It has a parameter called \"patience,\" which defines the number of epochs to wait for an improvement in the monitored metric before stopping the training. If the performance does not improve for a specified number of epochs, the training is stopped.\n",
        "\n",
        "3. **Restore Best Weights**: In some implementations, early stopping can also restore the model weights to the state that resulted in the best performance on the validation set.\n",
        "\n",
        "#### Benefits of early stopping include:\n",
        "\n",
        "- **Preventing Overfitting**: By stopping training when the model starts to overfit the training data, early stopping helps maintain good generalization performance.\n",
        "- **Saving Time and Resources**: It avoids unnecessary training epochs, saving computational resources and time.\n",
        "\n",
        "In the cell below, write the code to create an Early Stopping Monitor that monitors `val_loss`. Set the parameter `patience` to `10`.\n",
        "\n"
      ],
      "metadata": {
        "id": "AMF_1asAp9_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 13: Create early stopping monitor\n",
        "\n"
      ],
      "metadata": {
        "id": "arpKI32lqOXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should not see any output."
      ],
      "metadata": {
        "id": "-tz63waED2bp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 14: Train the Model**\n",
        "\n",
        "In the cell below, write the Python code to train the neural network that you constructed in **Step 11**. Set the number of epochs to `100`. Make sure the parameter `verbose` is set to `2` so that the output of each epoch is written out."
      ],
      "metadata": {
        "id": "KE-301Rqqa8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Train the Model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "341bu2ql028Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 15: Convert Prediction Probabilites into Actual Prediction Values**\n",
        "\n",
        "**REF: Class_04_2 (Example 11)**\n",
        "\n",
        "In the cell below, write the code to convert the prediction probabilities from your neural network model into actual prediction probabilities using the `df.armax()` method. Print out the prediction probabilites for the first 6 samples, and then print out the prediction values for the same 6 samples."
      ],
      "metadata": {
        "id": "Y9U0cIUT6cFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 15: Convert Prediction Probabilites into Actual Prediction Values\n",
        "\n"
      ],
      "metadata": {
        "id": "juq_nz616j65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP 16: Compute the Percent Accuracy**\n",
        "\n",
        "**REF: Class_04_2 (Example 11)**\n",
        "\n",
        "In the cell below, compute the percent accuracy of your neural network model. Print out the first 6 values in your `Y_compare` variable as well as your percent accuracy score.\n",
        "\n",
        "NOTE: **Step 16** uses the variable holding the actual prediction values generated in **Step 15**, so you need to run **Step 15** before your run this step."
      ],
      "metadata": {
        "id": "BpYpDwAbr3Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 16: Compute percent accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "opla_00453VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atkFq1eGxmjw"
      },
      "source": [
        "## **Assignment Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Assignment_01.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Poly-A Tail**\n",
        "\n",
        "## **DeepSeek**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/DeepSeek_logo.svg/1920px-DeepSeek_logo.svg.png)\n",
        "\n",
        "**DeepSeek** (Chinese: 深度求索; pinyin: Shēndù Qiúsuǒ) is a Chinese artificial intelligence company that develops open-source large language models (LLMs). Based in Hangzhou, Zhejiang, it is owned and funded by Chinese hedge fund High-Flyer, whose co-founder, Liang Wenfeng, established the company in 2023 and serves as its CEO.\n",
        "\n",
        "The DeepSeek-R1 model provides responses comparable to other contemporary large language models, such as OpenAI's GPT-4o and o1. It is trained at a significantly lower cost—stated at US$6 million compared to $100 million for OpenAI's GPT-4 in 2023—and approximately a tenth of the computing power used for Meta's comparable model, LLaMA 3.1. DeepSeek's AI models were developed amid United States sanctions on China and other countries for chips used to develop artificial intelligence, which were intended to restrict the ability of these countries to develop advanced AI systems. Lesser restrictions were later announced that would affect all but a few countries.\n",
        "\n",
        "On 10 January 2025, DeepSeek released its first free chatbot app, based on the DeepSeek-R1 model, for iOS and Android; by 27 January, DeepSeek had surpassed ChatGPT as the most-downloaded free app on the iOS App Store in the United States,[10] causing Nvidia's share price to drop by 18%. DeepSeek's success against larger and more established rivals has been described as \"upending AI\"[10] and ushering in \"a new era of AI brinkmanship\". DeepSeek's compliance with Chinese government censorship policies and its data collection practices have also raised concerns over privacy and information control in the model, prompting regulatory scrutiny in multiple countries.\n",
        "\n",
        "DeepSeek makes its generative artificial intelligence algorithms, models, and training details open-source, allowing its code to be freely available for use, modification, viewing, and designing documents for building purposes.However, reports indicate that the API version hosted in China applies content restrictions in accordance with local regulations, limiting responses on topics such as the Tiananmen Square massacre and Taiwan’s status. The company reportedly vigorously recruits young AI researchers from top Chinese universities, and hires from outside the computer science field to diversify its models' knowledge and abilities.\n",
        "\n",
        "**Background**\n",
        "\n",
        "In February 2016, High-Flyer was co-founded by AI enthusiast Liang Wenfeng, who had been trading since the 2007–2008 financial crisis while attending Zhejiang University. They began stock-trading with a deep learning model running on GPU on October 21, 2016. Prior to this, they used CPU-based models, mainly linear models. Most trading was done by AI by the end of 2017.\n",
        "\n",
        "By 2019, he established High-Flyer as a hedge fund focused on developing and using AI trading algorithms. By 2021, High-Flyer exclusively used AI in trading, often using Nvidia chips. DeepSeek has made its generative artificial intelligence chatbot open source, meaning its code is freely available for use, modification, and viewing. This includes permission to access and use the source code, as well as design documents, for building purposes.\n",
        "\n",
        "In 2021, while running High-Flyer, Liang began stockpiling Nvidia GPUs for an AI project.[20] According to 36Kr, Liang had built up a store of 10,000 Nvidia A100 GPUs, which are used to train AI, before the United States federal government imposed AI chip restrictions on China.\n",
        "\n",
        "On 14 April 2023,[22] High-Flyer announced the start of an artificial general intelligence lab dedicated to research developing AI tools separate from High-Flyer's financial business. Incorporated on 17 July 2023, with High-Flyer as the investor and backer, the lab became its own company, DeepSeek. Venture capital firms were reluctant to provide funding, as they considered it unlikely that the venture would be able to generate an \"exit\" in a short period of time.\n",
        "\n",
        "On May 16, 2023, the company Beijing DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd. incorporated under the control of Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd. As of May 2024, Liang Wenfeng held 84% of DeepSeek through two shell corporations.\n",
        "\n",
        "After releasing DeepSeek-V2 in May 2024, which offered strong performance for a low price, DeepSeek became known as the catalyst for China's AI model price war. It was quickly dubbed the \"Pinduoduo of AI\", and other major tech giants such as ByteDance, Tencent, Baidu, and Alibaba began to cut the price of their AI models to compete with the company. Despite the low price charged by DeepSeek, it was profitable compared to its rivals that were losing money.\n",
        "\n",
        "DeepSeek is focused on research and has no detailed plans for commercialization, which also allows its technology to avoid the most stringent provisions of China's AI regulations, such as requiring consumer-facing technology to comply with the government's controls on information.\n",
        "\n",
        "DeepSeek's hiring preferences target technical abilities rather than work experience, resulting in most new hires being either recent university graduates or developers whose AI careers are less established. Likewise, the company recruits individuals without any computer science background to help its technology understand other topics and knowledge areas, including being able to generate poetry and perform well on the notoriously difficult Chinese college admissions exams (Gaokao).\n",
        "\n",
        "**Training framework**\n",
        "\n",
        "High-Flyer/DeepSeek has built at least two computing clusters, Fire-Flyer (萤火一号) and Fire-Flyer 2 (萤火二号). Fire-Flyer began construction in 2019 and finished in 2020, at a cost of 200 million yuan. It contained 1,100 GPUs interconnected at a rate of 200 Gbps. It was 'retired' after 1.5 years in operation. Fire-Flyer 2 began construction in 2021 with a budget of 1 billion yuan.[18] It was reported that in 2022, Fire-Flyer 2's capacity had been utilized at over 96%, totaling 56.74 million GPU hours. Of those GPU hours, 27% was used to support scientific computing outside the company.\n",
        "\n",
        "Fire-Flyer 2 consisted of co-designed software and hardware architecture. On the hardware side, there are more GPUs with 200 Gbps interconnects. The cluster is divided into two \"zones\", and the platform supports cross-zone tasks. The network topology was two fat trees, chosen for its high bisection bandwidth. On the software side, there are\n",
        "\n",
        "* **3FS (Fire-Flyer File System):** A distributed parallel file system. It was specifically designed for asynchronous random reads from a dataset, and uses Direct I/O and RDMA Read. In contrast to standard Buffered I/O, Direct I/O does not cache data. Caching is useless for this case, since each data read is random, and would not be reused.\n",
        "* **hfreduce:** Library for asynchronous communication, originally designed to replace Nvidia Collective Communication Library (NCCL).[30] It was mainly used for allreduce, especially of gradients during backpropagation. It is asynchronously run on the CPU to avoid blocking kernels on the GPU.[28] It uses two-tree broadcast like NCCL.\n",
        "* **hfai.nn:** Software library of commonly used operators in neural network training, similar to torch.nn in PyTorch.\n",
        "* **HaiScale Distributed Data Parallel (DDP):** Parallel training library that implements various forms of parallelism in deep learning such as Data Parallelism (DP), Pipeline Parallelism (PP), Tensor Parallelism (TP), Experts Parallelism (EP), Fully Sharded Data Parallel (FSDP) and Zero Redundancy Optimizer (ZeRO). It is similar to PyTorch DDP, which uses NCCL on the backend.\n",
        "* **HAI Platform:** Various applications such as task scheduling, fault handling, and disaster recovery.\n",
        "During 2022, Fire-Flyer 2 had 5000 PCIe A100 GPUs in 625 nodes, each containing 8 GPUs. At the time, they chose to exclusively use PCIe instead of DGX version of A100, since at the time the models they trained could fit within a single 40 GB GPU VRAM, so there was no need for the higher bandwidth of DGX (i.e. they required only data parallelism but not model parallelism).[30] Later, they also incorporated NVLinks and NCCL, to train larger models that required model parallelism."
      ],
      "metadata": {
        "id": "jNtya3vqufwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwn0SR-xOuui"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}