{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Introduction to Large Language Models (LLMs)\n",
        "* Part 4.2: Chatbots\n",
        "* **Part 4.3: Image Generation with StableDiffusion**\n",
        "* Part 4.4: Image Generation with DALL-E\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded.\n"
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Accelerated Run-time Check**\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be running a Graphics Processing Unit (GPU)."
      ],
      "metadata": {
        "id": "0kumRAuI0_sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell second\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def check_device():\n",
        "    # Check for available devices\n",
        "    devices = tf.config.list_physical_devices()\n",
        "\n",
        "    # Initialize device flags\n",
        "    cpu = False\n",
        "    gpu = False\n",
        "    tpu = False\n",
        "\n",
        "    # Check device types\n",
        "    for device in devices:\n",
        "        if device.device_type == 'CPU':\n",
        "            cpu = True\n",
        "        elif device.device_type == 'GPU':\n",
        "            gpu = True\n",
        "        elif device.device_type == 'TPU':\n",
        "            tpu = True\n",
        "\n",
        "    # Output device status\n",
        "    if tpu:\n",
        "        print(\"Running on TPU\")\n",
        "        print(\"WARNING: You must run this assigment using a GPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now!\")\n",
        "    elif gpu:\n",
        "        print(\"Running on GPU\")\n",
        "        print(\"You are using a GPU hardware accelerator--You're good to go!\")\n",
        "    elif cpu:\n",
        "        print(\"Running on CPU\")\n",
        "        print(\"WARNING: You must run this assigment using a GPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now!\")\n",
        "    else:\n",
        "        print(\"No compatible device found\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU or a TPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now!\")\n",
        "\n",
        "# Call the function\n",
        "check_device()"
      ],
      "metadata": {
        "id": "Dgzi5SDT1Ips"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you current `Runtime` is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_1_image03B.png)\n",
        "\n",
        "However, if you received a warning message, you must go back and change your `Runtime` now before you continue."
      ],
      "metadata": {
        "id": "UBJv5vrm1dpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to Stable Diffusion**\n",
        "\n",
        "Run the next cell to see short introduction to Stable Diffusion. This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "LBzX5ggwOBFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"QdRP9pO89MY\"\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen>\n",
        "</iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "qgTsKhCyOB9A",
        "outputId": "6ef3e846-c0e3-4ccc-c38e-b62aaaab3862"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"560\" height=\"315\"\n",
              "  src=\"https://www.youtube.com/embed/QdRP9pO89MY\"\n",
              "  title=\"YouTube video player\"\n",
              "  frameborder=\"0\"\n",
              "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
              "  allowfullscreen>\n",
              "</iframe>\n"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stable Diffusion**\n",
        "\n",
        "**Stable Diffusion** is a deep learning model for **generative image synthesis**. It belongs to the class of **latent diffusion models (LDMs)**, which generate high-quality images from text prompts by operating in a compressed latent space rather than pixel space. This approach significantly reduces computational cost while maintaining image fidelity.\n",
        "\n",
        "Stable Diffusion is trained on large-scale image-text datasets and uses a combination of:\n",
        "\n",
        "- **Variational Autoencoders (VAEs)**: To encode images into a latent space.\n",
        "- **U-Net architecture**: For denoising latent representations.\n",
        "- **Text encoders (e.g., CLIP or BERT)**: To condition image generation on natural language prompts.\n",
        "\n",
        "The model works by iteratively denoising a random latent vector, guided by a text prompt, until a coherent image emerges.\n",
        "\n",
        "### **Key Features**\n",
        "\n",
        "- **Text-to-image synthesis**: Generate images from descriptive text.\n",
        "- **Image-to-image translation**: Modify existing images using prompts.\n",
        "- **Inpainting and outpainting**: Fill in missing regions or expand images.\n",
        "- **Custom fine-tuning**: Adapt the model to domain-specific data.\n",
        "\n",
        "### **Applications in Computational Biology**\n",
        "\n",
        "Stable Diffusion can be a powerful tool for computational biologists in several ways:\n",
        "\n",
        "#### **1. Scientific Visualization**\n",
        "Generate illustrative figures for:\n",
        "- Molecular structures\n",
        "- Cellular processes\n",
        "- Pathways and interactions\n",
        "- Anatomical diagrams\n",
        "\n",
        "This can enhance presentations, publications, and educational materials.\n",
        "\n",
        "#### **2. Data Augmentation**\n",
        "Use synthetic biological images to:\n",
        "- Augment training datasets for machine learning models\n",
        "- Improve robustness in image classification tasks (e.g., histopathology, microscopy)\n",
        "\n",
        "#### **3. Hypothesis Communication**\n",
        "Translate complex biological hypotheses into visual representations for:\n",
        "- Grant proposals\n",
        "- Interdisciplinary collaboration\n",
        "- Public outreach\n",
        "\n",
        "#### **4. Custom Model Training**\n",
        "Fine-tune Stable Diffusion on domain-specific datasets (e.g., microscopy images, protein structures) to:\n",
        "- Generate realistic biological imagery\n",
        "- Explore latent space representations of biological phenomena\n",
        "\n",
        "#### **5. Interactive Exploration**\n",
        "Use prompt-based generation to explore:\n",
        "- Morphological variations\n",
        "- Evolutionary traits\n",
        "- Synthetic biology designs"
      ],
      "metadata": {
        "id": "992nctfKybK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optional YorTube Video**\n",
        "\n",
        "If you are interested in knowing how diffusion models can be used to generate complex images, run the next cell to watch this YouTube video."
      ],
      "metadata": {
        "id": "d3EASu-A3-Xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Extracted video ID from the original URL\n",
        "video_id = \"iv-5mZ_9CPY\"\n",
        "\n",
        "# Construct the proper embed URL\n",
        "embed_url = f\"https://www.youtube.com/embed/{video_id}\"\n",
        "\n",
        "# Display the embedded video\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\" src=\"{embed_url}\"\n",
        "frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "allowfullscreen></iframe>\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "NEzVc9Ja5sw3",
        "outputId": "bf932f61-ae6e-4d73-d190-f2ae2e3ad969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/iv-5mZ_9CPY\"\n",
              "frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
              "allowfullscreen></iframe>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text to Images with StableDiffusion**\n",
        "\n",
        "We will now see how to use Stable Diffusion to create various images from textual prompts. There will be four settings that we will deal with as we generate these images.\n",
        "\n",
        "* **model**: We will use the trained/finetuned model. Different models are optimized for different types of images.\n",
        "* **prompt**: Text that you provide to describe what sort of image you would like created.\n",
        "* **negative prompt**: Text that you describe elements that should not be present in your image.\n",
        "* **seed**: The same image for the prompt/negative prompt will always be produced for the same seed. To get a different image for the same prompts, change the seed.\n"
      ],
      "metadata": {
        "id": "ZJrIgZkszA7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importance of \"Setting the Seed\"**\n",
        "\n",
        "In image generation models like **Stable Diffusion**, a **random seed** determines the initial noise used to create an image. This noise is gradually transformed into a coherent image based on the prompt you provide.\n",
        "\n",
        "A **random seed** is a number used to initialize a random number generator. In Stable Diffusion:\n",
        "\n",
        "- The model begins with a field of random noise.\n",
        "- It uses your prompt to guide the transformation of this noise into an image.\n",
        "- The seed controls the exact pattern of that starting noise.\n",
        "\n",
        "Even with the **same prompt**, changing the seed changes the initial noise pattern, which leads to a **different final image**.\n",
        "\n",
        "**Analogy**:  \n",
        "Imagine sculpting clay using the same instructions. If each lump of clay starts with a different shape (seed), the final sculptures will be similar but **not identical**.\n",
        "\n",
        "### Use Cases for Seeds\n",
        "\n",
        "- **Reproducibility**: Using the same seed and prompt will always generate the same image.\n",
        "- **Exploration**: Trying different seeds lets you explore variations of the same concept.\n",
        "- **Fine-tuning**: You can find a seed that gives you the best result and reuse it.\n",
        "\n"
      ],
      "metadata": {
        "id": "vRGHWr99vU92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Basic Pipeline\n",
        "\n",
        "To make use of Stable Diffusion we will use the `HuggingFace DiffusionPipeline`. When setting up the pipeline we specify to use the `CompVis/stable-diffusion-v1-4` model, which is a basic model created to be used with StableDiffusion.\n",
        "\n",
        "The following code sets up this model and downloads it from `HuggingFace`."
      ],
      "metadata": {
        "id": "9ftIaZHNBzgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup basic pipeline\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# Configuration\n",
        "MODEL_ID = \"CompVis/stable-diffusion-v1-4\"\n",
        "CUSTOM_PIPELINE = \"lpw_stable_diffusion\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
        "\n",
        "# Optional: Set a generator for reproducibility\n",
        "generator = torch.manual_seed(42)\n",
        "\n",
        "# Load and configure the pipeline\n",
        "try:\n",
        "    pipe = DiffusionPipeline.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        custom_pipeline=CUSTOM_PIPELINE,\n",
        "        torch_dtype=DTYPE,\n",
        "        generator=generator\n",
        "    )\n",
        "    pipe = pipe.to(DEVICE)\n",
        "    print(f\"Pipeline loaded successfully on {DEVICE}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load pipeline: {e}\")\n"
      ],
      "metadata": {
        "id": "T5TF55x_uF4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image01B.png)"
      ],
      "metadata": {
        "id": "ZMeBh_B9d8Su"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Generate Basic Image\n",
        "\n",
        "We will begin by using Stable Diffusion (`stable-diffusion-v1-4`) to create a simple picture of a Monarch butterfly. As you will see, the image we generate will depend to a great extent on the value to random see. For Example 1 we will set the seed = `100`.\n",
        "\n",
        "In **Stable Diffusion**, the terms `prompt` and `negative prompt` refer to two different types of input that guide the image generation process:\n",
        "\n",
        "**Prompt**\n",
        "\n",
        "* This is the **main description** of what you want the model to generate.\n",
        "* It includes **keywords, phrases, or detailed descriptions** of the desired scene, style, objects, characters, lighting, mood, etc.\n",
        "\n",
        "Here is the `prompt` for Example 1:\n",
        "```text\n",
        "# Set prompt\n",
        "prompt= \"\"\"\n",
        "a Monarch butterfly\"\"\"\n",
        "```\n",
        "**Negative Prompt**\n",
        "\n",
        "* This is used to specify **what you _don’t_ want** in the image.\n",
        "* It helps the model avoid unwanted elements, styles, or artifacts.\n",
        "\n",
        "Here is the negative prompt for Example 1:\n",
        "```type\n",
        "# Set negative prompt\n",
        "neg_prompt = \"\"\"\n",
        "signature, watermark, incomplete image\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "**NSFW**\n",
        "\n",
        "As you might imagine it is quite possible to generate images that are considered \"not safe for work\" (NSFW). The code in the cell below contains the following line of code that will protect you from inadvertently generating a pornographic and/or an extremely violent image:\n",
        "\n",
        "```type\n",
        "# Uncomment the next line to remove the safety checker\n",
        "# pipe.safety_checker = lambda images, clip_input: (images, False)\n",
        "```\n",
        "If such an image is generated, you will see the following message.\n",
        "```text\n",
        "Potential NSFW content was detected in one or more images. A black image will be returned instead.\n",
        "Try again with a different prompt and/or seed.\n",
        "```\n",
        "\n",
        "You may wish to disable this feature. To do this, uncomment the pipe.`safety_checker` line. Be cafeful, if you do disable this, as unsafe images may be generated containing NSFW themes, which might contain violence, nudity, or sexual themes.\n",
        "`"
      ],
      "metadata": {
        "id": "Gn4kx3Hy6Aw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Generate basic image\n",
        "\n",
        "import random\n",
        "\n",
        "# Set the seed\n",
        "seed = 100\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "print(f\"The seed =\", seed)\n",
        "\n",
        "# Use seed to create the generator\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Set prompt\n",
        "prompt= \"\"\"\n",
        "a Monarch butterfly\"\"\"\n",
        "\n",
        "# Set negative prompt\n",
        "neg_prompt = \"\"\"\n",
        "signature, watermark, incomplete image\n",
        "\"\"\"\n",
        "\n",
        "# Uncomment the next line to remove the safety checker\n",
        "# pipe.safety_checker = lambda images, clip_input: (images, False)\n",
        "\n",
        "# Generate image\n",
        "pipe.text2img(prompt, negative_prompt=neg_prompt, width=512,height=512,\n",
        "              max_embeddings_multiples=3,generator=generator).images[0]"
      ],
      "metadata": {
        "id": "2hC9hktBh0v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image09B.png)"
      ],
      "metadata": {
        "id": "-Ns1QM4VlGPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1A: Generate Basic Image**\n",
        "\n",
        "In the cell below write the code to generate the same image of a Monarch butterfly generated in Example 1, but set the `seed` to the number `1604`."
      ],
      "metadata": {
        "id": "262XLos2lQ31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1A here\n"
      ],
      "metadata": {
        "id": "dNG9lUFTlQ32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image10B.png)"
      ],
      "metadata": {
        "id": "1ymmHtrulQ32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1B: Generate Basic Image**\n",
        "\n",
        "In the cell below write the code to generate the same image of a Monarch butterfly but this time set the seed to a random number (i.e use a seed value of `-1`)."
      ],
      "metadata": {
        "id": "9Yymf7ZHmVBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1B here\n"
      ],
      "metadata": {
        "id": "NcVM1dv4mVBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image11B.png)\n",
        "\n",
        "but it's unlikely you will see the same image since you were to use a random seed."
      ],
      "metadata": {
        "id": "fJk4etOtmVBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1C: Generate Basic Image**\n",
        "\n",
        "In the cell below write the code to generate an image of your choice. It can be any image that you like. Also, the value you pick for your `seed` is also up to you. Show your imagination!"
      ],
      "metadata": {
        "id": "YgE6TObznEjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1C here\n"
      ],
      "metadata": {
        "id": "QkWX-uwunEjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output will depend upon your `prompt` and your seed value."
      ],
      "metadata": {
        "id": "N1GPgwFMnEjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Generate Reference Image\n",
        "\n",
        "The diffusion model that we have been using so far is adequate for similar images, but is less suitable for high resolution images. This is especially true when it comes to generating images with human faces.\n",
        "\n",
        "The code in the next cell generates the face of a young Japanese woman. We will use this image as an example of a basic image that can be generated with the `stable-diffusion-v1-4`  model.\n"
      ],
      "metadata": {
        "id": "KLnGWWoponB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Importance of model selection\n",
        "\n",
        "import random\n",
        "\n",
        "# Set the seed / Use -1 for random seed\n",
        "seed = 100\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "print(f\"The seed =\", seed)\n",
        "\n",
        "# Use seed to create generator\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Prompts to generate image\n",
        "prompt= \"\"\"\n",
        "the face of a young Japanese woman\"\"\"\n",
        "\n",
        "neg_prompt = \"\"\"\n",
        "signature, watermark, incomplete image\n",
        "\"\"\"\n",
        "\n",
        "# Uncomment the next line to remove the safety checker\n",
        "#pipe.safety_checker = lambda images, clip_input: (images, False)\n",
        "\n",
        "# Generate image\n",
        "pipe.text2img(prompt, negative_prompt=neg_prompt, width=512,height=512,\n",
        "              max_embeddings_multiples=3,generator=generator).images[0]"
      ],
      "metadata": {
        "id": "bcxQmV9aonB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image12B.png)"
      ],
      "metadata": {
        "id": "rqSeJDTdonB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Generate Reference Image**\n",
        "\n",
        "In the next cell write the code that will generate a human face. You are free to specify the kind of face you want, male or female.  \n"
      ],
      "metadata": {
        "id": "kEyqrmc2q2xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n"
      ],
      "metadata": {
        "id": "eb3a0y65q2xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output will depend upon your `prompt`."
      ],
      "metadata": {
        "id": "3hYFYjr2q2xR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stable Diffusion Model Comparison: v1.4 vs v2.1**\n",
        "\n",
        "In the previous section we used the `stable-diffusion-v1-4` model to generate basic images. In this section we will use the more advanced version of this model, stable-diffusion-2-1 which can generate more realistics images.\n",
        "\n",
        "Here are the main differences between these two model generations.\n",
        "\n",
        "\n",
        "**1. Architecture and Text Encoder**\n",
        "\n",
        "| Feature | v1.4 (CompVis) | v2.1 (StabilityAI) |\n",
        "|--------|----------------|--------------------|\n",
        "| Text Encoder | CLIP ViT-L/14 | OpenCLIP ViT-H/14 |\n",
        "| Architecture | Stable Diffusion 1.x | Updated architecture |\n",
        "| Native Resolution | 512x512 | 768x768 |\n",
        "\n",
        "\n",
        "**2. Training Data**\n",
        "- **v1.4**:\n",
        "  - Trained on **LAION-Aesthetics v2 5+**.\n",
        "  - Fine-tuned for 225k steps.\n",
        "  - Focused on aesthetic images.\n",
        "\n",
        "- **v2.1**:\n",
        "  - Trained on a **filtered subset of LAION-5B**.\n",
        "  - Improved safety and quality filtering.\n",
        "  - Better prompt alignment and reduced bias\n",
        "\n",
        "\n",
        "**3. Output Style and Quality**\n",
        "\n",
        "- **v1.4**:\n",
        "  - Good for general-purpose image generation.\n",
        "  - May struggle with complex compositions or high-resolution needs.\n",
        "\n",
        "- **v2.1**:\n",
        "  - Better at handling **complex prompts**, **realism**, and **fine details**.\n",
        "  - Improved **anatomical accuracy**, lighting, and texture rendering.\n",
        "\n",
        "**4. Use Cases**\n",
        "\n",
        "| Model | Best For | Limitations |\n",
        "|-------|----------|-------------|\n",
        "| **v1.4** | Quick, general image generation | Lower resolution, less prompt nuance |\n",
        "| **v2.1** | High-quality, detailed images; better realism | May interpret prompts differently due to OpenCLIP |\n"
      ],
      "metadata": {
        "id": "nR05ENEwtfvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Advanced Pipleline\n",
        "\n",
        "The code in the cell below sets up the more advanced pipeline `stabilityai/stable-diffusion-2-1`.\n"
      ],
      "metadata": {
        "id": "-ery1YvsDEbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup advanced pipeline\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    #'hakurei/waifu-diffusion',\n",
        "    #\"SG161222/Realistic_Vision_V2.0\",\n",
        "    'stabilityai/stable-diffusion-2-1',\n",
        "    custom_pipeline=\"lpw_stable_diffusion\",\n",
        "    generator=generator,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe=pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "5uC4JuHWYtEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image07B.png)"
      ],
      "metadata": {
        "id": "IY58w88xyMSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Realistic Image\n",
        "\n",
        "We now generate an image with a much more complex prompt. The positive and negative prompts describe how to generate an image of a young woman. Stable diffusion prompts are usually comma separated lists of attributes to draw.\n",
        "\n",
        "You will notice that some are enclosed in paranthesis; which designates that this attribute is more important. A number, near the end, separated by a colon specifies how important."
      ],
      "metadata": {
        "id": "DoXoxynov8UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Realistic Image\n",
        "\n",
        "# Set the seed\n",
        "seed = 14\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "print(f\"The seed =\", seed)\n",
        "\n",
        "# Create generator\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Prompt\n",
        "prompt= \"\"\"\n",
        "(woman age 26 standing by tree), (long blonde hair:1.2), ray traced shadows,\n",
        "RAW, 8k, (eczema:0.7), (sub-surface scattering:1.55), (sweat:1.22), (freckles:0.55),\n",
        "highly detailed skin, (Acne:0.7), (FACE1:0.5), (FACE2:1.2), (FACE3:0.85),\n",
        "perfect eyes, no makeup. (skin spores:1.05), (skin spores:1.05),\n",
        "ultra detailed face, ultra detailed skin, film grain, ray tracing, studio lighting\"\"\"\n",
        "\n",
        "# Negative prompt\n",
        "neg_prompt = \"\"\"\n",
        "signature, watermark, airbrush, photoshop, plastic doll,\n",
        "(ugly eyes, deformed iris, deformed pupils, fused lips and teeth:1.2),\n",
        "(un-detailed skin, semi-realistic, cgi, 3d, render, sketch, cartoon,\n",
        "drawing, anime:1.2), text, close up, cropped, out of frame, worst quality,\n",
        "low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers,\n",
        "mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry,\n",
        "dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured,\n",
        "gross proportions, malformed limbs, missing arms, missing legs, extra arms,\n",
        "extra legs, fused fingers, too many fingers, long neck, head wear, masculine,\n",
        "obese, fat, out of frame\"\"\"\n",
        "\n",
        "# Safety checker\n",
        "#pipe.safety_checker = lambda images, clip_input: (images, False)\n",
        "\n",
        "# Generate text-to-image\n",
        "pipe.text2img(prompt, negative_prompt=neg_prompt, width=512,height=512,\n",
        "              max_embeddings_multiples=3,generator=generator).images[0]"
      ],
      "metadata": {
        "id": "Vdjgbx1RzwR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image13B.png)"
      ],
      "metadata": {
        "id": "JKyvso3MwwXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Realistic Image**\n",
        "\n",
        "Generate an image of old Japanese woman in front of Mount Fuji. Set the seed to `2`.\n",
        "\n",
        "Define your prompt as:\n",
        "```text\n",
        "# Prompt\n",
        "prompt= \"\"\"\n",
        "(Japanese woman age 90+, standing before Mount Fuji), (gray thinning hair:1.2),\n",
        "ray traced shadows, RAW, 8k, (deep wrinkles:1.5), (sub-surface scattering:1.55),\n",
        "(age spots:1.2), highly detailed aged skin, (sagging skin:1.3), (FACE1:0.5),\n",
        "(FACE2:1.2), (FACE3:0.85), cloudy eyes, no makeup, (skin texture:1.05),\n",
        "ultra detailed face, ultra detailed skin, film grain, ray tracing, studio lighting\n",
        "\"\"\"\n",
        "```\n",
        "and your negative prompt as:\n",
        "```type\n",
        "# Negative prompt\n",
        "neg_prompt = \"\"\"\n",
        "signature, watermark, airbrush, photoshop, plastic doll,\n",
        "(ugly eyes, deformed iris, deformed pupils, fused lips and teeth:1.2),\n",
        "(un-detailed skin, semi-realistic, cgi, 3d, render, sketch, cartoon,\n",
        "drawing, anime:1.2), text, close up, cropped, out of frame, worst quality,\n",
        "low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers,\n",
        "mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry,\n",
        "dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured,\n",
        "gross proportions, malformed limbs, missing arms, missing legs, extra arms,\n",
        "extra legs, fused fingers, too many fingers, long neck, head wear, masculine,\n",
        "obese, fat, out of frame\"\"\"\n",
        "```\n",
        "\n",
        "The rest of the code is the same as Example 3."
      ],
      "metadata": {
        "id": "Ij-eAwnTyxAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n"
      ],
      "metadata": {
        "id": "WQ-OgZOpyxAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image14B.png)"
      ],
      "metadata": {
        "id": "naLpWkDkyxAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Anime**\n",
        "\n",
        "**Anime** is a form of animation that originated in **Japan** and has grown into a globally recognized medium. It includes a wide variety of genres, themes, and artistic styles, and is known for its unique storytelling and visual aesthetics.\n",
        "\n",
        "#### **Key Characteristics**\n",
        "- **Origin**: Japan\n",
        "- **Mediums**: TV series, films, web series, OVAs (Original Video Animations)\n",
        "- **Genres**: Action, Romance, Fantasy, Horror, Sci-Fi, Slice of Life, and more\n",
        "- **Audience**: All age groups, from children to adults\n",
        "\n",
        "#### **Notable Examples**\n",
        "- *Spirited Away* (Studio Ghibli)\n",
        "- *Naruto*\n",
        "- *Attack on Titan*\n",
        "- *One Piece*\n",
        "- *Demon Slayer*\n",
        "\n",
        "### **What Is Anime Style?**\n",
        "\n",
        "**Anime style** refers to the distinctive visual and artistic elements commonly found in anime. It is characterized by stylized character designs, expressive features, and dynamic visuals.\n",
        "\n",
        "#### **Visual Features**\n",
        "\n",
        "| Feature              | Description                                                                 |\n",
        "|----------------------|-----------------------------------------------------------------------------|\n",
        "| **Eyes**             | Large, expressive, often detailed to convey emotion                         |\n",
        "| **Facial Features**  | Small noses and mouths, exaggerated expressions                             |\n",
        "| **Hair**             | Stylized, often colorful, with unique shapes and movement                   |\n",
        "| **Body Proportions** | Can range from realistic to highly exaggerated depending on the genre       |\n",
        "| **Line Work**        | Clean and sharp outlines                                                     |\n",
        "| **Coloring**         | Flat or cel-shaded, with emphasis on contrast and mood                      |\n",
        "| **Backgrounds**      | Often highly detailed, especially in fantasy or sci-fi settings             |\n",
        "\n",
        "#### **Storytelling Elements**\n",
        "- **Emotional depth** and character development\n",
        "- **Symbolism** and metaphorical themes\n",
        "- **Cultural references** to Japanese traditions, language, and society\n",
        "- **Genre blending**, such as mixing romance with supernatural or comedy with horror\n",
        "\n",
        "#### **Anime vs. Western Animation**\n",
        "\n",
        "| Aspect              | Anime                                   | Western Animation                      |\n",
        "|---------------------|------------------------------------------|----------------------------------------|\n",
        "| **Art Style**       | Stylized, detailed, expressive           | Varies widely, often more cartoonish   |\n",
        "| **Themes**          | Often complex and mature                 | Often geared toward children/family    |\n",
        "| **Production**      | Typically serialized with long arcs      | Often episodic                         |\n",
        "| **Cultural Influence** | Strong Japanese cultural elements     | Western cultural norms and humor       |\n"
      ],
      "metadata": {
        "id": "kzaxKXEp61Vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Anime Models**\n",
        "\n",
        "If you would like to generate cartoon or Anime style images, the `waifu-diffusion model` will work nicely. The `Waifu Diffusion model` is a specialized AI image generation model designed to create anime-style artwork, particularly focusing on characters that resemble the \"waifu\" archetype—typically stylized, idealized female characters popular in anime and manga culture.\n",
        "\n",
        "#### **Key Features:**\n",
        "* **Anime-focused training data:** Trained on datasets like Danbooru (a large anime image repository).\n",
        "* **Text-to-image generation:** You input a prompt like \"a cute anime girl with blue hair in a school uniform\" and it generates an image.\n",
        "* **Customizable outputs:** You can adjust style, pose, background, and more using prompt engineering.\n",
        "* **Checkpoint versions:** Includes models like Waifu Diffusion 1.3, 1.4, and 1.5, each improving quality and prompt responsiveness.\n",
        "\n",
        "The code below loads the pipeline for the `Waifu Diffusion model`."
      ],
      "metadata": {
        "id": "ZrnDmq8AE1P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Waifu Diffusion model pipeline\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# Set seed\n",
        "seed = 102  # or -1 for random\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "print(f\"The seed =\", seed)\n",
        "\n",
        "# Create generator\n",
        "generator = torch.Generator(device='cuda').manual_seed(seed)\n",
        "\n",
        "# Load pipeline without generator\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"hakurei/waifu-diffusion\",\n",
        "    custom_pipeline=\"lpw_stable_diffusion\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# Use generator when calling the pipeline\n",
        "image = pipe(\n",
        "    prompt=\"your prompt here\",\n",
        "    negative_prompt=\"your negative prompt here\",\n",
        "    width=512,\n",
        "    height=512,\n",
        "    max_embeddings_multiples=3,\n",
        "    generator=generator\n",
        ").images[0]\n"
      ],
      "metadata": {
        "id": "BkL0UMyfcPbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image20B.png)"
      ],
      "metadata": {
        "id": "TmJAfJUtzJVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Create Anime Person\n",
        "\n",
        "The code in the cell below creates an Anime image of a young girl."
      ],
      "metadata": {
        "id": "Wf--JbSf9gfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Create anime image\n",
        "\n",
        "# Random number seed, -1 for random seed\n",
        "seed = 105\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "print(f\"The seed =\", seed)\n",
        "\n",
        "# Create generator\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Prompt\n",
        "prompt = (\n",
        "    \"best_quality (1girl:1.3) bride brown_hair closed_mouth frills (full_body:1.3) \"\n",
        "    \"fox_ear hair_bow happy hood kimono long_sleeves red_bow smile solo tabi \"\n",
        "    \"white_kimono wide_sleeves cherry_blossoms\"\n",
        ")\n",
        "\n",
        "# Negative prompt\n",
        "neg_prompt = (\n",
        "    \"lowres, bad_anatomy, error_body, error_hands, bad_hands, error_fingers, \"\n",
        "    \"missing_fingers, error_legs, bad_legs, error_lighting, error_shadow, \"\n",
        "    \"extra_digit, cropped, worst_quality, jpeg_artifacts, watermark, blurry\"\n",
        ")\n",
        "\n",
        "# Safety check\n",
        "#pipe.safety_checker = lambda images, clip_input: (images, False)\n",
        "\n",
        "# Generate the image\n",
        "pipe.text2img(prompt, negative_prompt=neg_prompt, width=512,height=512,\n",
        "              max_embeddings_multiples=3,generator=generator).images[0]"
      ],
      "metadata": {
        "id": "l5NuKnPCzKA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image16B.png)"
      ],
      "metadata": {
        "id": "sgKIwF11-rsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Create Anime Person**\n",
        "\n",
        "In the cell below write the code to generate an Anime image of a young boy. Make sure to set the seed = `100`.\n",
        "\n",
        "You can reuse the code in Example 4 but change the prompt to read:\n",
        "\n",
        "~~~text\n",
        "# Prompt\n",
        "prompt = (\n",
        "    \"best_quality, (1boy:1.3), heroic_pose, spiky_hair, intense_eyes, anime_uniform, \"\n",
        "    \"dramatic_lighting, cinematic_background, serious_expression, solo, full_body, \"\n",
        "    \"anime_style, masterpiece, high_detail, vibrant_colors, wind_effects, glowing_aura, \"\n",
        "    \"dynamic_composition, epic_scale, battle_ready, stormy_sky, energy_particles, depth_of_field\"\n",
        ")\n",
        "\n",
        "~~~"
      ],
      "metadata": {
        "id": "xIO0QRRTdbN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n"
      ],
      "metadata": {
        "id": "5vuRG-_cwcDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image23B.png)\n"
      ],
      "metadata": {
        "id": "aKJRxoqHAyHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Create Anime Scene\n",
        "\n",
        "The Anime world is full of magical as well as real creatures. One popular Anime TV show that is populated with a variety of creatures is called `Shirokuma Café`.\n",
        "\n",
        "**Shirokuma Café** (also known as Polar Bear Café) is a Japanese anime and manga series that blends slice-of-life comedy with a whimsical, anthropomorphic twist.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image24B.png)\n",
        "\n",
        "The code in the cell below generates an image that is in `the style of` `Shirokuma Café`. This particular prompt was generated by uploading this image to Microsoft 365 Copilot and asking it to create a prompt to duplicate the image."
      ],
      "metadata": {
        "id": "nLwM03Ldg5W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Create Anime Scene\n",
        "\n",
        "# Set the seed\n",
        "seed = 8\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "print(f\"The seed =\", seed)\n",
        "\n",
        "# Create generator\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Prompt generated by Microsoft 365 Copilot from Shirokuma Café image\n",
        "prompt = (\n",
        "    \"best_quality, anime_style, high_detail, vibrant_colors, cute_animal_characters, \"\n",
        "    \"panda_sitting_with_pink_swim_ring_and_yellow_shorts, waving_paw, penguin_standing_next_to_panda, \"\n",
        "    \"lush_green_foliage_background, stone_wall, summer_theme, friendly_expression, \"\n",
        "    \"casual_swimwear, relaxed_atmosphere, slice_of_life_scene, soft_shading\"\n",
        ")\n",
        "\n",
        "# Negative prompt\n",
        "neg_prompt = (\n",
        "    \"lowres, bad_anatomy, blurry, distorted_faces, error_limbs, bad_hands, missing_fingers, \"\n",
        "    \"extra_limbs, worst_quality, jpeg_artifacts, watermark, cropped, poor_lighting, \"\n",
        "    \"oversaturated_colors, unnatural_pose, messy_background, out_of_focus, bad_proportions\"\n",
        ")\n",
        "\n",
        "# Generate the image\n",
        "pipe.text2img(prompt, negative_prompt=neg_prompt, width=512,height=512,\n",
        "              max_embeddings_multiples=3,generator=generator).images[0]"
      ],
      "metadata": {
        "id": "9Eb-aLHn4L8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image25B.png)\n",
        "\n",
        "Here is the prompt that was used to generate this image:\n",
        "\n",
        "~~~type\n",
        "prompt = (\n",
        "    \"best_quality, anime_style, high_detail, vibrant_colors, cute_animal_characters, \"\n",
        "    \"panda_sitting_with_pink_swim_ring_and_yellow_shorts, waving_paw, penguin_standing_next_to_panda, \"\n",
        "    \"lush_green_foliage_background, stone_wall, summer_theme, friendly_expression, \"\n",
        "    \"casual_swimwear, relaxed_atmosphere, slice_of_life_scene, soft_shading\"\n",
        ")\n",
        "~~~\n",
        "\n",
        "As you can see the image generated was rather different that one might have expected from the prompt. This kind of _mismatch_ is very common with `text-to-image` generators, even when rather detailed prompts are used."
      ],
      "metadata": {
        "id": "iqNswVQPCNz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Create Anime Scene**\n",
        "\n",
        "In the cell below use exactly the same code that was shown in Example 5.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image24B.png)\n",
        "\n",
        " See if by changing the value of the seed, if you can generate an image that is more similar to the Shirokuma Café image that the prompt was based on."
      ],
      "metadata": {
        "id": "4W66FXxyi_4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n"
      ],
      "metadata": {
        "id": "TMzP4M8mi_4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What your output looks like will depend upon the value of your seed."
      ],
      "metadata": {
        "id": "ruyTjujwi_4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Image-to-Image Pipelines in Stable Diffusion**\n",
        "\n",
        "#### **Background**\n",
        "\n",
        "Stable Diffusion is a latent text-to-image diffusion model developed by **Stability AI**, first released in **August 2022**. It quickly became popular due to its open-source nature, high-quality outputs, and ability to run on consumer GPUs.\n",
        "\n",
        "While the original model focused on **text-to-image generation**, the community and developers soon expanded its capabilities to include **image-to-image** transformations, enabling users to guide generation using existing images.\n",
        "\n",
        "#### **What Is Image-to-Image in Stable Diffusion?**\n",
        "\n",
        "Image-to-image (img2img) generation allows users to provide:\n",
        "- A **source image** (e.g., a sketch, photo, or concept art)\n",
        "- A **text prompt** describing the desired transformation\n",
        "- A **strength parameter** controlling how much the output deviates from the input\n",
        "\n",
        "This technique uses the same latent diffusion process but starts from a **noised version of the input image**, allowing for creative reinterpretation while preserving structure.\n",
        "\n",
        "### **Evolution of Image-to-Image Pipelines**\n",
        "\n",
        "##### Stable Diffusion v1.5 (2022)\n",
        "- First widely used version for img2img tasks.\n",
        "- Introduced via the `StableDiffusionImg2ImgPipeline` in the `diffusers` library.\n",
        "- Supported basic transformations with prompt guidance and strength control.\n",
        "\n",
        "##### Stable Diffusion v2.x (Late 2022–2023)\n",
        "- Improved resolution and semantic understanding.\n",
        "- Introduced **depth-to-image** and **inpainting** pipelines.\n",
        "- Better handling of complex prompts and image conditioning.\n",
        "\n",
        "##### ControlNet Integration (2023)\n",
        "- Added fine-grained control using **edge maps**, **pose estimation**, **depth maps**, etc.\n",
        "- Enabled highly structured transformations while preserving artistic freedom.\n",
        "\n",
        "##### SDXL (2023–2024)\n",
        "- Major upgrade with richer visual fidelity and prompt comprehension.\n",
        "- Image-to-image support extended to **SDXLImg2ImgPipeline**.\n",
        "- Better performance on high-resolution inputs and nuanced prompts.\n",
        "\n",
        "### **Parameters in Image-to-Image Pipelines**\n",
        "\n",
        "| Parameter         | Description                                                                 |\n",
        "|------------------|-----------------------------------------------------------------------------|\n",
        "| `prompt`          | Text description guiding the transformation                                 |\n",
        "| `image`           | Input image to be transformed                                               |\n",
        "| `strength`        | Controls deviation from input (0.0 = faithful, 1.0 = creative)              |\n",
        "| `guidance_scale`  | Controls adherence to prompt (higher = more prompt-driven)                  |\n",
        "| `num_inference_steps` | Number of denoising steps (more = better quality, slower)               |\n",
        "| `generator`       | Random seed generator for reproducibility                                   |\n",
        "\n",
        "#### **Use Cases**\n",
        "\n",
        "- **Artistic reinterpretation** of student sketches\n",
        "- **Scientific visualization** from concept diagrams\n",
        "- **Creative storytelling** using visual prompts\n",
        "- **Biological image transformation** for simulations or hypothetical scenarios\n"
      ],
      "metadata": {
        "id": "o8kwkJocKUk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Image-2-Image Pipeline\n",
        "\n",
        "The code in the cell below sets up the Image-to-Image pipe line."
      ],
      "metadata": {
        "id": "V1rYZoSs2v0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup image-2-image pipe line\n",
        "\n",
        "\n",
        "# Import libraries\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# Load the pipeline\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "LfncOn4LyWbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image27B.png)"
      ],
      "metadata": {
        "id": "8voAgXjzCwg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Understanding the `strength` Parameter in Image-to-Image Generation**\n",
        "\n",
        "##### **What Is `strength`?**\n",
        "\n",
        "In Stable Diffusion's image-to-image pipeline, the `strength` parameter controls **how much the generated image deviates from the input image**. It determines the level of noise added to the input image before the diffusion process begins.\n",
        "\n",
        "##### **How It Works**\n",
        "\n",
        "- The image-to-image pipeline works by **encoding the input image into latent space**, adding noise, and then **denoising it guided by the text prompt**.\n",
        "- The `strength` parameter sets the **starting point** in the denoising process:\n",
        "  - **Low strength** → less noise → output is **closer to the input image**\n",
        "  - **High strength** → more noise → output is **more influenced by the prompt**\n",
        "\n",
        "\n",
        "#### **Typical Values**\n",
        "\n",
        "| Strength | Behavior                          | Use Case Example                          |\n",
        "|----------|-----------------------------------|-------------------------------------------|\n",
        "| 0.1–0.3  | Very close to input image         | Subtle edits, style transfer              |\n",
        "| 0.4–0.6  | Balanced between input and prompt | Concept transformation                    |\n",
        "| 0.7–0.9  | Highly creative, prompt-driven    | Abstract reinterpretation, surreal edits  |\n",
        "\n",
        "> ⚠️ Values above `0.9` may ignore the input image almost entirely."
      ],
      "metadata": {
        "id": "xEDV1MdGPPr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Image-to-Image Strength Parameter\n",
        "\n",
        "The code in the cell below uses this rather famous image of an astronaut riding a horse on the moon as the initial image.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/AstronautMoon.jpg)\n",
        "\n",
        "\n",
        "The **initial image** (also called the **input image**) is the starting point for the image-to-image generation process in Stable Diffusion. It provides the **visual structure** or **content** that the model will transform based on a given **text prompt**.\n",
        "\n",
        "#### **How It Works**\n",
        "\n",
        "1. The initial image is **encoded into latent space** using a Variational Autoencoder (VAE).\n",
        "2. A certain amount of **noise is added**, controlled by the `strength` parameter.\n",
        "3. The model then **denoises** the image while being guided by the **text prompt**, producing a new image that blends the original content with the prompt's intent.\n",
        "\n",
        "### **Prompt**\n",
        "\n",
        "Here is the text prompt for Example 6:\n",
        "\n",
        "```text\n",
        "# Prompt\n",
        "prompt = \"a evil clown riding a tiger on the moon\"\n",
        "```\n",
        "\n",
        "### **Strength Parameter**\n",
        "\n",
        "In this example we are going to look at how the `strength parameter` affects image creation.\n",
        "\n",
        "The code in the cell below has the `strength parameter` = `0.01`. This is an extremely low value so we would expect the output image to be relatively unchanged."
      ],
      "metadata": {
        "id": "JHKj9FsTJ0S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Image-to-Image Strength Parameter\n",
        "\n",
        "import torch\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "import requests\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "\n",
        "# Set strength parameter\n",
        "strength = 0.01  # Creativity level\n",
        "\n",
        "# Set the seed\n",
        "seed = 1\n",
        "print(f\"The seed = {seed}\")\n",
        "\n",
        "# Prompt\n",
        "prompt = \"a evil clown riding a tiger on the moon\"\n",
        "\n",
        "# Set additional paraemters\n",
        "guidance_scale = 7.5\n",
        "num_inference_steps = 50\n",
        "\n",
        "# Validate strength\n",
        "if not (0.01 <= strength <= 0.9):\n",
        "    raise ValueError(\"Strength should be between 0.1 and 0.9 for best results.\")\n",
        "\n",
        "# Load pipeline\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Override safety checker\n",
        "pipe.safety_checker = lambda images, clip_input: (images, [False] * len(images))\n",
        "\n",
        "# Create generator\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "\n",
        "# Load and preprocess inital image\n",
        "url = \"https://biologicslab.co/BIO1173/images/class_04/AstronautMoon.jpg\"\n",
        "init_image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
        "\n",
        "# Resize and crop to 512x512\n",
        "init_image = init_image.resize((512, 512))\n",
        "\n",
        "# Generate image\n",
        "try:\n",
        "    result = pipe(\n",
        "        prompt=prompt,\n",
        "        image=init_image,\n",
        "        strength=strength,\n",
        "        guidance_scale=guidance_scale,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        generator=generator\n",
        "    ).images[0]\n",
        "\n",
        "    # Display result\n",
        "    display(result)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error during image generation:\", e)\n"
      ],
      "metadata": {
        "id": "zuZtRR5QG_2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image29B.png)\n",
        "\n",
        "As expected the output image is relatively unchanged."
      ],
      "metadata": {
        "id": "TkwEY2HQQ7f5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6A: Image-to-Image Strength Parameter**\n",
        "\n",
        "In the cell below write the code to reproduce Example 6 but set the `strength parameter` = `0.5` but keep the `seed` = `1`.\n",
        "\n",
        "Let's see how a \"medium level\" of creativity affects the output image?"
      ],
      "metadata": {
        "id": "HHM5KmIlRNJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6A here\n"
      ],
      "metadata": {
        "id": "C80zMNWASaG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image31B.png)\n",
        "\n",
        "The horse in the output image seems to be morphing into a more `tiger-like` creature."
      ],
      "metadata": {
        "id": "p8Q1IeeyWB5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6B: Image-to-Image Strength Parameter**\n",
        "\n",
        "In the cell below write the code to reproduce Example 6 but now set the `strength parameter` = `0.75` and keep the `seed` = `1`.\n",
        "\n",
        "Let's see how a \"high level\" of creativity affects the output image?"
      ],
      "metadata": {
        "id": "vacrZm9aWUnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6B here\n"
      ],
      "metadata": {
        "id": "yKiHH1bBWUng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image30B.png)\n",
        "\n",
        "The output image is now relatively bizzare. The horse has been changed into a tiger albeit a mishapen tiger. The astronaut is also quite different.  "
      ],
      "metadata": {
        "id": "hsEXLGVMWUng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Class_04_3.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "iG6D_KMDNdIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lizard Tail**\n",
        "\n",
        "### **BACKPROGATION**\n",
        "\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png)\n",
        "\n",
        "\n",
        "In machine learning, **backpropagation** is a gradient estimation method commonly used for training a neural network to compute its parameter updates.\n",
        "\n",
        "It is an efficient application of the chain rule to neural networks. Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input–output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming.\n",
        "\n",
        "Strictly speaking, the term backpropagation refers only to an algorithm for efficiently computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm – including how the gradient is used, such as by stochastic gradient descent, or as an intermediate step in a more complicated optimizer, such as Adaptive Moment Estimation.[5] The local minimum convergence, exploding gradient, vanishing gradient, and weak control of learning rate are main disadvantages of these optimization algorithms. The Hessian and quasi-Hessian optimizers solve only local minimum convergence problem, and the backpropagation works longer. These problems caused researchers to develop hybrid and fractional optimization algorithms.\n",
        "\n",
        "Backpropagation had multiple discoveries and partial discoveries, with a tangled history and terminology. See the history section for details. Some other names for the technique include \"reverse mode of automatic differentiation\" or \"reverse accumulation\".\n",
        "\n",
        "## **Intuition**\n",
        "\n",
        "**Motivation**\n",
        "\n",
        "The goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output. The motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output.\n",
        "\n",
        "**Learning as an optimization problem**\n",
        "\n",
        "To understand the mathematical derivation of the backpropagation algorithm, it helps to first develop some intuition about the relationship between the actual output of a neuron and the correct output for a particular training example. Consider a simple neural network with two input units, one output unit and no hidden units, and in which each neuron uses a linear output (unlike most work on neural networks, in which mapping from inputs to outputs is non-linear) that is the weighted sum of its input.\n",
        "\n",
        "**History**\n",
        "\n",
        "Backpropagation had been derived repeatedly, as it is essentially an efficient application of the chain rule (first written down by Gottfried Wilhelm Leibniz in 1676) to neural networks.\n",
        "\n",
        "The terminology \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this. In any case, he only studied neurons whose outputs were discrete levels, which only had zero derivatives, making backpropagation impossible.\n",
        "\n",
        "Precursors to backpropagation appeared in optimal control theory since 1950s. Yann LeCun et al credits 1950s work by Pontryagin and others in optimal control theory, especially the adjoint state method, for being a continuous-time version of backpropagation. Hecht-Nielsen credits the Robbins–Monro algorithm (1951)[23] and Arthur Bryson and Yu-Chi Ho's Applied Optimal Control (1969) as presages of backpropagation. Other precursors were Henry J. Kelley 1960, and Arthur E. Bryson (1961). In 1962, Stuart Dreyfus published a simpler derivation based only on the chain rule. In 1973, he adapted parameters of controllers in proportion to error gradients. Unlike modern backpropagation, these precursors used standard Jacobian matrix calculations from one stage to the previous one, neither addressing direct links across several stages nor potential additional efficiency gains due to network sparsity.\n",
        "\n",
        "The ADALINE (1960) learning algorithm was gradient descent with a squared error loss for a single layer. The first multilayer perceptron (MLP) with more than one layer trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.[29] The MLP had 5 layers, with 2 learnable layers, and it learned to classify patterns not linearly separable.\n",
        "\n",
        "**Modern backpropagation**\n",
        "\n",
        "Modern backpropagation was first published by Seppo Linnainmaa as \"reverse mode of automatic differentiation\" (1970) for discrete connected networks of nested differentiable functions.\n",
        "\n",
        "In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. Werbos described how he developed backpropagation in an interview. In 1971, during his PhD work, he developed backpropagation to mathematicize Freud's \"flow of psychic energy\". He faced repeated difficulty in publishing the work, only managing in 1981. He also claimed that \"the first practical application of back-propagation was for estimating a dynamic model to predict nationalism and social communications in 1974\" by him.\n",
        "\n",
        "Around 1982, David E. Rumelhart independently developed backpropagation and taught the algorithm to others in his research circle. He did not cite previous work as he was unaware of them. He published the algorithm first in a 1985 paper, then in a 1986 Nature paper an experimental analysis of the technique These papers became highly cited, contributed to the popularization of backpropagation, and coincided with the resurging research interest in neural networks during the 1980s.\n",
        "\n",
        "In 1985, the method was also described by David Parker. Yann LeCun proposed an alternative form of backpropagation for neural networks in his PhD thesis in 1987.\n",
        "\n",
        "Gradient descent took a considerable amount of time to reach acceptance. Some early objections were: there were no guarantees that gradient descent could reach a global minimum, only local minimum; neurons were \"known\" by physiologists as making discrete signals (0/1), not continuous ones, and with discrete signals, there is no gradient to take. See the interview with Geoffrey Hinton,[36] who was awarded the 2024 Nobel Prize in Physics for his contributions to the field.\n",
        "\n",
        "**Early successes**\n",
        "\n",
        "Contributing to the acceptance were several applications in training neural networks via backpropagation, sometimes achieving popularity outside the research circles.\n",
        "\n",
        "In 1987, NETtalk learned to convert English text into pronunciation. Sejnowski tried training it with both backpropagation and Boltzmann machine, but found the backpropagation significantly faster, so he used it for the final NETtalk. The NETtalk program became a popular success, appearing on the Today show.\n",
        "\n",
        "In 1989, Dean A. Pomerleau published ALVINN, a neural network trained to drive autonomously using backpropagation.\n",
        "\n",
        "The LeNet was published in 1989 to recognize handwritten zip codes.\n",
        "\n",
        "In 1992, TD-Gammon achieved top human level play in backgammon. It was a reinforcement learning agent with a neural network with two layers, trained by backpropagation.\n",
        "\n",
        "In 1993, Eric Wan won an international pattern recognition contest through backpropagation.\n",
        "\n",
        "**After backpropagation**\n",
        "\n",
        "During the 2000s it fell out of favour, but returned in the 2010s, benefiting from cheap, powerful GPU-based computing systems. This has been especially so in speech recognition, machine vision, natural language processing, and language structure learning research (in which it has been used to explain a variety of phenomena related to first and second language learning.\n",
        "\n",
        "Error backpropagation has been suggested to explain human brain event-related potential (ERP) components like the N400 and P600.\n",
        "\n",
        "In 2023, a backpropagation algorithm was implemented on a photonic processor by a team at Stanford University.\n",
        "\n",
        "## **Backpropagation in Deep Neural Networks**\n",
        "\n",
        "Backpropagation is a fundamental algorithm used to train deep neural networks. It efficiently computes the **gradient of the loss function** with respect to each weight in the network, enabling the use of **gradient descent** to update the weights and minimize the loss.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/BackProp.jpg)\n",
        "\n",
        "### Overview of the Process\n",
        "\n",
        "1. **Forward Pass**: Input data is passed through the network to compute the output (prediction).\n",
        "2. **Loss Calculation**: The output is compared to the true label using a loss function (e.g., MSE, cross-entropy).\n",
        "3. **Backward Pass (Backpropagation)**:\n",
        "   - Gradients of the loss with respect to each parameter are computed using the **chain rule**.\n",
        "   - These gradients are used to update the weights via an optimization algorithm (e.g., SGD, Adam).\n",
        "\n",
        "## Mathematical Foundations\n",
        "\n",
        "Let’s consider a simple feedforward neural network with:\n",
        "- Input layer: \\( x \\)\n",
        "- Hidden layer: \\( h = f(Wx + b) \\)\n",
        "- Output layer: \\( \\hat{y} = g(Vh + c) \\)\n",
        "- Loss function: \\( L(\\hat{y}, y) \\)\n",
        "\n",
        "### Step 1: Compute Gradients\n",
        "\n",
        "Using the chain rule:\n",
        "\n",
        "- Gradient w.r.t. output weights \\( V \\):\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial V} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial V}\n",
        "  $$\n",
        "\n",
        "- Gradient w.r.t. hidden weights \\( W \\):\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial h} \\cdot \\frac{\\partial h}{\\partial W}\n",
        "  $$\n",
        "\n",
        "### Step 2: Update Weights\n",
        "\n",
        "Using gradient descent:\n",
        "$$\n",
        "V := V - \\eta \\cdot \\frac{\\partial L}{\\partial V}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W := W - \\eta \\cdot \\frac{\\partial L}{\\partial W}\n",
        "$$\n",
        "\n",
        "Where $\\eta$ is the learning rate.\n",
        "\n",
        "\n",
        "## Backpropagation Algorithm (Pseudocode)\n",
        "\n",
        "```python\n",
        "# Assume a simple 2-layer neural network\n",
        "for epoch in range(num_epochs):\n",
        "    for x, y in data:\n",
        "        # Forward pass\n",
        "        h = f(W @ x + b)\n",
        "        y_hat = g(V @ h + c)\n",
        "        loss = compute_loss(y_hat, y)\n",
        "\n",
        "        # Backward pass\n",
        "        dL_dyhat = compute_loss_gradient(y_hat, y)\n",
        "        dL_dV = dL_dyhat @ h.T\n",
        "        dL_dh = V.T @ dL_dyhat\n",
        "        dL_dW = (dL_dh * f_prime(W @ x + b)) @ x.T\n",
        "\n",
        "        # Update weights\n",
        "        V -= learning_rate * dL_dV\n",
        "        W -= learning_rate * dL_dW\n"
      ],
      "metadata": {
        "id": "6x8gMtgQxMMH"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}