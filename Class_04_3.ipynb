{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEEW13uCtiJ"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/master/Class_04_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIO 1173: Intro Computational Biology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Module 4: Training for Tabular Data**\n",
    "\n",
    "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
    "\n",
    "### Module 4 Material\n",
    "\n",
    "* Part 4.1: Encoding a Feature Vector for Keras Deep Learning\n",
    "* Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC\n",
    "* **Part 4.3: Keras Regression for Deep Neural Networks with RMSE**\n",
    "* Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Neural Network Training\n",
    "* Part 4.5: Neural Network RMSE and Log Loss Error Calculation from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Setup\n",
    "\n",
    "Run the next code cell to load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You MUST run this code cell first\n",
    "\n",
    "# Classification neural network\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "path = '/'\n",
    "memory = shutil.disk_usage(path)\n",
    "dirpath = os.getcwd()\n",
    "print(\"Your current working directory is : \" + dirpath)\n",
    "print(\"Disk\", memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4.3: Keras Regression for Deep Neural Networks with RMSE\n",
    "\n",
    "In Keras, regression models can be built using deep neural networks to predict continuous values. The RMSE (Root Mean Squared Error) is a common metric used to evaluate the performance of regression models. It measures the average magnitude of the errors between predicted and actual values. To train a regression model using Keras, you would typically define a deep neural network architecture with dense layers, activation functions, and optimizer. The target variable would be continuous and the loss function used would be mean squared error (MSE).\n",
    "\n",
    "During training, the model would minimize the MSE loss function by adjusting the weights and biases of the neural network using backpropagation. The model's performance can be evaluated using RMSE on a separate test dataset, where a lower RMSE indicates a better performing model. Overall, Keras Regression for Deep Neural Networks with RMSE involves building a neural network for regression tasks, training it using MSE loss, and evaluating its performance using RMSE.\n",
    "\n",
    "We evaluate regression results differently than classification.  Consider the following code that trains a neural network for regression on the data set **jh-simple-dataset.csv**.  We begin by preparing the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Fat Dataset for the Examples\n",
    "\n",
    "In this lesson we will be using the [Body Fat Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset) for the Examples. \n",
    "\n",
    "Volume, and hence body **_density_**, can be accurately measured a variety of ways. The technique of [underwater weighing](https://en.wikipedia.org/wiki/Hydrostatic_weighing), computes body volume as the difference between body weight measured in air and weight measured during water submersion. In other words, body volume is equal to the loss of weight in water with the appropriate temperature correction for the water's density. \n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/underwater-weigh.jpg)\n",
    "**Image of a woman having her body density measured by Hydrostatic Weighing**\n",
    "\n",
    "Using this technique,\n",
    "\n",
    "$Body Density = \\frac{W_{A}}{(W_{A} - W_{W}/c.f.- LV)}$        \n",
    "\n",
    "where:\n",
    "\n",
    "* $W_{A}$ = Weight in air (kg)\n",
    "* $W_{W}$ = Weight in water (kg)\n",
    "* $c.f.$ = Water correction factor =0.997 at 76-78 deg F)\n",
    "* $LV$ = Residual Lung Volume (liters)\n",
    "\n",
    "Determining a person's body density by water submersion is at best inconvienient. Wearing a swimsuit, you are completely immersed into a tank of water and are asked to expel as much air from your lungs as possible. A measurement is taken, and the displacement of water is measured to determine body density. Typically, 4 to 5 trials are repeated. Testing usually takes about 10-15 minutes.\n",
    "\n",
    "In the Examples below we will see if we can construct and train a deep neural network that can accurately predict body density using Keras linear regression and a set of clinical measurements.  \n",
    "  \n",
    "The factors (X-variables) for training your neural network are:\n",
    "* **Percent body fat:** from Siri's (1956) equation\n",
    "* **Age:** (years)\n",
    "* **Weight:** (lbs)\n",
    "* **Height:** (inches)\n",
    "* **Neck circumference:** (cm)\n",
    "* **Chest circumference:** (cm)\n",
    "* **Abdomen 2 circumference:** (cm)\n",
    "* **Hip circumference:** (cm)\n",
    "* **Thigh circumference:** (cm)\n",
    "* **Knee circumference:** (cm)\n",
    "* **Ankle circumference:** (cm)\n",
    "* **Biceps (extended) circumference:** (cm)\n",
    "* **Forearm circumference:** (cm)\n",
    "* **Wrist circumference:** (cm)\n",
    "\n",
    "The response variable (Y) that your neural network will try to predict is:\n",
    "* **Density:** determined from underwater weighing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical Costs Dataset for the **Exercises**\n",
    "\n",
    "In this lesson we will be using the [Medical Costs Personal Datasets](https://www.kaggle.com/datasets/mirichoi0218/insurance) for the **Exercises**. \n",
    "\n",
    "Understanding the factors involved in personal medical costs in the US is important for several reasons:\n",
    "\n",
    "* **Financial burden:** Medical costs can be a significant financial burden for individuals and families, impacting their ability to afford necessary healthcare services. Understanding the factors influencing these costs can help individuals make informed decisions about their healthcare spending and budgeting.\n",
    "* **Access to healthcare:** High medical costs can create barriers to accessing healthcare services, particularly for individuals with limited financial resources. By understanding the factors contributing to medical costs, policymakers and healthcare providers can work towards improving affordability and access to care.\n",
    "* **Health outcomes:** The cost of healthcare can influence individuals' decisions to seek treatment or adhere to medical recommendations. Understanding factors influencing medical costs can help identify disparities in access to care and develop interventions to improve health outcomes.\n",
    "* **Policy implications:** Knowledge of the factors shaping personal medical costs can inform healthcare policies and regulations aimed at controlling costs, improving quality of care, and expanding access to healthcare services. This understanding is crucial for policymakers seeking to address healthcare affordability and sustainability in the US.\n",
    "\n",
    "The Medical Costs dataset contains statistical information about the insurance medical bill (`charges`) for individuals (_n_=1338) living in 4 different areas of the US. \n",
    "\n",
    "The dataset included some of the factors (X-variables) that contribute to medical costs including: \n",
    "* **age:** age of primary beneficiary\n",
    "* **sex:** insurance contractor gender, female, male\n",
    "* **bmi:** Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,\n",
    "objective index of body weight (kg / $m^2$) using the ratio of height to weight, ideally 18.5 to 24.9\n",
    "* **children:** Number of children covered by health insurance / Number of dependents\n",
    "* **smoker:** Smoking\n",
    "* **region:** the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n",
    "\n",
    "The response variable (Y) that your neural network will try to predict will be:\n",
    "* **charges:** Individual medical costs billed by health insurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Read the datafile, create DataFrame and display\n",
    "\n",
    "The code in the cell below reads the Body Fat datafile, `bodyfat.csv` from the course HTTPS server and creates a DataFrame called `bfDF`. The display options are set for 6 rows and 8 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1: Read data, create DataFrame and display\n",
    "\n",
    "# Read the data set\n",
    "bfDF = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/bodyfat.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "\n",
    "# Display DataFrame\n",
    "display(bfDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you code is correct you should see the following table:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_04_3_Exe1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1: Read the datafile, create DataFrame and display**\n",
    "\n",
    "In the cell below, read `Medical Costs` datafile `medical_costs.csv` from the course HTTPS server and creates a DataFrame called `mcDF`. Set your display options for 6 rows and 7 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 1 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you code is correct you should see the following table:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_04_3_Exm1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Determine Preprocessing Steps\n",
    "\n",
    "In almost every instance, some degree of preprocessing must be done before data can be used for training a neural network. The cell below uses the Pandas method `pd.dtypes()` to print out a list of the data types in the different columns in a DataFrame. Example 2 prints out the data types in `bfDF`. What we are looking for are the columns with categorical (non-numeric) values. \n",
    "\n",
    "Dependent on the number of columns in the DataFrame, you may have to adjust the number of rows to display. Since the DataFrame `bfDF` has 15 columns, the number of rows to display had to be set to 15, as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Print data types\n",
    "\n",
    "# Set num of row to number of columns in DF\n",
    "pd.set_option('display.max_rows', 15)\n",
    "\n",
    "# Print data types\n",
    "print(bfDF.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "Density    float64\n",
    "BodyFat    float64\n",
    "Age          int64\n",
    "Weight     float64\n",
    "Height     float64\n",
    "Neck       float64\n",
    "Chest      float64\n",
    "Abdomen    float64\n",
    "Hip        float64\n",
    "Thigh      float64\n",
    "Knee       float64\n",
    "Ankle      float64\n",
    "Biceps     float64\n",
    "Forearm    float64\n",
    "Wrist      float64\n",
    "dtype: object\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns with data types that are either `int64` or `float64` are numeric while columns that are `object` are categorical (string) values which must be converted into numerical values during data preprocessing. At least for the Body Fat dataset, we don't have to worry about categorical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2: Determine Preprocessing Steps**\n",
    "\n",
    "In the cell below use the Pandas method `pd.dtypes()` to print out a list of the data types in the DataFrame `mcDF`. Since this DataFrame has 7 columns, set the number of rows to display to be set to 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 2 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "age           int64\n",
    "sex          object\n",
    "bmi         float64\n",
    "children      int64\n",
    "smoker       object\n",
    "region       object\n",
    "charges     float64\n",
    "dtype: object\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, columns with data types that are either `int64` or `float64` are numeric while columns that are `object` are categorical (string) values. In the Medical Costs data set there are 3 columns with categorical values, `sex`, `smoker` and `region`. You will have to convert these strings into numerical values during data preprocessing in **Exercise 3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Preprocess data, generate X and Y, split data\n",
    "\n",
    "From the results of Example 2, we know that the DataFrame `bfDF` only has numeric values. There are also no missing values to worry about. However, with numeric data it is generally a good idea to standardize the values. Standardizing numeric values helps ensure that features are on a similar scale, which can lead to faster convergence during training. Neural networks often perform better when input features are standardized as it makes it easier for the optimizer to find the optimal weights and biases.\n",
    "\n",
    "The code in the cell below standardizes all of the columns in the Body Fat dataset with the notable exception of the target column, `Density` by converting the values into their Z-scores \n",
    "\n",
    "Once the data has been standardize, the independent (X) values are generated in a 2-step process. First, a list of the columns that are to be included, called `bfX_columns`, is created using the Pandas command:\n",
    "\n",
    "> bfX_columns = bfDF.columns.drop('Density')`\n",
    "\n",
    "At this point we can drop one (or more) columns that we **don't** want to be included in the X-values. Since the column `Density` is going to be out Y-values, we drop this column. \n",
    "\n",
    "The second step in creating the X values is to use the column list, `bfX_columns` as part of the following command:\n",
    "\n",
    "> `bfX = bfDF[bfX_columns].values`\n",
    "\n",
    "This command creates the variable `bfX` by using the Pandas method `.values`. The variable `bfX` is **not** a DataFrame, but a large Numpy array containing the X-values. The next line of code is necessary for Keras to work correctly by making sure all values in `bfX` are type `float32`. \n",
    "\n",
    "> `bfX = np.asarray(bfX).astype('float32')`\n",
    "\n",
    "The next step is to generate the Y-values for the regression directly from the target column, `Density`. It is very important **not** to One-Hot Encode the Y-values in a regression analysis! We just want to use the numerical values as they are.\n",
    "\n",
    "The last preprocessing step is to split the X-values in `bfX` and the Y-values in `bfY` into training and test (validation) data sets. The parameter, `test_size=0.25` specifies that we want about 25% of the X and Y data going into the test data sets `bfX_test` and `bfY_test`, respectively, and the rest of the data going into the training data sets `bfX_train` and `bfY_train`. \n",
    "\n",
    "As a final check, the X data for the first 4 subjects in the test or validation set, `bfX_test`, is printed out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 3: Preprocess data, generate X and Y, split data\n",
    "\n",
    "\n",
    "# Standardize ranges to z-scores\n",
    "bfDF['BodyFat'] = zscore(bfDF['BodyFat'])\n",
    "bfDF['Age'] = zscore(bfDF['Age'])\n",
    "bfDF['Weight'] = zscore(bfDF['Weight'])\n",
    "bfDF['Neck'] = zscore(bfDF['Neck'])\n",
    "bfDF['Abdomen'] = zscore(bfDF['Abdomen'])\n",
    "bfDF['Hip'] = zscore(bfDF['Hip'])\n",
    "bfDF['Thigh'] = zscore(bfDF['Thigh'])\n",
    "bfDF['Knee'] = zscore(bfDF['Knee'])\n",
    "bfDF['Ankle'] = zscore(bfDF['Ankle'])\n",
    "bfDF['Biceps'] = zscore(bfDF['Biceps'])\n",
    "bfDF['Forearm'] = zscore(bfDF['Forearm'])\n",
    "bfDF['Wrist'] = zscore(bfDF['Wrist'])\n",
    "\n",
    "\n",
    "# Generate X\n",
    "bfX_columns = bfDF.columns.drop('Density')\n",
    "bfX = bfDF[bfX_columns].values\n",
    "bfX = np.asarray(bfX).astype('float32')\n",
    "\n",
    "# Generate Y\n",
    "bfY = bfDF['Density'].values\n",
    "bfY = np.asarray(bfY).astype('float32')\n",
    "\n",
    "\n",
    "# Create train/test\n",
    "bfX_train, bfX_test, bfY_train, bfY_test = train_test_split(    \n",
    "    bfX, bfY, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print out bfX_test\n",
    "print(bfX_test[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "[ 5.89148048e-03 -7.85951495e-01  1.29814422e+00  7.37500000e+01\n",
    "   1.03373802e+00  1.07500000e+02  2.36399174e-01  6.42705977e-01\n",
    "   1.02949166e+00  1.12567818e+00  1.47654676e+00  1.36856163e+00\n",
    "   2.49723125e+00  1.25598311e+00]\n",
    " [ 5.89148048e-03 -1.50154281e+00  7.07650632e-02  6.97500000e+01\n",
    "  -6.56227350e-01  1.05099998e+02 -1.72459662e-01  5.52793778e-02\n",
    "  -1.91993043e-01 -1.20679036e-01 -1.19643927e-01 -1.23840421e-01\n",
    "  -4.28372264e-01 -5.68578303e-01]\n",
    " [ 1.05951631e+00 -1.49870321e-01  1.47476256e-01  7.00000000e+01\n",
    "  -3.67696702e-01  1.08000000e+02  1.15633154e+00  4.32910770e-01\n",
    "   8.19549024e-01  5.85590065e-01  2.94183314e-01  4.06791419e-01\n",
    "  -4.28372264e-01 -8.90559793e-01]\n",
    " [ 1.61540598e-01 -7.85951495e-01 -5.70869297e-02  7.10000000e+01\n",
    "   1.68145999e-01  1.00500000e+02 -2.09628657e-01 -1.68502197e-01\n",
    "  -3.06507230e-01 -5.36131442e-01 -4.15234804e-01 -4.22320843e-01\n",
    "   1.79062374e-02 -5.68578303e-01]]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is the X-values for 4 subjects in the test set. For each subject there are 14 floating-point numbers representing the Z-score values, in order, for their:  `BodyFat`, `Age`, `Weight`, `Height`, `Neck`, `Chest`, `Abdomen`, `Hip`, `Thigh`, `Knee`, `Ankle`, `Biceps`, `Forearm` and `Wrist`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3: Preprocess data, generate X and Y, split data**\n",
    "\n",
    "From the results of **Exercise 2**, we know that the DataFrame `mcDF` has three columns with categorical values: `sex`, `smoker` and `region`. The columns `sex` and `smoker` are **_binary_**. In other words, these columns only contain two values. The column `sex` contains the strings `male` and `female`, while the column `smoker` contains the strings `yes` and `no`. As a general rule, _mapping_ is the most efficient way to handle binary categories. \n",
    "\n",
    "In the cell below, map the string `male` to a value of `1` and the string `female` to a value of `0` using the following line of code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "# Map sex\n",
    "mapping = {'male': 1, 'female': 0}\n",
    "mcDF['sex'] = mcDF['sex'].map(mapping)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same approach to map the strings `yes` to `1` and `no` to `0` for the column `smoker`.\n",
    "\n",
    "The column `region` contains 4 strings: `northeast`, `northwest`, `southeast` and `southwest`. In this instance, you should use `One-Hot Encoding`, instead of mapping, to preprocess the strings in `region` using the Pandas function `pd.get_dummies()` as shown as in the code example below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "# Generate dummies for region\n",
    "mcDF = pd.concat([mcDF,pd.get_dummies(mcDF['regions'],prefix=\"region\")],axis=1)\n",
    "mcDF.drop('region', axis=1, inplace=True)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two columns with numeric values, `age` and `bmi`, the need to be standardized to the Z-scores.\n",
    "\n",
    "Once the data has been standardize, generate the independent (X) values by first creating  a list of columns to be included, called `mcX_columns`, making sure to drop the target column, `charges`. \n",
    "\n",
    "Then generate the Y-values for the regression directly from the target column, `charges`. Again, it is very important **not** to One-Hot Encode the Y-values in a regression analysis! We just want to use the numerical values as they are.\n",
    "\n",
    "Finally, split the X-values in `mcX` and the Y-values in `mcY` into training and test (validation) data sets setting the parameter,`test_size=0.25`. The test data sets should be called `mcX_test` and `mcY_test`, and the training data sets called `mcX_train` and `mcY_train`. \n",
    "\n",
    "As a final check, print out the X data for the first 4 subjects in the test or validation set, `mcX_test`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 3 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "[[ 0.41246668  0.         -0.9003412   2.          0.          1.\n",
    "   0.          0.          0.        ]\n",
    " [-0.22834402  0.         -0.10554571  0.          0.          0.\n",
    "   1.          0.          0.        ]\n",
    " [ 1.7652893   0.         -0.6198251   0.          1.          0.\n",
    "   1.          0.          0.        ]\n",
    " [ 0.48366788  1.         -0.80683583  3.          0.          0.\n",
    "   1.          0.          0.        ]]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the values for: `age`, `sex`, `bmi`, `children`, `smoker` and the last 4 values are the dummy columns for `region`. \n",
    "\n",
    "You can tell the gender of these subjects by the 2nd value in each array. In this case, the first three subjects have `0` as the second value, so they are female, while the 4th subject has `1` making him a male.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Construct, compile and fit neural network\n",
    "\n",
    "The code in the cell below constructs a linear (\"sequential\") regression neural network called `bfModel` with 3 hidden layers, with 50 neurons in the 1st layer, 25 neurons in the second layer and 10 neurons in the 3rd layer. Since this is a regression neural network, there is only a single neuron in the output layer. The \"voltage\" in this neuron at the end of a run represents the neural network's prediction of an individual's body `density`.  \n",
    "\n",
    "Since the objective of the model `bfModel` is regression, we compile the model using the 'mean_squared_error' as the loss function along with `adam` as the optimizer.\n",
    "\n",
    "An EarlyStopping monitor called `bfMonitor` is created to stop the fitting process if the value for the `validation loss` doesn't increase after waiting 50 epochs. \n",
    "\n",
    "Finally, the model is run (\"fitted\") for 1000 epochs using the training and test data created in Example 3. The verbose setting (output) is set to 0. \n",
    "\n",
    "**WARNING---WARNING--WARNING--WARNING**\n",
    "\n",
    "The variable `verbose` is set to `0`.  \n",
    "You will **NOT** see anything happing when you run the next cell. \n",
    "\n",
    "> BE PATIENT. \n",
    "\n",
    "Depending upon your computer's speed, it will take some time to complete. Just relax. \n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/KeepCalm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Construct, compile and fit neural network\n",
    "\n",
    "# Construct\n",
    "bfModel = Sequential()\n",
    "bfModel.add(Dense(50, input_dim=bfX.shape[1], activation='relu')) # Hidden 1\n",
    "bfModel.add(Dense(25, activation='relu')) # Hidden 2\n",
    "bfModel.add(Dense(10, activation='relu')) # Hidden 3\n",
    "bfModel.add(Dense(1)) # Output\n",
    "\n",
    "# Compile\n",
    "bfModel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Create Monitor\n",
    "bfMonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "                        patience=50, verbose=1, mode='auto', \n",
    "                        restore_best_weights=True)\n",
    "\n",
    "# Fit\n",
    "bfModel.fit(bfX_train,bfY_train,validation_data=(bfX_test,bfY_test),\n",
    "          callbacks=[bfMonitor],verbose=0,epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming everything went smoothly, you should see something similar to the output below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "Restoring model weights from the end of the best epoch: 121.\n",
    "Epoch 171: early stopping\n",
    "\n",
    "<keras.callbacks.History at 0x242baf29880>\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example, the lowest validation loss occured after epoch 121. The model ran another 50 epochs before the monitor `bfMonitor` terminated the fitting and restored the connection weights between all of the neurons to the value they had after the 121 epoch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 4: Construct, compile and fit neural network**\n",
    "\n",
    "In the cell below constructs a linear (\"sequential\") regression neural network called `mcModel` with 3 hidden layers, with 50 neurons in the 1st layer, 25 neurons in the second layer and 10 neurons in the 3rd layer. Make sure that there is only a single neuron in the output layer.   \n",
    "\n",
    "Compile the model using the 'mean_squared_error' as the loss function along with `adam` as the optimizer.\n",
    "\n",
    "Create an EarlyStopping monitor called `mcMonitor` to stop the fitting process if the value for the `validation loss` doesn't increase after waiting 50 epochs. \n",
    "\n",
    "Finally, fit your model for 1000 epochs using the training and test data created in **Exercise 3**. \n",
    "\n",
    "You may set the verbose argument to either `0` (no output) or `2` (output after each epoch). In either case, your model `mcModel` will take significantly **LONGER** to run than the previous model. If you select `verbose=2` be prepared for several \"pages\" of screen output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 4 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the output from setting verbose=0 and waiting several minutes for the fitting to terminate after 645 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "Restoring model weights from the end of the best epoch: 645.\n",
    "Epoch 695: early stopping\n",
    "\n",
    "<keras.callbacks.History at 0x242cd496f70>\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Square Error\n",
    "\n",
    "Using **_Mean Squared Error (MSE)_** for regression neural networks is common for several reasons:\n",
    "\n",
    "* **Differentiable and continuous:** MSE is a differentiable and continuous loss function, making it suitable for optimization algorithms like gradient descent. This allows the neural network to update its parameters smoothly during training to minimize the error.\n",
    "* **Mathematically well-defined:** MSE calculates the average of the squared differences between predicted and actual values, providing a clear measure of how well the model is performing in terms of minimizing prediction errors. It provides a single, interpretable metric for assessing the model's performance.\n",
    "* **Emphasis on outliers:** Squaring the errors in MSE gives higher weights to larger errors, making the model more sensitive to outliers in the data. This can be useful in regression tasks where accurately predicting extreme values is important.\n",
    "* **Convex optimization:** MSE is convex, meaning it has a single global minimum, making it easier for optimization algorithms to find the optimal model parameters. This can lead to faster convergence during training.\n",
    "* **Widely used:** MSE is a commonly used loss function for regression tasks in neural networks, which means there are well-established techniques and frameworks for implementing and optimizing models with MSE as the loss function.\n",
    "\n",
    "The mean square error (MSE) is the sum of the squared differences between the prediction ($\\hat{y}$) and the expected ($y$).  MSE values are not of a particular unit. If an MSE value has decreased for a model, that is good. However, beyond this, there is not much more you can determine. We seek to achieve low MSE values. The following equation demonstrates how to calculate MSE.\n",
    "\n",
    "$$ \\mbox{MSE} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}_i - y_i\\right)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Compute MSE\n",
    "\n",
    "The code in the cell below uses the function `metrics.mean_squared_error()` from the Python library called `scikit-learn` (alias `sklearn`) to compute the MSE for the model `bfModel`. This function takes 2 arguments, an array containing the model's **_predicted_** value for body `Density` for every subject in the validation dataset `bfX_test`, and their **_actual_** body `Density` values, stores in `bfY_test`.  Finally, the code prints out the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 5: Compute MSE\n",
    "\n",
    "# Use model to predict values\n",
    "bfPred = bfModel.predict(bfX_test)\n",
    "\n",
    "# Compare predicted and actual values  \n",
    "score = metrics.mean_squared_error(bfPred,bfY_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Final score (MSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "2/2 [==============================] - 0s 4ms/step\n",
    "Final score (MSE): 0.0009884501341730356\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relatively small number so that's a good sign. However, when it comes to MSE, all you really know is that `smaller is better`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 5: Compute MSE**\n",
    "\n",
    "In the cell below use the function `metrics.mean_squared_error()` to compute the MSE for your model `mcModel`. Call your prediction `mcPred`. Print out the results as illustrated in Example 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 5 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see something similar to the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "11/11 [==============================] - 0s 2ms/step\n",
    "Final score (MSE): 19451004.0\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the MSE computed above in Example 5, your MSE for `mcModel`, 19451004.0, might seem to be extremely large. However, looks can be deceiving.  Your model was measuring the cost of medical treatment in the _tens of thousands_ of dollars, while the other model, `bfModel` was measuring body density, which has an average (mean) value of only 0.31558996."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Square Error\n",
    "\n",
    "Using Root Mean Squared Error (RMSE) for regression neural networks has several advantages:\n",
    "\n",
    "* **Scale interpretation:** RMSE is in the same units as the target variable, providing a more interpretable measure of error compared to MSE. This makes it easier to understand the magnitude of the errors in the predicted values.\n",
    "* **Outlier sensitivity:** RMSE penalizes large errors more heavily than smaller errors due to the square root operation, making the model more sensitive to outliers. This can be beneficial for regression tasks where accurately predicting extreme values is important.\n",
    "* **Averaging effect:** RMSE averages the errors across all samples in the dataset, providing a single metric that represents the overall model performance. This can simplify the evaluation process and make it easier to compare different models.\n",
    "\n",
    "The root mean square (RMSE) is essentially the square root of the MSE. Because of this, the RMSE error is in the same units as the training data outcome. We desire Low RMSE values. The following equation calculates RMSE.\n",
    "\n",
    "$$ \\mbox{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}_i - y_i\\right)^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Compute RMSE\n",
    "\n",
    "The code in the cell below again uses the function `metrics.mean_squared_error()` from the Python library called `scikit-learn` (alias `sklearn`) to compute the MSE for the model `bfModel`. To compute the _Root_ Mean Squared Error, you simple take the square root of the MSE, using the Numpy function `np.sqrt()` as shown in the next line of code:\n",
    "\n",
    "> `score = np.sqrt(metrics.mean_squared_error(mcPred,mcY_test))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Compute RMSE\n",
    "\n",
    "# Compute RMSE\n",
    "score = np.sqrt(metrics.mean_squared_error(bfPred,bfY_test))\n",
    "\n",
    "# Print RSME\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "# Print mean of Y\n",
    "print(f\"Average body `Density`:{bfY_test.mean()}\")\n",
    "\n",
    "# Print comparison\n",
    "print(f\"RMSE as percent of mean `Density`:  {score/bfY_test.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "Final score (RMSE): 0.03143962845206261\n",
    "Average body `Density`:1.0570443868637085\n",
    "RMSE as percent of mean `Density`:  0.029742959886789322\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RSME represents has about a 3% error in the `bfModel's` ability to accurately predict body `Density`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 6: Compute RMSE**\n",
    "\n",
    "In the cell below compute and print out the RMSE for your model `mcModel`, the average (mean) value of `charges` of the subjects in the validation set and \n",
    "\n",
    "To compute the _Root_ Mean Squared Error, you simple take the square root of the MSE, using the Numpy function `np.sqrt()` as shown in the next line of code:\n",
    "\n",
    "> `score = np.sqrt(metrics.mean_squared_error(mcPred,mcY_test))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Compute RMSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "Final score (RMSE): 4410.3291015625\n",
    "Average insurance `charges`:13277.865234375\n",
    "RMSE as percent of mean `charges`:  0.3321564793586731625\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, RMSE for your `mcModel` was \\\\$4,410. On average, the people in the validation group spent \\\\$13,278 on insurance `charges` each year. Therefore, when expressed as a percentage of the mean, the RMSE was about 33% of the average costs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lift Chart\n",
    "\n",
    "We often visualize the results of regression with a lift chart. Lift charts are graphical tools used to evaluate the performance of predictive models, including neural network models. The lift chart shows how much better the model is at predicting outcomes compared to a random model. They also help in understanding the model's ability to rank or classify instances correctly.\n",
    "\n",
    "To generate a lift chart, perform the following activities:\n",
    "\n",
    "* Sort the data by expected output and plot these values.\n",
    "* For every point on the x-axis, plot that same data point's predicted value in another color.\n",
    "* The x-axis is just 0 to 100% of the dataset. The expected always starts low and ends high.\n",
    "* The y-axis is ranged according to the values predicted.\n",
    "\n",
    "You can interpret the lift chart as follows:\n",
    "\n",
    "* The expected and predict lines should be close. Notice where one is above the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7: Plot Lift Chart\n",
    "\n",
    "The code in the cell below creates a function called `chart_regression()`. The function takes two arguments: (1) the model's **_predictions_** of the response variable for each subject in the dataset and (2) the **_actual_** response variable for each subject in the dataset. In this example, the `bfModel's` predictions are stored in the variable `bfPred` while the corresponding actual values are stored in `bfY`. \n",
    "\n",
    "The function begins by sorting the predicted and actual values by size, from small to large. The sorted predicted values are assigned to the variable `pred`. The actual values in `y` are 'flattened'. This means all of the values stored in 2-dimensional arrays are converted into a single, contiguous one-dimensional array. \n",
    "\n",
    "The function plots two lines. In this example, the blue line shows all of the 'Density` values in the Body Fat dataset, with the smallest value to the largest value plotted left-to-right. The orange line shows the model's predicted value for each actual value. Sometimes the model's predictions are greater than the actual values (the orange line is above the blue) and sometimes the predicted values are lower than the actual values (the orange line is below the blue). The difference between these two lines are the _'errors'_ that are used in the calculation of the **Root Mean Squared _ERRORS_ (RMSE)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: Plot Lift Chart\n",
    "\n",
    "# Define plot function\n",
    "def chart_regression(pred, y, sort=True):\n",
    "    t = pd.DataFrame({'pred': pred, 'y': y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'], inplace=True)\n",
    "    plt.plot(t['y'].tolist(), label='expected')\n",
    "    plt.plot(t['pred'].tolist(), label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Use function to plot the chart\n",
    "chart_regression(bfPred.flatten(),bfY_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see a Lift Plot similar to the one shown below.\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_04_3_Lift1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 7: Plot Lift Chart**\n",
    "\n",
    "In the cell below plot the Lift Chart for your `mcModel` predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 7 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see a Lift Plot similar to the one shown below.\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_04_3_Lift2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visual inspect your `mcModel's` ability to predict the insurance costs (`charges`) becomes less accurate as the insurance costs start to increase more rapidly starting around the X value of 240. The reason for this sudden increase is not immediately obvious. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lesson Turn-in**\n",
    "\n",
    "When you have completed all of the code cells, and run them in sequential order (the last code cell should be number 16), use the **File --> Print.. --> Save to PDF** to generate a PDF of your JupyterLab notebook. Save your PDF as `Class_04_3.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
