{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEEW13uCtiJ"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/master/Class_04_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BIO 1173: Intro Computational Biology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Module 4: Training for Tabular Data**\n",
    "\n",
    "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
    "\n",
    "### Module 4 Material\n",
    "\n",
    "* Part 4.1: Encoding a Feature Vector for Keras Deep Learning\n",
    "* Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC\n",
    "* **Part 4.3: Keras Regression for Deep Neural Networks with RMSE**\n",
    "* Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Neural Network Training\n",
    "* Part 4.5: Neural Network RMSE and Log Loss Error Calculation from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKQylnEiLDUM"
   },
   "source": [
    "### Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n",
    "  Running the following code will map your GDrive to ```/content/drive```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seXFCYH4LDUM",
    "outputId": "c05015aa-871e-4779-9265-5ad07e8bf617"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Setup\n",
    "\n",
    "Run the next code cell to load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You MUST run this code cell first\n",
    "\n",
    "# Classification neural network\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "path = '/'\n",
    "memory = shutil.disk_usage(path)\n",
    "dirpath = os.getcwd()\n",
    "print(\"Your current working directory is : \" + dirpath)\n",
    "print(\"Disk\", memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Datasets for Class_04_3\n",
    "\n",
    "For Class_04_3 we will be using the Body Fat dataset for the Examples, and the Medical Costs dataset for the **Exercises**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Body Fat Dataset**\n",
    "\n",
    "[Body Fat Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset) \n",
    "\n",
    "Volume, and hence body **_density_**, can be accurately measured in a variety of ways. One of the most precise ways in called [underwater weighing](https://en.wikipedia.org/wiki/Hydrostatic_weighing) or more formally, **_hydrostatic weighing_**\n",
    "\n",
    "This method computes body volume as the difference between body weight measured in air and weight measured during water submersion. In other words, body volume is equal to the loss of weight in water with the appropriate temperature correction for the water's density. \n",
    "\n",
    "**Image of a woman having her body density measured by Hydrostatic Weighing**\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/underwater-weigh.jpg)\n",
    "\n",
    "\n",
    "The mathematical formulas used in this technique are as follows:\n",
    "\n",
    "$Body Density = \\frac{W_{A}}{(W_{A} - W_{W}/c.f.- LV)}$        \n",
    "\n",
    "where:\n",
    "\n",
    "* $W_{A}$ = Weight in air (kg)\n",
    "* $W_{W}$ = Weight in water (kg)\n",
    "* $c.f.$ = Water correction factor =0.997 at 76-78 deg F)\n",
    "* $LV$ = Residual Lung Volume (liters)\n",
    "\n",
    "While very precise, determining a person's body density by water submersion is at best inconvienient. Wearing a swimsuit, you are completely immersed into a tank of water and are asked to expel as much air from your lungs as possible. A measurement is taken, and the displacement of water is measured to determine body density. Typically, 4 to 5 trials are repeated. Testing usually takes about 10-15 minutes.\n",
    "\n",
    "In the Examples below we will see if we can construct and train a deep neural network that can make accurate predictions body density with hydrostatic submersion, using a Keras linear regression neural network and a set of clinical measurements.  \n",
    "  \n",
    "The factors (X-values) for training your neural network are:\n",
    "\n",
    "* **Percent body fat:** from Siri's (1956) equation\n",
    "* **Age:** (years)\n",
    "* **Weight:** (lbs)\n",
    "* **Height:** (inches)\n",
    "* **Neck circumference:** (cm)\n",
    "* **Chest circumference:** (cm)\n",
    "* **Abdomen 2 circumference:** (cm)\n",
    "* **Hip circumference:** (cm)\n",
    "* **Thigh circumference:** (cm)\n",
    "* **Knee circumference:** (cm)\n",
    "* **Ankle circumference:** (cm)\n",
    "* **Biceps (extended) circumference:** (cm)\n",
    "* **Forearm circumference:** (cm)\n",
    "* **Wrist circumference:** (cm)\n",
    "\n",
    "The response variable (Y) that your neural network will try to predict is:\n",
    "* **Density:** determined from underwater weighing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Medical Costs Dataset**\n",
    "\n",
    "[Medical Costs Personal Datasets](https://www.kaggle.com/datasets/mirichoi0218/insurance)  \n",
    "\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/Health_Spending.jpg)\n",
    "\n",
    "Understanding the factors involved in personal medical costs in the US is important for several reasons:\n",
    "\n",
    "* **Financial burden:** Medical costs can be a significant financial burden for individuals and families, impacting their ability to afford necessary healthcare services. Understanding the factors influencing these costs can help individuals make informed decisions about their healthcare spending and budgeting.\n",
    "* **Access to healthcare:** High medical costs can create barriers to accessing healthcare services, particularly for individuals with limited financial resources. By understanding the factors contributing to medical costs, policymakers and healthcare providers can work towards improving affordability and access to care.\n",
    "* **Health outcomes:** The cost of healthcare can influence individuals' decisions to seek treatment or adhere to medical recommendations. Understanding factors influencing medical costs can help identify disparities in access to care and develop interventions to improve health outcomes.\n",
    "* **Policy implications:** Knowledge of the factors shaping personal medical costs can inform healthcare policies and regulations aimed at controlling costs, improving quality of care, and expanding access to healthcare services. This understanding is crucial for policymakers seeking to address healthcare affordability and sustainability in the US.\n",
    "\n",
    "The Medical Costs dataset contains statistical information about the insurance medical bill (`charges`) for individuals (_n_=1338) living in 4 different areas of the US. \n",
    "\n",
    "The dataset included some of the factors (X-variables) that contribute to medical costs including: \n",
    "* **age:** age of primary beneficiary\n",
    "* **sex:** insurance contractor gender, female, male\n",
    "* **bmi:** Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,\n",
    "objective index of body weight (kg / $m^2$) using the ratio of height to weight, ideally 18.5 to 24.9\n",
    "* **children:** Number of children covered by health insurance / Number of dependents\n",
    "* **smoker:** Smoking\n",
    "* **region:** the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n",
    "\n",
    "The response variable (Y) that your neural network will try to predict will be:\n",
    "* **charges:** Individual medical costs billed by health insurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4.3: Keras Regression for Deep Neural Networks with RMSE\n",
    "\n",
    "In Keras, regression models can be built using deep neural networks to predict continuous values. The RMSE (Root Mean Squared Error) is a common metric used to evaluate the performance of regression models. It measures the average magnitude of the errors between predicted and actual values. To train a regression model using Keras, you would typically define a deep neural network architecture with dense layers, activation functions, and optimizer. The target variable would be continuous and the loss function used would be mean squared error (MSE).\n",
    "\n",
    "During training, the model would minimize the MSE loss function by adjusting the weights and biases of the neural network using backpropagation. The model's performance can be evaluated using RMSE on a separate test dataset, where a lower RMSE indicates a better performing model. Overall, Keras Regression for Deep Neural Networks with RMSE involves building a neural network for regression tasks, training it using MSE loss, and evaluating its performance using RMSE.\n",
    "\n",
    "We evaluate regression results differently than classification.  Consider the following code that trains a neural network for regression on the data set **jh-simple-dataset.csv**.  We begin by preparing the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Read the datafile, create DataFrame and display\n",
    "\n",
    "The code in the cell below reads the Body Fat datafile, `bodyfat.csv` from the course HTTPS server and creates a DataFrame called `bfDF`. The display options are set for 6 rows and 8 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1: Read data, create DataFrame and display\n",
    "\n",
    "# Read the data set\n",
    "bfDF = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/bodyfat.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "\n",
    "# Display DataFrame\n",
    "display(bfDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you code is correct you should see the following table:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_04_3_Exe1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1: Read the datafile, create DataFrame and display**\n",
    "\n",
    "In the cell below, read `Medical Costs` datafile `medical_costs.csv` from the course HTTPS server and creates a DataFrame called `mcDF`. Set your display options for 6 rows and 7 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 1 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you code is correct you should see the following table:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_04_3_Exm1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Determine Preprocessing Steps\n",
    "\n",
    "In almost every instance, some degree of preprocessing must be done before data can be used for training a neural network. The cell below uses the Pandas method `df.info()` to print out a list of the data types in the different columns in a DataFrame. \n",
    "\n",
    "Dependent on the number of columns in the DataFrame, you may have to adjust the number of rows to display. Since the DataFrame `bfDF` has 15 columns, the number of rows to display had to be set to 15, as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Print data types\n",
    "\n",
    "# Set num of row to number of columns in DF\n",
    "pd.set_option('display.max_rows', 15)\n",
    "\n",
    "# Print data types\n",
    "print(bfDF.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "\n",
    "~~~text\n",
    "RangeIndex: 252 entries, 0 to 251\n",
    "Data columns (total 15 columns):\n",
    " #   Column   Non-Null Count  Dtype  \n",
    "---  ------   --------------  -----  \n",
    " 0   Density  252 non-null    float64\n",
    " 1   BodyFat  252 non-null    float64\n",
    " 2   Age      252 non-null    int64  \n",
    " 3   Weight   252 non-null    float64\n",
    " 4   Height   252 non-null    float64\n",
    " 5   Neck     252 non-null    float64\n",
    " 6   Chest    252 non-null    float64\n",
    " 7   Abdomen  252 non-null    float64\n",
    " 8   Hip      252 non-null    float64\n",
    " 9   Thigh    252 non-null    float64\n",
    " 10  Knee     252 non-null    float64\n",
    " 11  Ankle    252 non-null    float64\n",
    " 12  Biceps   252 non-null    float64\n",
    " 13  Forearm  252 non-null    float64\n",
    " 14  Wrist    252 non-null    float64\n",
    "dtypes: float64(14), int64(1)\n",
    "memory usage: 29.7 KB\n",
    "None\n",
    "~~~\n",
    "\n",
    "You should pay attentions to two features. First, see if there are any differences in the `Non-Null Count`. In this case, all of the Non-Null Counts are the same (`252`). If there were different values, that would indicate missing data. \n",
    "\n",
    "The second feature you should pay attention to is the data type (`Dtype`). Columns with data types that are either `int64` or `float64` are numeric while columns that are `object` are categorical (string) values which must be converted into numerical values during data preprocessing. \n",
    "\n",
    "At least for our Body Fat DataFrame, `bfDF`, we don't have to worry about preprocessing any categorical values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2: Determine Preprocessing Steps**\n",
    "\n",
    "In the cell below use the Pandas method `pd.info()` to print out a list of the data types in the DataFrame `mcDF`. Since this DataFrame has 7 columns, set the number of rows to display to be set to 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 2 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "~~~text\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 1338 entries, 0 to 1337\n",
    "Data columns (total 7 columns):\n",
    " #   Column    Non-Null Count  Dtype  \n",
    "---  ------    --------------  -----  \n",
    " 0   age       1338 non-null   int64  \n",
    " 1   sex       1338 non-null   object \n",
    " 2   bmi       1338 non-null   float64\n",
    " 3   children  1338 non-null   int64  \n",
    " 4   smoker    1338 non-null   object \n",
    " 5   region    1338 non-null   object \n",
    " 6   charges   1338 non-null   float64\n",
    "dtypes: float64(2), int64(2), object(3)\n",
    "memory usage: 73.3+ KB\n",
    "None\n",
    "~~~\n",
    "Again, columns with data types that are either `int64` or `float64` are numeric while columns that are `object` are categorical (string) values. In the Medical Costs data set there are 3 columns with categorical values, `sex`, `smoker` and `region`. You will have to convert these strings into numerical values during data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Preprocess, Create Feature Vectors and Split Data\n",
    "\n",
    "We know from Example 2 that `bfDF` only has numeric values so there are no string data to worry about. However, with numeric data it is generally a good idea to standardize the values. \n",
    "\n",
    "Standardizing numeric values helps ensure that features are on a similar scale, which can lead to faster convergence during training. Neural networks often perform better when input features are standardized as it makes it easier for the optimizer to find the optimal weights and biases.\n",
    "\n",
    "The code in the cell below standardizes all of the columns in the Body Fat dataset with the notable exception of the target column, `Density` by converting the values into their Z-scores using this code chunk:\n",
    "~~~text\n",
    "# Generate column list for preprocessing\n",
    "bfX_columns = bfDF.columns.drop('Density')\n",
    "\n",
    "# Replace values with their Z-scores\n",
    "for col in bfX_columns:\n",
    "    bfDF[col] = zscore(bfDF[col])\n",
    "~~~\n",
    "Once the data has been standardize, we can reuse the variable `bfX_columns` to create our X feature vector usin this code chunk:\n",
    "~~~text\n",
    "# Generate X feature vector\n",
    "bfX = bfDF[bfX_columns].values\n",
    "bfX = np.asarray(bfX).astype('float32')\n",
    "~~~~\n",
    "Since we are building a **_regression_** neural network, we **_don't_** want ti One-Hot Encode the response (Y) variable `density`. Instead, we simply convert the values in the column `Density` in a Numpy array using the `.values` method:\n",
    "~~~text\n",
    "# Generate Y feature vector\n",
    "bfY = bfDF['Density'].values\n",
    "bfY = np.asarray(bfY).astype('float32')\n",
    "~~~\n",
    "The last preprocessing step is to split our feature vectors `bfX` and `bfY` into training and test (validation) data sets. The parameter, `test_size=0.25` specifies that we want about 25% of the X and Y data going into the test data sets `bfX_test` and `bfY_test`, respectively, and the rest of the data going into the training data sets `bfX_train` and `bfY_train`.  \n",
    "\n",
    "As a final check, we print out the X data for the first 4 subjects in the test or validation set, `bfX_test`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 3: Preprocess data, generate X and Y, split data\n",
    "\n",
    "# Generate column list for preprocessing\n",
    "bfX_columns = bfDF.columns.drop('Density')\n",
    "\n",
    "# Replace values with their Z-scores\n",
    "for col in bfX_columns:\n",
    "    bfDF[col] = zscore(bfDF[col])\n",
    "\n",
    "# Generate X feature vector\n",
    "bfX = bfDF[bfX_columns].values\n",
    "bfX = np.asarray(bfX).astype('float32')\n",
    "\n",
    "# Generate Y feature vector\n",
    "bfY = bfDF['Density'].values\n",
    "bfY = np.asarray(bfY).astype('float32')\n",
    "\n",
    "# Split data\n",
    "bfX_train, bfX_test, bfY_train, bfY_test = train_test_split(    \n",
    "    bfX, bfY, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print out first 4 of bfX_test\n",
    "np.set_printoptions(suppress=True,precision=4)\n",
    "print(bfX_test[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "~~~text\n",
    "[[ 0.0059 -0.786   1.2981  0.9851  1.0337  0.7934  0.2364  0.6427  1.0295\n",
    "   1.1257  1.4765  1.3686  2.4972  1.256 ]\n",
    " [ 0.0059 -1.5015  0.0708 -0.1091 -0.6562  0.5082 -0.1725  0.0553 -0.192\n",
    "  -0.1207 -0.1196 -0.1238 -0.4284 -0.5686]\n",
    " [ 1.0595 -0.1499  0.1475 -0.0407 -0.3677  0.8529  1.1563  0.4329  0.8195\n",
    "   0.5856  0.2942  0.4068 -0.4284 -0.8906]\n",
    " [ 0.1615 -0.786  -0.0571  0.2328  0.1681 -0.0385 -0.2096 -0.1685 -0.3065\n",
    "  -0.5361 -0.4152 -0.4223  0.0179 -0.5686]]\n",
    "~~~\n",
    "This output is the X-values for 4 subjects in the test set (`bfX_test`). For each subject there are 14 floating-point numbers representing the Z-score values. The order of this values is the same order as the column names in `bfDF`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3: Preprocess data, generate X and Y, split data**\n",
    "\n",
    "From the results of **Exercise 2**, we know that your DataFrame `mcDF` has three columns with categorical values: `sex`, `smoker` and `region`. Since the columns `sex` and `smoker` are **_binary_** (only contain two values), you should use `mapping` to convert the categorical values into integers. Here is the code chunk for mapping the column `sex`:\n",
    "~~~text\n",
    "# Map sex\n",
    "mapping = {'male': 1, 'female': 0}\n",
    "mcDF['sex'] = mcDF['sex'].map(mapping)\n",
    "~~~\n",
    "You can use this as a template to map the strings `yes` and `no` in the `smoker` column to `1` and `0`.\n",
    "\n",
    "The column `region` contains 4 strings: `northeast`, `northwest`, `southeast` and `southwest`. In this instance, you should use `One-Hot Encoding`, instead of mapping, to preprocess the strings in `region` using the Pandas function `pd.get_dummies()` as shown as in this code chunk:\n",
    "~~~text\n",
    "# Generate dummies for region\n",
    "mcDF = pd.concat([mcDF,pd.get_dummies(mcDF['region'],prefix=\"region\")],axis=1)\n",
    "mcDF.drop('region', axis=1, inplace=True\n",
    "~~~\n",
    "There are two columns with numeric values, `age` and `bmi`, the need to be standardized to their Z-scores. In this situation, it probably makes more sense simply to specify individual column by name as shown in this code chunk:\n",
    "~~~text\n",
    "# Standardize ranges to z-scores\n",
    "mcDF['age'] = zscore(mcDF['age'])\n",
    "mcDF['bmi'] = zscore(mcDF['bmi'])\n",
    "~~~\n",
    "\n",
    "Once the data has been standardize, generate the independent (X) values by first creating  a list of columns to be included, called `mcX_columns`, making sure to drop the target column, `charges`. \n",
    "\n",
    "Then generate the Y-values for the regression directly from the target column, `charges`. Again, it is very important **not** to One-Hot Encode the Y-values in a regression analysis! We just want to use the numerical values as they are.\n",
    "\n",
    "Finally, split the X-values in `mcX` and the Y-values in `mcY` into training and test (validation) data sets setting the parameter,`test_size=0.25`. The test data sets should be called `mcX_test` and `mcY_test`, and the training data sets called `mcX_train` and `mcY_train`. \n",
    "\n",
    "As a final check, print out the X data for the first 4 subjects in the test or validation set, `mcX_test`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two columns with numeric values, `age` and `bmi`, the need to be standardized to the Z-scores.\n",
    "\n",
    "Once the data has been standardize, generate the independent (X) values by first creating  a list of columns to be included, called `mcX_columns`, making sure to drop the target column, `charges`. \n",
    "\n",
    "Then generate the Y-values for the regression directly from the target column, `charges`. Again, it is very important **not** to One-Hot Encode the Y-values in a regression analysis! We just want to use the numerical values as they are.\n",
    "\n",
    "Finally, split the X-values in `mcX` and the Y-values in `mcY` into training and test (validation) data sets setting the parameter,`test_size=0.25`. The test data sets should be called `mcX_test` and `mcY_test`, and the training data sets called `mcX_train` and `mcY_train`. \n",
    "\n",
    "As a final check, print out the X data for the first 4 subjects in the test or validation set, `mcX_test`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 3 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:\n",
    "~~~text\n",
    "[[ 0.4125  0.     -0.9003  2.      0.      1.      0.      0.      0.    ]\n",
    " [-0.2283  0.     -0.1055  0.      0.      0.      1.      0.      0.    ]\n",
    " [ 1.7653  0.     -0.6198  0.      1.      0.      1.      0.      0.    ]\n",
    " [ 0.4837  1.     -0.8068  3.      0.      0.      1.      0.      0.    ]]\n",
    "~~~\n",
    "These are the values for: `age` (z-score), sex, bmi (z-score), children, smoker and the last 4 values are the dummy columns for `region`. \n",
    "\n",
    "You can tell the gender of these subjects by the 2nd value in each array. In this particular example, the first three subjects have 0 as the second value, so they are female, while the 4th subject has 1, making him a male. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Construct, Compile and Train Model\n",
    "\n",
    "The code in the cell below constructs a linear (\"sequential\") regression neural network called `bfModel` with 3 hidden layers with: \n",
    "\n",
    "* **50 neurons** in Hidden #1\n",
    "* **25 neurons** in Hidden #2\n",
    "* **10 neurons** in Hidden #3\n",
    "\n",
    "All neurons are `Dense` connected.\n",
    "\n",
    "Since this is a regression neural network, there is only a single neuron in the output layer. The \"voltage\" in this neuron at the end of each run (epoch) represents the neural network's prediction of an individual's body `density`.  \n",
    "\n",
    "Since the objective of the model `bfModel` is regression, we compile the model using the 'mean_squared_error' as the loss function along with `adam` as the optimizer.\n",
    "\n",
    "An EarlyStopping monitor called `bfMonitor` is created to stop the training if the value for the `validation loss` doesn't increase after waiting 50 epochs. \n",
    "\n",
    "Finally, the model is trained (\"fitted\") for 1000 epochs using the training and test data created in Example 3. The verbose setting (output) is set to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Construct, compile and train model\n",
    "\n",
    "# Construct model---------------------------------------------------\n",
    "bfModel = Sequential()\n",
    "bfModel.add(Dense(50, input_dim=bfX.shape[1], \n",
    "                  activation='relu')) # Hidden 1\n",
    "bfModel.add(Dense(25, activation='relu')) # Hidden 2\n",
    "bfModel.add(Dense(10, activation='relu')) # Hidden 3\n",
    "bfModel.add(Dense(1)) # Output\n",
    "\n",
    "# Compile model-------------------------------------------------------\n",
    "bfModel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Create monitor------------------------------------------------------\n",
    "bfMonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "                        patience=50, verbose=1, mode='auto', \n",
    "                        restore_best_weights=True)\n",
    "\n",
    "# Train model---------------------------------------------------------\n",
    "bfModel.fit(bfX_train,bfY_train,validation_data=(bfX_test,bfY_test),\n",
    "          callbacks=[bfMonitor],verbose=0,epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the variable `verbose` is set to `0`, you will **NOT** see anything happening when you run the next cell. It will take some time to complete training before you can see any output. \n",
    "\n",
    "> ### **SO PLEASE BE PATIENT.** \n",
    "\n",
    "Assuming everything went smoothly, you should see something similar to the output below:\n",
    "~~~text\n",
    "Restoring model weights from the end of the best epoch: 184.\n",
    "Epoch 234: early stopping\n",
    "\n",
    "<keras.callbacks.History at 0x26bf045ba30>\n",
    "~~~\n",
    "In this particular example, the lowest validation loss occured after epoch 184. The model ran another 50 epochs before the monitor `bfMonitor` terminated the fitting and restored the connection weights between all of the neurons to the value they had at the end of the 184st epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 4: Construct, Compile and Train Model**\n",
    "\n",
    "In the cell below constructs a linear (\"sequential\") regression neural network called `mcModel` with 3 hidden layers. Put 50 neurons in the 1st layer, 25 neurons in the second layer and 10 neurons in the 3rd layer. Make sure that there is only a single neuron in the output layer.   \n",
    "\n",
    "Compile the model using the 'mean_squared_error' as the loss function along with `adam` as the optimizer.\n",
    "\n",
    "Create an EarlyStopping monitor called `mcMonitor` to stop the fitting process if the value for the `validation loss` doesn't increase after waiting 50 epochs. \n",
    "\n",
    "Finally, fit your model for 1000 epochs using the training and test data created in **Exercise 3**. \n",
    "\n",
    "You may set the verbose argument to either `0` (no output) or `2` (output after each epoch). In either case, your model `mcModel` will take significantly **LONGER** to run than the previous model. If you select `verbose=2` be prepared for several \"pages\" of screen output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 4 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set the argument `verbose` to `0`, don't expect to see any output for a few minutes. \n",
    "\n",
    "Here is the output with `verbose=0` after waiting a few minutes for training to terminate. \n",
    "~~~text\n",
    "Restoring model weights from the end of the best epoch: 443.\n",
    "Epoch 493: early stopping\n",
    "\n",
    "<keras.callbacks.History at 0x26beb859d60>\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Square Error\n",
    "\n",
    "Using **_Mean Squared Error (MSE)_** for regression neural networks is common for several reasons:\n",
    "\n",
    "* **Differentiable and continuous:** MSE is a differentiable and continuous loss function, making it suitable for optimization algorithms like gradient descent. This allows the neural network to update its parameters smoothly during training to minimize the error.\n",
    "* **Mathematically well-defined:** MSE calculates the average of the squared differences between predicted and actual values, providing a clear measure of how well the model is performing in terms of minimizing prediction errors. It provides a single, interpretable metric for assessing the model's performance.\n",
    "* **Emphasis on outliers:** Squaring the errors in MSE gives higher weights to larger errors, making the model more sensitive to outliers in the data. This can be useful in regression tasks where accurately predicting extreme values is important.\n",
    "* **Convex optimization:** MSE is convex, meaning it has a single global minimum, making it easier for optimization algorithms to find the optimal model parameters. This can lead to faster convergence during training.\n",
    "* **Widely used:** MSE is a commonly used loss function for regression tasks in neural networks, which means there are well-established techniques and frameworks for implementing and optimizing models with MSE as the loss function.\n",
    "\n",
    "The mean square error (MSE) is the sum of the squared differences between the prediction ($\\hat{y}$) and the expected ($y$).  MSE values are not of a particular unit. If an MSE value has decreased for a model, that is good. However, beyond this, there is not much more you can determine. We seek to achieve low MSE values. The following equation demonstrates how to calculate MSE.\n",
    "\n",
    "$$ \\mbox{MSE} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}_i - y_i\\right)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Compute MSE\n",
    "\n",
    "The code in the cell below uses the function `metrics.mean_squared_error()` from the Python library called `scikit-learn` (alias `sklearn`) to compute the MSE for the model `bfModel`. This function takes 2 arguments. \n",
    "\n",
    "The first argument is an array containing what the model **_predicted_** for body `Density` for every subject in the validation dataset `bfX_test`. In the code cell below, this array is called `bfPred`.\n",
    "\n",
    "The second argument is an array contain the  **_actual_** body `Density` for each subject in the validation dataset. These actual values were previously stored in the array called  `bfY_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 5: Compute MSE\n",
    "\n",
    "# Use model to predict values\n",
    "bfPred = bfModel.predict(bfX_test)\n",
    "\n",
    "# Compare predicted and actual values  \n",
    "score = metrics.mean_squared_error(bfPred,bfY_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Final score (MSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "\n",
    "~~~text\n",
    "2/2 [==============================] - 0s 3ms/step\n",
    "Final score (MSE): 0.015650872141122818\n",
    "~~~\n",
    "\n",
    "The final MSE is a relatively small number so that's a good sign. However, when it comes to MSE its hard to know if that means our `bfModel` is highly accurate, or fairly inaccurate. When it comes to MSE, all you really know is that `smaller is better`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 5: Compute MSE**\n",
    "\n",
    "In the cell below use the function `metrics.mean_squared_error()` to compute the MSE for your model `mcModel`. Call your prediction `mcPred`. Print out the results as illustrated in Example 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 5 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see something similar to the following output:\n",
    "~~~text\n",
    "11/11 [==============================] - 0s 2ms/step\n",
    "Final score (MSE): 19570264.0\n",
    "~~~\n",
    "Compared to the MSE computed above in Example 5, your final MSE for `mcModel`, 19570264.0, might seem to be extremely big number. However, looks can be deceiving.  Your model was measuring the cost of medical treatment in the _tens of thousands_ of dollars, while the other model, `bfModel` was measuring body density, which has an average (mean) value of only 0.31558996. That's one of the problems with MSE, its magnitude depends upon the magnitude of the variable that you are trying to predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Square Error\n",
    "\n",
    "Using Root Mean Squared Error (RMSE) for regression neural networks has several advantages:\n",
    "\n",
    "* **Scale interpretation:** RMSE is in the same units as the target variable, providing a more interpretable measure of error compared to MSE. This makes it easier to understand the magnitude of the errors in the predicted values.\n",
    "* **Outlier sensitivity:** RMSE penalizes large errors more heavily than smaller errors due to the square root operation, making the model more sensitive to outliers. This can be beneficial for regression tasks where accurately predicting extreme values is important.\n",
    "* **Averaging effect:** RMSE averages the errors across all samples in the dataset, providing a single metric that represents the overall model performance. This can simplify the evaluation process and make it easier to compare different models.\n",
    "\n",
    "The root mean square (RMSE) is essentially the square root of the MSE. Because of this, the RMSE error is in the same units as the training data outcome. We desire Low RMSE values. The following equation calculates RMSE.\n",
    "\n",
    "$$ \\mbox{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}_i - y_i\\right)^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Compute RMSE\n",
    "\n",
    "The code in the cell below again uses the function `metrics.mean_squared_error()` from the Python library called `scikit-learn` (alias `sklearn`) to compute the MSE for the model `bfModel`. To compute the _Root_ Mean Squared Error, you simple take the square root of the MSE, using the Numpy function `np.sqrt()` as shown in the next line of code:\n",
    "~~~text\n",
    "score = np.sqrt(metrics.mean_squared_error(mcPred,mcY_test))\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Compute RMSE\n",
    "\n",
    "# Compute RMSE\n",
    "score = np.sqrt(metrics.mean_squared_error(bfPred,bfY_test))\n",
    "\n",
    "# Print RSME\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "# Print mean of Y\n",
    "print(f\"Average body `Density`:{bfY_test.mean()}\")\n",
    "\n",
    "# Print comparison\n",
    "print(f\"RMSE as percent of mean `Density`:  {score/bfY_test.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:\n",
    "~~~text\n",
    "Final score (RMSE): 0.1251034438610077\n",
    "Average body `Density`:1.0570443868637085\n",
    "RMSE as percent of mean `Density`:  0.11835212260484695\n",
    "~~~\n",
    "In this particular example, the error in our `bfModel's` predictions is equal to about 11%. In other words, our model either predicted a value that was, on average, about 10% too high, or too low.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 6: Compute RMSE**\n",
    "\n",
    "In the cell below compute and print out the RMSE for your model `mcModel`, the average (mean) value of `charges` of the subjects in the validation set and \n",
    "\n",
    "To compute the _Root_ Mean Squared Error, you simple take the square root of the MSE, using the Numpy function `np.sqrt()` as shown in the next line of code:\n",
    "\n",
    "~~~text\n",
    "score = np.sqrt(metrics.mean_squared_error(mcPred,mcY_test))\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 6 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:\n",
    "~~~text\n",
    "Final score (RMSE): 4423.8291015625\n",
    "Average insurance `charges`:13277.865234375\n",
    "RMSE as percent of mean `charges`:  0.3331732153892517\n",
    "~~~\n",
    "In this example, RMSE for your `mcModel` was \\\\$4,424. On average, the people in the validation group spent \\\\$13,278 on insurance `charges` each year. Therefore, when expressed as a percentage of the mean, the RMSE of your model was about 33% of the average costs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lift Chart\n",
    "\n",
    "We often visualize the results of regression with a **_lift chart_**. Lift charts are graphical tools used to evaluate the performance of predictive models, including neural network models. The lift chart shows how much better the model is at predicting outcomes compared to a random model. They also help in understanding the model's ability to rank or classify instances correctly.\n",
    "\n",
    "To generate a lift chart, perform the following activities:\n",
    "\n",
    "* Sort the data by expected output and plot these values.\n",
    "* For every point on the x-axis, plot that same data point's predicted value in another color.\n",
    "* The x-axis is just 0 to 100% of the dataset. The expected always starts low and ends high.\n",
    "* The y-axis is ranged according to the values predicted.\n",
    "\n",
    "You can interpret the lift chart as follows:\n",
    "\n",
    "* The expected and predict lines should be close. Notice where one is above the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7: Plot Lift Chart\n",
    "\n",
    "The code in the cell below creates a function called `chart_regression()`. The function takes two arguments: (1) the model's **_predictions_** of the response variable for each subject in the dataset and (2) the **_actual_** response variable for each subject in the dataset. In this example, the `bfModel's` predictions are stored in the variable `bfPred` while the corresponding actual values are stored in `bfY`. \n",
    "\n",
    "The function begins by sorting the predicted and actual values by size, from small to large. The sorted predicted values are assigned to the variable `pred`. The actual values in `Y` are 'flattened'. This means all of the values stored in 2-dimensional arrays are converted into a single, contiguous one-dimensional array. \n",
    "\n",
    "The function plots two lines. In this example, the blue line shows all of the 'Density` values in the Body Fat dataset, with the smallest value to the largest value plotted left-to-right. \n",
    "\n",
    "The orange line shows the model's predicted value for each actual value. Sometimes the model's predictions are greater than the actual values (the orange line is above the blue) and sometimes the predicted values are lower than the actual values (the orange line is below the blue). \n",
    "\n",
    "The difference between these two lines are the _'errors'_ that are used in the calculation of the **Root Mean Squared _ERRORS_ (RMSE)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: Plot Lift Chart\n",
    "\n",
    "# Define plot function\n",
    "def chart_regression(pred, y, sort=True):\n",
    "    t = pd.DataFrame({'pred': pred, 'y': y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'], inplace=True)\n",
    "    plt.plot(t['y'].tolist(), label='expected')\n",
    "    plt.plot(t['pred'].tolist(), label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Use function to plot the chart\n",
    "chart_regression(bfPred.flatten(),bfY_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see a Lift Plot similar to the one shown below.\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_04_3_Lift1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 7: Plot Lift Chart**\n",
    "\n",
    "In the cell below plot the Lift Chart for your `mcModel` predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 7 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see a Lift Plot similar to the one shown below.\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_04_3_Lift2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visual inspect your `mcModel's` ability to predict the insurance costs (`charges`) becomes less accurate as the insurance costs start to increase more rapidly starting around an X value of 240. The reason for this sudden increase in variability is not immediately obvious. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lesson Turn-in**\n",
    "\n",
    "When you have completed all of the code cells, and run them in sequential order (the last code cell should be number 16), use the **File --> Print.. --> Save to PDF** to generate a PDF of your JupyterLab notebook. Save your PDF as `Class_04_3.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tf-sum24)",
   "language": "python",
   "name": "tf-sum24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
