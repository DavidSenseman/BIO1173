{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Introduction to Large Language Models (LLMs)\n",
        "* Part 4.2: Chatbots\n",
        "* **Part 4.3: Image Generation with StableDiffusion**\n",
        "* Part 4.4: Agentic AI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded.\n"
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Accelerated Run-time Check**\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be running a Graphics Processing Unit (GPU). You don't need a \"high-end\" GPU like the `A100` but you can select something less powerful like the `L4 GPU` to save your Colab credits."
      ],
      "metadata": {
        "id": "0kumRAuI0_sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell second\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check for GPU\n",
        "def check_colab_gpu():\n",
        "    print(\"=== Colab GPU Check ===\")\n",
        "\n",
        "    # Check PyTorch\n",
        "    pt_gpu = torch.cuda.is_available()\n",
        "    print(f\"PyTorch GPU available: {pt_gpu}\")\n",
        "\n",
        "    if pt_gpu:\n",
        "        print(f\"PyTorch device count: {torch.cuda.device_count()}\")\n",
        "        print(f\"PyTorch current device: {torch.cuda.current_device()}\")\n",
        "        print(f\"PyTorch device name: {torch.cuda.get_device_name()}\")\n",
        "        print(\"You are good to go!\")\n",
        "\n",
        "    else:\n",
        "        print(\"No compatible device found\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now and start over!\")\n",
        "\n",
        "check_colab_gpu()"
      ],
      "metadata": {
        "id": "Dgzi5SDT1Ips"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you current `Runtime` is correct you should see something _similar_ to the following output\n",
        "\n",
        "```text\n",
        "=== Colab GPU Check ===\n",
        "PyTorch GPU available: True\n",
        "PyTorch device count: 1\n",
        "PyTorch current device: 0\n",
        "PyTorch device name: NVIDIA L4\n",
        "You are good to go!\n",
        "```\n",
        "However, if you received a warning message, you must go back and change your `Runtime` now before you continue."
      ],
      "metadata": {
        "id": "UBJv5vrm1dpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to Stable Diffusion**\n",
        "\n",
        "Run the next cell to see short introduction to Stable Diffusion. This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "LBzX5ggwOBFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id =  \"QdRP9pO89MY\"\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\"> </iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "n356UUBvCp9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stable Diffusion**\n",
        "\n",
        "**Stable Diffusion** is a deep learning model for **generative image synthesis**. It belongs to the class of **latent diffusion models (LDMs)**, which generate high-quality images from text prompts by operating in a compressed latent space rather than pixel space. This approach significantly reduces computational cost while maintaining image fidelity.\n",
        "\n",
        "Stable Diffusion is trained on large-scale image-text datasets and uses a combination of:\n",
        "\n",
        "- **Variational Autoencoders (VAEs)**: To encode images into a latent space.\n",
        "- **U-Net architecture**: For denoising latent representations.\n",
        "- **Text encoders (e.g., CLIP or BERT)**: To condition image generation on natural language prompts.\n",
        "\n",
        "The model works by iteratively denoising a random latent vector, guided by a text prompt, until a coherent image emerges.\n",
        "\n",
        "### **Key Features**\n",
        "\n",
        "- **Text-to-image synthesis**: Generate images from descriptive text.\n",
        "- **Image-to-image translation**: Modify existing images using prompts.\n",
        "- **Inpainting and outpainting**: Fill in missing regions or expand images.\n",
        "- **Custom fine-tuning**: Adapt the model to domain-specific data.\n",
        "\n",
        "### **Applications in Computational Biology**\n",
        "\n",
        "Stable Diffusion can be a powerful tool for computational biologists in several ways:\n",
        "\n",
        "#### **1. Scientific Visualization**\n",
        "Generate illustrative figures for:\n",
        "- Molecular structures\n",
        "- Cellular processes\n",
        "- Pathways and interactions\n",
        "- Anatomical diagrams\n",
        "\n",
        "This can enhance presentations, publications, and educational materials.\n",
        "\n",
        "#### **2. Data Augmentation**\n",
        "Use synthetic biological images to:\n",
        "- Augment training datasets for machine learning models\n",
        "- Improve robustness in image classification tasks (e.g., histopathology, microscopy)\n",
        "\n",
        "#### **3. Hypothesis Communication**\n",
        "Translate complex biological hypotheses into visual representations for:\n",
        "- Grant proposals\n",
        "- Interdisciplinary collaboration\n",
        "- Public outreach\n",
        "\n",
        "#### **4. Custom Model Training**\n",
        "Fine-tune Stable Diffusion on domain-specific datasets (e.g., microscopy images, protein structures) to:\n",
        "- Generate realistic biological imagery\n",
        "- Explore latent space representations of biological phenomena\n",
        "\n",
        "#### **5. Interactive Exploration**\n",
        "Use prompt-based generation to explore:\n",
        "- Morphological variations\n",
        "- Evolutionary traits\n",
        "- Synthetic biology designs"
      ],
      "metadata": {
        "id": "992nctfKybK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text to Images with StableDiffusion**\n",
        "\n",
        "We will now see how to use Stable Diffusion to create various images from textual prompts. There will be four settings that we will deal with as we generate these images.\n",
        "\n",
        "* **model**: We will use the trained/finetuned model. Different models are optimized for different types of images.\n",
        "* **prompt**: Text that you provide to describe what sort of image you would like created.\n",
        "* **negative prompt**: Text that you describe elements that should not be present in your image.\n",
        "* **seed**: The same image for the prompt/negative prompt will always be produced for the same seed. To get a different image for the same prompts, change the seed.\n"
      ],
      "metadata": {
        "id": "ZJrIgZkszA7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importance of \"Setting the Seed\"**\n",
        "\n",
        "In image generation models like **Stable Diffusion**, a **random seed** determines the initial noise used to create an image. This noise is gradually transformed into a coherent image based on the prompt you provide.\n",
        "\n",
        "A **random seed** is a number used to initialize a random number generator. In Stable Diffusion:\n",
        "\n",
        "- The model begins with a field of random noise.\n",
        "- It uses your prompt to guide the transformation of this noise into an image.\n",
        "- The seed controls the exact pattern of that starting noise.\n",
        "\n",
        "Even with the **same prompt**, changing the seed changes the initial noise pattern, which leads to a **different final image**.\n",
        "\n",
        "**Analogy**:  \n",
        "Imagine sculpting clay using the same instructions. If each lump of clay starts with a different shape (seed), the final sculptures will be similar but **not identical**.\n",
        "\n",
        "#### **Use Cases for Seeds**\n",
        "\n",
        "- **Reproducibility**: Using the same seed and prompt will always generate the same image.\n",
        "- **Exploration**: Trying different seeds lets you explore variations of the same concept.\n",
        "- **Fine-tuning**: You can find a seed that gives you the best result and reuse it.\n",
        "\n"
      ],
      "metadata": {
        "id": "vRGHWr99vU92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Basic Pipeline\n",
        "\n",
        "To make use of Stable Diffusion we will use the `HuggingFace DiffusionPipeline`. When setting up the pipeline we specify to use the `CompVis/stable-diffusion-v1-4` model, which is a basic model created to be used with StableDiffusion.\n",
        "\n",
        "The following code sets up this model and downloads it from `HuggingFace`."
      ],
      "metadata": {
        "id": "9ftIaZHNBzgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Basic Pipeline\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "MODEL_ID = \"CompVis/stable-diffusion-v1-4\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=DTYPE,\n",
        ")\n",
        "pipe = pipe.to(DEVICE)\n",
        "print(f\"Pipeline loaded successfully on {DEVICE}.\")\n"
      ],
      "metadata": {
        "id": "Mda9sZK2D2u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image01F.png)"
      ],
      "metadata": {
        "id": "ZMeBh_B9d8Su"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Generate Basic Image\n",
        "\n",
        "We will begin by using Stable Diffusion (`stable-diffusion-v1-4`) to create a simple picture of a Monarch butterfly. As you will see, the image we generate will depend to a great extent on the value to random seed. For Example 1 we will set the seed = `100`.\n",
        "\n",
        "In **Stable Diffusion**, the terms `prompt` and `negative prompt` refer to two different types of input that guide the image generation process:\n",
        "\n",
        "**Prompt**\n",
        "\n",
        "* This is the **main description** of what you want the model to generate.\n",
        "* It includes **keywords, phrases, or detailed descriptions** of the desired scene, style, objects, characters, lighting, mood, etc.\n",
        "\n",
        "Here is the `prompt` for Example 1:\n",
        "```text\n",
        "# Set prompt\n",
        "prompt= \"\"\"\n",
        "a Monarch butterfly\"\"\"\n",
        "```\n",
        "**Negative Prompt**\n",
        "\n",
        "* This is used to specify **what you _don’t_ want** in the image.\n",
        "* It helps the model avoid unwanted elements, styles, or artifacts.\n",
        "\n",
        "Here is the negative prompt for Example 1:\n",
        "```type\n",
        "# Set negative prompt\n",
        "neg_prompt = \"\"\"\n",
        "signature, watermark, incomplete image\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "**NSFW**\n",
        "\n",
        "As you might imagine it is quite possible to generate images that are considered \"not safe for work\" (NSFW). The code in the cell below contains the following line of code that will protect you from inadvertently generating a pornographic and/or an extremely violent image:\n",
        "\n",
        "```type\n",
        "# Uncomment the next line to remove the safety checker\n",
        "# pipe.safety_checker = lambda images, clip_input: (images, False)\n",
        "```\n",
        "If such an image is generated, you will see the following message.\n",
        "```text\n",
        "Potential NSFW content was detected in one or more images. A black image will be returned instead.\n",
        "Try again with a different prompt and/or seed.\n",
        "```\n",
        "\n",
        "You may wish to disable this feature. To do this, uncomment the pipe.`safety_checker` line. Be cafeful, if you do disable this, as unsafe images may be generated containing NSFW themes, which might contain violence, nudity, or sexual themes.\n",
        "`"
      ],
      "metadata": {
        "id": "Gn4kx3Hy6Aw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Generate basic image\n",
        "\n",
        "import random\n",
        "\n",
        "# Set the seed\n",
        "seed = 100\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "print(f\"The seed =\", seed)\n",
        "\n",
        "# Use seed to create the generator\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Set prompt\n",
        "prompt= \"\"\"\n",
        "a Monarch butterfly\"\"\"\n",
        "\n",
        "# Set negative prompt\n",
        "neg_prompt = \"\"\"\n",
        "signature, watermark, incomplete image\n",
        "\"\"\"\n",
        "\n",
        "# Uncomment the next line to remove the safety checker\n",
        "# pipe.safety_checker = lambda images, clip_input: (images, False)\n",
        "\n",
        "# Generate image\n",
        "pipe(prompt, negative_prompt=neg_prompt,\n",
        "              width=512,height=512,generator=generator).images[0]"
      ],
      "metadata": {
        "id": "2hC9hktBh0v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image09B.png)"
      ],
      "metadata": {
        "id": "-Ns1QM4VlGPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1A: Generate Basic Image**\n",
        "\n",
        "In the cell below write the code to generate the same image of a Monarch butterfly generated in Example 1, but set the `seed` to the number `1604`."
      ],
      "metadata": {
        "id": "262XLos2lQ31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1A here\n"
      ],
      "metadata": {
        "id": "dNG9lUFTlQ32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image10B.png)"
      ],
      "metadata": {
        "id": "1ymmHtrulQ32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1B: Generate Basic Image**\n",
        "\n",
        "In the cell below write the code to generate the same image of a Monarch butterfly but this time set the seed to a **random number**. This is easy to do by just setting the seed value of `-1`."
      ],
      "metadata": {
        "id": "9Yymf7ZHmVBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1B here\n",
        "\n"
      ],
      "metadata": {
        "id": "NcVM1dv4mVBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image11B.png)\n",
        "\n",
        "but it's unlikely you will see the same image since you were to use a random seed."
      ],
      "metadata": {
        "id": "fJk4etOtmVBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1C: Generate Basic Image**\n",
        "\n",
        "In the cell below write the code to generate an image of your choice. It can be any image that you like. Also, the value you pick for your `seed` is also up to you. Show your imagination!"
      ],
      "metadata": {
        "id": "YgE6TObznEjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1C here\n",
        "\n"
      ],
      "metadata": {
        "id": "QkWX-uwunEjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output will depend upon your `prompt` and your seed value."
      ],
      "metadata": {
        "id": "N1GPgwFMnEjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Generate Reference Image\n",
        "\n",
        "The diffusion model that we have been using so far is adequate for similar images, but is less suitable for high resolution images. This is especially true when it comes to generating images with human faces.\n",
        "\n",
        "The code in the next cell generates the face of a young Japanese woman. We will use this image as an example of a basic image that can be generated with the `stable-diffusion-v1-4`  model.\n"
      ],
      "metadata": {
        "id": "KLnGWWoponB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Importance of model selection\n",
        "\n",
        "import random\n",
        "\n",
        "# Set the seed / Use -1 for random seed\n",
        "seed = 100\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "print(f\"The seed =\", seed)\n",
        "\n",
        "# Use seed to create generator\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Prompts to generate image\n",
        "prompt= \"\"\"\n",
        "the face of a young Japanese woman\"\"\"\n",
        "\n",
        "neg_prompt = \"\"\"\n",
        "signature, watermark, incomplete image\n",
        "\"\"\"\n",
        "\n",
        "# Uncomment the next line to remove the safety checker\n",
        "#pipe.safety_checker = lambda images, clip_input: (images, False)\n",
        "\n",
        "# Generate image\n",
        "pipe(prompt, negative_prompt=neg_prompt, width=512,height=512,\n",
        "              max_embeddings_multiples=3,generator=generator).images[0]"
      ],
      "metadata": {
        "id": "bcxQmV9aonB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image12B.png)"
      ],
      "metadata": {
        "id": "rqSeJDTdonB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Generate Reference Image**\n",
        "\n",
        "In the next cell write the code that will generate a human face. You are free to specify the kind of face you want, male or female.  \n"
      ],
      "metadata": {
        "id": "kEyqrmc2q2xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eb3a0y65q2xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output will depend upon your `prompt`."
      ],
      "metadata": {
        "id": "3hYFYjr2q2xR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stable Diffusion Model Comparison: v1.4 vs v2.1**\n",
        "\n",
        "In the previous section we used the `stable-diffusion-v1-4` model to generate basic images. In this section we will use the more advanced version of this model, stable-diffusion-2-1 which can generate more realistics images.\n",
        "\n",
        "Here are the main differences between these two model generations.\n",
        "\n",
        "\n",
        "**1. Architecture and Text Encoder**\n",
        "\n",
        "| Feature | v1.4 (CompVis) | v2.1 (StabilityAI) |\n",
        "|--------|----------------|--------------------|\n",
        "| Text Encoder | CLIP ViT-L/14 | OpenCLIP ViT-H/14 |\n",
        "| Architecture | Stable Diffusion 1.x | Updated architecture |\n",
        "| Native Resolution | 512x512 | 768x768 |\n",
        "\n",
        "\n",
        "**2. Training Data**\n",
        "- **v1.4**:\n",
        "  - Trained on **LAION-Aesthetics v2 5+**.\n",
        "  - Fine-tuned for 225k steps.\n",
        "  - Focused on aesthetic images.\n",
        "\n",
        "- **v2.1**:\n",
        "  - Trained on a **filtered subset of LAION-5B**.\n",
        "  - Improved safety and quality filtering.\n",
        "  - Better prompt alignment and reduced bias\n",
        "\n",
        "\n",
        "**3. Output Style and Quality**\n",
        "\n",
        "- **v1.4**:\n",
        "  - Good for general-purpose image generation.\n",
        "  - May struggle with complex compositions or high-resolution needs.\n",
        "\n",
        "- **v2.1**:\n",
        "  - Better at handling **complex prompts**, **realism**, and **fine details**.\n",
        "  - Improved **anatomical accuracy**, lighting, and texture rendering.\n",
        "\n",
        "**4. Use Cases**\n",
        "\n",
        "| Model | Best For | Limitations |\n",
        "|-------|----------|-------------|\n",
        "| **v1.4** | Quick, general image generation | Lower resolution, less prompt nuance |\n",
        "| **v2.1** | High-quality, detailed images; better realism | May interpret prompts differently due to OpenCLIP |\n"
      ],
      "metadata": {
        "id": "nR05ENEwtfvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Login into Huggingface\n",
        "\n",
        "Run the next cell to log into your Hugginface account using your HF_API stored in your Colab Secrets. Information about obtaining a Huggingface API token has already been provided to you."
      ],
      "metadata": {
        "id": "0puJLQWOJX_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log into Huggingface\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "try:\n",
        "    login(token=userdata.get('HF_Token'))\n",
        "    print(\"Logged in to Hugging Face successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to log in to Hugging Face: {e}\")\n",
        "    print(\"Please check that your HF_Token is correctly stored in Colab Secrets.\")\n"
      ],
      "metadata": {
        "id": "g9507RtYOvLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your Huggingface toekn is correctly installed in your Colab's Secrets, you should see the following output:\n",
        "\n",
        "```text\n",
        "Logged in to Hugging Face successfully.\n",
        "```\n",
        "\n",
        "See your course Instructor or a TA if you can't get your Huggingface token to work."
      ],
      "metadata": {
        "id": "G7zPMsm0OJQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Advanced Pipleline\n",
        "\n",
        "The code in the cell below sets up the more advanced pipeline `stabilityai/stable-diffusion-2-1`.\n"
      ],
      "metadata": {
        "id": "-ery1YvsDEbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Advanced Pipeline\n",
        "\n",
        "!pip install -q compel\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    'runwayml/stable-diffusion-v1-5',\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "print(\"Pipeline loaded successfully on cuda.\")\n"
      ],
      "metadata": {
        "id": "oEsCCGfJRVtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image25F.png)"
      ],
      "metadata": {
        "id": "IY58w88xyMSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Realistic Image\n",
        "\n",
        "We now generate an image with a much more complex prompt. The positive and negative prompts describe how to generate an image of a young woman. Stable diffusion prompts are usually comma separated lists of attributes to draw.\n",
        "\n",
        "You will notice that some aspects of the prompt (e.g. `(FACE1:0.5)`) are enclosed in paranthesis; which designates that this attribute is more important. A number, near the end, separated by a colon specifies how important."
      ],
      "metadata": {
        "id": "DoXoxynov8UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Realistic Image\n",
        "\n",
        "from compel import Compel\n",
        "import random\n",
        "\n",
        "# Set the seed\n",
        "seed = 8\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "print(f\"The seed =\", seed)\n",
        "\n",
        "# Create generator\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Create compel processor\n",
        "compel = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder)\n",
        "\n",
        "# Prompt\n",
        "prompt = \"\"\"(woman age 26 standing by tree), (long blonde hair:1.2), ray traced shadows,\n",
        "RAW, 8k, (eczema:0.7), (sub-surface scattering:1.55), (sweat:1.22), (freckles:0.55),\n",
        "highly detailed skin, (Acne:0.7), (FACE1:0.5), (FACE2:1.2), (FACE3:0.85),\n",
        "perfect eyes, no makeup, (skin spores:1.05), (skin spores:1.05),\n",
        "ultra detailed face, ultra detailed skin, film grain, ray tracing, studio lighting\"\"\"\n",
        "\n",
        "# Negative prompt\n",
        "neg_prompt = \"\"\"signature, watermark, airbrush, photoshop, plastic doll,\n",
        "(ugly eyes, deformed iris, deformed pupils, fused lips and teeth:1.2),\n",
        "(un-detailed skin, semi-realistic, cgi, 3d, render, sketch, cartoon,\n",
        "drawing, anime:1.2), text, close up, cropped, out of frame, worst quality,\n",
        "low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers,\n",
        "mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry,\n",
        "dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured,\n",
        "gross proportions, malformed limbs, missing arms, missing legs, extra arms,\n",
        "extra legs, fused fingers, too many fingers, long neck, head wear, masculine,\n",
        "obese, fat, out of frame\"\"\"\n",
        "\n",
        "# Generate embeddings with compel\n",
        "prompt_embeds = compel(prompt)\n",
        "neg_embeds = compel(neg_prompt)\n",
        "\n",
        "# Generate image using embeddings\n",
        "image = pipe(\n",
        "    prompt_embeds=prompt_embeds,\n",
        "    negative_prompt_embeds=neg_embeds,\n",
        "    width=512,\n",
        "    height=512,\n",
        "    generator=generator\n",
        ").images[0]\n",
        "\n",
        "image\n"
      ],
      "metadata": {
        "id": "IQzTJfIHSnhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image14F.png)"
      ],
      "metadata": {
        "id": "JKyvso3MwwXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Realistic Image**\n",
        "\n",
        "Generate an image of old Japanese woman in front of Mount Fuji. Set the seed to `6`.\n",
        "\n",
        "Define your prompt as:\n",
        "```text\n",
        "# Prompt\n",
        "prompt= \"\"\"\n",
        "(Japanese woman age 90+, standing before Mount Fuji), (gray thinning hair:1.2),\n",
        "ray traced shadows, RAW, 8k, (deep wrinkles:1.5), (sub-surface scattering:1.55),\n",
        "(age spots:1.2), highly detailed aged skin, (sagging skin:1.3), (FACE1:0.5),\n",
        "(FACE2:1.2), (FACE3:0.85), cloudy eyes, no makeup, (skin texture:1.05),\n",
        "ultra detailed face, ultra detailed skin, film grain, ray tracing, studio lighting\n",
        "\"\"\"\n",
        "```\n",
        "and your negative prompt as:\n",
        "```type\n",
        "# Negative prompt\n",
        "neg_prompt = \"\"\"\n",
        "signature, watermark, airbrush, photoshop, plastic doll,\n",
        "(ugly eyes, deformed iris, deformed pupils, fused lips and teeth:1.2),\n",
        "(un-detailed skin, semi-realistic, cgi, 3d, render, sketch, cartoon,\n",
        "drawing, anime:1.2), text, close up, cropped, out of frame, worst quality,\n",
        "low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers,\n",
        "mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry,\n",
        "dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured,\n",
        "gross proportions, malformed limbs, missing arms, missing legs, extra arms,\n",
        "extra legs, fused fingers, too many fingers, long neck, head wear, masculine,\n",
        "obese, fat, out of frame\"\"\"\n",
        "```\n",
        "\n",
        "The rest of the code is the same as Example 3."
      ],
      "metadata": {
        "id": "Ij-eAwnTyxAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WQ-OgZOpyxAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image15F.png)"
      ],
      "metadata": {
        "id": "naLpWkDkyxAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Anime**\n",
        "\n",
        "**Anime** is a form of animation that originated in **Japan** and has grown into a globally recognized medium. It includes a wide variety of genres, themes, and artistic styles, and is known for its unique storytelling and visual aesthetics.\n",
        "\n",
        "#### **Key Characteristics**\n",
        "- **Origin**: Japan\n",
        "- **Mediums**: TV series, films, web series, OVAs (Original Video Animations)\n",
        "- **Genres**: Action, Romance, Fantasy, Horror, Sci-Fi, Slice of Life, and more\n",
        "- **Audience**: All age groups, from children to adults\n",
        "\n",
        "#### **Notable Examples**\n",
        "- *Spirited Away* (Studio Ghibli)\n",
        "- *Naruto*\n",
        "- *Attack on Titan*\n",
        "- *One Piece*\n",
        "- *Demon Slayer*\n",
        "\n",
        "### **What Is Anime Style?**\n",
        "\n",
        "**Anime style** refers to the distinctive visual and artistic elements commonly found in anime. It is characterized by stylized character designs, expressive features, and dynamic visuals.\n",
        "\n",
        "#### **Visual Features**\n",
        "\n",
        "| Feature              | Description                                                                 |\n",
        "|----------------------|-----------------------------------------------------------------------------|\n",
        "| **Eyes**             | Large, expressive, often detailed to convey emotion                         |\n",
        "| **Facial Features**  | Small noses and mouths, exaggerated expressions                             |\n",
        "| **Hair**             | Stylized, often colorful, with unique shapes and movement                   |\n",
        "| **Body Proportions** | Can range from realistic to highly exaggerated depending on the genre       |\n",
        "| **Line Work**        | Clean and sharp outlines                                                     |\n",
        "| **Coloring**         | Flat or cel-shaded, with emphasis on contrast and mood                      |\n",
        "| **Backgrounds**      | Often highly detailed, especially in fantasy or sci-fi settings             |\n",
        "\n",
        "#### **Storytelling Elements**\n",
        "- **Emotional depth** and character development\n",
        "- **Symbolism** and metaphorical themes\n",
        "- **Cultural references** to Japanese traditions, language, and society\n",
        "- **Genre blending**, such as mixing romance with supernatural or comedy with horror\n",
        "\n",
        "#### **Anime vs. Western Animation**\n",
        "\n",
        "| Aspect              | Anime                                   | Western Animation                      |\n",
        "|---------------------|------------------------------------------|----------------------------------------|\n",
        "| **Art Style**       | Stylized, detailed, expressive           | Varies widely, often more cartoonish   |\n",
        "| **Themes**          | Often complex and mature                 | Often geared toward children/family    |\n",
        "| **Production**      | Typically serialized with long arcs      | Often episodic                         |\n",
        "| **Cultural Influence** | Strong Japanese cultural elements     | Western cultural norms and humor       |\n"
      ],
      "metadata": {
        "id": "kzaxKXEp61Vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Anime Models**\n",
        "\n",
        "If you would like to generate cartoon or Anime style images, the `waifu-diffusion model` will work nicely. The `Waifu Diffusion model` is a specialized AI image generation model designed to create anime-style artwork, particularly focusing on characters that resemble the \"waifu\" archetype—typically stylized, idealized female characters popular in anime and manga culture.\n",
        "\n",
        "#### **Key Features:**\n",
        "* **Anime-focused training data:** Trained on datasets like Danbooru (a large anime image repository).\n",
        "* **Text-to-image generation:** You input a prompt like \"a cute anime girl with blue hair in a school uniform\" and it generates an image.\n",
        "* **Customizable outputs:** You can adjust style, pose, background, and more using prompt engineering.\n",
        "* **Checkpoint versions:** Includes models like Waifu Diffusion 1.3, 1.4, and 1.5, each improving quality and prompt responsiveness.\n",
        "\n",
        "The code below loads the pipeline for the `Waifu Diffusion model`."
      ],
      "metadata": {
        "id": "ZrnDmq8AE1P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Waifu Diffusion model pipeline\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from compel import Compel\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# Load pipeline\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"hakurei/waifu-diffusion\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# Create compel processor\n",
        "compel = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder)\n",
        "\n",
        "print(\"Waifu Diffusion pipeline loaded successfully on cuda.\")\n"
      ],
      "metadata": {
        "id": "0UIxSaiJYJow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image16F.png)"
      ],
      "metadata": {
        "id": "TmJAfJUtzJVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Create Anime Person\n",
        "\n",
        "The code in the cell below creates an Anime image of a young girl."
      ],
      "metadata": {
        "id": "Wf--JbSf9gfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Create anime image\n",
        "\n",
        "import random\n",
        "\n",
        "# Random number seed, -1 for random seed\n",
        "seed = 105\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "print(f\"The seed =\", seed)\n",
        "\n",
        "# Create generator\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Prompt\n",
        "prompt = (\n",
        "    \"best_quality (1girl:1.3) bride brown_hair closed_mouth frills (full_body:1.3) \"\n",
        "    \"fox_ear hair_bow happy hood kimono long_sleeves red_bow smile solo tabi \"\n",
        "    \"white_kimono wide_sleeves cherry_blossoms\"\n",
        ")\n",
        "\n",
        "# Negative prompt\n",
        "neg_prompt = (\n",
        "    \"lowres, bad_anatomy, error_body, error_hands, bad_hands, error_fingers, \"\n",
        "    \"missing_fingers, error_legs, bad_legs, error_lighting, error_shadow, \"\n",
        "    \"extra_digit, cropped, worst_quality, jpeg_artifacts, watermark, blurry\"\n",
        ")\n",
        "\n",
        "# Generate embeddings\n",
        "prompt_embeds = compel(prompt)\n",
        "neg_embeds = compel(neg_prompt)\n",
        "\n",
        "# Generate the image\n",
        "image = pipe(\n",
        "    prompt_embeds=prompt_embeds,\n",
        "    negative_prompt_embeds=neg_embeds,\n",
        "    width=512,\n",
        "    height=512,\n",
        "    generator=generator\n",
        ").images[0]\n",
        "\n",
        "image\n"
      ],
      "metadata": {
        "id": "R7YHlHZ5Y8zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image16B.png)"
      ],
      "metadata": {
        "id": "sgKIwF11-rsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Create Anime Person**\n",
        "\n",
        "In the cell below write the code to generate an Anime image of a young boy. Make sure to set the seed = `3`.\n",
        "\n",
        "You can reuse the code in Example 4 but change the prompt to read:\n",
        "\n",
        "~~~text\n",
        "# Prompt\n",
        "prompt = (\n",
        "    \"best_quality, (1boy:1.3), heroic_pose, spiky_hair, intense_eyes, anime_uniform, \"\n",
        "    \"dramatic_lighting, cinematic_background, serious_expression, solo, full_body, \"\n",
        "    \"anime_style, masterpiece, high_detail, vibrant_colors, wind_effects, glowing_aura, \"\n",
        "    \"dynamic_composition, epic_scale, battle_ready, stormy_sky, energy_particles, depth_of_field\"\n",
        ")\n",
        "\n",
        "~~~"
      ],
      "metadata": {
        "id": "xIO0QRRTdbN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n",
        "import random\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5vuRG-_cwcDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image17F.png)\n"
      ],
      "metadata": {
        "id": "aKJRxoqHAyHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Create Anime Scene\n",
        "\n",
        "The Anime world is full of magical as well as real creatures. One popular Anime TV show that is populated with a variety of creatures is called `Shirokuma Café`.\n",
        "\n",
        "**Shirokuma Café** (also known as Polar Bear Café) is a Japanese anime and manga series that blends slice-of-life comedy with a whimsical, anthropomorphic twist.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image24B.png)\n",
        "\n",
        "The code in the cell below generates an image that is in `the style of` `Shirokuma Café`. This particular prompt was generated by uploading this image to Microsoft 365 Copilot and asking it to create a prompt to duplicate the image."
      ],
      "metadata": {
        "id": "nLwM03Ldg5W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Create Anime Scene\n",
        "\n",
        "import random\n",
        "\n",
        "# Set the seed\n",
        "seed = 8\n",
        "seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "print(f\"The seed =\", seed)\n",
        "\n",
        "# Create generator\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "# Prompt generated by Microsoft 365 Copilot from Shirokuma Café image\n",
        "prompt = (\n",
        "    \"best_quality, anime_style, high_detail, vibrant_colors, cute_animal_characters, \"\n",
        "    \"panda_sitting_with_pink_swim_ring_and_yellow_shorts, waving_paw, penguin_standing_next_to_panda, \"\n",
        "    \"lush_green_foliage_background, stone_wall, summer_theme, friendly_expression, \"\n",
        "    \"casual_swimwear, relaxed_atmosphere, slice_of_life_scene, soft_shading\"\n",
        ")\n",
        "\n",
        "# Negative prompt\n",
        "neg_prompt = (\n",
        "    \"lowres, bad_anatomy, blurry, distorted_faces, error_limbs, bad_hands, missing_fingers, \"\n",
        "    \"extra_limbs, worst_quality, jpeg_artifacts, watermark, cropped, poor_lighting, \"\n",
        "    \"oversaturated_colors, unnatural_pose, messy_background, out_of_focus, bad_proportions\"\n",
        ")\n",
        "\n",
        "# Generate embeddings\n",
        "prompt_embeds = compel(prompt)\n",
        "neg_embeds = compel(neg_prompt)\n",
        "\n",
        "# Generate the image\n",
        "image = pipe(\n",
        "    prompt_embeds=prompt_embeds,\n",
        "    negative_prompt_embeds=neg_embeds,\n",
        "    width=512,\n",
        "    height=512,\n",
        "    generator=generator\n",
        ").images[0]\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "nr3AkpWVaXYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image18F.png)\n",
        "\n",
        "Here is the prompt that was used to generate this image:\n",
        "\n",
        "~~~type\n",
        "prompt = (\n",
        "    \"best_quality, anime_style, high_detail, vibrant_colors, cute_animal_characters, \"\n",
        "    \"panda_sitting_with_pink_swim_ring_and_yellow_shorts, waving_paw, penguin_standing_next_to_panda, \"\n",
        "    \"lush_green_foliage_background, stone_wall, summer_theme, friendly_expression, \"\n",
        "    \"casual_swimwear, relaxed_atmosphere, slice_of_life_scene, soft_shading\"\n",
        ")\n",
        "~~~\n",
        "\n",
        "As you can see the image generated was rather different that one might have expected from the prompt. This kind of _mismatch_ is very common with `text-to-image` generators, even when rather detailed prompts are used."
      ],
      "metadata": {
        "id": "iqNswVQPCNz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Create Anime Scene**\n",
        "\n",
        "In the cell below use exactly the same code that was shown in Example 5.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image24B.png)\n",
        "\n",
        " See if by changing the value of the seed, if you can generate an image that is more similar to the Shirokuma Café image that the prompt was based on."
      ],
      "metadata": {
        "id": "4W66FXxyi_4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n"
      ],
      "metadata": {
        "id": "TMzP4M8mi_4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What your output looks like will depend upon the value of your seed."
      ],
      "metadata": {
        "id": "ruyTjujwi_4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Image-to-Image Pipelines in Stable Diffusion**\n",
        "\n",
        "#### **Background**\n",
        "\n",
        "Stable Diffusion is a latent text-to-image diffusion model developed by **Stability AI**, first released in **August 2022**. It quickly became popular due to its open-source nature, high-quality outputs, and ability to run on consumer GPUs.\n",
        "\n",
        "While the original model focused on **text-to-image generation**, the community and developers soon expanded its capabilities to include **image-to-image** transformations, enabling users to guide generation using existing images.\n",
        "\n",
        "#### **What Is Image-to-Image in Stable Diffusion?**\n",
        "\n",
        "Image-to-image (img2img) generation allows users to provide:\n",
        "- A **source image** (e.g., a sketch, photo, or concept art)\n",
        "- A **text prompt** describing the desired transformation\n",
        "- A **strength parameter** controlling how much the output deviates from the input\n",
        "\n",
        "This technique uses the same latent diffusion process but starts from a **noised version of the input image**, allowing for creative reinterpretation while preserving structure.\n",
        "\n",
        "### **Evolution of Image-to-Image Pipelines**\n",
        "\n",
        "##### Stable Diffusion v1.5 (2022)\n",
        "- First widely used version for img2img tasks.\n",
        "- Introduced via the `StableDiffusionImg2ImgPipeline` in the `diffusers` library.\n",
        "- Supported basic transformations with prompt guidance and strength control.\n",
        "\n",
        "##### Stable Diffusion v2.x (Late 2022–2023)\n",
        "- Improved resolution and semantic understanding.\n",
        "- Introduced **depth-to-image** and **inpainting** pipelines.\n",
        "- Better handling of complex prompts and image conditioning.\n",
        "\n",
        "##### ControlNet Integration (2023)\n",
        "- Added fine-grained control using **edge maps**, **pose estimation**, **depth maps**, etc.\n",
        "- Enabled highly structured transformations while preserving artistic freedom.\n",
        "\n",
        "##### SDXL (2023–2024)\n",
        "- Major upgrade with richer visual fidelity and prompt comprehension.\n",
        "- Image-to-image support extended to **SDXLImg2ImgPipeline**.\n",
        "- Better performance on high-resolution inputs and nuanced prompts.\n",
        "\n",
        "### **Parameters in Image-to-Image Pipelines**\n",
        "\n",
        "| Parameter         | Description                                                                 |\n",
        "|------------------|-----------------------------------------------------------------------------|\n",
        "| `prompt`          | Text description guiding the transformation                                 |\n",
        "| `image`           | Input image to be transformed                                               |\n",
        "| `strength`        | Controls deviation from input (0.0 = faithful, 1.0 = creative)              |\n",
        "| `guidance_scale`  | Controls adherence to prompt (higher = more prompt-driven)                  |\n",
        "| `num_inference_steps` | Number of denoising steps (more = better quality, slower)               |\n",
        "| `generator`       | Random seed generator for reproducibility                                   |\n",
        "\n",
        "#### **Use Cases**\n",
        "\n",
        "- **Artistic reinterpretation** of student sketches\n",
        "- **Scientific visualization** from concept diagrams\n",
        "- **Creative storytelling** using visual prompts\n",
        "- **Biological image transformation** for simulations or hypothetical scenarios\n"
      ],
      "metadata": {
        "id": "o8kwkJocKUk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Image-2-Image Pipeline\n",
        "\n",
        "The code in the cell below sets up the Image-to-Image pipe line."
      ],
      "metadata": {
        "id": "V1rYZoSs2v0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup image-2-image pipeline\n",
        "\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "from compel import Compel\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# Load the pipeline\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Create compel processor\n",
        "compel = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder)\n",
        "\n",
        "print(\"Image-to-image pipeline loaded successfully on cuda.\")\n"
      ],
      "metadata": {
        "id": "ttLunXG_bmoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image19F.png)"
      ],
      "metadata": {
        "id": "8voAgXjzCwg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Understanding the `strength` Parameter in Image-to-Image Generation**\n",
        "\n",
        "##### **What Is `strength`?**\n",
        "\n",
        "In Stable Diffusion's image-to-image pipeline, the `strength` parameter controls **how much the generated image deviates from the input image**. It determines the level of noise added to the input image before the diffusion process begins.\n",
        "\n",
        "##### **How It Works**\n",
        "\n",
        "- The image-to-image pipeline works by **encoding the input image into latent space**, adding noise, and then **denoising it guided by the text prompt**.\n",
        "- The `strength` parameter sets the **starting point** in the denoising process:\n",
        "  - **Low strength** → less noise → output is **closer to the input image**\n",
        "  - **High strength** → more noise → output is **more influenced by the prompt**\n",
        "\n",
        "\n",
        "#### **Typical Values**\n",
        "\n",
        "| Strength | Behavior                          | Use Case Example                          |\n",
        "|----------|-----------------------------------|-------------------------------------------|\n",
        "| 0.1–0.3  | Very close to input image         | Subtle edits, style transfer              |\n",
        "| 0.4–0.6  | Balanced between input and prompt | Concept transformation                    |\n",
        "| 0.7–0.9  | Highly creative, prompt-driven    | Abstract reinterpretation, surreal edits  |\n",
        "\n",
        "> ⚠️ Values above `0.9` may ignore the input image almost entirely."
      ],
      "metadata": {
        "id": "xEDV1MdGPPr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Image-to-Image Strength Parameter\n",
        "\n",
        "The code in the cell below uses this rather famous image of an astronaut riding a horse on the moon as the initial image.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/AstronautMoon.jpg)\n",
        "\n",
        "\n",
        "The **initial image** (also called the **input image**) is the starting point for the image-to-image generation process in Stable Diffusion. It provides the **visual structure** or **content** that the model will transform based on a given **text prompt**.\n",
        "\n",
        "#### **How It Works**\n",
        "\n",
        "1. The initial image is **encoded into latent space** using a Variational Autoencoder (VAE).\n",
        "2. A certain amount of **noise is added**, controlled by the `strength` parameter.\n",
        "3. The model then **denoises** the image while being guided by the **text prompt**, producing a new image that blends the original content with the prompt's intent.\n",
        "\n",
        "### **Prompt**\n",
        "\n",
        "Here is the text prompt for Example 6:\n",
        "\n",
        "```text\n",
        "# Prompt\n",
        "prompt = \"a evil clown riding a tiger on the moon\"\n",
        "```\n",
        "\n",
        "### **Strength Parameter**\n",
        "\n",
        "In this example we are going to look at how the `strength parameter` affects image creation.\n",
        "\n",
        "The code in the cell below has the `strength parameter` = `0.01`. This is an extremely low value so we would expect the output image to be relatively unchanged."
      ],
      "metadata": {
        "id": "JHKj9FsTJ0S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Image-to-Image Strength Parameter\n",
        "\n",
        "import torch\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# Set strength parameter\n",
        "strength = 0.01  # Creativity level\n",
        "\n",
        "# Set the seed\n",
        "seed = 1\n",
        "print(f\"The seed = {seed}\")\n",
        "\n",
        "# Prompt\n",
        "prompt = \"a evil clown riding a tiger on the moon\"\n",
        "\n",
        "# Negative prompt\n",
        "neg_prompt = \"blurry, low quality, distorted\"\n",
        "\n",
        "# Set additional parameters\n",
        "guidance_scale = 7.5\n",
        "num_inference_steps = 50\n",
        "\n",
        "# Validate strength\n",
        "if not (0.01 <= strength <= 0.9):\n",
        "    raise ValueError(\"Strength should be between 0.1 and 0.9 for best results.\")\n",
        "\n",
        "# Create generator\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "\n",
        "# Load and preprocess initial image\n",
        "url = \"https://biologicslab.co/BIO1173/images/class_04/AstronautMoon.jpg\"\n",
        "init_image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
        "\n",
        "# Resize and crop to 512x512\n",
        "init_image = init_image.resize((512, 512))\n",
        "\n",
        "# Generate embeddings\n",
        "prompt_embeds = compel(prompt)\n",
        "neg_embeds = compel(neg_prompt)\n",
        "\n",
        "# Generate image\n",
        "try:\n",
        "    result = pipe(\n",
        "        prompt_embeds=prompt_embeds,\n",
        "        negative_prompt_embeds=neg_embeds,\n",
        "        image=init_image,\n",
        "        strength=strength,\n",
        "        guidance_scale=guidance_scale,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        generator=generator\n",
        "    ).images[0]\n",
        "\n",
        "    # Display result\n",
        "    display(result)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error during image generation:\", e)\n"
      ],
      "metadata": {
        "id": "Myvx5RRFcEz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image29B.png)\n",
        "\n",
        "As expected the output image is relatively unchanged."
      ],
      "metadata": {
        "id": "TkwEY2HQQ7f5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6A: Image-to-Image Strength Parameter**\n",
        "\n",
        "In the cell below copy-and-paste the code from Example 6. Keep the `seed` = `1` but set the `strength parameter` = `0.5`. In other words, we want to know what happens when increase the `creativity` to a \"medium level\"?\n"
      ],
      "metadata": {
        "id": "HHM5KmIlRNJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6A here\n",
        "\n"
      ],
      "metadata": {
        "id": "C80zMNWASaG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image31B.png)\n",
        "\n",
        "The horse in the output image seems to be morphing into a more `tiger-like` creature."
      ],
      "metadata": {
        "id": "p8Q1IeeyWB5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6B: Image-to-Image Strength Parameter**\n",
        "\n",
        "In the cell below again copy-and-paste Example 6, keep the `seed` = `1` but now set the `strength parameter` = `0.75`.\n",
        "\n",
        "Let's see how a \"high level\" of creativity will effect the output image?"
      ],
      "metadata": {
        "id": "vacrZm9aWUnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6B here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yKiHH1bBWUng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image20F.png)\n",
        "\n",
        "The output image is now relatively bizzare. The horse has been changed into a tiger albeit a mishapen tiger. The astronaut is also quite different.  "
      ],
      "metadata": {
        "id": "hsEXLGVMWUng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Biomedical Use Cases?**\n",
        "\n",
        "In theory, we could image a wide range of biomedical use cases for the text-to-image Stable Diffusion pipeline demonstrated above. Here are several potential prompts that could, in theory, generate useful images for biomedical education:\n",
        "\n",
        "```python\n",
        "# Biomedical prompt\n",
        "prompt = (\n",
        "    \"highly detailed scientific illustration of a human heart, anatomically accurate, \"\n",
        "    \"cross-section view showing chambers and valves, medical textbook style, \"\n",
        "    \"labeled diagram aesthetic, clean white background, professional medical illustration, \"\n",
        "    \"arteries and veins visible, realistic tissue texture, educational anatomy poster, \"\n",
        "    \"high resolution, sharp details, clinical precision\"\n",
        ")\n",
        "\n",
        "# Cell biology prompt\n",
        "prompt = (\n",
        "    \"detailed scientific illustration of a eukaryotic cell, cross-section view, \"\n",
        "    \"nucleus with chromatin visible, mitochondria, endoplasmic reticulum, golgi apparatus, \"\n",
        "    \"ribosomes, cell membrane, medical textbook style, educational diagram, \"\n",
        "    \"labeled structure aesthetic, high detail, professional biology illustration\"\n",
        ")\n",
        "\n",
        "# Neuroscience prompt\n",
        "prompt = (\n",
        "    \"scientific illustration of a neuron, detailed axon and dendrites, \"\n",
        "    \"myelin sheath visible, synaptic terminals, action potential visualization, \"\n",
        "    \"medical textbook quality, anatomically accurate, clean background, \"\n",
        "    \"neuroscience educational poster, high resolution microscopy style\"\n",
        ")\n",
        "\n",
        "# Molecular biology prompt\n",
        "prompt = (\n",
        "    \"3D rendering of DNA double helix structure, accurate base pairs, \"\n",
        "    \"phosphate backbone visible, scientific visualization, molecular biology, \"\n",
        "    \"high detail protein structure, medical research aesthetic, \"\n",
        "    \"clean dark background, glowing nucleotides, educational science poster\"\n",
        ")\n",
        "\n",
        "# Pathology prompt\n",
        "prompt = (\n",
        "    \"medical illustration of healthy lung tissue versus diseased tissue comparison, \"\n",
        "    \"histology slide aesthetic, microscopic view, alveoli structure visible, \"\n",
        "    \"clinical pathology style, educational medical diagram, high detail, \"\n",
        "    \"professional medical textbook illustration, accurate cellular structure\"\n",
        ")\n",
        "```\n",
        "\n",
        "The ability to rapidly generate high-quality medical illustrations has obvious appeal. In Example 7, we will see if this potential is met?"
      ],
      "metadata": {
        "id": "oUf34g0sdmgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set-up Text-to-Image Pipeline\n",
        "\n",
        "Run the code in the next cell to setup an text-to-image pipeline to generate biomedical images."
      ],
      "metadata": {
        "id": "qbmOAkiBEB9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup text-to-image pipeline for biomedical images\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from compel import Compel\n",
        "import torch\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# Create compel processor\n",
        "compel = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder)\n",
        "\n",
        "print(\"Text-to-image pipeline loaded successfully on cuda.\")\n"
      ],
      "metadata": {
        "id": "03sUnGL3eOuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image26F.png)\n"
      ],
      "metadata": {
        "id": "q7w8MybmHBT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7: Biomedical Image Generation\n",
        "\n",
        "The code in the cell uses the Stable Diffusion text-to-image pipeline `runwayml/stable-diffusion-v1-5` to create a medical illustration showing the difference between health and diseased lung tissue. The prompt used in this example is same `Pathology prompt` shown above.\n"
      ],
      "metadata": {
        "id": "GBWYgDB6rbat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: Biomedical Image Generation\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from IPython.display import display\n",
        "\n",
        "# Set the seed\n",
        "seed = 749991061\n",
        "seed =random.randint(0, 2**32) if seed == -1 else seed\n",
        "print(f\"The seed = {seed}\")\n",
        "\n",
        "# Create generator\n",
        "generator = torch.Generator(device='cuda').manual_seed(int(seed))\n",
        "\n",
        "\n",
        "# Pathology prompt - Motor neuron with action potential initiation\n",
        "prompt = (\n",
        "    \"medical illustration of healthy lung tissue versus diseased tissue comparison, \"\n",
        "    \"histology slide aesthetic, microscopic view, alveoli structure visible, \"\n",
        "    \"clinical pathology style, educational medical diagram, high detail, \"\n",
        "    \"professional medical textbook illustration, accurate cellular structure\"\n",
        ")\n",
        "\n",
        "# Negative prompt\n",
        "neg_prompt = (\n",
        "    \"low quality, blurry, distorted anatomy, incorrect proportions, \"\n",
        "    \"abstract, cartoon, anime, watermark, signature, \"\n",
        "    \"low resolution, jpeg artifacts, fantasy style, \"\n",
        "    \"poorly drawn, amateur, sketch, doodle, multiple neurons, network\"\n",
        ")\n",
        "\n",
        "# Generate embeddings\n",
        "prompt_embeds = compel(prompt)\n",
        "neg_embeds = compel(neg_prompt)\n",
        "\n",
        "# Generate the image\n",
        "image = pipe(\n",
        "    prompt_embeds=prompt_embeds,\n",
        "    negative_prompt_embeds=neg_embeds,\n",
        "    width=512,\n",
        "    height=512,\n",
        "    num_inference_steps=50,\n",
        "    guidance_scale=7.5,\n",
        "    generator=generator\n",
        ").images[0]\n",
        "\n",
        "display(image)\n"
      ],
      "metadata": {
        "id": "isD7TVnNreQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image24F.png)\n",
        "\n",
        "Remember, we wanted Stable Diffusion to generate an image showing healthy vs diseased lung tissue. What Stable Diffusion generated is, quite frankky, totally bizzare. Technically, it is a \"visual hallucination\".\n",
        "\n",
        "While you can definitely get _really_ different output images from the code above by just changing the value of the `seed`--and you are encouraged to try this--I will bet you a dollar that you will _never_ generate anything that is remotely useful. Believe me, I have tried.\n",
        "\n",
        "## **Nano Banana Pro for Biomedical Illustrations**\n",
        "\n",
        "**Nano Banana Pro** was only released by Google around November 20-21, 2025. It is an advanced AI image generation model built on `Gemini 3 Pro Image`. You may recall that we have already used Nano Banana Pro in an earlier lesson.\n",
        "\n",
        "If you give Nano Banana Pro _exactly_ the same prompt used in Example 7, here is the image that Nano Banana Pro generates:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image23F.png)\n",
        "\n",
        "By any objective measure, this is a much more useful image for biomedical education than the image generated by Stable Diffusion.\n",
        "\n",
        "This raises the following question: \"Why is Nano Banana Pro so much better so much better than Stable Diffusion in medical image generation, especially when it comes to labelling the image?\"\n",
        "\n",
        "## **Model Comparison: Nano Banana Pro vs. Stable Diffusion v1.5**\n",
        "\n",
        "**Nano Banana Pro**, built on the Gemini 3 Pro Image architecture, uses a **multimodal reasoning architecture**. This treats text as semantic symbols to be written. **Stable Diffusion v1.5** utilizes a **Latent Diffusion** architecture that treats text as visual texture to be hallucinated.\n",
        "\n",
        "#### **1. Architectural Differences**\n",
        "\n",
        "| Feature | **Nano Banana Pro** (Gemini 3 Pro Image) | **Stable Diffusion v1.5** (RunwayML) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Core Mechanism** | **Autoregressive / Reasoning Engine** | **Latent Diffusion (LDM)** |\n",
        "| **Generation Flow** | **Sequential & Iterative:** Uses a \"Plan → Evaluate → Improve\" loop. | **Global Denoising:** Refines random noise into an image simultaneously across the entire canvas. |\n",
        "| **Text Perception** | **Symbolic:** Understands \"A-P-P-L-E\" as a specific sequence of character tokens. | **Visual:** Sees \"text\" as a shape pattern (like the texture of brick or leaves). |\n",
        "| **Resolution** | **Native High-Res (up to 4K):** Renders text directly at target resolution. | **Compressed Latent Space:** Generates at 512x512 (compressed 8x), often crushing fine text details. |\n",
        "\n",
        "\n",
        "#### **2. Why Nano Banana Pro Generates Correct Text**\n",
        "\n",
        "Nano Banana Pro does not simply \"dream\" text based on visual training data. It uses a **token-based validation** system:\n",
        "\n",
        "*   **Character-by-Character Planning:** It plans the layout of text strings *before* rendering the pixels, ensuring letters appear in the correct order (e.g., \"SALE\" instead of \"SLAE\").\n",
        "*   **No Latent Compression Loss:** It avoids the heavy autoencoder compression (VAE) used by Stable Diffusion, which typically blurs the high-frequency edges required for sharp fonts.\n",
        "*   **Multimodal Grounding:** It aligns the visual output with internal dictionary knowledge, preventing the \"alien hieroglyphics\" common in older diffusion models.\n",
        "\n",
        "#### **3. Why Stable Diffusion v1.5 Fails at Text**\n",
        "\n",
        "Stable Diffusion v1.5 struggles with text because it lacks a concept of \"language\":\n",
        "\n",
        "*   **The \"Texture\" Problem:** To SD v1.5, a letter is just a shape. It attempts to mimic the *look* of writing without understanding the *spelling*, often resulting in letter-like shapes that form nonsense words.\n",
        "*   **Spatial Incoherence:** Because it denoises the entire image at once, it struggles to maintain the strict horizontal coherence required for sentences (e.g., letters often merge or float apart).\n"
      ],
      "metadata": {
        "id": "LYyuaOfe28xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-In**\n",
        "\n",
        "When you have completed and run all of the code cells, use the `File --> Print.. --> Microsoft Print to PDF` to generate your PDF if you are running `MS Windows`. If you have a Mac, use the `File --> Print.. --> Save as PDF`\n",
        "\n",
        "In either case, save your PDF as Copy of Class_04_4.lastname.pdf where lastname is your last name, and upload the file to Canvas.\n",
        "\n",
        "**NOTE TO WINDOWS USERS:** Your grade will be reduced by 10% if your PDF is found to be missing pages when it is being graded in Canvas. This penalty is simply meant to prevent the grader from having to take the additional steps of (1) downloading your PDF, (2) printing it out using the `Microsoft Print to PDF` and (3) having to resubmit to Canvas so they can grade it"
      ],
      "metadata": {
        "id": "iG6D_KMDNdIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lizard Tail**\n",
        "\n",
        "### **BACKPROGATION**\n",
        "\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png)\n",
        "\n",
        "\n",
        "In machine learning, **backpropagation** is a gradient estimation method commonly used for training a neural network to compute its parameter updates.\n",
        "\n",
        "It is an efficient application of the chain rule to neural networks. Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input–output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming.\n",
        "\n",
        "Strictly speaking, the term backpropagation refers only to an algorithm for efficiently computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm – including how the gradient is used, such as by stochastic gradient descent, or as an intermediate step in a more complicated optimizer, such as Adaptive Moment Estimation.[5] The local minimum convergence, exploding gradient, vanishing gradient, and weak control of learning rate are main disadvantages of these optimization algorithms. The Hessian and quasi-Hessian optimizers solve only local minimum convergence problem, and the backpropagation works longer. These problems caused researchers to develop hybrid and fractional optimization algorithms.\n",
        "\n",
        "Backpropagation had multiple discoveries and partial discoveries, with a tangled history and terminology. See the history section for details. Some other names for the technique include \"reverse mode of automatic differentiation\" or \"reverse accumulation\".\n",
        "\n",
        "## **Intuition**\n",
        "\n",
        "**Motivation**\n",
        "\n",
        "The goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output. The motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output.\n",
        "\n",
        "**Learning as an optimization problem**\n",
        "\n",
        "To understand the mathematical derivation of the backpropagation algorithm, it helps to first develop some intuition about the relationship between the actual output of a neuron and the correct output for a particular training example. Consider a simple neural network with two input units, one output unit and no hidden units, and in which each neuron uses a linear output (unlike most work on neural networks, in which mapping from inputs to outputs is non-linear) that is the weighted sum of its input.\n",
        "\n",
        "**History**\n",
        "\n",
        "Backpropagation had been derived repeatedly, as it is essentially an efficient application of the chain rule (first written down by Gottfried Wilhelm Leibniz in 1676) to neural networks.\n",
        "\n",
        "The terminology \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this. In any case, he only studied neurons whose outputs were discrete levels, which only had zero derivatives, making backpropagation impossible.\n",
        "\n",
        "Precursors to backpropagation appeared in optimal control theory since 1950s. Yann LeCun et al credits 1950s work by Pontryagin and others in optimal control theory, especially the adjoint state method, for being a continuous-time version of backpropagation. Hecht-Nielsen credits the Robbins–Monro algorithm (1951)[23] and Arthur Bryson and Yu-Chi Ho's Applied Optimal Control (1969) as presages of backpropagation. Other precursors were Henry J. Kelley 1960, and Arthur E. Bryson (1961). In 1962, Stuart Dreyfus published a simpler derivation based only on the chain rule. In 1973, he adapted parameters of controllers in proportion to error gradients. Unlike modern backpropagation, these precursors used standard Jacobian matrix calculations from one stage to the previous one, neither addressing direct links across several stages nor potential additional efficiency gains due to network sparsity.\n",
        "\n",
        "The ADALINE (1960) learning algorithm was gradient descent with a squared error loss for a single layer. The first multilayer perceptron (MLP) with more than one layer trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.[29] The MLP had 5 layers, with 2 learnable layers, and it learned to classify patterns not linearly separable.\n",
        "\n",
        "**Modern backpropagation**\n",
        "\n",
        "Modern backpropagation was first published by Seppo Linnainmaa as \"reverse mode of automatic differentiation\" (1970) for discrete connected networks of nested differentiable functions.\n",
        "\n",
        "In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. Werbos described how he developed backpropagation in an interview. In 1971, during his PhD work, he developed backpropagation to mathematicize Freud's \"flow of psychic energy\". He faced repeated difficulty in publishing the work, only managing in 1981. He also claimed that \"the first practical application of back-propagation was for estimating a dynamic model to predict nationalism and social communications in 1974\" by him.\n",
        "\n",
        "Around 1982, David E. Rumelhart independently developed backpropagation and taught the algorithm to others in his research circle. He did not cite previous work as he was unaware of them. He published the algorithm first in a 1985 paper, then in a 1986 Nature paper an experimental analysis of the technique These papers became highly cited, contributed to the popularization of backpropagation, and coincided with the resurging research interest in neural networks during the 1980s.\n",
        "\n",
        "In 1985, the method was also described by David Parker. Yann LeCun proposed an alternative form of backpropagation for neural networks in his PhD thesis in 1987.\n",
        "\n",
        "Gradient descent took a considerable amount of time to reach acceptance. Some early objections were: there were no guarantees that gradient descent could reach a global minimum, only local minimum; neurons were \"known\" by physiologists as making discrete signals (0/1), not continuous ones, and with discrete signals, there is no gradient to take. See the interview with Geoffrey Hinton,[36] who was awarded the 2024 Nobel Prize in Physics for his contributions to the field.\n",
        "\n",
        "**Early successes**\n",
        "\n",
        "Contributing to the acceptance were several applications in training neural networks via backpropagation, sometimes achieving popularity outside the research circles.\n",
        "\n",
        "In 1987, NETtalk learned to convert English text into pronunciation. Sejnowski tried training it with both backpropagation and Boltzmann machine, but found the backpropagation significantly faster, so he used it for the final NETtalk. The NETtalk program became a popular success, appearing on the Today show.\n",
        "\n",
        "In 1989, Dean A. Pomerleau published ALVINN, a neural network trained to drive autonomously using backpropagation.\n",
        "\n",
        "The LeNet was published in 1989 to recognize handwritten zip codes.\n",
        "\n",
        "In 1992, TD-Gammon achieved top human level play in backgammon. It was a reinforcement learning agent with a neural network with two layers, trained by backpropagation.\n",
        "\n",
        "In 1993, Eric Wan won an international pattern recognition contest through backpropagation.\n",
        "\n",
        "**After backpropagation**\n",
        "\n",
        "During the 2000s it fell out of favour, but returned in the 2010s, benefiting from cheap, powerful GPU-based computing systems. This has been especially so in speech recognition, machine vision, natural language processing, and language structure learning research (in which it has been used to explain a variety of phenomena related to first and second language learning.\n",
        "\n",
        "Error backpropagation has been suggested to explain human brain event-related potential (ERP) components like the N400 and P600.\n",
        "\n",
        "In 2023, a backpropagation algorithm was implemented on a photonic processor by a team at Stanford University.\n",
        "\n",
        "## **Backpropagation in Deep Neural Networks**\n",
        "\n",
        "Backpropagation is a fundamental algorithm used to train deep neural networks. It efficiently computes the **gradient of the loss function** with respect to each weight in the network, enabling the use of **gradient descent** to update the weights and minimize the loss.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/BackProp.jpg)\n",
        "\n",
        "### Overview of the Process\n",
        "\n",
        "1. **Forward Pass**: Input data is passed through the network to compute the output (prediction).\n",
        "2. **Loss Calculation**: The output is compared to the true label using a loss function (e.g., MSE, cross-entropy).\n",
        "3. **Backward Pass (Backpropagation)**:\n",
        "   - Gradients of the loss with respect to each parameter are computed using the **chain rule**.\n",
        "   - These gradients are used to update the weights via an optimization algorithm (e.g., SGD, Adam).\n",
        "\n",
        "## Mathematical Foundations\n",
        "\n",
        "Let’s consider a simple feedforward neural network with:\n",
        "- Input layer: \\( x \\)\n",
        "- Hidden layer: \\( h = f(Wx + b) \\)\n",
        "- Output layer: \\( \\hat{y} = g(Vh + c) \\)\n",
        "- Loss function: \\( L(\\hat{y}, y) \\)\n",
        "\n",
        "### Step 1: Compute Gradients\n",
        "\n",
        "Using the chain rule:\n",
        "\n",
        "- Gradient w.r.t. output weights \\( V \\):\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial V} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial V}\n",
        "  $$\n",
        "\n",
        "- Gradient w.r.t. hidden weights \\( W \\):\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial h} \\cdot \\frac{\\partial h}{\\partial W}\n",
        "  $$\n",
        "\n",
        "### Step 2: Update Weights\n",
        "\n",
        "Using gradient descent:\n",
        "$$\n",
        "V := V - \\eta \\cdot \\frac{\\partial L}{\\partial V}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W := W - \\eta \\cdot \\frac{\\partial L}{\\partial W}\n",
        "$$\n",
        "\n",
        "Where $\\eta$ is the learning rate.\n",
        "\n",
        "\n",
        "## Backpropagation Algorithm (Pseudocode)\n",
        "\n",
        "```python\n",
        "# Assume a simple 2-layer neural network\n",
        "for epoch in range(num_epochs):\n",
        "    for x, y in data:\n",
        "        # Forward pass\n",
        "        h = f(W @ x + b)\n",
        "        y_hat = g(V @ h + c)\n",
        "        loss = compute_loss(y_hat, y)\n",
        "\n",
        "        # Backward pass\n",
        "        dL_dyhat = compute_loss_gradient(y_hat, y)\n",
        "        dL_dV = dL_dyhat @ h.T\n",
        "        dL_dh = V.T @ dL_dyhat\n",
        "        dL_dW = (dL_dh * f_prime(W @ x + b)) @ x.T\n",
        "\n",
        "        # Update weights\n",
        "        V -= learning_rate * dL_dV\n",
        "        W -= learning_rate * dL_dW\n",
        "```\n",
        "\n",
        "#### **Vectorized Backpropagation**\n",
        "\n",
        "Below is a minimal, fully vectorized implementation for a 2‑layer perceptron (one hidden layer). It demonstrates the key steps without relying on a deep learning library.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_deriv(x):\n",
        "    return 1.0 - np.tanh(x)**2\n",
        "\n",
        "def forward(x, W1, b1, W2, b2):\n",
        "    z1 = W1 @ x + b1\n",
        "    a1 = tanh(z1)\n",
        "    z2 = W2 @ a1 + b2\n",
        "    a2 = z2          # linear output (e.g., regression)\n",
        "    cache = (x, z1, a1, z2, a2)\n",
        "    return a2, cache\n",
        "\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return 0.5 * np.sum((y_pred - y_true)**2)\n",
        "\n",
        "def backward(y_true, cache, W2):\n",
        "    x, z1, a1, z2, a2 = cache\n",
        "    \n",
        "    # Output layer error (MSE + linear)\n",
        "    delta2 = (a2 - y_true)          # shape (output_dim,)\n",
        "    \n",
        "    # Gradients for W2, b2\n",
        "    grad_W2 = np.outer(delta2, a1)   # shape (output_dim, hidden_dim)\n",
        "    grad_b2 = delta2                 # shape (output_dim,)\n",
        "    \n",
        "    # Hidden layer error\n",
        "    delta1 = (W2.T @ delta2) * tanh_deriv(z1)   # shape (hidden_dim,)\n",
        "    \n",
        "    # Gradients for W1, b1\n",
        "    grad_W1 = np.outer(delta1, x)              # shape (hidden_dim, input_dim)\n",
        "    grad_b1 = delta1\n",
        "    \n",
        "    grads = {'W1': grad_W1, 'b1': grad_b1,\n",
        "             'W2': grad_W2, 'b2': grad_b2}\n",
        "    return grads\n",
        "\n",
        "#### **Training Loop (SGD)**\n",
        "\n",
        "```Python\n",
        "# Dummy data\n",
        "X = np.random.randn(5, 3)   # 5 samples, 3 features\n",
        "Y = np.random.randn(5, 1)   # 5 targets\n",
        "\n",
        "# Initialize parameters\n",
        "W1 = np.random.randn(4, 3) * 0.01\n",
        "b1 = np.zeros(4)\n",
        "W2 = np.random.randn(1, 4) * 0.01\n",
        "b2 = np.zeros(1)\n",
        "\n",
        "eta = 0.01   # learning rate\n",
        "\n",
        "for epoch in range(1000):\n",
        "    for x, y in zip(X.T, Y.T):   # iterate over samples\n",
        "        y_hat, cache = forward(x, W1, b1, W2, b2)\n",
        "        grads = backward(y, cache, W2)\n",
        "        \n",
        "        # SGD update\n",
        "        W1 -= eta * grads['W1']\n",
        "        b1 -= eta * grads['b1']\n",
        "        W2 -= eta * grads['W2']\n",
        "        b2 -= eta * grads['b2']\n",
        "    \n",
        "    if epoch % 100 == 0:\n",
        "        y_pred, _ = forward(X.T, W1, b1, W2, b2)\n",
        "        loss = compute_loss(Y.T, y_pred)\n",
        "        print(f'Epoch {epoch}: loss={loss:.4f}')\n",
        "\n",
        "```\n",
        "\n",
        "#### **Variants of the Basic Algorithm**\n",
        "\n",
        "\n",
        "| Variant                | Key Idea                                 | Where It Helps                                 |\n",
        "|------------------------|------------------------------------------|------------------------------------------------|\n",
        "| Mini‑Batch SGD         | Update weights after a small batch (32–256 samples) | Faster convergence, better generalization     |\n",
        "| Momentum               | Keeps running average of gradients       | Accelerates along relevant directions           |\n",
        "| Adam                   | Adaptive learning rates per parameter    | Works well out‑of‑the‑box for many tasks         |\n",
        "| Batch Normalization    | Normalizes activations                   | Reduces covariate shift, improves training speed |\n",
        "| Weight Sharing         | Same weights reused across time/space    | CNNs, RNNs, Transformer attention              |\n",
        "\n"
      ],
      "metadata": {
        "id": "6x8gMtgQxMMH"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}