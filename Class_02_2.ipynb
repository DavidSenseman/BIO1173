{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_02_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uui7bLnihYnj"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlJeYqWdhYnk"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x8Dye6ehYnk"
      },
      "source": [
        "**Module 2: Machine Learning**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 2 Material\n",
        "\n",
        "* Part 2.1: Pandas DataFrame Operations\n",
        "* **Part 2.2: Categorical Values**\n",
        "* Part 2.3: Grouping, Sorting and Shuffling on Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmkZN0lrhYnk"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to ```/content/drive``` and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your GMAIL address is visible in the output above."
      ],
      "metadata": {
        "id": "DJsoSxlSpK7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Functions\n",
        "\n",
        "The code in the next cell creates one (or more) functions that are used later in this class lesson. If you don't run the next cell, you will get an error."
      ],
      "metadata": {
        "id": "8fb5FI0W_TLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create functions for this lesson\n",
        "\n",
        "def list_float_columns(dataframe):\n",
        "    \"\"\"\n",
        "    Create a list of all columns in a DataFrame that contain float values.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (pd.DataFrame): The DataFrame to check.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of column names that contain float values.\n",
        "    \"\"\"\n",
        "    float_columns = [col for col in dataframe.columns if dataframe[col].dtype == 'float64']\n",
        "    return float_columns"
      ],
      "metadata": {
        "id": "aXVJgnIS_VwV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTP35EsPhYnl"
      },
      "source": [
        "# **Categorical and Continuous Values**\n",
        "\n",
        "Neural networks require their input to be a fixed number of columns. This input format is very similar to spreadsheet data; it must be entirely numeric. It is essential to represent the data so that the neural network can train from it. Before we look at specific ways to preprocess data, it is important to consider four basic types of data, as defined by [[Cite:stevens1946theory]](http://psychology.okstate.edu/faculty/jgrice/psyc3214/Stevens_FourScales_1946.pdf). Statisticians commonly refer to as the [levels of measure](https://en.wikipedia.org/wiki/Level_of_measurement):\n",
        "\n",
        "* Character Data (strings)\n",
        "    * **Nominal** - Individual discrete items, no order. For example, color, zip code, and shape.\n",
        "    * **Ordinal** - Individual distinct items have an implied order. For example, grade level, job title, Starbucks(tm) coffee size (tall, vente, grande)\n",
        "* Numeric Data\n",
        "    * **Interval** - Numeric values, no defined start.  For example, temperature. You would never say, \"yesterday was twice as hot as today.\"\n",
        "    * **Ratio** - Numeric values, clearly defined start.  For example, speed. You could say, \"The first car is going twice as fast as the second.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3k1f6lvhYnl"
      },
      "source": [
        "## **Datasets for Class_02_2**\n",
        "\n",
        "In this class we will be using the **_Obesity Prediction_** dataset for the Examples and the **_Heart Failure_** dataset for the **Exercises**. Both of these datasets will be downloaded from the course HTTPS server [https://biologicslab.co](https://biologicslab.co)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "gLiz8yN2hYnl"
      },
      "source": [
        "## **Obesity Prediction Dataset**\n",
        "\n",
        "[Obesity Prediction Dataset](https://www.kaggle.com/datasets/mrsimple07/obesity-prediction)\n",
        "\n",
        "**Description**\n",
        "\n",
        "The dataset provides comprehensive information on individuals' demographic characteristics, physical attributes, and lifestyle habits, aiming to facilitate the analysis and prediction of obesity prevalence. It includes key variables such as age, gender, height, weight, body mass index (BMI), physical activity level, and obesity category.\n",
        "\n",
        "* **Age:** The age of the individual, expressed in years.\n",
        "* **Gender:** The gender of the individual, categorized as male or female.\n",
        "* **Height:** The height of the individual, typically measured in centimeters or inches.\n",
        "* **Weight:** The weight of the individual, typically measured in kilograms or pounds.\n",
        "* **BMI:** A calculated metric derived from the individual's weight and height\n",
        "* **PhysicalActivityLevel:** This variable quantifies the individual's level of physical activity\n",
        "* **ObesityCategory:** Categorization of individuals based on their BMI into different obesity categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WojSa_BhYnl"
      },
      "source": [
        "### Example 1: Read data file and create Pandas DataFrame\n",
        "\n",
        "The cell below use the Pandas `read_csv()` method to read the `obesity_prediction.csv` file using the code chunk below:\n",
        "~~~text\n",
        "opDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/obesity_prediction.csv\",\n",
        "    na_values=['NA','?'])\n",
        "~~~\n",
        "The function `read_csv()` is an important Pandas method to read CSV files. In the cell below, the `read_csv()` method takes 2 arguments.\n",
        "\n",
        "The first argument, `\"https://biologicslab.co/BIO1173/data/obesity_prediction.csv\"` is a string that provides the filepath and filename. The second argument, `na_values=['NA','?']` is used to recognize `?` as NaN (Not a Number).\n",
        "\n",
        "As the file is read, Pandas creates a DataFrame called `opDF` to hold the information.\n",
        "\n",
        "After reading the datafile into a DataFrame, it is always a good idea to use the function `display()` to print out a specified number of rows and columns to make sure the data was read correctly.\n",
        "\n",
        "The code in the cell below, sets the maximum number of rows to 6 and the maximum number of columns to 6.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZeNqs_NhYnl"
      },
      "outputs": [],
      "source": [
        "# Example 1: Read data file and create Pandas DataFrame\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the datafile\n",
        "opDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/obesity_prediction.csv\",\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "# Set max rows and max columns\n",
        "pd.set_option('display.max_rows', 6)\n",
        "pd.set_option('display.max_columns', 6)\n",
        "\n",
        "# Display DataFrame\n",
        "display(opDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7FgW6oFhYnm"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_02_2_Exm1.png)\n",
        "\n",
        "You can see from looking at the last line of the output, the DataFrame `opDF` has 7 columns and 1000 rows. This means `opDF` has clinical measurements for 1000 patients and for each patient, there are 7 separate clinical measurements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "5PzwziGXhYnm"
      },
      "source": [
        "## **Heart Disease Dataset**\n",
        "\n",
        "[Heart Disease Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)\n",
        "\n",
        "**Description**\n",
        "\n",
        "Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n",
        "\n",
        "People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n",
        "\n",
        "* **Age:** age of the patient [years]\n",
        "* **Sex:** sex of the patient [M: Male, F: Female]\n",
        "* **ChestPainType:** chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n",
        "* **RestingBP:** resting blood pressure [mm Hg]\n",
        "* **Cholesterol:** serum cholesterol [mm/dl]\n",
        "* **FastingBS:** fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n",
        "* **RestingECG:** resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
        "* **MaxHR:** maximum heart rate achieved [Numeric value between 60 and 202]\n",
        "* **ExerciseAngina:** exercise-induced angina [Y: Yes, N: No]\n",
        "* **Oldpeak:** oldpeak = ST [Numeric value measured in depression]\n",
        "* **ST_Slope:** the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n",
        "* **HeartDisease:** output class [1: heart disease, 0: Normal]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5BOGzschYnm"
      },
      "source": [
        "### **Exercise 1: Read data file into a Pandas DataFrame**\n",
        "\n",
        "In the cell below use the Pandas `read_csv()` method to read the `heart_disease.csv` file that is located on the course HTTPS server using this code chunk:\n",
        "~~~text\n",
        "# Read the datafile\n",
        "hdDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/heart_disease.csv\",\n",
        "    na_values=['NA','?'])\n",
        "~~~\n",
        "As the file is read, have Pandas create a DataFrame called `hdDF`. to hold the heart disease data.\n",
        "\n",
        "Use the `display()` function to print out 6 rows and 6 columns of `hdDF`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpq-EkdZhYnm"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUBTJ7smhYnm"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_02_2_Exe1.png)\n",
        "\n",
        "You can see by looking at the last line of the output, your DataFrame, `hdDF`, has 12 columns and 918 rows. In other words `hdDF` has clinical measurements for 918 patients and 12 separate clinical measurements for each patient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "3iMAEGS7hYnm"
      },
      "source": [
        "## **Data Normalization**\n",
        "\n",
        "Neural network datasets need to be _normalized_ for the following reasons:\n",
        "\n",
        "* **Improving Convergence:** Normalization helps ensure that the input values to a neural network are within a similar range. This prevents certain features from dominating others and avoids issues such as slow convergence or the network getting stuck in local optima. By normalizing the data, we can achieve a more balanced training process and faster convergence.\n",
        "* **Avoiding Gradient Instability:** During the training of a neural network, backpropagation is used to adjust the weights based on the gradient of the loss function. If the input features have significantly different scales, the gradient updates may become unstable. Normalizing the data mitigates this problem by keeping the input values at a similar magnitude, leading to more stable and reliable gradient updates.\n",
        "* **Efficient Computation:** Normalizing the data to a common range between 0 and 1 or -1 and 1 can improve the efficiency of computations within the neural network. Many activation functions and optimization algorithms are designed to work well with inputs in this range. By normalizing the data, we can leverage these computational efficiencies and speed up the training process.\n",
        "* **Generalization:** Normalization helps the neural network generalize better to unseen data. If the input features have different scales or distributions in the training and test datasets, the network may struggle to generalize its learned patterns effectively. By normalizing the data, we ensure that the network receives consistent input representations, improving its ability to handle new, unseen samples.\n",
        "* **Better Weight Initialization:** Normalizing the data can facilitate better weight initialization in a neural network. Weight initialization methods like Xavier or He initialization assume that the input features are normalized to have zero mean and unit variance. By normalizing the data, we align the network's expectations with these weight initialization techniques, enhancing the overall training process.\n",
        "* **Handling Outliers:** Normalization can help handle outliers in the data. Outliers can disproportionately influence the learning process and bias the network's decisions. By normalizing the data, outliers are brought closer to the range of other values, minimizing their impact on the network's behavior.\n",
        "\n",
        "In summary, normalizing neural network datasets improves convergence, avoids gradient instability, enhances computational efficiency, promotes generalization, aids in weight initialization, and helps handle outliers. These benefits contribute to more effective training and improved performance of neural networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5fJhDLihYnm"
      },
      "source": [
        "## **Encoding Continuous Values**\n",
        "\n",
        "One common transformation for data **_normalization_** is to convert the input values into Z-scores.  Normalizing numeric inputs into a standard form makes it easier for a program to compare values.  Consider if a friend told you that he received a 10-dollar discount.  Is this a good deal?  Maybe.  But the cost is not normalized.  If your friend purchased a car, the discount is not that good.  If your friend bought lunch, this is an excellent discount!\n",
        "\n",
        "Converting a number into a percentage is a common form of normalization.  If your friend tells you they got 10% off, we know that this is a better discount than 5%.  It does not matter how much the purchase price was.  \n",
        "\n",
        "For machine learning, a better form of normalization than percentages is the Z-Score:\n",
        "\n",
        "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
        "\n",
        "To calculate the Z-Score, you also need to calculate the mean(&mu; or $\\bar{x}$) and the standard deviation (&sigma;).  You can calculate the mean with this equation:\n",
        "\n",
        "$$ \\mu = \\bar{x} = \\frac{x_1+x_2+\\cdots +x_n}{n} $$\n",
        "\n",
        "The standard deviation is calculated as follows:\n",
        "\n",
        "$$ \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} $$\n",
        "\n",
        "Example 2 and Exercise 2 below will demostrate how to replace numerical values in a DataFrame with their Z-scores. Average values will end up having Z-scores near zero, values that are greater than average will have positive Z-scores while below values below average will end up having negative Z-scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfhuGksRhYnm"
      },
      "source": [
        "---------------------\n",
        "\n",
        "## **Z-scores**\n",
        "\n",
        "**_Z-scores_**, also known as _standard scores_, are statistical measures that indicate how far a particular value is from the mean of a dataset, measured in terms of standard deviations. They are important because they allow us to compare and analyze data points from different distributions with different units and scales.\n",
        "\n",
        "The calculation of a Z-score involves subtracting the mean of the distribution from the specific value and then dividing the result by the standard deviation. This transforms the original value into a standardized value that represents its relative position within the distribution.\n",
        "\n",
        "Z-scores are important for several reasons:\n",
        "\n",
        "* **Standardization:** Z-scores provide a way to standardize data, making it easier to compare values from different datasets and variables. By converting values to a common scale, we can compare observations, identify outliers, and analyze data more accurately.\n",
        "* **Normal distribution:** Z-scores are frequently used with normally distributed data, where the mean is 0 and the standard deviation is 1. When data is standardized to a Z-score distribution, it becomes easier to apply statistical techniques and make meaningful interpretations based on the standard normal distribution.\n",
        "* **Identification of extreme values:** Z-scores help in identifying extreme values, known as outliers. Values with Z-scores greater than a certain threshold (e.g., 2 or 3) are considered outliers, indicating that they deviate significantly from the mean.\n",
        "* **Probability calculations:** Z-scores also enable us to calculate probabilities and determine the likelihood of a value occurring in a normal distribution. By converting a value to its Z-score, we can look up the probability associated with that Z-score in a standard normal distribution table.\n",
        "\n",
        "Overall, Z-scores provide a standardized way to analyze, compare, and interpret data, making them an essential tool in statistical analysis and research.\n",
        "\n",
        "-----------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc0Nf6_WhYnm"
      },
      "source": [
        "### Example 2: Convert to Z-scores\n",
        "\n",
        "The cell below shows how to use the `zscore` package from the `scipy.stats` module to compute Z-score values for float values in the Obesity Prediction dataset. Float values are found in the `Height`,`Weight` and `BMI` columns. These measurements are good candidates for converting to Z-scores since their values can be quite different."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Convert Floats to Z-score\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Use function defined at the start of this notebook\n",
        "float_columns = list_float_columns(opDF)\n",
        "print(f\"Columns with float values: {float_columns}\")\n",
        "\n",
        "for col in float_columns:\n",
        "    opDF[col] = zscore(opDF[col])\n",
        "\n",
        "# Print the first 5 values of each float column\n",
        "for col in float_columns:\n",
        "    print(f\"First 5 values in column '{col}': {opDF[col].head().tolist()}\")"
      ],
      "metadata": {
        "id": "52UfeJxP94KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoE_ejz-hYnm"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "~~~text\n",
        "Columns with float values: ['Height', 'Weight', 'BMI']\n",
        "First 5 values in column 'Height': [0.3418640392162576, -0.5749847402902392, -0.19216404055993186, -0.15456698221858464, 1.3116345561014482]\n",
        "First 5 values in column 'Weight': [0.0500759354697495, 1.2097390434498765, 0.11126628433386448, 0.8825352755671579, -0.1397761977927924]\n",
        "First 5 values in column 'BMI': [-0.16096979006604262, 1.3741152139851138, 0.1501289782474755, 0.8115135795924825, -0.7107971919502696]\n",
        "~~~\n",
        "\n",
        "As you can see, after converting to their Z-scores, the height, weight and BMI measurements have gone from relatively large, all positive numbers, to small values, near zero, that are both positive and negative. A value equal to `0` means average, while positive values are above average and negative values are below average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj4ZnoGGhYnm"
      },
      "source": [
        "### **Exercise 2: Convert to Z-scores**\n",
        "\n",
        "In the cell below, use the `zscore` package from the `scipy.stats` module to compute Z-score values for the resting blood pressue (`RestingBP`) values and the serum cholesterol values (`Cholesterol`) in the Heart Disease dataset. Display only the `h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "rfy0gilUhYnm"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 2 here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKBic4SKhYnm"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "\n",
        "~~~text\n",
        "Columns with float values: ['Oldpeak']\n",
        "First 5 values in column 'Oldpeak': [-0.8324323931317043, 0.10566352743655559, -0.8324323931317043, 0.5747114877206856, -0.8324323931317043]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bbcaCZShYnn"
      },
      "source": [
        "------------------------------------\n",
        "\n",
        "## **Encoding Categorical Values as Dummies**\n",
        "\n",
        "The traditional means of encoding categorical values (i.e. converting string data to numerical values) is to replace them with **_dummy variables_**.  This technique is also called One-Hot Encoding.\n",
        "\n",
        "**One-Hot Encoding** is a technique used in data preprocessing and feature engineering to convert categorical variables into a numerical representation that can be used by machine learning algorithms. It is important because many machine learning models require numerical input, and categorical variables cannot be directly used in their raw form.\n",
        "\n",
        "In One-Hot Encoding, each category in a categorical variable is converted into a new binary feature column (i.e. `0` or `1`). For a variable with _N_ categories, _N_ new binary columns are created, where each column represents a specific category. If an observation belongs to a particular category, the corresponding feature column is assigned a value of 1, otherwise 0.\n",
        "\n",
        "There are a few reasons why One-Hot Encoding is important:\n",
        "\n",
        "* **Retaining categorical information:** One-Hot Encoding allows us to retain the categorical information that would otherwise be lost if we simply assigned numerical labels to each category. By creating separate binary columns, we preserve the distinctiveness of each category, enabling the model to understand and utilize this information.\n",
        "* **Avoiding numerical assumptions:** By converting categorical variables into numerical representations, we eliminate any numerical order or relationship assumptions that may not exist in the original data. This prevents the model from mistakenly interpreting the numerical values as meaningful in terms of order or magnitude.\n",
        "* **Compatibility with machine learning algorithms:** Many machine learning algorithms require numerical input. By converting categorical variables into a binary representation, One Hot Encoding makes it possible to feed categorical data into these algorithms, expanding the range of models that can be utilized.\n",
        "* **Handling multi-class categories:** One Hot Encoding is particularly useful when dealing with categorical variables with multiple classes. By creating binary columns for each class, we allow the model to learn distinct patterns and relationships between the categories.\n",
        "\n",
        "It is important to note that One-Hot Encoding can increase the dimensionality of the dataset, especially if the categorical variable has a large number of classes. This can potentially lead to the **\"curse of dimensionality\"** and affect the performance of the model. However, it is a widely used and effective technique for incorporating categorical variables into machine learning models.\n",
        "\n",
        "-------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7vOEF9LhYnn"
      },
      "source": [
        "## Example 3: Preprocessing data\n",
        "\n",
        "Example 3 has been broken down into 5 separate steps. Each step illustrates an important technique used to **_preprocess_** data before it can be used with deep neural networks. You will be using these steps over-and-over again in this course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jceYBZ_bhYnn"
      },
      "source": [
        "### Example 3-Step 1: Determine categories that are not numeric\n",
        "\n",
        "The first step in encoding categorical variables is determine which column(s) have non numerical values.\n",
        "\n",
        "We can use the Pandas method `df.select_dtypes(exclude='number'0.columns` to generate a list of column names.\n",
        "\n",
        "The code in the cell below also uses the `starred` print option.\n",
        "~~~text\n",
        "# Print result\n",
        "print(*non_numerical_columns)\n",
        "~~~\n",
        "\n",
        "By simply inserting an asterisk `*` before the list variable, the print statement only prints out the column names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxLBBBgHhYnn"
      },
      "outputs": [],
      "source": [
        "# Example 3-Step 1: Determine columns with non-numeric values\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Select columns\n",
        "non_numerical_columns = opDF.select_dtypes(exclude='number').columns\n",
        "\n",
        "# Print result\n",
        "print(*non_numerical_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sy1gBuMhYnn"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "~~~text\n",
        "Gender ObesityCategory\n",
        "~~~\n",
        "\n",
        "There are two columns in the `opDF` Dataframe, `Gender` and `ObesityCategory`, that we will need to convert string values into numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ke04aqIhYnn"
      },
      "source": [
        "### Example 3-Step 2: Print out a list of the category values\n",
        "\n",
        "There are basically two different ways to convert string values into numerical values\n",
        "\n",
        "1. Mapping string values to integers\n",
        "2. One-Hot Encoding\n",
        "\n",
        "In most situations, either method would work. I will usually use \"mapping\" if the number of categorical values is small. For example, in many biomedical datasets, a string value is used to denote the gender of the subject/patient (e.g., `M` for male and `F` for female). When the number of categorical values is small, my preference is to use `mapping`. In other situations, where the number of categorical values is 3 or more, my preference is to use One-Hot Encoding. Again, in most situations, either method should work.\n",
        "\n",
        "The code in the cell below shows how to determine the number of different categories that are used in `ObesityCategory` column in the `opDF` DataFrame. This step is not really necessary for this particular dataset since the number of columns is relatively small. However in a dataset with a large number of columns, this step might be very helpful.\n",
        "\n",
        "The trick here is to use the Python `list()` function with the `unique()` method when creating the category list. This insures that the list contains only the name of each _different_ category in the column and not simply a list containing all of the category names repeated hundred of times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "NweQXwTwhYnn"
      },
      "outputs": [],
      "source": [
        "# Example 3 Step 2: Print category values\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Generate a list with only unique values\n",
        "numOpCat = list(opDF['ObesityCategory'].unique())\n",
        "\n",
        "# Print out the results\n",
        "print(f'Number of obesity categories: {len(numOpCat)}')\n",
        "print(f'numOpCat: {numOpCat}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEzNfFi8hYnn"
      },
      "source": [
        "As you can see from the output above:\n",
        "~~~text\n",
        "Number of obesity categories: 4\n",
        "numOpCat: ['Normal weight', 'Obese', 'Overweight', 'Underweight']\n",
        "~~~\n",
        "there are four different strings used as categorical values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dvc38v3hYnn"
      },
      "source": [
        "### Example 3- Step 3: One-Hot Encode the column\n",
        "\n",
        "From the output above we know that there are exactly 4 different category values used in the `ObesityCategory` column: `Normal weight`, `Obese`, `Overweight` and `Underweight`. We need to One-Hot Encode these values.\n",
        "\n",
        "The code in the cell below uses Pandas' `pd.get_dummies()` function to create dummy columns that can be used to replace the `ObesityCategory` column in the `opDF` DataFrame. To make it easier to remember what the dummy columns represent, we are going to add the prefix `OBCat` to each of the new dummy columns. What you use as a prefix is totally up to you, since it doesn't have any effect on how the data is processed. The prefix is just a reminder of what the original data was."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-S6aVcghYnn"
      },
      "outputs": [],
      "source": [
        "# Example 3 - Step 3: Encode the actual column\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Encode the ObesityCategory column\n",
        "dummies = pd.get_dummies(opDF['ObesityCategory'],prefix='OBCat', dtype=int)\n",
        "\n",
        "# Display dummies DataFrame\n",
        "display (dummies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma1av2dohYnn"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_02_2_Exm3.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIsUClR7hYnn"
      },
      "source": [
        "The variable called `dummies`, that was created by the One-Hot Encoding, is actually a new, **_separate_** DataFrame which is displayed above.\n",
        "\n",
        "These four dummy columns encode the categorical data that is in the `ObesityCategory` column.\n",
        "\n",
        "It is important to know how dummy columns encode numerical information. Notice that for each row, only **_one_** column that has a value of `1`, while the other columns in that row contain `0`.\n",
        "\n",
        "For example, the first patient (index value `0`) has a `1` only in the column `OBCat_Normal weight` while the second patient (index value `1`) only has a `1` in the column `OBCat_Obese`.  \n",
        "\n",
        "For this reason, this type of encoding is called **_One-Hot_** Encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0K2xTjWhYnn"
      },
      "source": [
        "### Example 3-Step 4: Merge dummy columns into dataset\n",
        "\n",
        "As mentioned above, One-Hot Encoding only generates a new, separate DataFrame called `dummies`. In order to use it, it is up to you to **_merge_** the `dummies` DataFrame with the DataFrame containing the dataset, in this example, `opDF`.   \n",
        "\n",
        "The code in the cell below, shows how to merge these two DataFrames using the Pandas function `pd.concat()`. The word 'concat' in this command is short for `concatenate`.\n",
        "\n",
        "**_Concatenate_** means combining multiple strings, lists, or other sequences into a single sequence. It can be done using various methods including the `concat()` function in Pandas.\n",
        "\n",
        "The code fragment that accomplishes the concatenation is:\n",
        "~~~text\n",
        "# Merge dummies with the DataFrame\n",
        "opDF = pd.concat([opDF,dummies],axis=1)\n",
        "~~~\n",
        "\n",
        "The argument `axis=1` specifies that the concatenation should be done along **_columns_**. This means that the DataFrames are joined _horizontally_, with the columns from the second DataFrame (`dummies`) added next to the columns from the first DataFrame (`opDF`).\n",
        "\n",
        "The code below also illustrates how you can display only **_selected columns_** in a large DataFrame by specifying their column name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce1ywWaYhYnn"
      },
      "outputs": [],
      "source": [
        "# Example 3 - Step 4: Merge dummies with dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Merge dummies with the DataFrame\n",
        "opDF = pd.concat([opDF,dummies],axis=1)\n",
        "\n",
        "# Set max rows and max columns\n",
        "pd.set_option('display.max_rows', 10)\n",
        "pd.set_option('display.max_columns', 6)\n",
        "\n",
        "# Display certain columns in the DataFrame\n",
        "display(opDF[['BMI','ObesityCategory','OBCat_Normal weight',\n",
        "                  'OBCat_Obese','OBCat_Overweight','OBCat_Underweight']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3bfcdVfhYnq"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_02_2obdummies.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i28yiqTphYnq"
      },
      "source": [
        "From the output above, we can see that the four dummy columns, with the pre-fix `OBCat_` have been added to the `opDF` DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1XfnKq1hYnq"
      },
      "source": [
        "### Example 3-Step 5: Remove the original column\n",
        "\n",
        "Usually, you will need to remove the column that was One-Hot Encoded from your DataFrame for two reasons. First, the column still contains string values which you can't use. And second, the informat in that column is **_redundant_** -- the information is encoded in the dummy columns.\n",
        "\n",
        "The cell below shows how to use the Pandas `df.drop()` method to drop the `ObesityCategory` column from the `opDF` DataFrame. As above, the argument `axis=1` tells the method that you want to drop the column instead of a row.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guKUd-qEhYnq"
      },
      "outputs": [],
      "source": [
        "# Example 3-Step 5: Remove the orginal column\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Use drop method to drop the ObesityCategory column\n",
        "opDF.drop('ObesityCategory', axis=1, inplace=True)\n",
        "\n",
        "# Set max rows and max columns\n",
        "pd.set_option('display.max_rows', 6)\n",
        "pd.set_option('display.max_columns', 6)\n",
        "\n",
        "# Display DataFrame\n",
        "display(opDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a07C4SChYnq"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_02_2_Exm3-5.png)\n",
        "\n",
        "You should note the the column `ObesityCategory` is no longer present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYhBJjw0hYnr"
      },
      "source": [
        "Instead of see the image above, you might see the following error message (it won't be this color):\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_02_2_error.png)\n",
        "\n",
        "If you see this error, DON'T panic! It simply means that you ran the code in Example 3-Step 5 more than once.\n",
        "\n",
        "Since the first time you ran the cell, it removed the `ObesityCategory` column from the DataFrame `opDF`. The problem comes when try to run it again. You will get this error since the column had already been removed.\n",
        "\n",
        "The solution is to simply SAVE your work and then restart your notebook from the very beginning. Your notebook will **_re-read_** the Obesity Prediction dataset and re-create the `opDF` DataFrame will all of its columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akZtGA5FhYnr"
      },
      "source": [
        "## **Exercise 3**\n",
        "\n",
        "In **Exercise 3** you are to repeat the same 5 steps from Example 3, but using the the Heart Disease dataset in the DataFrame `hdDF`. In other words, you are to follow the same 5 steps shown in Example 3 to Hot-Encode data in the Heart Failure dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbGhXXZNhYnr"
      },
      "source": [
        "### **Exercise 3-Step 1: Determine categories that are not numeric**\n",
        "\n",
        "In the first step, use the Pandas function `df.select_dtypes()` to select all of the columns in your DataFrame `hdDF` that contain non-numeric (string) values.\n",
        "\n",
        "Print out the results using the `starred` print statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "scrolled": true,
        "id": "By7H7JymhYnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb85abe1-d135-4904-9000-cd0fbe0aed2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sex ChestPainType RestingECG ExerciseAngina ST_Slope\n"
          ]
        }
      ],
      "source": [
        "# Insert your code for Exercise 3 Step 1 here\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Select columns\n",
        "non_numerical_columns = hdDF.select_dtypes(exclude='number').columns\n",
        "\n",
        "# Print result\n",
        "print(*non_numerical_columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm62DqDIhYnr"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "~~~text\n",
        "Sex ChestPainType RestingECG ExerciseAngina ST_Slope\n",
        "~~~\n",
        "There are 4 columns that have non numeric values: `Sex`, `RestingECG`, `ExerciseAngina` and `ST_Slope`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PphZUTw7hYnr"
      },
      "source": [
        "### **Exercise 3-Step 2: Print out a list of the category values**\n",
        "\n",
        "Even though there are 4 columns with non numerical values, for Exercise 3 you are to only encode the `ChestPainType` column.\n",
        "\n",
        "In the cell below write the Python code to print out a list showing the number of different categories in the `ChestPainType` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EJy04uUThYnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b23388-b206-457b-ec18-e7678128b8a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chest pain categories: 4\n",
            "numHCat: ['ATA', 'NAP', 'ASY', 'TA']\n"
          ]
        }
      ],
      "source": [
        "# Insert your code for Exercise 3 Step 2 here\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Generate a list with only unique values\n",
        "numHfCat = list(hdDF['ChestPainType'].unique())\n",
        "\n",
        "# Print out the results\n",
        "print(f'Number of chest pain categories: {len(numHfCat)}')\n",
        "print(f'numHCat: {numHfCat}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ah1eGPrhYnr"
      },
      "source": [
        "If your code is correct you should see the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXTD0C1ihYnr"
      },
      "source": [
        "~~~text\n",
        "Number of chest pain categories: 4\n",
        "numHfCat: ['ATA', 'NAP', 'ASY', 'TA'\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvpwXbkZhYnr"
      },
      "source": [
        "### **Exercise 3-Step 3: One-Hot Encode the column**\n",
        "\n",
        "In the cell below use the `pd.get_dummies()` function to create dummy columns for the column `ChestPainType`. To make it easier to remember what the dummy columns represent, add the prefix `Pain` to each dummy column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1c1mB1RhYnr"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 Step 3 here\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Encode the ObesityCategory column\n",
        "dummies = pd.get_dummies(hdDF['ChestPainType'],prefix='Pain', dtype=int)\n",
        "\n",
        "# Display dummies DataFrame\n",
        "display (dummies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjikSlmthYnr"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02_2_dummie2.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FU-JuyDhYnr"
      },
      "source": [
        "Notice that for each row, there is only one column that has a value of `1`, while the rest of the columns in that row contain `0`. The first patient (index `0`) has the pain type `ATA` since he/she has a `1` in that column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj6QBUdehYnr"
      },
      "source": [
        "### **Exercise 3-Step 4: Merge dummy columns into dataset**\n",
        "\n",
        "In the cell below write the code to add the dummy columns back into the `hdDF` DataFrame using the Pandas' `pd.concat()` function. Set the display option to print out 6 rows and 6 columns. Unlike Example 3-Step 4, don't print out specific columns. Instead just use the command `display(hdDF)` to see your DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qc069GohYnr"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 Step 4 here\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Merge dummies with the DataFrame\n",
        "hdDF = pd.concat([hdDF,dummies],axis=1)\n",
        "\n",
        "# Set max rows and max columns\n",
        "pd.set_option('display.max_rows', 6)\n",
        "pd.set_option('display.max_columns', 6)\n",
        "\n",
        "# Display DataFrame\n",
        "display(hdDF)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qH187zshYnr"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_02_2_Exe3-4.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKQp0vkThYnr"
      },
      "source": [
        "### **Exercise 3-Step 5: Remove the orignal column**\n",
        "\n",
        "In the cell below, write the code to drop the `ChestPainType` column from the `hdDF` DataFrame.\n",
        "\n",
        "Display 6 rows and 6 columns of your modified DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "B7wqrh3ThYnr"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 Step 5 here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7BW1g9RhYnr"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_02_2_Exe3-5.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrxivRLFhYnr"
      },
      "source": [
        "### Removing the First Level\n",
        "\n",
        "The **pd.concat** function also includes a parameter named *drop_first*, which specifies whether to get k-1 dummies out of k categorical levels by removing the first level.\n",
        "\n",
        "Why would you want to remove the first level?\n",
        "\n",
        "Consider the category `Gender` in the `opDF` dataframe. This column contains the string variables `Male` and `Female`. Suppose we were to `Hot-One-Encode` the `Gender` column with the following line of Python code:\n",
        "\n",
        "> `dummies = pd.get_dummies(opDF['Gender'],prefix='Gender', dtype=int)`\n",
        ">\n",
        "\n",
        "Here is what the two dummy columns would look like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0g23SpfhYnr"
      },
      "source": [
        "~~~text\n",
        "    Gender_Female  Gender_Male\n",
        "0               0            1\n",
        "1               0            1\n",
        "2               1            0\n",
        "..            ...          ...\n",
        "7               0            1\n",
        "8               0            1\n",
        "9               0            1\n",
        "\n",
        "[10 rows x 2 columns]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ99IhcshYnr"
      },
      "source": [
        "Now, ask yourself the question, \"Do we really need **both** columns to know if a subject was a female or a male?\"\n",
        "\n",
        "The answer is, \"Not really\".\n",
        "\n",
        "Suppose we removed the `First Level` using the following line of code:\n",
        "\n",
        "> `dummies = pd.get_dummies(obDF['Gender'],prefix='Gender', dtype=int, drop_first=True)`\n",
        ">\n",
        "Here is the output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwbS06NPhYns"
      },
      "source": [
        "~~~text\n",
        "    Gender_Male\n",
        "0             1\n",
        "1             1\n",
        "2             0\n",
        "..          ...\n",
        "7             1\n",
        "8             1\n",
        "9             1\n",
        "\n",
        "[10 rows x 1 columns]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtWVHNvthYns"
      },
      "source": [
        "Since the first level `Gender_Female` was dropped, there is now only the single dummy column `Gender_Male` left. However, you only need this column to know the gender of each subject in the `opDF` dataframe!\n",
        "\n",
        "Consider the subject in Row 0. This subject is a male since he has a `1` in the `Gender_Male` column. This is also true of the subject in Row 1. But the subject in Row 2 _must_ be a female because she has a `0` in the `Gender_Male` column. In other words, we can determine the gender of every subject by the values in a single column.\n",
        "\n",
        "It turns out that this idea is not limited to situations were there are only two possible choices. You can **always** drop one column in _any_ series of dummy columns **without** losing any information. For this reason the command `pd.get_dummies()` is often used with the argument `drop_first` set to `True` to simplify this process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBV24wdYhYns"
      },
      "source": [
        "### Example 4: Using `drop_first=True`\n",
        "\n",
        "The code in the cell below, begins by regenerating the original `opDF` DataFrame by re-reading the datafile.\n",
        "\n",
        "The code then creates dummy columns for the `ObesityCategory` column in the DataFrame `opDF` but drops the first category before merging the remaining 3 dummy columns with the DataFrame and then drops the `ObesityCategory` column from the `opDF` DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0uSo_nOhYns"
      },
      "outputs": [],
      "source": [
        "# Example 4 - Use drop_first = True\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the datafile\n",
        "opDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/obesity_prediction.csv\",\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "# Encode the ObesityCategory column as dummy variables\n",
        "dummies = pd.get_dummies(opDF['ObesityCategory'], drop_first=True,\n",
        "                         dtype= int, prefix='OBCat')\n",
        "\n",
        "# Merge the dummie with the DataFrame\n",
        "opDF = pd.concat([opDF,dummies],axis=1)\n",
        "\n",
        "# Drop the column replaced by the dummies\n",
        "opDF.drop('ObesityCategory', axis=1, inplace=True)\n",
        "\n",
        "# Set max rows and max columns\n",
        "pd.set_option('display.max_rows', 4)\n",
        "pd.set_option('display.max_columns', 6)\n",
        "\n",
        "# Display the DataFrame\n",
        "display(opDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO2Oe9y7hYns"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_02_2_Exm4.png)\n",
        "\n",
        "Notice that the first subject (row `0`) who is a male, age 56. He must also have a _normal weight_, even though in the first category `OBCat_Normal Weight` was dropped.\n",
        "\n",
        "Why?\n",
        "\n",
        "Since he has a zero in the remaining 3 obesity categories, the only possibilty is that he had a `1` in the dropped category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVuv_NWphYns"
      },
      "source": [
        "### **Exercise 4: Using `drop_first=True`**\n",
        "\n",
        "In the cell below, start by regenerating the complete (original) Heart Failure DataFrame using your code from **Exercise 1**.\n",
        "\n",
        "Then write the Python code to One-Hot encode the column `ChestPainType` column setting the `drop_first` argument to `True` to drop the first dummy column. Merge the remaining 3 dummy columns with the `hfDF` DataFrame before dropping the `ChestPainType` column.  Set the display options to print out 6 rows and 8 columns and print out the updated DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEE3s_HAhYns"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ej1cbJhYns"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_02_2_Drop1.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRaItKFWhYns"
      },
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_02_2.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Poly-A Tail**\n",
        "\n",
        "\n",
        "\n",
        "![___](https://upload.wikimedia.org/wikipedia/commons/e/ee/Pdp-11-40.jpg)\n",
        "\n",
        "\n",
        "The **PDP11** is a series of 16-bit minicomputers originally sold by Digital Equipment Corporation (DEC) from 1970 into the late 1990s, one of a set of products in the Programmed Data Processor (PDP) series. In total, around 600,000 PDP-11s of all models were sold, making it one of DEC's most successful product lines. The PDP-11 is considered by some experts to be the most popular minicomputer.\n",
        "\n",
        "The PDP11 included a number of innovative features in its instruction set and additional general-purpose registers that made it easier to program than earlier models in the PDP series. Further, the innovative Unibus system allowed external devices to be more easily interfaced to the system using direct memory access, opening the system to a wide variety of peripherals. The PDP11 replaced the PDP8 in many real-time computing applications, although both product lines lived in parallel for more than 10 years. The ease of programming of the PDP11 made it popular for general-purpose computing.\n",
        "\n",
        "The design of the PDP11 inspired the design of late-1970s microprocessors including the Intel x86[1] and the Motorola 68000. The design features of PDP11 operating systems, and other operating systems from Digital Equipment, influenced the design of operating systems such as CP/M and hence also MS-DOS. The first officially named version of Unix ran on the PDP11/20 in 1970. It is commonly stated that the C programming language took advantage of several low-level PDP11dependent programming features, albeit not originally by design.\n",
        "\n",
        "An effort to expand the PDP11 from 16- to 32-bit addressing led to the VAX-11 design, which took part of its name from the PDP11.\n",
        "\n",
        "**History**\n",
        "\n",
        "Previous machines\n",
        "In 1963, DEC introduced what is considered to be the first commercial minicomputer in the form of the PDP5. This was a 12-bit design adapted from the 1962 LINC machine that was intended to be used in a lab setting. DEC slightly simplified the LINC system and instruction set, aiming the PDP-5 at smaller settings that did not need the power of their larger 18-bit PDP-4. The PDP-5 was a success, ultimately selling about 1,000 machines. This led to the PDP8, a further cost-reduced 12-bit model that sold about 50,000 units.\n",
        "\n",
        "During this period, the computer market was moving from computer word lengths based on units of 6 bits to units of 8 bits, following the introduction of the 7-bit ASCII standard. In 19671968, DEC engineers designed a 16-bit machine, the PDPX,[5] but management ultimately canceled the project as it did not appear to offer a significant advantage over their existing 12- and 18-bit platforms.\n",
        "\n",
        "This prompted several of the engineers from the PDP-X program to leave DEC and form Data General. The next year they introduced the 16-bit Data General Nova.[6] The Nova sold tens of thousands of units and launched what would become one of DEC's major competitors through the 1970s and 1980s.\n",
        "\n",
        "**Release**\n",
        "\n",
        "Ken Olsen, president and founder of DEC, was more interested in a small 8-bit machine than the larger 16-bit system. This became the \"Desk Calculator\" project. Not long after, Datamation published a note about a desk calculator being developed at DEC, which caused concern at Wang Laboratories, who were heavily invested in that market. Before long, it became clear that the entire market was moving to 16-bit, and the Desk Calculator began a 16-bit design as well.\n",
        "\n",
        "The team decided that the best approach to a new architecture would be to minimize the memory bandwidth needed to execute the instructions. Larry McGowan coded a series of assembly language programs using the instruction sets of various existing platforms and examined how much memory would be exchanged to execute them. Harold McFarland joined the effort and had already written a very complex instruction set that the team rejected, but a second one was simpler and would ultimately form the basis for the PDP11.\n",
        "\n",
        "When they first presented the new architecture, the managers were dismayed. It lacked single instruction-word immediate data and short addresses, both of which were considered essential to improving memory performance. McGowan and McFarland were eventually able to convince them that the system would work as expected, and suddenly \"the Desk Calculator project got hot\". Much of the system was developed using a PDP-10 where the SIM-11 simulated what would become the PDP11/20 and Bob Bowers wrote an assembler for it.\n",
        "\n",
        "At a late stage, the marketing team wanted to ship the system with 2K of memory[a] as the minimal configuration. When McGowan stated this would mean an assembler could not run on the system, the minimum was expanded to 4K. The marketing team also wanted to use the forward slash character for comments in the assembler code, as was the case in the PDP8 assembler. McGowan stated that he would then have to use semicolon to indicate division, and the idea was dropped.[7]\n",
        "\n",
        "The PDP11 family was announced in January 1970 and shipments began early that year. DEC sold over 170,000 PDP11s in the 1970s.\n",
        "\n",
        "Initially manufactured of small-scale transistortransistor logic, a single-board large-scale integration version of the processor was developed in 1975. A two- or three-chip processor, the J-11 was developed in 1979.\n",
        "\n",
        "The last models of the PDP11 line were the single board PDP11/94 and PDP11/93 introduced in 1990."
      ],
      "metadata": {
        "id": "4L0ngX__6Ofn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0p76IiST7S94"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}