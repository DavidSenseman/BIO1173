{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_02_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYZVwSpdbE3Y"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExN-OzpYbE3Y"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4imk1kbE3Y"
      },
      "source": [
        "##### **Module 2: Neural Networks with PyTorch**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 2 Material\n",
        "\n",
        "* Part 2.1: Introduction to Neural Networks with PyTorch\n",
        "* **Part 2.2: Encoding Feature Vectors**\n",
        "* Part 2.3: Controlling Overfitting\n",
        "* Part 2.4: Saving and Loading a PyTorch Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_-lPkxLbE3Z"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "xG3_sXTDfyjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Custom Function\n",
        "\n",
        "The cell below creates a custom function called `hms_string()`. This function is needed to record the time required to train your neural network model.\n",
        "\n",
        "If you fail to run this cell now, you will receive one (or more) error message(s) later in this lesson."
      ],
      "metadata": {
        "id": "DFngTyq_X2cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom function\n",
        "\n",
        "# Create hms_string() ----------------------------------------------------\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "cDRAqKoNX9pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdeDojEDbE3Z"
      },
      "source": [
        "## **Datasets for Class_02_2**\n",
        "\n",
        "For Class_02_2 we will be using the `Wisconsin Breast Cancer` dataset for the Examples and the `Heart Disease` dataset for the **Exercises**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "g9F_7m3ObE3Z"
      },
      "source": [
        "### **`Breast Cancer Wisconsin (Diagnostic)` Data Set**\n",
        "\n",
        "[Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data)\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/breast_cancer.png)\n",
        "\n",
        "\n",
        "The average risk of developing breast cancer in the United States is 13%, or 1 in 8. Approximately 42,000 women in the US die from breast cancer each year. Like most cancers, early detection and treatment is singularily important in preventing mortality.\n",
        "\n",
        "The Breast Cancer Wisconsin dataset contains detailed microscopic measurements of cell nuclei obtained by fine needle aspirates (FNAs) from breast tumors found in 569 women. Some of these tumors were later determined to be **_malignant_** (cancerous), while other tumors were found to be **_benign_** (non-cancerous). Being able to differentiate cancerous from non-cancerous tumors is of obvious importance.  \n",
        "\n",
        "Fine needle aspiration (FNA), also called a fine needle aspiration biopsy, is a minimally invasive procedure that uses a thin needle and syringe to extract a sample of cells, tissue, or fluid from an abnormal area or lump in the body. The sample is then examined under a microscope to confirm a diagnosis or guide treatment.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/fna_tech.png)\n",
        "\n",
        "\n",
        "The list of features computed from digitized images of breast mass cell nuclei obtained from by FNA in the Breast Cancer Wisconsin datasete are as follows:\n",
        "\n",
        "**Attribute Information:**\n",
        "\n",
        "* **ID number**\n",
        "* **Diagnosis:** (M = malignant, B = benign)\n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus:\n",
        "\n",
        "*  **radius:** (mean of distances from center to points on the perimeter)\n",
        "* **texture:** (standard deviation of gray-scale values)\n",
        "* **perimeter:**\n",
        "* **area:**\n",
        "* **smoothness:** (local variation in radius lengths)\n",
        "* **compactness:** (perimeter<sup>2</sup> / area - 1.0)\n",
        "* **concavity:** (severity of concave portions of the contour)\n",
        "* **concave points:** (number of concave portions of the contour)\n",
        "* **symmetry:**\n",
        "* **fractal dimension:** (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error and \"worst\" or largest (mean of the three\n",
        "largest values) of these features were computed for each image,\n",
        "resulting in 30 features. For instance, field 3 is Mean Radius, field\n",
        "13 is Radius SE, field 23 is Worst Radius.1) ID number\n",
        "2) Diagnosis (M = malignant, B = benign)\n",
        "3-32)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlZI4yH4bE3Z"
      },
      "source": [
        "### **Heart Disease Dataset**\n",
        "\n",
        "[Heart Disease Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)\n",
        "\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/HD.jpg)\n",
        "\n",
        "**Description**\n",
        "\n",
        "Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n",
        "\n",
        "People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n",
        "\n",
        "* **Age:** age of the patient [years]\n",
        "* **Sex:** sex of the patient [M: Male, F: Female]\n",
        "* **ChestPainType:** chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n",
        "* **RestingBP:** resting blood pressure [mm Hg]\n",
        "* **Cholesterol:** serum cholesterol [mm/dl]\n",
        "* **FastingBS:** fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n",
        "* **RestingECG:** resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
        "* **MaxHR:** maximum heart rate achieved [Numeric value between 60 and 202]\n",
        "* **ExerciseAngina:** exercise-induced angina [Y: Yes, N: No]\n",
        "* **Oldpeak:** oldpeak = ST [Numeric value measured in depression]\n",
        "* **ST_Slope:** the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n",
        "* **HeartDisease:** output class [1: heart disease, 0: Normal]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVa1JySMbE3Z"
      },
      "source": [
        "# **Encoding a Feature Vector for Deep Learning**\n",
        "\n",
        "Neural networks can accept many types of data. We will continue our focus on tabular data, where there are well-defined rows and columns. This kind of data is what you would typically see in Microsoft Excel spreadsheet. Tabular data can contain both numbers or words (e.g. `male` or `female`).\n",
        "\n",
        "Neural networks require numeric input. This numeric form is called a **_feature vector_**. If the tabular data contains any words, we will need to convert each different word into a specific number. Each input neuron receives one feature (or column) from this vector. Each row of training data typically becomes one vector.\n",
        "\n",
        "In this lesson, we will see how to encode tabular data stored in a Pandas DataFrame into a feature vector that can be used by two types of neural networks: (1) classification and (2) regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX3Hp1tybE3a"
      },
      "source": [
        "### Example 1 - Step 1: Read Dataset and Store Values in a DataFrame\n",
        "\n",
        "Data is the essence of neural networks and deep learning. Neural networks are of little use until they have been trained on **large** datasets. Only by making repeated adjustments in the weights of their neural connections, during many rounds of training (epochs) on a particular dataset can a neural network **learn** to make accurate predictions.  \n",
        "\n",
        "Not surprisingly, building and training neural networks begins with a dataset. The code in the cell below reads the Breast Cancer Wisconsin dataset file, `wcbreast.csv`. With few exceptions, all of the dataset that we will use in the course are stored on a dedicated HTTPS server for this course: https://biologicslab.co.  \n",
        "\n",
        "For tabular data, like the Breast Cancer dataset, we wiil use the Pandas `pd.read_csv()` function to read the data from the course web server and store this information in a DataFrame called `bc_df`\n",
        "~~~text\n",
        "# Read file and create DataFrame\n",
        "bc_df = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/wcbreast.csv\",\n",
        "    index_col=0,\n",
        "    na_values=['NA','?'])\n",
        "~~~\n",
        "In Python programming, DataFrames are usually just called `df`. However, in this course we need to give DataFrames more explicit names since we will typically be using two different DataFrames at the same time, one for the Examples and one for the **Exercises**.\n",
        "\n",
        "The name `bc_df` was chosen to remind us that the DataFrame contains the breast cancer dataset.\n",
        "\n",
        "As a general rule, it is always a good idea to display at least part of your new DataFrame to make sure it was read correctly. Since large DataFrames can have many columns and many, many rows--too many to display in Colab notebook-- it is helpful to specify the maximum number of rows and columns to display. This is accomplished in the cell below using this code chunk:\n",
        "\n",
        "~~~text\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', 4)\n",
        "pd.set_option('display.max_rows', 4)\n",
        "~~~\n",
        "\n",
        "Since different datasets have different numbers of rows and columns, you will be asked frequently to adjust the maximum number of rows and columns to display.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmjrLkDwK1AM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Example 1 - Step 1: Read data and create dataframe\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Read file and create DataFrame\n",
        "bc_df = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/wcbreast.csv\",\n",
        "    index_col=0,\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', 4)\n",
        "pd.set_option('display.max_rows', 4)\n",
        "\n",
        "# Display DataFrame\n",
        "display(bc_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19UPLcZQbE3a"
      },
      "source": [
        "If the code is correct you should see the following table:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image01B.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJTs4WQQbE3a"
      },
      "source": [
        "There are several observations that you should make from this table. First, looking at the very bottom you see:\n",
        "~~~text\n",
        "569 rows x 32 columns\n",
        "~~~\n",
        "This means that our DataFrame `bc_df` has clinical information for `569` subjects (i.e. 1 row/subject) and that there are `32` clinical features (1 feature/column) recorded for each subject.\n",
        "\n",
        "By inspection, we can see at least one column, `diagnosis` has non-numerical values (the strings \"M\" and \"B\"), but there could be more, since we can only see a small fraction of the entire 32 columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZy-feihbE3a"
      },
      "source": [
        "### Example 1 - Step 2: Display Data Types\n",
        "\n",
        "In order to create a feature vector, we will need to know which column(s) in our DataFrame are non-numeric, i.e., contain string values. We can easily print out the different data types in a DataFrame using the Pandas method `df.info`.   \n",
        "\n",
        "However, in order see **_all_** of the different data types, we need to change the number of rows to display. While we could simply set this option to `33`, (i.e. the number of columns in the DataFrame), but it would be difficult to display all of these columns on your computer's monitor. Here we will use a slightly more elegant method using `len(bc_df.columns)`. This will automatically compute the number of rows we will need to display in order to see all of the different columns and the data types that are stored in them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "lABXUQ8nbE3a"
      },
      "outputs": [],
      "source": [
        "# Example 1 - Step 2: Find data types\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Set max rows to the number of columns\n",
        "pd.set_option('display.max_rows', len(bc_df.columns))\n",
        "\n",
        "# Print data types\n",
        "bc_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8otBpc4rbE3a"
      },
      "source": [
        "If the code is correct you should see the following table:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image02B.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiFxcRkHbE3b"
      },
      "source": [
        "You should make the following observations from this output:\n",
        "* The target column is the column that you seek to predict. Usually, the target column containing the `Y-values`, will be the rightmost column in a display, or last column in a list. However, in this particular example, the target column, `diagnosis`, is the second column in the list.   \n",
        "* There is a column called `id` which identifies each subject. We should exclude this columns from our analysis because it contains no information useful for making a prediction.\n",
        "* From the data types output, we can see that with the exception of the column `diagnosis`, all of the fields, are **_numeric_**. Non-numeric values are classified as `object` while numeric values are classified as being either `int64` or `float64`. Numeric columns might not require further processing before there are used to generate our `X-values`.\n",
        "* Categorical values (strings) are only found in the target column (`diagnosis`) which we will take care of later when we generate our `Y-values`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOAb250SbE3b"
      },
      "source": [
        "### **Exercise 1 - Step 1: Read dataset and store values in a DataFrame**\n",
        "\n",
        "In the cell below, use the Pandas function `pd.read_csv()` to read the Heart Disease data file `heart_disease.csv` located on the course HTTPS server. Save the data to a new DataFrame called `hd_df`.\n",
        "\n",
        "_Code Hints:_\n",
        "\n",
        "1. In order to read this file correctly, you **must** comment out the following line of code:\n",
        "\n",
        "~~~text    \n",
        "# index_col=0,\n",
        "~~~\n",
        "The `index_col=0` parameter in `pandas.read_csv()` tells `Pandas` to use the first column of the CSV file as the index of the resulting DataFrame. Whether or not you need to specify it depends on the structure of the CSV file you're reading.\n",
        "\n",
        "##### **When to Use `index_col=0`**\n",
        "Use it when the first column contains row labels (i.e., meaningful identifiers like patient IDs, sample names, etc.) rather than actual data.\n",
        "\n",
        "##### **When to Omit `index_col`**\n",
        "If the first column is just another data column (like age, cholesterol, etc.), and not meant to be the index, then you should not use index_col=0. The easiest way to \"omit\" this argument is to `comment it out` by placing a `#` at the start of the line.\n",
        "\n",
        "**WARNING:** If you don't comment out that line the column `Age` will not be placed in your DataFrame `hd_df` correctly.\n",
        "\n",
        "\n",
        "Set the display for 6 rows and 6 columns and then print out a display of `hd_df`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy7HbReObE3b"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1 - Step 1 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJHzhkBHbE3b"
      },
      "source": [
        "If your code is correct, you should see the following table.\n",
        "\n",
        "![Heart Failure DataFrame](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01a.png)\n",
        "\n",
        "Check your output carefully. From left to right, there should be an unnamed index column with descending numbers (i.e, 0, 1, 2, ...) and just to the right of the index column there should be a column called `Age`. If your output doesn't have an index column, and the column `Age` is the first column on the left, go back and re-read the instruction for **Exercise 1A**.\n",
        "\n",
        "Your `hd_df` DataFrame has information on `918` subjects (number of rows = 918) and `12` clinical values for each subject (number of columns = 12). There are clearly more than one column with non-numeric values, but you won't know exactly how many until you run **Exercise 1B**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bsPm7cabE3b"
      },
      "source": [
        "### **Exercise 1 - Step 2: Display Data Types**\n",
        "\n",
        "In the cell below, write the code to print out the different data types in your DataFrame `hd_df` using the Pandas method `df.info()`. Use `len(hd_df.columns)` to set the number of rows to display, before you print out the data types.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "SN86CHXrbE3b"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1 - Step 2 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HwK4dm6bE3b"
      },
      "source": [
        "If the code is correct you should see the following table:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image03B.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dCjDEVJbE3b"
      },
      "source": [
        "You should make the following observations from the above output:\n",
        "* The target column is the column that you usually want to predict with a classification neural network. In this instance, the last column in this list, `HeartDisease`, will be your **target column** (y-values). All of the other columns are called `feature columns`.\n",
        "* The column `FastingBS` doesn't appear to contain information that would be especially useful for predicting heart disease, so you will need to drop it.\n",
        "* Some fields are numeric (data type `int64` or `float64`) and might not require further processing.\n",
        "* There are categorical values (data type `object`) in 5 columns including: `Sex`, `ChestPainType`, `RestingECG`, `ExerciseAngina` and `ST_Slope`. The categorical values (strings) in these columns will need to be taken care of, before you can use them in generating your `X-values`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyVh55smbE3b"
      },
      "source": [
        "### Example 2: Drop Unecessary Columns\n",
        "\n",
        "The `id` column in the `bc_df` DataFrame does not contain information useful for predicting breast cancer, so we need to exclude from the feature vector containing the `X-values`. To do this we will use the Pandas method `df.drop()` as shown by the next code chunk:\n",
        "~~~text\n",
        "bc_df.drop('id', axis=1, inplace=True)\n",
        "~~~\n",
        "The method `df.drop()` has three arguments. The first argument `id` is the sname of the column to be dropped. The second argument `axis=1` means to drop the entire column, while the third argument `inplace=True` means to change the DataFrame **_permanently_**.\n",
        "\n",
        "**NOTE:** After you run a code cell where you drop a column, you will get an error if you try to re-run the same cell, since there is no longer any column to drop.\n",
        "\n",
        "If you need to run the cell again, you will first need to re-read the datafile and re-create the DataFrame `bc_df` with **all** of the original columns by running Example 1 again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmIasmvIbE3b"
      },
      "outputs": [],
      "source": [
        "# Example 2: Drop unnecessary columns\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Drop specific column\n",
        "bc_df.drop('id', axis=1, inplace=True)\n",
        "\n",
        "# Set the max rows and max columns\n",
        "pd.set_option('display.max_columns', 4)\n",
        "pd.set_option('display.max_rows', 4)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "display(bc_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZglkxDzDbE3c"
      },
      "source": [
        "If you code is correct you should see the following table:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image02a.png)\n",
        "\n",
        "You should note that the column `id` has been removed. Instead of the original `32` columns, there are now only `31` columns.\n",
        "\n",
        "**NOTE:** If you get an error that column `id` doesn't exist, it probably means that you have already run this cell and dropped the column. To correct this error, simply go back and re-read the datafile by re-running Example 1 to create a fresh copy of `bc_df`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image19D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmUbjPt6bE3c"
      },
      "source": [
        "### **Exercise 2: Drop Unecessary Columns**\n",
        "\n",
        "Since the column `FastingBS` in the Heart Disease dataset doesn't contain information that will be especially useful for predicting heart disease, this column should not be included in the analysis. In the cell below, write the code to drop the `FastingBS` column. Set your display to show 6 rows and 6 columns of your updated DataFrame and print out your updated DataFrame `hd_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "fo895DRibE3c"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fURmKEkbE3c"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image04B.png)\n",
        "\n",
        "Since the column `FastingBS` wasn't displayed previously (**Example 1A**), you can't tell if it was dropped. However, the number of columns is now `11`, instead of the original `12` so you can assume your code was successful.\n",
        "\n",
        "**NOTE:** If you get an error that column `FastingBS` doesn't exist, it probably means that you have already run this cell and dropped the column. To correct this error, simply go back and re-read the datafile to create a fresh copy of `hd_df` by running all of the code cells starting with **Exercise 1** again."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------\n",
        "## **One-Hot Encoding**\n",
        "\n",
        "**One-hot encoding** is a technique used to convert categorical variables into a binary matrix (0s and 1s). Each category is represented as a vector where only one element is \"hot\" (i.e., 1) and the rest are 0.\n",
        "\n",
        "**Example:**\n",
        "For a feature `Color` with values `[\"Red\", \"Green\", \"Blue\"]`, `One-Hot Encoding` transforms it as:\n",
        "\n",
        "| Original Value | Red | Green | Blue |\n",
        "|----------------|-----|-------|------|\n",
        "| Red            |  1  |   0   |  0   |\n",
        "| Green          |  0  |   1   |  0   |\n",
        "| Blue           |  0  |   0   |  1   |\n",
        "\n",
        "**Why it's used in neural networks:**\n",
        "- Neural networks require numerical input.\n",
        "- One-Hot Encoding avoids assigning arbitrary numerical values to categories, which could mislead the model into thinking there's an ordinal relationship.\n",
        "- It ensures each category is treated independently and equally.\n",
        "\n",
        "##### **Note:** For high-cardinality features, `One-Hot Encoding` can lead to a large number of input dimensions, which may affect performance and memory usage.\n",
        "-------------------"
      ],
      "metadata": {
        "id": "H0EET9D4NBET"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8P-sfz-bE3c"
      },
      "source": [
        "### Example 3: One-Hot Encode Categorical Variables\n",
        "\n",
        "In general neural networks can only process **numerical** data, not string (categorical) data. String/categorical data are classified as Python type `object`.\n",
        "\n",
        "The code in the cell below shows how to automatically one-hot encode all the `feature columns` that contain `object` (\"string\") data type.\n",
        "\n",
        "As shown in Example 1 - Step 2, the DataFrame `bc_df` had only one column called `diagnosis` with `object` data.\n",
        "\n",
        "However, since `diagnosis` is the **target column**, that contains the y-values (\"targets\"), it is important **not** to one-hot encode this column yet. We therefore separate the `target column` from the `feature columns` from the DataFrame _before_ encoding using these code chunks:\n",
        "\n",
        "```python\n",
        "# Set target name\n",
        "target_name = 'diagnosis'\n",
        "\n",
        "# Separate the target column\n",
        "y_column = bc_df[target_name]\n",
        "\n",
        "# Drop target from features before encoding\n",
        "bc_df_features = bc_df.drop(columns=[target_name])\n",
        "```\n",
        "Once the target column has been removed, we can one-hot encode any `feature column` that contains strings using this code chunk:\n",
        "\n",
        "```python\n",
        "# One-hot encode only the features (not the target)\n",
        "bc_df_encoded_features = pd.get_dummies(bc_df_features, dtype=int, drop_first=True)\n",
        "```\n",
        "After encoding the features, the unencoded target column is added back to the DataFrame using this code chunk:\n",
        "\n",
        "```python\n",
        "# Add the target column back (without encoding it)\n",
        "bc_df_encoded = bc_df_encoded_features.copy()\n",
        "bc_df_encoded[target_name] = y_column\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: One-Hot Encode Categorical Variables\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set target name\n",
        "target_name = 'diagnosis'\n",
        "\n",
        "# Separate the target column\n",
        "y_column = bc_df[target_name]\n",
        "\n",
        "# Drop target from features before encoding\n",
        "bc_df_features = bc_df.drop(columns=[target_name])\n",
        "\n",
        "# One-hot encode only the features (not the target)\n",
        "bc_df_encoded_features = pd.get_dummies(bc_df_features, dtype=int, drop_first=True)\n",
        "\n",
        "# Add the target column back (without encoding it)\n",
        "bc_df_encoded = bc_df_encoded_features.copy()\n",
        "bc_df_encoded[target_name] = y_column\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', 4)\n",
        "pd.set_option('display.max_rows', 4)\n",
        "\n",
        "# Display DataFrame\n",
        "print(\"Before One-hot encoding\")\n",
        "display(bc_df)\n",
        "print(\"After One-hot encoding\")\n",
        "display(bc_df_encoded)"
      ],
      "metadata": {
        "id": "4TS0GPE_f4vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image18D.png)\n",
        "\n",
        "It should be noted that the target column `diagnosis` was **not** encoded and it still contains `object` (string) values (i.e \"M\" and \"B\")."
      ],
      "metadata": {
        "id": "RIRy5xjGYKbl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNqMz8swc1xv"
      },
      "source": [
        "### **Exercise 3: One-Hot Encode Categorical Variables**\n",
        "\n",
        "Your `hd_df` DataFrame has 5 columns that have categorical variables (strings):\n",
        "1. `Sex`\n",
        "2. `ChestPainType`\n",
        "3. `RestingECG`\n",
        "4. `ExerciseAngina`\n",
        "5. `ST_Slope`.\n",
        "\n",
        "For the heart disease dataset, the `target column` is `diagnosis`.  \n",
        "In the cell below, write the code to one-hot encode any `feature column` containing the `object` data type but do NOT encode the target column.\n",
        "\n",
        "**Code Hints**\n",
        "\n",
        "1. Use the code in Example 3 but change the prefix `bc_` to `hd_`.\n",
        "2. Set the display options to show 6 rows and 6 columns."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n"
      ],
      "metadata": {
        "id": "HbohXc8Qc1xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image20D.png)"
      ],
      "metadata": {
        "id": "kBbTU3tOc1xv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should note that your DataFrame `hd_df` is now very different after being one-hot encoded. Since there are so many columns in `hd_df_encoded`, it's difficult to see all of these changes using the `display(df)` command."
      ],
      "metadata": {
        "id": "tPpmMb3Mc1xw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZabPdNCkbE3g"
      },
      "source": [
        "You should notice that the number of columns in your DataFrame, `hd_df`, has increased from the `12` original columns to total of **20** columns!. This a clear example of **_column inflation_** in an invariable consequence of using One-Hot Encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzTd6KMWbE3g"
      },
      "source": [
        "### Example 4: Print Column Names\n",
        "\n",
        "As you saw in **Exercise 3** above, it is not always practical to see all of the column names using the command `display(df)`.\n",
        "\n",
        "The code in the cell below shows how to use the `df.columns` method in conjunction with a `for loop` to print all the column names in a DataFrame in a single vertical column."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Print Column Names\n",
        "\n",
        "from itertools import zip_longest\n",
        "\n",
        "# define a fixed width for the left column so the right one aligns perfectly\n",
        "left_width = 50\n",
        "\n",
        "# Print Headers\n",
        "print(f\"{'Columns Before Encoding':<{left_width}}Columns After Encoding\")\n",
        "print(\"-\" * (left_width + 30))\n",
        "\n",
        "# Use zip_longest to iterate both lists at once\n",
        "# fillvalue=\"\" ensures we print an empty string if one list runs out of items\n",
        "for idx, (col_before, col_after) in enumerate(zip_longest(bc_df.columns, bc_df_encoded.columns, fillvalue=\"\"), start=1):\n",
        "\n",
        "    # Format the strings (only add the index number if the column name exists)\n",
        "    left_str = f\"{idx}. {col_before}\" if col_before else \"\"\n",
        "    right_str = f\"{idx}. {col_after}\" if col_after else \"\"\n",
        "\n",
        "    # Print side-by-side using f-string formatting\n",
        "    # :<{left_width} tells Python to align text to the left and pad it to 50 chars\n",
        "    print(f\"{left_str:<{left_width}}{right_str}\")"
      ],
      "metadata": {
        "id": "7l0WNCytIHLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image21D.png)  \n"
      ],
      "metadata": {
        "id": "_yeMSD1uPlrL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7npA25wCJ_9"
      },
      "source": [
        "### **Exercise 4: Print Column Names**\n",
        "\n",
        "If your coding has been correct so far, your DataFrame, `hd_df` should have 20 columns. This is an inconveniently large number of columns to display on your computer screen using the command `display(df)`.\n",
        "\n",
        "In the cell below, use the code in Example 4 to print out 2 complete lists of all the column names in your DataFrame `hd_df` before and after you one-hot encoded it.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n"
      ],
      "metadata": {
        "id": "S--kedQUKPEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image04D.png)  \n",
        "\n",
        "By comparing the two lists you can see that encoding created different number of new columns. Only one new column was needed for `Sex` and `ExerciseAngina`, two new columns were created for `RestingECG` and `ST_Slope` and three new columns were created for `ChestPainType`. The number of column depends on how many different strings need to be encoded. Since there were three different string values in `ChestPainType` (i.e. `ATA`, `NAP` and `TA`) it was necessary to create three new columns as shown in this table.\n",
        "\n",
        "| Original Value | ChestPainType_ATA | ChestPainType_NAP | ChestPainType_TA |\n",
        "|----------------|-------------------|-------------------|------------------|\n",
        "| ATA            |     1             |      0            |        0         |\n",
        "| NAP            |         0         |      1            |        0         |\n",
        "| TA             |         0         |      0            |        1         |"
      ],
      "metadata": {
        "id": "o6LXXD6vCJ_-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu1059YiCJ_-"
      },
      "source": [
        "**WARNING:** If your output does _not_ start with the name `Age`, it probably means that you didn't read the datafile correctly. You need to go back and re-read the instructions if you want to receive a passing grade for this lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFw_Ff5qbE3h"
      },
      "source": [
        "## **Generate X and Y for a Classification Neural Network**\n",
        "\n",
        "Now that unecessary columns have been dropped, and all of the string data has been `one-hot encoded`, we are ready to use the data stored in the updated DataFrame as input for a neural network.\n",
        "\n",
        "There are two basic ways to used tabular data as input into a neural network. The neural network can perform either **_classification_** or **_regression_**.\n",
        "\n",
        "There are small number of very important differences in how you generate X and Y values for these different functions.\n",
        "\n",
        "We will begin creating a feature vector for a **classification** neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onW_b5thbE3h"
      },
      "source": [
        "### Example 5: Create Feature Vector for _Classification_ Neural Network\n",
        "\n",
        "The code in the cell below creates a feature vector called `bc_X` to hold the `X-values` and another feature vector called `bc_y` to hold the `y-values`.\n",
        "\n",
        "Since we previously dropped columns that contained data that was not useful for our analysis (i.e Example 2 and **Exercise 2**) the code in the cell below uses the data in all of the columns to create the feature vector `bc_X` with the notable exception if the `target column` containing the `y-values`. In this example, the `target column` is `diagnosis`.\n",
        "\n",
        "When building the X feature vector `bc_X`, the code _normalizes_ the numerical data by converting it to its Z-value. Normalization is critical when creating feature vectors for neural networks because it puts all input features on an \"equal playing field.\" Without it, features with larger numerical ranges can dominate the learning process, leading to slow training or poor performance.\n",
        "\n",
        "The exception to this rule are the 0s and 1s that were created by one-hot encoding. Normalizing one-hot encoded data (specifically using Z-score standardization) is generally discouraged for three main reasons: it destroys sparsity, loses semantic meaning, and is mathematically unnecessary.\n",
        "\n",
        "The code in the cell below checks the data in each column and normalizes the data in that column only if it contains values other than `0s` and `1s`. There are a few different ways to normalize data. In this case were are using the Z-score method (see below).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Create Feature Vector for Classification Neural Network\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set target name\n",
        "target_name = 'diagnosis'\n",
        "\n",
        "# 1. Create the X DataFrame by dropping the target column\n",
        "bc_X_df = bc_df_encoded.drop(columns=[target_name])\n",
        "\n",
        "# 2. Create the y Series (Target)\n",
        "bc_y_temp = bc_df_encoded[target_name]\n",
        "\n",
        "# 3. Intelligent Normalization\n",
        "# Create a copy to avoid SettingWithCopy warnings\n",
        "bc_X_normalized = bc_X_df.copy()\n",
        "\n",
        "for col in bc_X_normalized.columns:\n",
        "    # Check if the column contains only 0s and 1s\n",
        "    unique_vals = bc_X_normalized[col].dropna().unique()\n",
        "    is_binary = np.array_equal(np.sort(unique_vals), np.array([0, 1])) or \\\n",
        "                np.array_equal(np.sort(unique_vals), np.array([0])) or \\\n",
        "                np.array_equal(np.sort(unique_vals), np.array([1]))\n",
        "\n",
        "    # If it is NOT binary, apply Z-score normalization\n",
        "    if not is_binary:\n",
        "        mean = bc_X_normalized[col].mean()\n",
        "        std = bc_X_normalized[col].std()\n",
        "        # Avoid division by zero if std is 0\n",
        "        if std != 0:\n",
        "            bc_X_normalized[col] = (bc_X_normalized[col] - mean) / std\n",
        "\n",
        "# 4. Convert both to Numpy arrays for PyTorch compatibility\n",
        "bc_X = bc_X_normalized.values\n",
        "bc_y = bc_y_temp.values\n",
        "\n",
        "# 5. REMAP CLASS LABELS TO START FROM 0\n",
        "# PyTorch expects class indices to be in range [0, num_classes-1]\n",
        "unique_classes = np.unique(bc_y)\n",
        "print(f\"Original class labels: {unique_classes}\")\n",
        "\n",
        "# Create mapping from original labels to 0-indexed labels\n",
        "class_mapping = {original: idx for idx, original in enumerate(unique_classes)}\n",
        "print(f\"Class mapping: {class_mapping}\")\n",
        "\n",
        "# Apply the mapping\n",
        "bc_y = np.array([class_mapping[label] for label in bc_y])\n",
        "\n",
        "# 6. Quick sanity-check print outs\n",
        "np.set_printoptions(suppress=True, precision=4)\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {bc_X.shape}\")\n",
        "print(f\"Target matrix shape: {bc_y.shape}\")\n",
        "\n",
        "# Print number of unique classes\n",
        "print(f\"Number of unique classes: {len(np.unique(bc_y))}\")\n",
        "print(f\"Remapped class labels: {np.unique(bc_y)}\")\n",
        "\n",
        "print(\"\\nFirst 4 feature vectors:\")\n",
        "print(bc_X[:4])\n",
        "\n",
        "print(\"\\nCorresponding targets:\")\n",
        "print(bc_y[:4])"
      ],
      "metadata": {
        "id": "tfFabsg9r9_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image22D.png)  \n",
        "\n",
        "\n",
        "**IMPORTANT:** You should never see any but numeric values in your feature vectors and their targets. If you see and \"words\" or \"letters\" (i.e. strings) in your feature vector or targets, they were not generated correctly. If you try to feed a feature vector containing strings to your neural network for training, the training will immediately terminate with an error message."
      ],
      "metadata": {
        "id": "e14aRNCOSvjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------\n",
        "\n",
        "## **What is a Z-score?**\n",
        "\n",
        "\n",
        "A Z-score (also called a **standard score**) is a statistical measurement that describes a value's relationship to the mean (average) of a group of values. It is measured in terms of **standard deviations** from the mean.\n",
        "\n",
        "#### The Formula\n",
        "The Z-score of a data point $x$ is calculated as:\n",
        "\n",
        "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "Where:\n",
        "* $x$ is the raw score (the original data point).\n",
        "* $\\mu$ (mu) is the mean of the population.\n",
        "* $\\sigma$ (sigma) is the standard deviation of the population.\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image17D.png)  \n",
        "\n",
        "\n",
        "\n",
        "#### What It Tells You\n",
        "* **Z = 0:** The data point is exactly the average.\n",
        "* **Z = +1.0:** The data point is one standard deviation *above* the average.\n",
        "* **Z = -2.5:** The data point is 2.5 standard deviations *below* the average.\n",
        "\n",
        "#### Why It Is Used in Neural Networks (Normalization)\n",
        "In the context of the code you are writing, Z-scores are used to **normalize** input features (like tumor radius or texture).\n",
        "\n",
        "1.  **Centering:** Subtracting the mean ($\\mu$) shifts the data so that the center is at 0.\n",
        "2.  **Scaling:** Dividing by the standard deviation ($\\sigma$) squishes or stretches the data so that most values fall between -1 and 1 (or -3 and 3).\n",
        "\n",
        "This ensures that a feature with a large range (e.g., `Area` = 500 to 2500) doesn't overpower a feature with a small range (e.g., `Smoothness` = 0.05 to 0.15) just because the numbers are bigger.\n",
        "\n",
        "-----------------"
      ],
      "metadata": {
        "id": "WY49yR9yIPqy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-TUKZcJbE3h"
      },
      "source": [
        "### **Exercise 5: Create Feature Vectors for _Classification_ Neural Network**\n",
        "\n",
        "In the cell below, create `X-` and `Y-` feature vectors for a classification neural network from the data in your Heart Disease DataFrame `hd_df`. Use the column `HeartDisease` for your **target column** (i.e. your `Y-values`), and all of the other columns for your `X-values`. Call your X-feature vector **`hd_X`** and your `Y-feature vector` **`hd_Y`**. Don't forget that you **MUST** `One-Hot Encode` the values in the target column for a classification neural network.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Set the target name to \"HeartDisease\"\n",
        "2. Change the prefix `bc_` to `hd_` everywhere in the code that you copied from Example 5.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n"
      ],
      "metadata": {
        "id": "6h9ZxH2lS-FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8G5KZX4bE3h"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image23D.png)  \n",
        "\n",
        "You might notice that there are a lot of `1s` and `0s` in your `X feature vector`. This is because you `one-hot encoded` several non-numeric columns in **Exercise 4**.\n",
        "\n",
        "Also note that there are no \"words\" or \"letters\" in either the `X-` feature vectors or in their corresponding targets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-SEwUxCbE3i"
      },
      "source": [
        "### Example 6:  Construct, Compile and Train _Classification_ Neural Network\n",
        "\n",
        "When building a **classification** neural network, there are two important points to remember:\n",
        "\n",
        "* Classification neural networks have an output neuron count equal to the number of classes.\n",
        "* Classification neural networks should use the **softmax** activation function in the output layer and **categorical_crossentropy** as the loss function when you compile your neural network.\n",
        "\n",
        "The code in the cell below starts out by defining several parmeters using the following code snippet:\n",
        "\n",
        "```python\n",
        "# ---------------------------------------------------------------------------\n",
        "# Define parameters\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "EPOCHS=200\n",
        "PATIENCE=20\n",
        "VERBOSE=2\n",
        "lr=0.0010\n",
        "```\n",
        "This makes it easier to change one (or more) of these values later if you want to \"tune\" your training of your neural network.\n",
        "\n",
        "**Creating a neural network using PyTorch**\n",
        "\n",
        "To create a neural network using PyTorch, we begin by creating a class definition. In this example the class is called `class BreastCancerModel(nn.Module)`. The actual name you give to your class is not important. The class definition defines the model's layers, their activation functions and the number of neurons in each layer. In other words, the class defines the **model's architecture**.\n",
        "\n",
        "```python\n",
        "# ---------------------------------------------------------------------------\n",
        "# Define Model architecture\n",
        "# ---------------------------------------------------------------------------\n",
        "class BreastCancerModel(nn.Module):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        super(BreastCancerModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_shape, 25)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.layer2 = nn.Linear(25, 50)\n",
        "        self.output = nn.Linear(50, output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.output(x) # Returns logits\n",
        "        return x\n",
        "\n",
        "```\n",
        "\n",
        "While the code above creates a neural network class definition, it doesn't actually create our model. For that we need to create an **instance** of the class by initializing the model as shown in this code chunk:\n",
        "\n",
        "```python\n",
        "# Initialize Model\n",
        "input_dim = bc_X.shape[1]\n",
        "bc_class_model = BreastCancerModel(input_dim, output_dim).to(device)\n",
        "```\n",
        "\n",
        "Here is the first part of the code snippet that performs the actual training of our neural network model `bc_class_model`:\n",
        "\n",
        "```python\n",
        "# ---------------------------------------------------------------------------\n",
        "# Training Loop\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"-- Training (classification) is starting for {EPOCHS} epochs ----------------------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "early_stop_counter = 0\n",
        "\n",
        "# Initialize history with all 4 metrics\n",
        "bc_class_history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "checkpoint_path = \"bc_best_classification_model.pth\"\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    bc_class_model.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for inputs, targets in bc_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = bc_class_model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Calculate Batch Accuracy\n",
        "        _, predicted = torch.max(outputs, 1)      # Get the class with highest probability\n",
        "        train_total += targets.size(0)            # Count total samples in batch\n",
        "        train_correct += (predicted == targets).sum().item() # Count correct predictions\n",
        "\n",
        "```\n",
        "This code snippet is only the start of the training loop. As you can see from inspection, additional code is required to track and update various metrics (e.g. `train_loss` and `train_accuracy`). These metrics are stored in a variable called `bc_class_history` that we will use latter to visualize the training process."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Construct and Train Classification Neural Network\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Define parameters\n",
        "# ---------------------------------------------------------------------------\n",
        "EPOCHS = 200\n",
        "PATIENCE = 20\n",
        "lr = 0.0010\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Data Preparation (Numpy -> PyTorch Tensors)\n",
        "# ---------------------------------------------------------------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use sklearn for proper stratified splitting\n",
        "bc_X_train, bc_X_val, bc_y_train, bc_y_val = train_test_split(\n",
        "    bc_X, bc_y,\n",
        "    test_size=0.2,  # 80/20 split\n",
        "    random_state=42,  # For reproducibility\n",
        "    stratify=bc_y  # Ensures balanced classes in both sets\n",
        ")\n",
        "\n",
        "# Determine Output Dimension from actual unique classes\n",
        "unique_classes = np.unique(bc_y)\n",
        "output_dim = len(unique_classes)\n",
        "print(f\"Output dimension (number of classes): {output_dim}\")\n",
        "print(f\"Training samples: {len(bc_y_train)}, Validation samples: {len(bc_y_val)}\")\n",
        "\n",
        "# Check class distribution\n",
        "print(f\"\\nTraining class distribution: {np.bincount(bc_y_train)}\")\n",
        "print(f\"Validation class distribution: {np.bincount(bc_y_val)}\")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "bc_y_train_tensor = torch.tensor(bc_y_train, dtype=torch.long).to(device)\n",
        "bc_y_val_tensor = torch.tensor(bc_y_val, dtype=torch.long).to(device)\n",
        "bc_X_train_tensor = torch.tensor(bc_X_train, dtype=torch.float32).to(device)\n",
        "bc_X_val_tensor = torch.tensor(bc_X_val, dtype=torch.float32).to(device)\n",
        "\n",
        "# Create DataLoaders\n",
        "bc_train_dataset = TensorDataset(bc_X_train_tensor, bc_y_train_tensor)\n",
        "bc_train_loader = DataLoader(bc_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Define Model architecture\n",
        "# ---------------------------------------------------------------------------\n",
        "class BreastCancerModel(nn.Module):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        super(BreastCancerModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_shape, 25)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.layer2 = nn.Linear(25, 50)\n",
        "        self.output = nn.Linear(50, output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.output(x) # Returns logits\n",
        "        return x\n",
        "\n",
        "# Initialize Model\n",
        "input_dim = bc_X.shape[1]\n",
        "bc_class_model = BreastCancerModel(input_dim, output_dim).to(device)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Define Loss and Optimizer\n",
        "# ---------------------------------------------------------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(bc_class_model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=PATIENCE)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Training Loop\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"-- Training (classification) is starting for {EPOCHS} epochs ----------------------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "early_stop_counter = 0\n",
        "\n",
        "# Initialize history with all 4 metrics\n",
        "bc_class_history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "checkpoint_path = \"bc_best_classification_model.pth\"\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    bc_class_model.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for inputs, targets in bc_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = bc_class_model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Calculate Batch Accuracy\n",
        "        _, predicted = torch.max(outputs, 1)      # Get the class with highest probability\n",
        "        train_total += targets.size(0)            # Count total samples in batch\n",
        "        train_correct += (predicted == targets).sum().item() # Count correct predictions\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    avg_train_loss = train_loss / len(bc_train_loader)\n",
        "    avg_train_acc = train_correct / train_total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    bc_class_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = bc_class_model(bc_X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, bc_y_val_tensor).item()\n",
        "\n",
        "        # Calculate Validation Accuracy\n",
        "        _, val_predicted = torch.max(val_outputs, 1)\n",
        "        val_correct = (val_predicted == bc_y_val_tensor).sum().item()\n",
        "        val_accuracy = val_correct / bc_y_val_tensor.size(0)\n",
        "\n",
        "    # Update history with all metrics\n",
        "    bc_class_history['train_loss'].append(avg_train_loss)\n",
        "    bc_class_history['train_accuracy'].append(avg_train_acc)\n",
        "    bc_class_history['val_loss'].append(val_loss)\n",
        "    bc_class_history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Checkpoint & Early Stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stop_counter = 0\n",
        "        torch.save(bc_class_model.state_dict(), checkpoint_path)\n",
        "        print(f\"Epoch {epoch+1}: Val Loss improved to {val_loss:.4f}, Val Acc: {val_accuracy:.4f} [Saved]\")\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if epoch % 10 == 0:\n",
        "             print(f\"Epoch {epoch+1}: Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "    if early_stop_counter >= PATIENCE:\n",
        "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\nTraining complete.\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Best validation accuracy: {max(bc_class_history['val_accuracy']):.4f}\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")\n"
      ],
      "metadata": {
        "id": "5zh9ys-svnXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following final output from the training\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image24D.png)  \n",
        "\n",
        "You should notice 3 things about the output:\n",
        "\n",
        "1. The training stop well short of 200 epochs. This occurred due to Early Stopping. Since we set `PATIENCE = 20` the training waited for 20 epochs for the validation accuracy to improve after reaching a low value before \"early stopping\".\n",
        "\n",
        "2. The `Best validation accuracy` was pretty high, over 80% accurate meaning our neural network `bc_class_model` learned how to classify tumors into `malignant` or `benign` to a high degree of precision based on the clinical measurements of the tumor.\n",
        "\n",
        "3. Training time was very short, less than 2 seconds. This was accomplished using only a `cpu` without any hardware acceleration (e.g. `GPU` or `TPU`). This speed was basically a consequence of the relatively small sample size."
      ],
      "metadata": {
        "id": "fS878utAeNS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7:  Visualize Training Curves\n",
        "\n",
        "Visualizing training curvesspecifically **train loss vs. validation loss** and **train accuracy vs. validation accuracy**is incredibly useful for diagnosing and improving the performance of a neural network. Here's why:\n",
        "\n",
        "##### **Monitor Learning Progress**\n",
        "* **Train Loss/Accuracy** shows how well the model is fitting the training data.\n",
        "* **Validation Loss/Accuracy** indicates how well the model generalizes to unseen data.\n",
        "\n",
        "These curves help you understand whether the model is learning effectively or struggling.\n",
        "\n",
        "##### **Detect Overfitting**\n",
        "* If training loss keeps decreasing while validation loss starts increasing, the model is likely **overfitting**memorizing training data rather than learning general patterns.\n",
        "* Similarly, if **training accuracy increases** but **validation accuracy plateaus or drops**, it's another sign of overfitting.\n",
        "\n",
        "##### **Detect Underfitting**\n",
        "* If both training and validation metrics are poor and dont improve, the model might be **underfitting**too simple to capture the data's complexity.\n",
        "\n",
        "##### **Identify Optimal Stopping Point**\n",
        "These curves help determine when to stop training (e.g., using early stopping) to avoid wasting time and resources once validation performance stops improving.\n",
        "\n",
        "The code in the cell below uses the graphics package `matplotlib.pyplot` to generate a plot of **`training loss vs validation loss`** and a plot of **`training accuracy vs validation accuracy`**."
      ],
      "metadata": {
        "id": "YpJ9gRhmeegL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: Visualize Training\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 1: Loss (Left Graph)\n",
        "# ------------------------------------------------------------------\n",
        "# Plot Training Loss if available\n",
        "if 'train_loss' in bc_class_history:\n",
        "    ax1.plot(bc_class_history['train_loss'], label='Train Loss')\n",
        "\n",
        "# Plot Validation Loss\n",
        "ax1.plot(bc_class_history['val_loss'], label='Val Loss')\n",
        "\n",
        "ax1.set_title('Model Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 2: Accuracy (Right Graph)\n",
        "# ------------------------------------------------------------------\n",
        "# Plot Training Accuracy if available\n",
        "if 'train_accuracy' in bc_class_history:\n",
        "    ax2.plot(bc_class_history['train_accuracy'], label='Train Acc')\n",
        "\n",
        "# Plot Validation Accuracy\n",
        "ax2.plot(bc_class_history['val_accuracy'], label='Val Acc')\n",
        "\n",
        "ax2.set_title('Model Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r2uttQ3sywRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image25D.png)  \n",
        "\n",
        "Here is an analysis of these two graphs:\n",
        "\n",
        "**1. Model Loss (Left Graph)**\n",
        "* **Rapid Convergence:** Both the training loss (blue) and validation loss (orange) drop sharply within the first 58 epochs. This indicates the learning rate is appropriate and the model is quickly finding the underlying patterns in the data.\n",
        "* **Generalization:** The training and validation curves stay very close to each other for most of the training. This is a positive sign that the model is **generalizing well** and not just memorizing the training data.\n",
        "* **Potential Overfitting:** After epoch 20, the validation loss begins to fluctuate slightly and drift upward (diverge) while the training loss continues to drop. This is a classic early sign of overfitting, where the model starts to learn noise. However, the divergence is minor.\n",
        "\n",
        "**2. Model Accuracy (Right Graph)**\n",
        "* **High Performance:** The model reaches a high accuracy (>95%) very quickly.\n",
        "* **Stability:** The validation accuracy (orange) plateaus early and remains relatively stable, though it appears \"stepped\" or jagged. This is common with smaller validation sets where a single misclassified sample can cause a visible jump in the metric.\n",
        "* **Train vs. Val:** The training accuracy eventually surpasses the validation accuracy around epoch 10, which is expected behavior as the model fine-tunes itself on the training set.\n",
        "\n",
        "**3. Conclusion**\n",
        "The model is **well-trained**. It achieves high accuracy without severe overfitting. The \"Early Stopping\" mechanism likely triggered (or would trigger soon) around epoch 20-30 to prevent the rising validation loss seen at the tail end of the graph."
      ],
      "metadata": {
        "id": "D_cPBiQyd7tu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2WAXWYqhK82"
      },
      "source": [
        "### **Exercise 6:  Construct and Train _Classification_ Neural Network**\n",
        "\n",
        "In the cell below write the code to construct and train a classification neural network called `hd_class_model` to analyze the data in the DataFrame `hd_df_encoded`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change the prefix `bc_` to `hd_` **_everywhere_** in the code that you copied from Example 6.\n",
        "\n",
        "**NOTE:** If you get an error when you try to run your code, it probably means that you missed one (or more) places that had the pre-fix `bc_`.\n",
        "\n",
        "2. You will also need to change the class name when defining your model's architecture. Specifically, you need to change this code:\n",
        "\n",
        "```text\n",
        "class BreastCancerModel(nn.Module):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        super(BreastCancerModel, self).__init__()\n",
        "```\n",
        "to read as:\n",
        "\n",
        "```text\n",
        "class HeartDiseaseModel(nn.Module):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        super(HeartDiseaseModel, self).__init__()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6 here\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dFRbS5rHixRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following final output from the training\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image26D.png)  "
      ],
      "metadata": {
        "id": "pw2-CH-imgaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7:  Visualize Training**\n",
        "\n",
        "In the cell below write the code to visualize the training curves for your `hd_class_model`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Copy the code in Example 7 and change `bc_class_history` to `hd_class_history` in every instance that `bc_class_history` appears in the code."
      ],
      "metadata": {
        "id": "R1dJwj6jhK83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 7 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gWVjKk-jhK83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image27D.png)  \n",
        "\n",
        "Here is an analysis of these two graphs:\n",
        "\n",
        "#### **Model Loss (Left Graph)**\n",
        "**Observation:**\n",
        "* **The \"Generalization Gap\":** The most striking feature is the widening space between the Blue line (Train) and the Orange line (Validation).\n",
        "* **Training Behavior:** The Training Loss (Blue) drops consistently and smoothly from `0.65` down to `0.24`. The model keeps learning the training data better and better.\n",
        "* **Validation Behavior:** The Validation Loss (Orange) drops quickly at first, but hits a \"floor\" around **Epoch 5**. After that, it stays stuck around `0.45` for the remaining 45 epochs.\n",
        "\n",
        "**Diagnosis: Overfitting (High Variance)**\n",
        "* The model is **memorizing** the training data.\n",
        "* It continues to improve on the data it has seen (Blue line goes down), but it is failing to translate that improvement to new data (Orange line stays flat).\n",
        "* **Correction:** You are wasting computational resources after Epoch 5. You should likely stop training earlier (\"Early Stopping\") or increase regularization (Dropout/L2) to force the Blue and Orange lines closer together.\n",
        "\n",
        "#### **Model Accuracy (Right Graph)**\n",
        "**Observation:**\n",
        "* **Tight Convergence:** The Blue and Orange lines track each other very closely.\n",
        "* **Val > Train (The \"Cross-over\"):** You might notice that in the first 10-15 epochs, the Validation Accuracy (Orange) is actually **higher** than the Training Accuracy (Blue).\n",
        "    * *Why?* This is common when using **Dropout**. Dropout \"cripples\" the neural network during training (making it harder to learn), but is turned off during validation (allowing the full network to work). This gives the validation metrics a slight artificial boost compared to training metrics.\n",
        "* **Jagged Lines:** The Validation line is \"stepped\" or jagged. This indicates a **small validation dataset**. A single misclassified point causes a noticeable jump in the percentage.\n",
        "\n",
        "**Diagnosis: Excellent Fit**\n",
        "* This model is **healthy**.\n",
        "* The fact that the Validation Loss drops extremely low (`< 0.1`) alongside the Training Loss proves the model has learned the actual rules of the data, not just the noise.\n",
        "* The slight uptick in Validation Loss at the very end (Epoch 28-30) suggests mild overfitting is *just starting* to begin, so stopping around Epoch 25-30 is perfect."
      ],
      "metadata": {
        "id": "TvA93FYyhK84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------\n",
        "\n",
        "## **Classification vs. Regression: A General Overview**\n",
        "\n",
        "In supervised machine learning, tasks are typically categorized into **classification** or **regression**, depending on the nature of the output variable.\n",
        "\n",
        "### **Classification**\n",
        "\n",
        "- **Goal**: Predict a **discrete label** or **category**.\n",
        "- **Output**: Categorical values (e.g., \"spam\" or \"not spam\", \"cat\", \"dog\", \"bird\").\n",
        "- **Examples**:\n",
        "  - Email spam detection\n",
        "  - Disease diagnosis (e.g., predicting if a patient has a disease)\n",
        "  - Image recognition (e.g., identifying objects in photos)\n",
        "- **Algorithms**:\n",
        "  - Logistic Regression\n",
        "  - Decision Trees\n",
        "  - Random Forest\n",
        "  - Support Vector Machines (SVM)\n",
        "  - **Neural Networks (for multi-class classification)**\n",
        "\n",
        "### **Regression**\n",
        "\n",
        "- **Goal**: Predict a **continuous value**.\n",
        "- **Output**: Real numbers (e.g., price, temperature, age).\n",
        "- **Examples**:\n",
        "  - Predicting medical costs\n",
        "  - Estimating a person's weight based on height\n",
        "  - Forecasting the spread of infections\n",
        "- **Algorithms**:\n",
        "  - Linear Regression\n",
        "  - Polynomial Regression\n",
        "  - Decision Trees\n",
        "  - Random Forest\n",
        "  - **Neural Networks (for regression tasks)**\n",
        "\n",
        "##### **Key Differences Between Classification and Regression**\n",
        "\n",
        "| Feature | Classification                 | Regression                       |\n",
        "|---------|--------------------------------|----------------------------------|\n",
        "| Output  | Discrete categories            | Continuous values                |\n",
        "| Evaluation | Accuracy, Precision, Recall, F1 Score | MSE, RMSE, MAE, R |\n",
        "| Use Case | Label prediction               | Value estimation                 |\n",
        "\n",
        "-------------------------------"
      ],
      "metadata": {
        "id": "XaMmJYIrUtaf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SytSo7xnbE3i"
      },
      "source": [
        "## **Generate Feature Vectors for a Regression Neural Network**\n",
        "\n",
        "As mentioned above, the procedure for generating `X-` and `Y ` feature vectors for a regression neural network is somewhat different the procedure used above. Even though these differences are not large, they are important. If your `X` and `Y` feature vectors are not generated in the correct format, your neural network will not compile and run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfYNaXbKbE3i"
      },
      "source": [
        "### Example 8: Generate Feature Vectors for _Regression_ Neural Network\n",
        "\n",
        "For regression, we want to predict a variable that has a **_range of values_**. In other words, we are looking for a variable that can have a range of different values. For Example 8, are target column will be the `mean_area` of the breast cancer tumor. For the 569 tumor samples in our Breast Cancer dataset, the mean area is $655 \\pm 352 \\text{ mm}^2$ ($\\pm$ STD). This is quite a range of values with a maximum turmor size equal to $2501 \\text{ mm}^2$ (See Appendix below).\n",
        "\n",
        "The goal of our `bc_class_model` neural network model will be to predict the area of any particular breast tumor based on the turmor's other measurements (`radius`, `texture`, `perimeter`, `smoothness`, etc.)  \n",
        "\n",
        "In general, the code is similar to that should above in Example 5."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 8: Generate Feature Vectors for Regression Neural Network\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Identify feature / target columns\n",
        "# ------------------------------------------------------------------\n",
        "TARGET_COL = \"mean_area\"  # continuous variable we want to predict\n",
        "\n",
        "# 1. Create the X DataFrame by dropping the target column\n",
        "bc_X_df = bc_df_encoded.drop(columns=[TARGET_COL])\n",
        "\n",
        "# 2. Create the y Series (Target)\n",
        "bc_y_temp = bc_df_encoded[TARGET_COL]\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# DEBUG: Check data types before normalization\n",
        "# ------------------------------------------------------------------\n",
        "print(\"Data types in bc_X_df:\")\n",
        "print(bc_X_df.dtypes)\n",
        "print(\"\\nColumns with non-numeric types:\")\n",
        "non_numeric_cols = bc_X_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "print(non_numeric_cols)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Intelligent Normalization (Standardize Continuous, Skip Binary)\n",
        "# ------------------------------------------------------------------\n",
        "# Create a copy to avoid warnings\n",
        "bc_X_normalized = bc_X_df.copy()\n",
        "\n",
        "for col in bc_X_normalized.columns:\n",
        "    # Skip non-numeric columns entirely\n",
        "    if bc_X_normalized[col].dtype == 'object' or bc_X_normalized[col].dtype.name == 'category':\n",
        "        print(f\"Warning: Skipping non-numeric column '{col}' with dtype {bc_X_normalized[col].dtype}\")\n",
        "        continue\n",
        "\n",
        "    # Check if the column contains only 0s and 1s\n",
        "    unique_vals = bc_X_normalized[col].dropna().unique()\n",
        "    is_binary = np.array_equal(np.sort(unique_vals), np.array([0, 1])) or \\\n",
        "                np.array_equal(np.sort(unique_vals), np.array([0])) or \\\n",
        "                np.array_equal(np.sort(unique_vals), np.array([1]))\n",
        "\n",
        "    # If it is NOT binary, apply Z-score normalization\n",
        "    if not is_binary:\n",
        "        mean = bc_X_normalized[col].mean()\n",
        "        std = bc_X_normalized[col].std()\n",
        "        if std != 0:\n",
        "            bc_X_normalized[col] = (bc_X_normalized[col] - mean) / std\n",
        "\n",
        "# Convert X to Numpy (only numeric columns)\n",
        "bc_X_normalized_numeric = bc_X_normalized.select_dtypes(include=[np.number])\n",
        "bc_X = bc_X_normalized_numeric.values\n",
        "bc_Y = bc_y_temp.values\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Split into train / test sets\n",
        "# ------------------------------------------------------------------\n",
        "test_size = 0.2\n",
        "\n",
        "bc_X_train, bc_X_val, bc_y_train, bc_y_val = train_test_split(\n",
        "    bc_X, bc_Y,\n",
        "    test_size=test_size,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Scale the target (Crucial for Regression NN convergence)\n",
        "# ------------------------------------------------------------------\n",
        "scale_y = True\n",
        "y_scaler = None\n",
        "\n",
        "if scale_y:\n",
        "    y_scaler = StandardScaler()\n",
        "    bc_y_train = y_scaler.fit_transform(bc_y_train.reshape(-1, 1)).ravel()\n",
        "    bc_y_val = y_scaler.transform(bc_y_val.reshape(-1, 1)).ravel()\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Convert to PyTorch Tensors\n",
        "# ------------------------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "X_train_tensor = torch.tensor(bc_X_train, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(bc_y_train, dtype=torch.float32).to(device)\n",
        "\n",
        "X_val_tensor = torch.tensor(bc_X_val, dtype=torch.float32).to(device)\n",
        "y_val_tensor = torch.tensor(bc_y_val, dtype=torch.float32).to(device)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Inspect the first few rows\n",
        "# ------------------------------------------------------------------\n",
        "np.set_printoptions(suppress=True, precision=4)\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {bc_X_train.shape}\")\n",
        "print(f\"Target matrix shape: {bc_y_train.shape}\")\n",
        "\n",
        "print(\"\\nFirst 4 rows of processed X (Train):\")\n",
        "print(X_train_tensor[:4].cpu().numpy())\n",
        "\n",
        "print(\"\\nCorresponding scaled targets (Train):\")\n",
        "print(y_train_tensor[:4].cpu().numpy())\n"
      ],
      "metadata": {
        "id": "tnqtKfc95FWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image28D.png)  "
      ],
      "metadata": {
        "id": "fivH43LLKhha"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mLXqkStK53s"
      },
      "source": [
        "### **Exercise 8: Generate Feature Vectors for Regression Neural Network**\n",
        "\n",
        "In the cell below write the code to generate feature vectors for your `hd_df` DataFrame. Set your `TARGET_COL=\"MaxHR\"` since you are to predict the maximum heart rate.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Change the prefix `bc_` to `hd_` everywhere in the code that you copied."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 8 here\n",
        "\n"
      ],
      "metadata": {
        "id": "D34sPMMbxOb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image29D.png)  "
      ],
      "metadata": {
        "id": "gnnHGygfK53t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------\n",
        "\n",
        "## **Classification vs. Regression in Neural Networks**\n",
        "\n",
        "While **classification** and **regression** are fundamentally different tasksclassification predicts **discrete categories**, whereas regression predicts **continuous values**the **core architecture** of the neural networks used for both can be remarkably similar. This includes shared components like:\n",
        "\n",
        "- Hidden layers\n",
        "- Activation functions (e.g., ReLU)\n",
        "- Optimizers (e.g., Adam)\n",
        "\n",
        "However, there are **critical differences** that must be addressed when switching between these tasks. These differences primarily affect the **output layer**, **loss function**, **label representation**, and **evaluation metrics**.\n",
        "\n",
        "#### **Key Differences**\n",
        "\n",
        "| Component      | Classification                                            | Regression                          |\n",
        "|----------------|-----------------------------------------------------------|-------------------------------------|\n",
        "| Output Layer   | `Dense(num_classes, activation='softmax')`                | `Dense(1)` (linear activation by default) |\n",
        "| Loss Function  | `categorical_crossentropy` or `sparse_categorical_crossentropy` | `mean_squared_error`, `mean_absolute_error` |\n",
        "| Label Format   | Onehot encoded or integer class labels                  | Continuous numeric values           |\n",
        "| Metrics        | `accuracy`, `precision`, `recall`                        | `mse`, `mae`, `r`                  |\n",
        "\n",
        "#### **Summary**\n",
        "In essence, while the internal structure of classification and regression models can be nearly identical, the task-specific componentsespecially the output layer and loss functionmust be carefully tailored to the nature of the prediction problem. This ensures that the model learns appropriately and that its performance is evaluated meaningfully.\n",
        "\n",
        "-----------------------------------"
      ],
      "metadata": {
        "id": "B7VquxyU56-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 9: Construct, Compile and Train _Regression_ Neural Network\n",
        "\n",
        "The code in the cell below performs a regression analysis of the data in the Breast Cancer Wisconsin dataset store in the `hd_df` DataFrame.\n",
        "\n",
        "Here is the code that builds the neural network\n",
        "```text\n",
        "class EgBreastCancerRegressionModel(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(EgBreastCancerRegressionModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_shape, 25)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.layer2 = nn.Linear(25, 50)\n",
        "        self.output = nn.Linear(50, 1) # Linear output for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "# Initialize Model\n",
        "input_dim = bc_X_train_tensor.shape[1]\n",
        "bc_reg_model = EgBreastCancerRegressionModel(input_dim).to(device)\n",
        "\n",
        "```\n",
        "You should note that name of our regression model is `bc_reg_model`.\n",
        "As before, we will train our model `bc_class_model` for `200` epochs unless the `EarlyStopping` callback kicks in sooner."
      ],
      "metadata": {
        "id": "Aug3ZvCsPmsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 9: Construct and Train Regression Neural Network\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Define parameters\n",
        "# ---------------------------------------------------------------------------\n",
        "EPOCHS = 200\n",
        "PATIENCE = 20\n",
        "lr = 0.0010\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# DATA SETUP: Create Unique Tensors for this Example\n",
        "# ---------------------------------------------------------------------------\n",
        "bc_X_train_tensor = torch.tensor(bc_X_train, dtype=torch.float32).to(device)\n",
        "bc_y_train_tensor = torch.tensor(bc_y_train, dtype=torch.float32).to(device)\n",
        "\n",
        "bc_X_val_tensor = torch.tensor(bc_X_val, dtype=torch.float32).to(device)\n",
        "bc_y_val_tensor = torch.tensor(bc_y_val, dtype=torch.float32).to(device)\n",
        "\n",
        "# Create DataLoader\n",
        "bc_train_dataset = TensorDataset(bc_X_train_tensor, bc_y_train_tensor)\n",
        "bc_train_loader = DataLoader(bc_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Define Model Architecture\n",
        "# ---------------------------------------------------------------------------\n",
        "class BreastCancerRegressionModel(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(BreastCancerRegressionModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_shape, 25)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.layer2 = nn.Linear(25, 50)\n",
        "        self.output = nn.Linear(50, 1) # Linear output for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "# Initialize Model\n",
        "input_dim = bc_X_train_tensor.shape[1]\n",
        "bc_reg_model = BreastCancerRegressionModel(input_dim).to(device)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Define Loss and Optimizer\n",
        "# ---------------------------------------------------------------------------\n",
        "criterion = nn.MSELoss()\n",
        "mae_metric = nn.L1Loss()\n",
        "\n",
        "bc_reg_optimizer = optim.Adam(bc_reg_model.parameters(), lr=lr)\n",
        "bc_reg_scheduler = optim.lr_scheduler.ReduceLROnPlateau(bc_reg_optimizer, mode='min', factor=0.5, patience=PATIENCE)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Training Loop\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"-- Training (regression) starting for {EPOCHS} epochs --\")\n",
        "start_time = time.time()\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "early_stop_counter = 0\n",
        "\n",
        "bc_reg_history = {'train_loss': [], 'train_mae': [], 'val_loss': [], 'val_mae': []}\n",
        "checkpoint_path = \"bc_reg_best_regression_model.pth\"\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    bc_reg_model.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_mae = 0.0\n",
        "\n",
        "    for inputs, targets in  bc_train_loader:\n",
        "        bc_reg_optimizer.zero_grad()\n",
        "        outputs = bc_reg_model(inputs)\n",
        "\n",
        "        # Reshape targets to match output (batch, 1)\n",
        "        targets = targets.view(-1, 1)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        mae = mae_metric(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        bc_reg_optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_mae += mae.item()\n",
        "\n",
        "    # Calculate average epoch metrics\n",
        "    avg_train_loss = train_loss / len( bc_train_loader)\n",
        "    avg_train_mae = train_mae / len( bc_train_loader)\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    bc_reg_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = bc_reg_model( bc_X_val_tensor)\n",
        "        val_targets_reshaped =  bc_y_val_tensor.view(-1, 1)\n",
        "\n",
        "        val_loss = criterion(val_outputs, val_targets_reshaped).item()\n",
        "        val_mae = mae_metric(val_outputs, val_targets_reshaped).item()\n",
        "\n",
        "    # Update history\n",
        "    bc_reg_history['train_loss'].append(avg_train_loss)\n",
        "    bc_reg_history['train_mae'].append(avg_train_mae)\n",
        "    bc_reg_history['val_loss'].append(val_loss)\n",
        "    bc_reg_history['val_mae'].append(val_mae)\n",
        "\n",
        "    bc_reg_scheduler.step(val_loss)\n",
        "\n",
        "    # Checkpoint & Early Stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stop_counter = 0\n",
        "        torch.save(bc_reg_model.state_dict(), checkpoint_path)\n",
        "        print(f\"Epoch {epoch+1}: Val Loss improved to {val_loss:.4f}, Val MAE: {val_mae:.4f} [Saved]\")\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if epoch % 10 == 0:\n",
        "             print(f\"Epoch {epoch+1}: Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}\")\n",
        "\n",
        "    if early_stop_counter >= PATIENCE:\n",
        "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\nTraining complete.\")\n",
        "print(f\"Best validation Loss (MSE): {best_val_loss:.4f}\")\n",
        "print(f\"Best validation MAE: {min(bc_reg_history['val_mae']):.4f}\")\n",
        "\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return f\"{h}:{m:>02}:{s:>05.2f}\"\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")"
      ],
      "metadata": {
        "id": "IbG0FpgwBKPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following final output from the training\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image13D.png)  \n",
        "\n",
        "In the example above, EarlyStopping terminated the training at `Epoch 87` after only `18` seconds of training."
      ],
      "metadata": {
        "id": "9aecl0mFT0-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 9: Construct, Compile and Train Regression Neural Network**\n",
        "\n",
        "In the cell below write the code to contruct, compile and train a regression neural network called `hd_class_model` on the Heart Disease dataset using the `X-` and `y-` feature vectors that you created in **Exercise 8**.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Change the prefix `bc_` to `hd_` everywhere in the code that you copied from Example 9."
      ],
      "metadata": {
        "id": "bw7Z5Mk_UO7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 9 here\n",
        "\n"
      ],
      "metadata": {
        "id": "kAMUbn3DBYIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following final output from the training\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image14D.png)  "
      ],
      "metadata": {
        "id": "2mVrbU_-UO7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 10: Plot Predicted vs Actual Values\n",
        "\n",
        "The code in the cell below shows the code needed to plot the Mean Tumor size predicted by the regression `bc_class_model` vs the Actual tumor size using the common Python plotting program `matplotlib.pyplot`."
      ],
      "metadata": {
        "id": "7_nXgFc8Wnze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 10: Plot Predicted vs Actual Values\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# 1. Set model to evaluation mode\n",
        "bc_reg_model.eval()\n",
        "\n",
        "# 2. Predict using validation tensor\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = bc_reg_model( bc_X_val_tensor)\n",
        "    y_pred_scaled = y_pred_tensor.cpu().numpy()\n",
        "\n",
        "# 3. Inverse transform (Uses y_scaler from Ex 8)\n",
        "y_pred = y_scaler.inverse_transform(y_pred_scaled).ravel()\n",
        "y_true = y_scaler.inverse_transform(bc_y_val.reshape(-1, 1)).ravel()\n",
        "\n",
        "# 4. Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(y_true, y_pred, alpha=0.6, color='blue', edgecolor='k')\n",
        "\n",
        "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()],\n",
        "         'r--', lw=2, label='Perfect Prediction')\n",
        "\n",
        "plt.xlabel(\"Actual Mean Area\")\n",
        "plt.ylabel(\"Predicted Mean Area\")\n",
        "plt.title(\"Predicted vs Actual (Regression)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8RX1CbZk-5iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image15D.png)  \n",
        "\n",
        "Based on the scatter plot of **Predicted vs. Actual Values** in the Wisconsin Breast Cancer dataset, we can conclude the following:\n",
        "\n",
        "\n",
        "**Overall Performance: Excellent Fit**\n",
        "* **Strong Correlation:** The blue data points are tightly clustered around the red dashed line (which represents a perfect prediction where $Actual = Predicted$). This indicates a very high correlation coefficient ($R^2$), meaning your model explains the variance in the target variable (`mean_area`) extremely well.\n",
        "* **Linearity:** The relationship is clearly linear across the entire range of values (from approx. 100 to 220). There is no obvious curvature, suggesting the neural network has successfully captured the linear relationship between the input features and the target.\n",
        "\n",
        "**Bias and Variance**\n",
        "* **Low Variance (Precision):** The spread of the points away from the red line is minimal. This means the model's predictions are consistent and precise.\n",
        "* **Low Bias (Accuracy):** There is no significant systematic error. The points are distributed fairly evenly above and below the red line. The model isn't consistently over-predicting or under-predicting at any specific range.\n",
        "\n",
        "**Subtle Observations**\n",
        "* **Slight Heteroscedasticity:** If you look closely at the higher values (top right, area > 180), the points are slightly more spread out compared to the lower values (bottom left, area < 120). This is common in regression; predicting larger values often comes with slightly higher absolute error than predicting smaller, more common values.\n",
        "* **Outliers:** There are very few significant outliers. One point around `Actual=210` is slightly under-predicted, but it is still reasonably close to the line.\n",
        "\n",
        "**Conclusion**\n",
        "This graph indicates a **highly successful regression model**. The model is robust, reliable, and capable of predicting the `mean_area` of tumor nuclei with high accuracy. No major changes (like architecture complexity or regularization) are immediately necessary based on this plot."
      ],
      "metadata": {
        "id": "8kOt86ye-YSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 10: Plot Predicted vs Actual Values**\n",
        "\n",
        "In the cell below write the code needed to plot the Maximum Heart Rate (`MaxHR`) predicted by your regression `hd_class_model` vs the Actual `MaxHR` using the common Python plotting program `matplotlib.pyplot`."
      ],
      "metadata": {
        "id": "TAZuFXb18tsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 10 here\n",
        "\n"
      ],
      "metadata": {
        "id": "NEIBaQHABuzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![_ _](https://biologicslab.co/BIO1173/images/class_02/class_02_2_image30D.png)  \n",
        "\n",
        "Based on the scatter plot of **Predicted vs Actual Values** from the Heart Disease dataset, heres what we can conclude:\n",
        "\n",
        "**Visual Overview**\n",
        "* **Graph Type:** A scatter plot comparing the **Actual Values** (Ground Truth) on the X-axis against the **Predicted Values** (Model Output) on the Y-axis.\n",
        "* **Reference Line:** The red dashed line indicates perfect prediction ($y=x$).\n",
        "\n",
        "**Key Observations (A \"Noisy\" Fit)**\n",
        "* **Moderate Correlation:** Unlike the previous Breast Cancer example (which was very tight), this graph shows a much \"cloudier\" or more scattered pattern. While there is definitely a positive trend (as Actual increases, Predicted increases), the relationship is not nearly as strong.\n",
        "* **High Variance:** The blue points are spread far away from the red line. For example, for an **Actual Value of 130**, the model predicts values ranging anywhere from **110 to 170**. This indicates a high error margin for individual predictions.\n",
        "\n",
        "**Systematic Bias: \"Regression to the Mean\"**\n",
        "A specific error pattern is visible here:\n",
        "* **Over-predicting Low Values:** Look at the left side (Actual < 100). Most blue points are *above* the red line. The model thinks these patients have higher values than they actually do.\n",
        "* **Under-predicting High Values:** Look at the right side (Actual > 160). Most blue points are *below* the red line. The model fails to capture the severity of the highest cases.\n",
        "* **Interpretation:** The model is playing it safe. Instead of boldly predicting high or low extremes, it tends to guess numbers closer to the **average** (mean) of the dataset.\n",
        "\n",
        "**Conclusion**\n",
        "* **Model Status:** **Sub-optimal / Needs Improvement.**\n",
        "* **Diagnosis:** The model has learned the general direction of the data but struggles with precision. This is common in medical datasets where the target variable (like heart disease progression) depends on complex factors not fully captured by the available input features.\n",
        "* **Next Steps:** To improve this, you would typically need **more training data**, **better features**, or **hyperparameter tuning** (e.g., deeper network, different learning rate)."
      ],
      "metadata": {
        "id": "FmnUwRTX8sYq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MhC_-6ebE3l"
      },
      "source": [
        "# **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Microsoft Print to PDF** in your are running Microsoft Windows to generate a PDF of your Colab notebook. If you are on a Mac, use the **File --> Print --> Save as PDF**.  Call your PDF as `Class_02_2_lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas for grading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgxZvma-bE3l"
      },
      "source": [
        "## Appendix\n",
        "\n",
        "The code in the cells use the Pandas method `pd.describe()` to print out a statistical summary of the column `mean_area` in the `Wisconsin Breast Cancer` dataset and the column `MaxHR` in the `Heart Disease` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Dkl5Os1bE3l"
      },
      "outputs": [],
      "source": [
        "bc_df['mean_area'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **mean_area** variable shows a wide range of tumor sizes with a positively skewed distribution. This suggests that while most tumors are relatively small to moderately sized, there are a few with very large areas that could be clinically significant and may warrant further investigation."
      ],
      "metadata": {
        "id": "xlAiywFE47RR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pdjo4WFsbE3m"
      },
      "outputs": [],
      "source": [
        "hd_df['MaxHR'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **MaxHR** variable shows a relatively symmetric distribution with a moderate spread. Most individuals have a maximum heart rate between **110** and **160**, but there are a few with very high values (up to 202), which could be outliers or clinically significant cases."
      ],
      "metadata": {
        "id": "BbZO7Vv86TaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard's Tail**\n",
        "\n",
        "## **UNIVAC**\n",
        "\n",
        "![___](https://upload.wikimedia.org/wikipedia/commons/2/2f/Univac_I_Census_dedication.jpg)\n",
        "\n",
        "**UNIVAC (Universal Automatic Computer)** was a line of electronic digital stored-program computers starting with the products of the EckertMauchly Computer Corporation. Later the name was applied to a division of the Remington Rand company and successor organizations.\n",
        "\n",
        "### **Historical Overview of UNIVAC**\n",
        "\n",
        "**UNIVAC** (Universal Automatic Computer) was the first commercially produced digital computer in the United States. It was designed primarily for business and administrative use, marking a significant shift from earlier computers that were mostly used for scientific and military purposes.\n",
        "\n",
        "#### Key Milestones\n",
        "\n",
        "- **19461951**: Developed by **J. Presper Eckert** and **John Mauchly**, the creators of the ENIAC, under the company **Eckert-Mauchly Computer Corporation**.\n",
        "- **1951**: The first UNIVAC I was delivered to the **U.S. Census Bureau**.\n",
        "- **1952**: UNIVAC I gained national attention when it successfully predicted the outcome of the U.S. presidential election on live television, favoring Eisenhower over Stevenson.\n",
        "\n",
        "#### Technical Specifications\n",
        "\n",
        "- **Memory**: Used mercury delay lines for memory storage.\n",
        "- **Storage**: Featured magnetic tape for data storage, a novel concept at the time.\n",
        "- **Speed**: Could perform about 1,000 calculations per second.\n",
        "- **Size**: Occupied over 35 square meters and weighed approximately 13 tons.\n",
        "\n",
        "#### Impact and Legacy\n",
        "\n",
        "- UNIVAC I was the first computer to be widely used for **business applications**, including payroll, inventory, and accounting.\n",
        "- It helped establish the **commercial computer industry**, paving the way for companies like IBM to enter the market.\n",
        "- The name \"UNIVAC\" became synonymous with \"computer\" in the 1950s and early 1960s.\n",
        "\n",
        "#### Fun Fact\n",
        "\n",
        "The UNIVAC I's prediction of the 1952 election was so unexpected that CBS initially hesitated to air it. The prediction turned out to be accurate, boosting public confidence in computing technology.\n",
        "\n",
        "---\n",
        "\n",
        "> UNIVAC represents a pivotal moment in computing history, transitioning from experimental machines to practical tools that shaped modern data processing.\n",
        "\n",
        "\n",
        "The BINAC, built by the EckertMauchly Computer Corporation, was the first general-purpose computer for commercial use, but it was not a success. The last UNIVAC-badged computer was produced in 1986.\n",
        "\n",
        "### **History and structure**\n",
        "\n",
        "**UNIVAC Sperry Rand label**\n",
        "\n",
        "J. Presper Eckert and John Mauchly built the ENIAC (Electronic Numerical Integrator and Computer) at the University of Pennsylvania's Moore School of Electrical Engineering between 1943 and 1946. A 1946 patent rights dispute with the university led Eckert and Mauchly to depart the Moore School to form the Electronic Control Company, later renamed EckertMauchly Computer Corporation (EMCC), based in Philadelphia, Pennsylvania. That company first built a computer called BINAC (BINary Automatic Computer) for Northrop Aviation (which was little used, or perhaps not at all). Afterwards, the development of UNIVAC began in April 1946.[1] UNIVAC was first intended for the Bureau of the Census, which paid for much of the development, and then was put in production.\n",
        "\n",
        "With the death of EMCC's chairman and chief financial backer Henry L. Straus in a plane crash on October 25, 1949, EMCC was sold to typewriter, office machine, electric razor, and gun maker Remington Rand on February 15, 1950. Eckert and Mauchly now reported to Leslie Groves, the retired army general who had previously managed building The Pentagon and led the Manhattan Project.\n",
        "\n",
        "The most famous UNIVAC product was the UNIVAC I mainframe computer of 1951, which became known for predicting the outcome of the U.S. presidential election the following year: this incident is noteworthy because the computer correctly predicted an Eisenhower landslide over Adlai Stevenson, whereas the final Gallup poll had Eisenhower winning the popular vote 5149 in a close contest.\n",
        "\n",
        "The prediction led CBS's news boss in New York, Siegfried Mickelson, to believe the computer was in error, and he refused to allow the prediction to be read. Instead, the crew showed some staged theatrics that suggested the computer was not responsive, and announced it was predicting 87 odds for an Eisenhower win (the actual prediction was 1001 in his favour).\n",
        "\n",
        "When the predictions proved trueEisenhower defeated Stevenson in a landslide, with UNIVAC coming within 3.5% of his popular vote total and four votes of his Electoral College totalCharles Collingwood, the on-air announcer, announced that they had failed to believe the earlier prediction.\n",
        "\n",
        "The United States Army requested a UNIVAC computer from Congress in 1951. Colonel Wade Heavey explained to the Senate subcommittee that the national mobilization planning involved multiple industries and agencies: \"This is a tremendous calculating process...there are equations that can not be solved by hand or by electrically operated computing machines because they involve millions of relationships that would take a lifetime to figure out.\" Heavey told the subcommittee it was needed to help with mobilization and other issues similar to the invasion of Normandy that were based on the relationships of various groups.\n",
        "\n",
        "The UNIVAC was manufactured at Remington Rand's former Eckert-Mauchly Division plant on W Allegheny Avenue in Philadelphia, Pennsylvania. Remington Rand also had an engineering research lab in Norwalk, Connecticut, and later bought Engineering Research Associates (ERA) in St. Paul, Minnesota. In 1953 or 1954 Remington Rand merged their Norwalk tabulating machine division, the ERA \"scientific\" computer division, and the UNIVAC \"business\" computer division into a single division under the UNIVAC name. This severely annoyed those who had been with ERA and with the Norwalk laboratory.\n",
        "\n",
        "In 1955 Remington Rand merged with Sperry Corporation to become Sperry Rand. General Douglas MacArthur, then the chairman of the Board of Directors of Remington Rand, was chosen to continue in that role in the new company. Harry Franklin Vickers, then the President of Sperry Corporation, continued as president and CEO of Sperry Rand. The UNIVAC division of Remington Rand was renamed the Remington Rand Univac division of Sperry Rand. William Norris was put in charge as Vice-President and General Manager reporting to the President of the Remington Rand Division (of Sperry Rand).\n",
        "\n",
        "## **UNIVAC - A Quick Snapshot**\n",
        "\n",
        "| Topic | Key Facts |\n",
        "|-------|-----------|\n",
        "| **Founded** | 1946 by J. Presper Eckert & John Mauchly (the same inventors of the ENIAC) |\n",
        "| **Full Name** | _Univac, Inc._ (short for *UNiversal Automatic Computer*) |\n",
        "| **First Product** | **UNIVAC I**  the worlds first commercially available electronic digital computer |\n",
        "| **First U.S. Government Use** | 1949: The U.S. Census Bureau used UNIVAC I to process the 1950 census in a record 30 days |\n",
        "| **First Commercial Sales** | 1950: UNIVAC I sold to the U.S. Department of Defense and later to companies like AT&T |\n",
        "| **Notable Programs** | 1952: Ran the first computersimulated nuclear war scenario for the U.S. Strategic Air Command |\n",
        "| **MassMarket Success** | 1950s1960s: UNIVAC sold more than 300 computers, dominating the early mainframe market |\n",
        "| **Key Innovations** |  Use of **vacuum tubes** and later **transistors** in a single product line<br> First to use magnetic drum memory for data storage<br> Developed the **UNIVAC 1107**  the worlds first minicomputer in 1962 |\n",
        "| **Corporate Evolution** | 1976: Acquired by Sperry Corporation  became **Sperry UNIVAC**<br>1990: Merged with Burroughs to form **Unisys** |\n",
        "| **Legacy** |  Introduced the phrase *computing* to mainstream culture<br> Paved the way for the **computer revolution** in business and government<br> Influenced the development of **software engineering** and **computer architecture** |\n",
        "| **Fun Trivia** |  The name UNIVAC was chosen by a contest in 1946  the winning entry was a 7word sentence: Universal Computer, for Universal Use.<br> UNIVACs first machine was a 90meter tall building in Kansas Citys industrial park.<br> The 1977 UNIVAC 1108 was used by NASAs Apollo missions for trajectory calculations. |\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Takeaway\n",
        "\n",
        "UNIVAC didnt just build computers; it built the **foundation of modern computing**. From the first commercial mainframe to pioneering minicomputers, its innovations shaped the entire industry and made the concept of software and programming part of everyday language. Even today, the legacy lives on in the name **Unisys**, the company that traces its lineage back to these early pioneers.\n",
        "\n",
        "## **The History and Legacy of UNIVAC**\n",
        "\n",
        "**UNIVAC** (Universal Automatic Computer) stands as a monumental pillar in the history of computing, representing the transition from experimental, government-funded projects to the dawn of the commercial computer industry. Originally developed by J. Presper Eckert and John Mauchly, the inventors of the ENIAC, UNIVAC was the first general-purpose electronic digital computer design for business and administrative use in the United States.\n",
        "\n",
        "#### **Origins and Early Struggles**\n",
        "The story of UNIVAC began in 1946 when Eckert and Mauchly left the University of Pennsylvania's Moore School of Electrical Engineering following a patent dispute. They formed the **Eckert-Mauchly Computer Corporation (EMCC)** in Philadelphia. Their initial goal was to build a machine for the Bureau of the Census. However, financial difficulties plagued the young company. Following the tragic death of their chief financial backer, Henry L. Straus, in a plane crash in 1949, EMCC was sold to **Remington Rand** in 1950. This acquisition provided the necessary capital to bring their vision to life, with Eckert and Mauchly staying on to report to Leslie Groves, the retired general who had managed the Manhattan Project.\n",
        "\n",
        "#### **The UNIVAC I and the 1952 Election**\n",
        "The first fruit of this labor was the **UNIVAC I**, delivered to the U.S. Census Bureau in 1951. It was a technological marvel of its time, utilizing mercury delay lines for memory and magnetic tape for data storagea significant innovation over the punch cards used by competitors like IBM. The machine weighed approximately 13 tons and occupied over 35 square meters of floor space.\n",
        "\n",
        "UNIVAC gained legendary status during the **1952 U.S. Presidential Election**. CBS News used the computer to predict the outcome between Dwight D. Eisenhower and Adlai Stevenson. With only a small sample of votes counted, UNIVAC predicted an Eisenhower landslide (100-to-1 odds), contradicting widespread polling that predicted a close race. CBS executives, fearing a humiliating error, refused to air the prediction initially. When the final results confirmed Eisenhower's massive victorywithin 3.5% of UNIVAC's calculationit cemented the computer's reputation for accuracy and introduced the public to the power of data processing.\n",
        "\n",
        "#### **Corporate Evolution**\n",
        "The brand's history is also one of complex corporate mergers. In 1955, Remington Rand merged with the Sperry Corporation to form **Sperry Rand**. The UNIVAC division continued to innovate, producing successors like the UNIVAC 1107 and 1108, which were used in NASA's Apollo missions. By the 1960s, UNIVAC was one of the few competitors to IBM, often referred to as part of \"the BUNCH\" (Burroughs, UNIVAC, NCR, Control Data, and Honeywell).\n",
        "\n",
        "#### **Legacy**\n",
        "The UNIVAC name eventually faded as the industry consolidated; Sperry merged with Burroughs in 1986 to form **Unisys**, which exists today. However, UNIVAC's legacy is undeniable. It was the first computer to handle both numerical and textual data effectively, making it suitable for business tasks like payroll and inventory, not just scientific calculation. For a generation, \"UNIVAC\" was synonymous with \"computer,\" setting the stage for the digital revolution that followed.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JjtoU6RQk199"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6Mup7QGl-A8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}