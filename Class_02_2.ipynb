{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/master/Class_02_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BIO 1173: Intro Computational Biology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module 2: Machine Learning**\n",
    "\n",
    "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
    "\n",
    "\n",
    "### Module 2 Material\n",
    "\n",
    "* Part 2.1: Pandas DataFrame Operations\n",
    "* **Part 2.2: Categorical Values** \n",
    "* Part 2.3: Grouping, Sorting and Shuffling on Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Setup\n",
    "\n",
    "Run the next code cell to load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You MUST run this code cell first\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "path = '/'\n",
    "memory = shutil.disk_usage(path)\n",
    "dirpath = os.getcwd()\n",
    "print(\"Your current working directory is : \" + dirpath)\n",
    "print(\"Disk\", memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.2: Categorical and Continuous Values\n",
    "\n",
    "Neural networks require their input to be a fixed number of columns. This input format is very similar to spreadsheet data; it must be entirely numeric. It is essential to represent the data so that the neural network can train from it. Before we look at specific ways to preprocess data, it is important to consider four basic types of data, as defined by [[Cite:stevens1946theory]](http://psychology.okstate.edu/faculty/jgrice/psyc3214/Stevens_FourScales_1946.pdf). Statisticians commonly refer to as the [levels of measure](https://en.wikipedia.org/wiki/Level_of_measurement):\n",
    "\n",
    "* Character Data (strings)\n",
    "    * **Nominal** - Individual discrete items, no order. For example, color, zip code, and shape.\n",
    "    * **Ordinal** - Individual distinct items have an implied order. For example, grade level, job title, Starbucks(tm) coffee size (tall, vente, grande) \n",
    "* Numeric Data\n",
    "    * **Interval** - Numeric values, no defined start.  For example, temperature. You would never say, \"yesterday was twice as hot as today.\"\n",
    "    * **Ratio** - Numeric values, clearly defined start.  For example, speed. You could say, \"The first car is going twice as fast as the second.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets for Class_02_2\n",
    "\n",
    "In this class we will be using the **_Obesity Prediction_** dataset for the Examples and the **_Heart Failure_** dataset for the **Exercises**. Both of these datasets will be downloaded from the course HTTPS server [https://biologicslab.co](https://biologicslab.co)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Obesity Prediction Dataset\n",
    "\n",
    "[Obesity Prediction Dataset](https://www.kaggle.com/datasets/mrsimple07/obesity-prediction)\n",
    "\n",
    "**Description**\n",
    "\n",
    "The dataset provides comprehensive information on individuals' demographic characteristics, physical attributes, and lifestyle habits, aiming to facilitate the analysis and prediction of obesity prevalence. It includes key variables such as age, gender, height, weight, body mass index (BMI), physical activity level, and obesity category. \n",
    "\n",
    "* **Age:** The age of the individual, expressed in years.\n",
    "* **Gender:** The gender of the individual, categorized as male or female.\n",
    "* **Height:** The height of the individual, typically measured in centimeters or inches.\n",
    "* **Weight:** The weight of the individual, typically measured in kilograms or pounds.\n",
    "* **BMI:** A calculated metric derived from the individual's weight and height\n",
    "* **PhysicalActivityLevel:** This variable quantifies the individual's level of physical activity\n",
    "* **ObesityCategory:** Categorization of individuals based on their BMI into different obesity categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Read data file and create Pandas DataFrame\n",
    "\n",
    "The cell below use the Pandas `read_csv()` method to read the `obesity_prediction.csv` file using the code chunk below:\n",
    "~~~text\n",
    "opDF = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/obesity_prediction.csv\",\n",
    "    na_values=['NA','?'])\n",
    "~~~\n",
    "The function `read_csv()` is an important Pandas method to read CSV files. In the cell below, the `read_csv()` method takes 2 arguments. \n",
    "\n",
    "The first argument, `\"https://biologicslab.co/BIO1173/data/obesity_prediction.csv\"` is a string that provides the filepath and filename. The second argument, `na_values=['NA','?']` is used to recognize `?` as NaN (Not a Number).\n",
    "\n",
    "As the file is read, Pandas creates a DataFrame called `opDF` to hold the information. \n",
    "\n",
    "After reading the datafile into a DataFrame, it is always a good idea to use the function `display()` to print out a specified number of rows and columns to make sure the data was read correctly. \n",
    "\n",
    "The code in the cell below, sets the maximum number of rows to 6 and the maximum number of columns to 6.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Read data file and create Pandas DataFrame\n",
    "\n",
    "# Read the datafile \n",
    "opDF = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/obesity_prediction.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Set max rows and max columns\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 6) \n",
    "\n",
    "# Display DataFrame\n",
    "display(opDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "\n",
    "![_](https://biologicslab.co/BIO1173/images/class_02_2_Exm1.png)\n",
    "\n",
    "You can see from looking at the last line of the output, the DataFrame `opDF` has 7 columns and 1000 rows. This means `opDF` has clinical measurements for 1000 patients and for each patient, there are 7 separate clinical measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Heart Disease Dataset\n",
    "\n",
    "[Heart Disease Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)\n",
    "\n",
    "**Description**\n",
    "\n",
    "Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n",
    "\n",
    "People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n",
    "\n",
    "* **Age:** age of the patient [years]\n",
    "* **Sex:** sex of the patient [M: Male, F: Female]\n",
    "* **ChestPainType:** chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n",
    "* **RestingBP:** resting blood pressure [mm Hg]\n",
    "* **Cholesterol:** serum cholesterol [mm/dl]\n",
    "* **FastingBS:** fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n",
    "* **RestingECG:** resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
    "* **MaxHR:** maximum heart rate achieved [Numeric value between 60 and 202]\n",
    "* **ExerciseAngina:** exercise-induced angina [Y: Yes, N: No]\n",
    "* **Oldpeak:** oldpeak = ST [Numeric value measured in depression]\n",
    "* **ST_Slope:** the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n",
    "* **HeartDisease:** output class [1: heart disease, 0: Normal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1: Read data file into a Pandas DataFrame**\n",
    "\n",
    "In the cell below use the Pandas `read_csv()` method to read the `heart_disease.csv` file that is located on the course HTTPS server using this code chunk:\n",
    "~~~text\n",
    "# Read the datafile \n",
    "hdDF = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/heart_disease.csv\",\n",
    "    na_values=['NA','?'])\n",
    "~~~\n",
    "As the file is read, have Pandas create a DataFrame called `hdDF`. to hold the heart disease data. \n",
    "\n",
    "Use the `display()` function to print out 6 rows and 6 columns of `hdDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 1 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "\n",
    "![_](https://biologicslab.co/BIO1173/images/class_02_2_Exe1.png)\n",
    "\n",
    "You can see by looking at the last line of the output, your DataFrame, `hdDF`, has 12 columns and 918 rows. In other words `hdDF` has clinical measurements for 918 patients and 12 separate clinical measurements for each patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Normalization\n",
    "\n",
    "Neural network datasets need to be normalized for the following reasons:\n",
    "\n",
    "* **Improving Convergence:** Normalization helps ensure that the input values to a neural network are within a similar range. This prevents certain features from dominating others and avoids issues such as slow convergence or the network getting stuck in local optima. By normalizing the data, we can achieve a more balanced training process and faster convergence.\n",
    "* **Avoiding Gradient Instability:** During the training of a neural network, backpropagation is used to adjust the weights based on the gradient of the loss function. If the input features have significantly different scales, the gradient updates may become unstable. Normalizing the data mitigates this problem by keeping the input values at a similar magnitude, leading to more stable and reliable gradient updates.\n",
    "* **Efficient Computation:** Normalizing the data to a common range between 0 and 1 or -1 and 1 can improve the efficiency of computations within the neural network. Many activation functions and optimization algorithms are designed to work well with inputs in this range. By normalizing the data, we can leverage these computational efficiencies and speed up the training process.\n",
    "* **Generalization:** Normalization helps the neural network generalize better to unseen data. If the input features have different scales or distributions in the training and test datasets, the network may struggle to generalize its learned patterns effectively. By normalizing the data, we ensure that the network receives consistent input representations, improving its ability to handle new, unseen samples.\n",
    "* **Better Weight Initialization:** Normalizing the data can facilitate better weight initialization in a neural network. Weight initialization methods like Xavier or He initialization assume that the input features are normalized to have zero mean and unit variance. By normalizing the data, we align the network's expectations with these weight initialization techniques, enhancing the overall training process.\n",
    "* **Handling Outliers:** Normalization can help handle outliers in the data. Outliers can disproportionately influence the learning process and bias the network's decisions. By normalizing the data, outliers are brought closer to the range of other values, minimizing their impact on the network's behavior.\n",
    "\n",
    "In summary, normalizing neural network datasets improves convergence, avoids gradient instability, enhances computational efficiency, promotes generalization, aids in weight initialization, and helps handle outliers. These benefits contribute to more effective training and improved performance of neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Continuous Values\n",
    "\n",
    "One common transformation for data **_normalization_** is to convert the input values into Z-scores.  Normalizing numeric inputs into a standard form makes it easier for a program to compare values.  Consider if a friend told you that he received a 10-dollar discount.  Is this a good deal?  Maybe.  But the cost is not normalized.  If your friend purchased a car, the discount is not that good.  If your friend bought lunch, this is an excellent discount!\n",
    "\n",
    "Converting a number into a percentage is a common form of normalization.  If your friend tells you they got 10% off, we know that this is a better discount than 5%.  It does not matter how much the purchase price was.  \n",
    "\n",
    "For machine learning, a better form of normalization than percentages is the Z-Score:\n",
    "\n",
    "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
    "\n",
    "To calculate the Z-Score, you also need to calculate the mean(&mu; or $\\bar{x}$) and the standard deviation (&sigma;).  You can calculate the mean with this equation:\n",
    "\n",
    "$$ \\mu = \\bar{x} = \\frac{x_1+x_2+\\cdots +x_n}{n} $$\n",
    "\n",
    "The standard deviation is calculated as follows:\n",
    "\n",
    "$$ \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} $$\n",
    "\n",
    "Example 2 and Exercise 2 below will demostrate how to replace numerical values in a DataFrame with their Z-scores. Average values will end up having Z-scores near zero, values that are greater than average will have positive Z-scores while below values below average will end up having negative Z-scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "### **Z-scores**\n",
    "\n",
    "**_Z-scores_**, also known as _standard scores_, are statistical measures that indicate how far a particular value is from the mean of a dataset, measured in terms of standard deviations. They are important because they allow us to compare and analyze data points from different distributions with different units and scales.\n",
    "\n",
    "The calculation of a Z-score involves subtracting the mean of the distribution from the specific value and then dividing the result by the standard deviation. This transforms the original value into a standardized value that represents its relative position within the distribution.\n",
    "\n",
    "Z-scores are important for several reasons:\n",
    "\n",
    "* **Standardization:** Z-scores provide a way to standardize data, making it easier to compare values from different datasets and variables. By converting values to a common scale, we can compare observations, identify outliers, and analyze data more accurately.\n",
    "* **Normal distribution:** Z-scores are frequently used with normally distributed data, where the mean is 0 and the standard deviation is 1. When data is standardized to a Z-score distribution, it becomes easier to apply statistical techniques and make meaningful interpretations based on the standard normal distribution.\n",
    "* **Identification of extreme values:** Z-scores help in identifying extreme values, known as outliers. Values with Z-scores greater than a certain threshold (e.g., 2 or 3) are considered outliers, indicating that they deviate significantly from the mean.\n",
    "* **Probability calculations:** Z-scores also enable us to calculate probabilities and determine the likelihood of a value occurring in a normal distribution. By converting a value to its Z-score, we can look up the probability associated with that Z-score in a standard normal distribution table.\n",
    "\n",
    "Overall, Z-scores provide a standardized way to analyze, compare, and interpret data, making them an essential tool in statistical analysis and research.\n",
    "\n",
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Convert to Z-scores\n",
    "\n",
    "The cell below shows how to use the `zscore` package from the `scipy.stats` module to compute Z-score values for the height and weight measurements in the Obesity Prediction dataset. Height and weight measurements are good candidates for converting to Z-scores since their values can be quite different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Convert to Z-scores\n",
    "\n",
    "# Import zscore package\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Set max rows and max columns\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 6)\n",
    "\n",
    "# Save original value\n",
    "origDF=opDF[['Height','Weight']]\n",
    "\n",
    "# Convert height values to Z-scores\n",
    "opDF['Height'] = zscore(opDF['Height'])\n",
    "\n",
    "# Convert weight values to Z-scores\n",
    "opDF['Weight'] = zscore(opDF['Weight']) \n",
    "\n",
    "# Display Original values\n",
    "print(\"Values before Z-score conversion\")\n",
    "display(origDF)\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"Values after Z-score conversion\")\n",
    "display(opDF[['Height','Weight']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following table:\n",
    "\n",
    "![__](https://biologicslab.co/BIO1173/images/class_02_2_HtWt.png)\n",
    "\n",
    "As you can see, after converting to their Z-scores, the height and weight measurements have gone from relatively large, all positive numbers, to small values, near zero, that are both positive and negative. A value equal to `0` means average, while positive values are above average and negative values are below average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2: Convert to Z-scores**\n",
    "\n",
    "In the cell below, use the `zscore` package from the `scipy.stats` module to compute Z-score values for the resting blood pressue (`RestingBP`) values and the serum cholesterol values (`Cholesterol`) in the Heart Disease dataset. Display only the `h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 2 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following table:\n",
    "\n",
    "![__](https://biologicslab.co/BIO1173/images/class_02_2_RestBPChol.png)\n",
    "\n",
    "As you can see, after converting to Z-scores, the values for resting blood pressure (`RestingBP`) and serum cholesterol (`Cholesterol`) are much more similar which makes it easier to train a neural network on this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "\n",
    "### **Encoding Categorical Values as Dummies**\n",
    "\n",
    "The traditional means of encoding categorical values (i.e. converting string data to numerical values) is to replace them with **_dummy variables_**.  This technique is also called One-Hot Encoding. \n",
    "\n",
    "**One-Hot Encoding** is a technique used in data preprocessing and feature engineering to convert categorical variables into a numerical representation that can be used by machine learning algorithms. It is important because many machine learning models require numerical input, and categorical variables cannot be directly used in their raw form.\n",
    "\n",
    "In One-Hot Encoding, each category in a categorical variable is converted into a new binary feature column (i.e. `0` or `1`). For a variable with _N_ categories, _N_ new binary columns are created, where each column represents a specific category. If an observation belongs to a particular category, the corresponding feature column is assigned a value of 1, otherwise 0.\n",
    "\n",
    "There are a few reasons why One-Hot Encoding is important:\n",
    "\n",
    "* **Retaining categorical information:** One-Hot Encoding allows us to retain the categorical information that would otherwise be lost if we simply assigned numerical labels to each category. By creating separate binary columns, we preserve the distinctiveness of each category, enabling the model to understand and utilize this information.\n",
    "* **Avoiding numerical assumptions:** By converting categorical variables into numerical representations, we eliminate any numerical order or relationship assumptions that may not exist in the original data. This prevents the model from mistakenly interpreting the numerical values as meaningful in terms of order or magnitude.\n",
    "* **Compatibility with machine learning algorithms:** Many machine learning algorithms require numerical input. By converting categorical variables into a binary representation, One Hot Encoding makes it possible to feed categorical data into these algorithms, expanding the range of models that can be utilized.\n",
    "* **Handling multi-class categories:** One Hot Encoding is particularly useful when dealing with categorical variables with multiple classes. By creating binary columns for each class, we allow the model to learn distinct patterns and relationships between the categories.\n",
    "\n",
    "It is important to note that One-Hot Encoding can increase the dimensionality of the dataset, especially if the categorical variable has a large number of classes. This can potentially lead to the **\"curse of dimensionality\"** and affect the performance of the model. However, it is a widely used and effective technique for incorporating categorical variables into machine learning models.\n",
    "\n",
    "-------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Preprocessing data\n",
    "\n",
    "Example 3 has been broken down into 5 separate steps. Each step illustrates an important technique used to **_preprocess_** data before it can be used with deep neural networks. You will be using these steps over-and-over again in this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3-Step 1: Determine categories that are not numeric\n",
    "\n",
    "The first step in encoding categorical variables is determine which column(s) have non numerical values. \n",
    "\n",
    "We can use the Pandas method `df.select_dtypes(exclude='number'0.columns` to generate a list of column names.\n",
    "\n",
    "The code in the cell below also uses the `starred` print option. \n",
    "~~~text\n",
    "# Print result\n",
    "print(*non_numerical_columns)\n",
    "~~~\n",
    "\n",
    "By simply inserting an asterisk `*` before the list variable, the print statement only prints out the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3-Step 1: Determine columns with non-numeric values\n",
    "\n",
    "# Select columns\n",
    "non_numerical_columns = opDF.select_dtypes(exclude='number').columns\n",
    "\n",
    "# Print result\n",
    "print(*non_numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "~~~text\n",
    "Gender ObesityCategory\n",
    "~~~\n",
    "\n",
    "There are two columns in the `opDF` Dataframe, `Gender` and `ObesityCategory`, that we will need to convert string values into numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3-Step 2: Print out a list of the category values \n",
    "\n",
    "There are basically two different ways to convert string values into numerical values\n",
    "\n",
    "1. Mapping string values to integers\n",
    "2. One-Hot Encoding\n",
    "\n",
    "In most situations, either method would work. I will usually use \"mapping\" if the number of categorical values is small. For example, in many biomedical datasets, a string value is used to denote the gender of the subject/patient (e.g., `M` for male and `F` for female). When the number of categorical values is small, my preference is to use `mapping`. In other situations, where the number of categorical values is 3 or more, my preference is to use One-Hot Encoding. Again, in most situations, either method should work.\n",
    "\n",
    "The code in the cell below shows how to determine the number of different categories that are used in `ObesityCategory` column in the `opDF` DataFrame. This step is not really necessary for this particular dataset since the number of columns is relatively small. However in a dataset with a large number of columns, this step might be very helpful.\n",
    "\n",
    "The trick here is to use the Python `list()` function with the `unique()` method when creating the category list. This insures that the list contains only the name of each _different_ category in the column and not simply a list containing all of the category names repeated hundred of times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 3 Step 2: Print category values\n",
    "\n",
    "# Generate a list with only unique values\n",
    "numOpCat = list(opDF['ObesityCategory'].unique())\n",
    "\n",
    "# Print out the results\n",
    "print(f'Number of obesity categories: {len(numOpCat)}')\n",
    "print(f'numOpCat: {numOpCat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output above:\n",
    "~~~text\n",
    "Number of obesity categories: 4\n",
    "numOpCat: ['Normal weight', 'Obese', 'Overweight', 'Underweight']\n",
    "~~~\n",
    "there are four different strings used as categorical values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3- Step 3: One-Hot Encode the column\n",
    "\n",
    "From the output above we know that there are exactly 4 different category values used in the `ObesityCategory` column: `Normal weight`, `Obese`, `Overweight` and `Underweight`. We need to One-Hot Encode these values.\n",
    "\n",
    "The code in the cell below uses Pandas' `pd.get_dummies()` function to create dummy columns that can be used to replace the `ObesityCategory` column in the `opDF` DataFrame. To make it easier to remember what the dummy columns represent, we are going to add the prefix `OBCat` to each of the new dummy columns. What you use as a prefix is totally up to you, since it doesn't have any effect on how the data is processed. The prefix is just a reminder of what the original data was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3 - Step 3: Encode the actual column\n",
    "\n",
    "# Encode the ObesityCategory column\n",
    "dummies = pd.get_dummies(opDF['ObesityCategory'],prefix='OBCat', dtype=int)\n",
    "\n",
    "# Display dummies DataFrame\n",
    "display (dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_02_2_Exm3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable called `dummies`, that was created by the One-Hot Encoding, is actually a new, **_separate_** DataFrame which is displayed above. \n",
    "\n",
    "These four dummy columns encode the categorical data that is in the `ObesityCategory` column. \n",
    "\n",
    "It is important to know how dummy columns encode numerical information. Notice that for each row, only **_one_** column that has a value of `1`, while the other columns in that row contain `0`. \n",
    "\n",
    "For example, the first patient (index value `0`) has a `1` only in the column `OBCat_Normal weight` while the second patient (index value `1`) only has a `1` in the column `OBCat_Obese`.  \n",
    "\n",
    "For this reason, this type of encoding is called **_One-Hot_** Encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3-Step 4: Merge dummy columns into dataset\n",
    "\n",
    "As mentioned above, One-Hot Encoding only generates a new, separate DataFrame called `dummies`. In order to use it, it is up to you to **_merge_** the `dummies` DataFrame with the DataFrame containing the dataset, in this example, `opDF`.   \n",
    "\n",
    "The code in the cell below, shows how to merge these two DataFrames using the Pandas function `pd.concat()`. The word 'concat' in this command is short for `concatenate`. \n",
    "\n",
    "**_Concatenate_** means combining multiple strings, lists, or other sequences into a single sequence. It can be done using various methods including the `concat()` function in Pandas.\n",
    "\n",
    "The code fragment that accomplishes the concatenation is:\n",
    "~~~text\n",
    "# Merge dummies with the DataFrame\n",
    "opDF = pd.concat([opDF,dummies],axis=1)\n",
    "~~~\n",
    "\n",
    "The argument `axis=1` specifies that the concatenation should be done along **_columns_**. This means that the DataFrames are joined _horizontally_, with the columns from the second DataFrame (`dummies`) added next to the columns from the first DataFrame (`opDF`).\n",
    "\n",
    "The code below also illustrates how you can display only **_selected columns_** in a large DataFrame by specifying their column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3 - Step 4: Merge dummies with dataset\n",
    "\n",
    "# Merge dummies with the DataFrame\n",
    "opDF = pd.concat([opDF,dummies],axis=1)\n",
    "\n",
    "# Set max rows and max columns\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 6)\n",
    "\n",
    "# Display certain columns in the DataFrame\n",
    "display(opDF[['BMI','ObesityCategory','OBCat_Normal weight',\n",
    "                  'OBCat_Obese','OBCat_Overweight','OBCat_Underweight']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_02_2obdummies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, we can see that the four dummy columns, with the pre-fix `OBCat_` have been added to the `opDF` DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3-Step 5: Remove the original column\n",
    "\n",
    "Usually, you will need to remove the column that was One-Hot Encoded from your DataFrame for two reasons. First, the column still contains string values which you can't use. And second, the informat in that column is **_redundant_** -- the information is encoded in the dummy columns. \n",
    "\n",
    "The cell below shows how to use the Pandas `df.drop()` method to drop the `ObesityCategory` column from the `opDF` DataFrame. As above, the argument `axis=1` tells the method that you want to drop the column instead of a row.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3-Step 5: Remove the orginal column\n",
    "\n",
    "# Use drop method to drop the ObesityCategory column\n",
    "opDF.drop('ObesityCategory', axis=1, inplace=True)\n",
    "\n",
    "# Set max rows and max columns\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 6)\n",
    "\n",
    "# Display DataFrame\n",
    "display(opDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_02_2_Exm3-5.png)\n",
    "\n",
    "You should note the the column `ObesityCategory` is no longer present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of see the image above, you might see the following error message:\n",
    "\n",
    "![___](https://biologicslab.co/BIO1173/images/class_02_2_error.png)\n",
    "\n",
    "If you see this error, DON'T panic! It simply means that you ran the code in Example 3-Step 5 more than once. \n",
    "\n",
    "Since the first time you ran the cell, it removed the `ObesityCategory` column from the DataFrame `opDF`. The problem comes when try to run it again. You will get this error since the column had already been removed. \n",
    "\n",
    "The solution is to simply SAVE your work and then restart your notebook from the very beginning. Your notebook will **_re-read_** the Obesity Prediction dataset and re-create the `opDF` DataFrame will all of its columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 3**\n",
    "\n",
    "In **Exercise 3** you are to repeat the same 5 steps from Example 3, but using the the Heart Disease dataset in the DataFrame `hdDF`. In other words, you are to follow the same 5 steps shown in Example 3 to Hot-Encode data in the Heart Failure dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3-Step 1: Determine categories that are not numeric**\n",
    "\n",
    "In the first step, use the Pandas function `df.select_dtypes()` to select all of the columns in your DataFrame `hdDF` that contain non-numeric (string) values.\n",
    "\n",
    "Print out the results using the `starred` print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 3 Step 1 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "~~~text\n",
    "Sex ChestPainType RestingECG ExerciseAngina ST_Slope\n",
    "~~~\n",
    "There are 4 columns that have non numeric values: `Sex`, `RestingECG`, `ExerciseAngina` and `ST_Slope`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3-Step 2: Print out a list of the category values** \n",
    "\n",
    "Even though there are 4 columns with non numerical values, for Exercise 3 you are to only encode the `ChestPainType` column. \n",
    "\n",
    "In the cell below write the Python code to print out a list showing the number of different categories in the `ChestPainType` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 3 Step 2 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "Number of chest pain categories: 4\n",
    "numHfCat: ['ATA', 'NAP', 'ASY', 'TA'\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3-Step 3: One-Hot Encode the column**\n",
    "\n",
    "In the cell below use the `pd.get_dummies()` function to create dummy columns for the column `ChestPainType`. To make it easier to remember what the dummy columns represent, add the prefix `Pain` to each dummy column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 3 Step 3 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "![__](https://biologicslab.co/BIO1173/images/class_02_2_dummie2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for each row, there is only one column that has a value of `1`, while the rest of the columns in that row contain `0`. The first patient (index `0`) has the pain type `ATA` since he/she has a `1` in that column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3-Step 4: Merge dummy columns into dataset**\n",
    "\n",
    "In the cell below write the code to add the dummy columns back into the `hdDF` DataFrame using the Pandas' `pd.concat()` function. Set the display option to print out 6 rows and 6 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 3 Step 4 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "![_](https://biologicslab.co/BIO1173/images/class_02_2_Exe3-4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3-Step 5: Remove the orignal column**\n",
    "\n",
    "In the cell below, write the code to drop the `ChestPainType` column from the `hdDF` DataFrame. \n",
    "\n",
    "Display 6 rows and 6 columns of your modified DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 3 Step 5 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "![_](https://biologicslab.co/BIO1173/images/class_02_2_Exe3-5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the First Level\n",
    "\n",
    "The **pd.concat** function also includes a parameter named *drop_first*, which specifies whether to get k-1 dummies out of k categorical levels by removing the first level. \n",
    "\n",
    "Why would you want to remove the first level? \n",
    "\n",
    "Consider the category `Gender` in the `opDF` dataframe. This column contains the string variables `Male` and `Female`. Suppose we were to `Hot-One-Encode` the `Gender` column with the following line of Python code:\n",
    "\n",
    "> `dummies = pd.get_dummies(opDF['Gender'],prefix='Gender', dtype=int)`\n",
    ">\n",
    "\n",
    "Here is what the two dummy columns would look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "    Gender_Female  Gender_Male\n",
    "0               0            1\n",
    "1               0            1\n",
    "2               1            0\n",
    "..            ...          ...\n",
    "7               0            1\n",
    "8               0            1\n",
    "9               0            1\n",
    "\n",
    "[10 rows x 2 columns]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ask yourself the question, \"Do we really need **both** columns to know if a subject was a female or a male?\" \n",
    "\n",
    "The answer is, \"Not really\". \n",
    "\n",
    "Suppose we removed the `First Level` using the following line of code:\n",
    "\n",
    "> `dummies = pd.get_dummies(obDF['Gender'],prefix='Gender', dtype=int, drop_first=True)`\n",
    ">\n",
    "Here is the output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~text\n",
    "    Gender_Male\n",
    "0             1\n",
    "1             1\n",
    "2             0\n",
    "..          ...\n",
    "7             1\n",
    "8             1\n",
    "9             1\n",
    "\n",
    "[10 rows x 1 columns]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first level `Gender_Female` was dropped, there is now only the single dummy column `Gender_Male` left. However, you only need this column to know the gender of each subject in the `opDF` dataframe! \n",
    "\n",
    "Consider the subject in Row 0. This subject is a male since he has a `1` in the `Gender_Male` column. This is also true of the subject in Row 1. But the subject in Row 2 _must_ be a female because she has a `0` in the `Gender_Male` column. In other words, we can determine the gender of every subject by the values in a single column. \n",
    "\n",
    "It turns out that this idea is not limited to situations were there are only two possible choices. You can **always** drop one column in _any_ series of dummy columns **without** losing any information. For this reason the command `pd.get_dummies()` is often used with the argument `drop_first` set to `True` to simplify this process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Using `drop_first=True`\n",
    "\n",
    "The code in the cell below, begins by regenerating the original `opDF` DataFrame by re-reading the datafile.\n",
    "\n",
    "The code then creates dummy columns for the `ObesityCategory` column in the DataFrame `opDF` but drops the first category before merging the remaining 3 dummy columns with the DataFrame and then drops the `ObesityCategory` column from the `opDF` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4 - Use drop_first = True\n",
    "\n",
    "# Read the datafile \n",
    "opDF = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/obesity_prediction.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Encode the ObesityCategory column as dummy variables\n",
    "dummies = pd.get_dummies(opDF['ObesityCategory'], drop_first=True, \n",
    "                         dtype= int, prefix='OBCat')\n",
    "\n",
    "# Merge the dummie with the DataFrame\n",
    "opDF = pd.concat([opDF,dummies],axis=1)\n",
    "\n",
    "# Drop the column replaced by the dummies\n",
    "opDF.drop('ObesityCategory', axis=1, inplace=True)\n",
    "\n",
    "# Set max rows and max columns\n",
    "pd.set_option('display.max_rows', 4)\n",
    "pd.set_option('display.max_columns', 6)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(opDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "![_](https://biologicslab.co/BIO1173/images/class_02_2_Exm4.png)\n",
    "\n",
    "Notice that the first subject (row `0`) who is a male, age 56. He must also have a _normal weight_, even though in the first category `OBCat_Normal Weight` was dropped. \n",
    "\n",
    "Why? \n",
    "\n",
    "Since he has a zero in the remaining 3 obesity categories, the only possibilty is that he had a `1` in the dropped category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 4: Using `drop_first=True`**\n",
    "\n",
    "In the cell below, start by regenerating the complete (original) Heart Failure DataFrame using the the command `hdDF = hdOrigDF.copy()`. Then write the Python code to One-Hot encode the column `ChestPainType` column setting the `drop_first` argument to `True` to drop the first dummy column. Merge the remaining 3 dummy columns with the `hfDF` DataFrame before dropping the `ChestPainType` column.  Set the display options to print out 6 rows and 8 columns and print out the updated DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code for Exercise 4 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "![_](https://biologicslab.co/BIO1173/images/class_02_2_Drop1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lesson Turn-in**\n",
    "\n",
    "When you have completed all of the code cells, and run them in sequential order (the last code cell should be number 18), use the **File --> Print.. --> Save to PDF** to generate a PDF of your JupyterLab notebook. Save your PDF as `Class_02_2.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
