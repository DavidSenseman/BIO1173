{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNRQlC8fnYJxREDRrtJeohX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_DICOM_v11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ],
      "metadata": {
        "id": "Uyra_V-5wfDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "xKg8niINwjH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Module 6: Advanced Topics**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 6 Material\n",
        "\n",
        "* Part 3.1: Using Convolutional Neural Networks\n",
        "* **Part 3.2: Using Pre-Trained Neural Networks with Keras**\n",
        "* Part 3.3: Facial Recognition and Analysis\n",
        "* Part 3.4: Introduction to GAN's for Image and Data Generation"
      ],
      "metadata": {
        "id": "hUi7q-ZEwnDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Change your Runtime Now!**\n",
        "\n",
        "For this lesson you must have a GPU hardware accelerator (e.g. `A100` with `High-RAM` if available)."
      ],
      "metadata": {
        "id": "Zd4-bdhmw0TW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ],
      "metadata": {
        "id": "UMb9nq1kw-c9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6ROH85hxC1V",
        "outputId": "6951cb07-5a3b-4722-9890-e5a6fd27c0f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: Using Google CoLab\n",
            "david.senseman@gmail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_01/class_01_6_image01A.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "PAhCLsfpxOpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerated Run-time Check\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be running a Graphics Processing Unit (GPU)."
      ],
      "metadata": {
        "id": "91TSMEo0xRgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell second\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 0️⃣  Create check_device() function\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "def check_device():\n",
        "    # Check for available devices\n",
        "    devices = tf.config.list_physical_devices()\n",
        "\n",
        "    # Initialize device flags\n",
        "    cpu = False\n",
        "    gpu = False\n",
        "    tpu = False\n",
        "\n",
        "    # Check device types\n",
        "    for device in devices:\n",
        "        if device.device_type == 'CPU':\n",
        "            cpu = True\n",
        "        elif device.device_type == 'GPU':\n",
        "            gpu = True\n",
        "        elif device.device_type == 'TPU':\n",
        "            tpu = True\n",
        "\n",
        "    # Output device status\n",
        "    if tpu:\n",
        "        print(\"Running on TPU\")\n",
        "        print(\"WARNING: You must run this assigment using a GPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now!\")\n",
        "    elif gpu:\n",
        "        print(\"Running on GPU\")\n",
        "        gpu_info = !nvidia-smi\n",
        "        gpu_info = '\\n'.join(gpu_info)\n",
        "        print(gpu_info)\n",
        "        print(\"You are using a GPU hardware accelerator--You're good to go!\")\n",
        "    elif cpu:\n",
        "        print(\"Running on CPU\")\n",
        "        print(\"WARNING: You must run this assigment using a GPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now!\")\n",
        "    else:\n",
        "        print(\"No compatible device found\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU or a TPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now!\")\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 1️⃣  Call function\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "check_device()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up1eHXtTxUlM",
        "outputId": "29cee87a-2b46-496f-d3bd-137639b9fc93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on GPU\n",
            "Thu Sep 25 00:27:27 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   34C    P0             54W /  400W |       5MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "You are using a GPU hardware accelerator--You're good to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you current `Runtime` is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_05/class_05_4_image11A.png)\n",
        "\n",
        "However, if you received this warning message\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_05/class_05_4_image14A.png)\n",
        "\n",
        "You **MUST** go back and change your `Runtime` **NOW** before you continue."
      ],
      "metadata": {
        "id": "K0_4X6Wtxdlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Custom Function\n",
        "\n",
        "The cell below creates a custom function called `hms_string()`. This function is needed to record the time required to train your neural network model.\n",
        "\n",
        "If you fail to run this cell now, you will receive one (or more) error message(s) later in this lesson."
      ],
      "metadata": {
        "id": "z1MHa1rzDCwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom function\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 0️⃣  Create hms_string()\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "ExLJjlbUDDQp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Packages\n",
        "\n",
        "Run the code in the next cell to install necessary packages for this lesson."
      ],
      "metadata": {
        "id": "VylfOuznXs9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RqoUEz9CXfy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5570031-ef99-4379-f434-56ae52127e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-3.0.1\n"
          ]
        }
      ],
      "source": [
        "# Install Packages\n",
        "\n",
        "!pip install pydicom"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n"
      ],
      "metadata": {
        "id": "QY9Yifcdxxmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and Extract Data\n",
        "\n",
        "Run the code in the next cell to download and extract the DICOM image dataset for this lesson."
      ],
      "metadata": {
        "id": "rbacVvpBX0fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UECDpBB8RSH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "#  Lazy DICOM loader\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import pydicom\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "import pydicom\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "\n",
        "class DicomImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 file_paths,\n",
        "                 labels,\n",
        "                 transform=None,\n",
        "                 gray_scale=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            file_paths (Iterable[str]): Paths to DICOM files.\n",
        "            labels (Iterable[int]): Class indices.\n",
        "            transform (callable, optional): torchvision transform pipeline.\n",
        "            gray_scale (bool, optional): If True, convert the image to a\n",
        "                                         single‑channel (grayscale) tensor.\n",
        "        \"\"\"\n",
        "        self.file_paths = list(file_paths)\n",
        "        self.labels = np.array(labels, dtype=np.int64)\n",
        "        self.gray_scale = gray_scale\n",
        "\n",
        "        # ---- 1. Default transform (PIL → Tensor) ------------------------------\n",
        "        # The compose must be *outside* of any other list or property.\n",
        "        self.transform = transform or T.Compose([\n",
        "            T.Resize(256),          # (optional) change to your size\n",
        "            T.CenterCrop(224),\n",
        "            T.ToTensor(),           # converts PIL image to FloatTensor [C, H, W]\n",
        "            T.Normalize(mean=[0.5], std=[0.5])   # single‑channel stats\n",
        "        ])\n",
        "\n",
        "    # ---- 2. Property: number of classes ------------------------------------\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        \"\"\"Return the number of distinct classes in the dataset.\"\"\"\n",
        "        return len(set(self.labels.tolist()))\n",
        "\n",
        "    # ---- 3. Dataset interface -----------------------------------------------\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Load the DICOM *only when requested*\n",
        "        dicom_path = self.file_paths[idx]\n",
        "        ds = pydicom.dcmread(dicom_path)\n",
        "\n",
        "        # Grab the pixel data and cast to float32 for later scaling\n",
        "        img = ds.pixel_array.astype(np.float32)\n",
        "\n",
        "        # If the image is 16‑bit (or any bit depth) we rescale to 0‑255.\n",
        "        # (pixel_array is usually uint16 for CT/MRI scans)\n",
        "        if img.dtype != np.uint8:\n",
        "            img = img - np.min(img)          # shift to zero\n",
        "            img = img / np.max(img)           # scale to [0, 1]\n",
        "            img = (img * 255).astype(np.uint8)\n",
        "\n",
        "        # Handle 3‑channel data: if the pixel array already has 3 channels\n",
        "        # we keep it as‑is; otherwise we treat it as single‑channel.\n",
        "        if img.ndim == 3 and img.shape[2] > 1:\n",
        "            # If you want RGB, keep the 3 channels\n",
        "            mode = \"RGB\"\n",
        "        else:\n",
        "            # Convert to 2‑D (single channel) if necessary\n",
        "            mode = \"L\"  # PIL mode for grayscale\n",
        "\n",
        "        # Convert to PIL image\n",
        "        pil_img = Image.fromarray(img, mode=mode)\n",
        "\n",
        "        # If gray_scale is True we convert the image to grayscale *again*\n",
        "        # (PIL will convert to mode 'L' anyway, but this keeps the logic\n",
        "        # explicit for clarity).\n",
        "        if self.gray_scale and pil_img.mode != \"L\":\n",
        "            pil_img = pil_img.convert(\"L\")\n",
        "\n",
        "        # Apply the transform (if any). The Compose expects a PIL image.\n",
        "        if self.transform:\n",
        "            pil_img = self.transform(pil_img)\n",
        "\n",
        "        # Return the tensor + label\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return pil_img, label\n",
        "\n"
      ],
      "metadata": {
        "id": "UMDlf9A1RSnX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Eg8OVPE3Rg85"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c-60hZhxRhnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and Extract Data\n",
        "\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import zipfile\n",
        "import sys\n",
        "import shutil\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
        "from torchvision import transforms, models\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import zipfile\n",
        "import sys\n",
        "import shutil\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Configuration – change only if you want a different URL / filename\n",
        "# ------------------------------------------------------------------\n",
        "URL = \"https://biologicslab.co/BIO1173/data/\"\n",
        "ZIP_FILENAME = \"pna_data.zip\"\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Download the zip file (streamed, so it works with large files)\n",
        "# ------------------------------------------------------------------\n",
        "def download_zip(url: str, dest: Path, chunk_size: int = 8192) -> None:\n",
        "    \"\"\"Download a file from `url` and write it to `dest`.\"\"\"\n",
        "    print(f\"Downloading {ZIP_FILENAME} to {dest}...\", end='')\n",
        "    with requests.get(url, stream=True, timeout=30) as r:\n",
        "        r.raise_for_status()           # will raise for 4xx/5xx\n",
        "        with dest.open(\"wb\") as f_out:\n",
        "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                if chunk:               # filter out keep‑alive new chunks\n",
        "                    f_out.write(chunk)\n",
        "    print(\"done\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Un‑zip the downloaded archive into a *named* directory\n",
        "# ------------------------------------------------------------------\n",
        "def unzip_file(zip_path: Path, extract_to: Path) -> None:\n",
        "    \"\"\"Extract all members of `zip_path` into `extract_to`.\"\"\"\n",
        "    print(f\"Unzipping {ZIP_FILENAME} to {extract_to}...\", end='')\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(extract_to)\n",
        "    print(\"done\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Optional – delete the zip after extraction\n",
        "# ------------------------------------------------------------------\n",
        "def clean_up_zip(zip_path: Path) -> None:\n",
        "    \"\"\"Delete the zip file – only if you no longer need it.\"\"\"\n",
        "    zip_path.unlink()\n",
        "    print(f\"Removed temporary archive: {zip_path}... done\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Main routine\n",
        "# ------------------------------------------------------------------\n",
        "def main() -> None:\n",
        "    cwd          = Path.cwd()            # current working directory\n",
        "    zip_path     = cwd / ZIP_FILENAME\n",
        "    extract_dir  = cwd / zip_path.stem   # e.g. /pna_data\n",
        "\n",
        "    # Ensure the extraction directory exists\n",
        "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download\n",
        "    download_zip(URL+ZIP_FILENAME, zip_path)\n",
        "\n",
        "    # Un‑zip\n",
        "    unzip_file(zip_path, extract_dir)\n",
        "\n",
        "    # Clean‑up the downloaded archive\n",
        "    clean_up_zip(zip_path)\n",
        "\n",
        "    print(f\"Files have been extracted to {extract_dir}\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC6aegypX05d",
        "outputId": "ec213286-5243-473f-9da0-fdde96bbc6a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading pna_data.zip to /content/pna_data.zip...done\n",
            "Unzipping pna_data.zip to /content/pna_data...done\n",
            "Removed temporary archive: /content/pna_data.zip... done\n",
            "Files have been extracted to /content/pna_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n"
      ],
      "metadata": {
        "id": "uGnJx86Uyk1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Process DICOM datafiles\n",
        "\n"
      ],
      "metadata": {
        "id": "_WCOBTIFYRGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process DICOM Files\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import warnings\n",
        "from matplotlib import pyplot as plt\n",
        "#import seaborn as sns\n",
        "\n",
        "# Global settings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "#sns.set_palette(\"husl\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Helper: read a single DICOM file\n",
        "# -------------------------------------------------------------\n",
        "def read_dicom_file(file_path: str):\n",
        "    \"\"\"Read a DICOM file and extract image data + basic metadata.\"\"\"\n",
        "    ds = pydicom.dcmread(file_path)\n",
        "\n",
        "    # Basic metadata\n",
        "    metadata = {\n",
        "        'filename': os.path.basename(file_path),\n",
        "        'patient_name': getattr(ds, 'PatientName', 'Unknown'),\n",
        "        'patient_id': getattr(ds, 'PatientID', 'Unknown'),\n",
        "        'study_date': getattr(ds, 'StudyDate', 'Unknown'),\n",
        "        'study_time': getattr(ds, 'StudyTime', 'Unknown'),\n",
        "        'modality': getattr(ds, 'Modality', 'Unknown'),\n",
        "        'manufacturer': getattr(ds, 'Manufacturer', 'Unknown'),\n",
        "        'institution_name': getattr(ds, 'InstitutionName', 'Unknown'),\n",
        "        'series_description': getattr(ds, 'SeriesDescription', 'Unknown'),\n",
        "        'bits_allocated': getattr(ds, 'BitsAllocated', 'Unknown'),\n",
        "        'rows': getattr(ds, 'Rows', 'Unknown'),\n",
        "        'columns': getattr(ds, 'Columns', 'Unknown'),\n",
        "        'pixel_spacing': getattr(ds, 'PixelSpacing', 'Unknown')\n",
        "    }\n",
        "\n",
        "    # Image data\n",
        "    if hasattr(ds, 'pixel_array'):\n",
        "        image_array = ds.pixel_array\n",
        "\n",
        "        # Normalise to 0‑255 if needed\n",
        "        if image_array.dtype != np.uint8:\n",
        "            image_array = ((image_array - image_array.min()) /\n",
        "                           (image_array.max() - image_array.min()) * 255).astype(np.uint8)\n",
        "\n",
        "        metadata['image_available'] = True\n",
        "        metadata['image_shape'] = image_array.shape\n",
        "    else:\n",
        "        metadata['image_available'] = False\n",
        "        metadata['image_shape'] = 'No image data'\n",
        "\n",
        "    return ds, metadata\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Helper: fast drop‑check\n",
        "# -------------------------------------------------------------\n",
        "def is_file_dropped(file_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Quick guard that tells us whether a DICOM file is already\n",
        "    missing / unreadable.\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(file_path):\n",
        "        return True\n",
        "\n",
        "    if os.path.getsize(file_path) == 0:\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        pydicom.dcmread(file_path, stop_before_pixels=True)\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Load & preprocess – merge CSVs, keep only valid DICOM rows\n",
        "# -------------------------------------------------------------\n",
        "def load_and_preprocess_data(\n",
        "    data_dir: str = '.',\n",
        "    log_dropped: bool = True\n",
        "):\n",
        "    # Load the two CSVs, merge on patient ID, and keep only rows\n",
        "    # that have an intact DICOM image.\n",
        "    info_df   = pd.read_csv(os.path.join(data_dir, 'pna_detailed_class_info.csv'))\n",
        "    labels_df = pd.read_csv(os.path.join(data_dir, 'pna_train_labels.csv'))\n",
        "\n",
        "    # Define patient ID variable\n",
        "    info_id_col   = 'patientId'\n",
        "    labels_id_col = 'patientId'\n",
        "\n",
        "    merged_df = pd.merge(info_df, labels_df, left_on=info_id_col,\n",
        "                         right_on=labels_id_col, how='inner')\n",
        "\n",
        "    dicom_dir = os.path.join(data_dir, 'pna_train_images')\n",
        "    valid_rows = []\n",
        "    dropped_ids = []\n",
        "\n",
        "    for idx, row in merged_df.iterrows():\n",
        "        patient_id = row[info_id_col]\n",
        "        dicom_file = os.path.join(dicom_dir, f\"{patient_id}.dcm\")\n",
        "\n",
        "        if is_file_dropped(dicom_file):\n",
        "            dropped_ids.append(patient_id)\n",
        "        else:\n",
        "            valid_rows.append(idx)\n",
        "\n",
        "    filtered_df = merged_df.loc[valid_rows].copy()\n",
        "    print(f\"Filtered DataFrame shape (with valid DICOM files): {filtered_df.shape}\")\n",
        "\n",
        "    return filtered_df, dropped_ids\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "#  Update load_and_preprocess_data to return only file‑paths + labels\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "def load_file_paths_and_labels(filtered_df, max_samples=None):\n",
        "    \"\"\"\n",
        "    Return two lists: `file_paths` and `labels`.  The function keeps the\n",
        "    *original* logic that filtered out NaNs / duplicated rows – we simply\n",
        "    strip it down to the very few objects that the new lazy loader needs.\n",
        "    \"\"\"\n",
        "    file_paths = filtered_df[\"file_path\"].tolist()\n",
        "    labels = filtered_df[\"label\"].tolist()\n",
        "\n",
        "    if max_samples is not None:\n",
        "        file_paths = file_paths[:max_samples]\n",
        "        labels = labels[:max_samples]\n",
        "\n",
        "    return file_paths, labels\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Main block – run the whole pipeline\n",
        "# -------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the data root\n",
        "    data_root = os.path.join('.', 'pna_data')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wmZJ68e3YRnj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "ZFObqnpMmtEN",
        "outputId": "c1c742c0-d97d-46d5-9173-ee5cb77db6bf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtered_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3630372315.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_df\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'filtered_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select Number of DICOM Images to Process\n",
        "\n",
        "The variable `MAX_SAMPLES` specifies how many of the 9337 images that will be used in this lesson."
      ],
      "metadata": {
        "id": "F3rLZ4vkz6fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of images to use\n",
        "MAX_SAMPLES=9337\n",
        "\n",
        "# Generate filtered_df\n",
        "filtered_df, _ = load_and_preprocess_data(data_dir=data_root)\n",
        "\n",
        "# Build X, y\n",
        "X, y = create_dataset(filtered_df, data_dir=data_root, target_column='Target', max_samples=MAX_SAMPLES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "Z9ywWJ2aZIvg",
        "outputId": "5505cd99-b039-475c-bb49-3aae0f858277"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered DataFrame shape (with valid DICOM files): (9337, 7)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'create_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-409386021.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Build X, y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'create_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output"
      ],
      "metadata": {
        "id": "AFot5HGaZISY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Data\n",
        "\n",
        "num_files = X.shape[0]      # <-- first element of the tuple\n",
        "\n",
        "# Print image shapes\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "print(\"Sample data shape:\", X[0].shape if len(X) > 0 else \"No data\")\n",
        "\n",
        "\n",
        "# Ensure the data type is correct\n",
        "print(f\"Processing {num_files} DICOM files...\", end='')\n",
        "X = X.astype(np.float32)\n",
        "print(\"done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24oQNU7PZlin",
        "outputId": "d0e861ef-49cf-48dc-921b-bebed7ba103d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (5000, 1024, 1024, 3)\n",
            "Shape of y: (5000,)\n",
            "Sample data shape: (1024, 1024, 3)\n",
            "Processing 5000 DICOM files...done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Plotting Functions"
      ],
      "metadata": {
        "id": "UOGPR4qUZ3g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### **Create Plotting Functions**\n",
        "import seaborn as sns\n",
        "\n",
        "# Global settings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "\n",
        "# Create Plotting Functions\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Show a single DICOM image (for sanity checks)\n",
        "# -------------------------------------------------------------\n",
        "def display_dicom_image(file_path: str, figsize: tuple = (10, 10)):\n",
        "    \"\"\"Show a single DICOM image with proper orientation.\"\"\"\n",
        "    ds = pydicom.dcmread(file_path)\n",
        "    if hasattr(ds, 'pixel_array'):\n",
        "        img = ds.pixel_array\n",
        "        if getattr(ds, 'PhotometricInterpretation', None) == 'MONOCHROME1':\n",
        "            img = np.max(img) - img\n",
        "\n",
        "        plt.figure(figsize=figsize)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(ds.SOPClassUID)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"This DICOM has no pixel data.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Visualise class / target distributions\n",
        "# -------------------------------------------------------------\n",
        "def visualize_data_distribution(filtered_df: pd.DataFrame):\n",
        "    if filtered_df is None:\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Class distribution\n",
        "    plt.subplot(1, 2, 1)\n",
        "    if 'class' in filtered_df.columns:\n",
        "        class_counts = filtered_df['class'].value_counts()\n",
        "        class_counts.plot(kind='bar')\n",
        "        plt.title('Class Distribution')\n",
        "        plt.ylabel('Count')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No class column found', ha='center')\n",
        "        plt.title('Class Distribution')\n",
        "\n",
        "    # Target distribution\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if 'Target' in filtered_df.columns:\n",
        "        target_counts = filtered_df['Target'].value_counts()\n",
        "        target_counts.plot(kind='bar')\n",
        "        plt.title('Target Distribution')\n",
        "        plt.ylabel('Count')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No Target column found', ha='center')\n",
        "        plt.title('Target Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Plot train and val history\n",
        "# -------------------------------------------------------------\n",
        "def plot_train_val(history: dict,\n",
        "                 num_epochs: int,\n",
        "                 batch_size: int,\n",
        "                 title: str | None = None) -> None:\n",
        "    \"\"\"\n",
        "    Plot a two‑panel figure:\n",
        "        • Left panel – train & val accuracy.\n",
        "        • Right panel – train & val loss.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    history : dict\n",
        "        Must contain the keys 'train_acc', 'val_acc',\n",
        "        'train_loss', 'val_loss'.\n",
        "    num_epochs : int\n",
        "        Total number of epochs that were run.\n",
        "    batch_size : int\n",
        "        Batch size that was used (not plotted but kept for\n",
        "        API compatibility with the original function).\n",
        "    title : str | None, optional\n",
        "        Figure title.  If None, a default title is used.\n",
        "    \"\"\"\n",
        "    epochs = np.arange(1, num_epochs + 1)\n",
        "\n",
        "    # ---- 2‑panel layout ---------------------------------------\n",
        "    fig, (ax_acc, ax_loss) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # ---------- Left panel – accuracy --------------------------------\n",
        "    ax_acc.plot(epochs, history[\"train_acc\"], label=\"train acc\", color=\"C0\")\n",
        "    ax_acc.plot(epochs, history[\"val_acc\"],   label=\"val acc\",   color=\"C1\")\n",
        "    ax_acc.set_xlabel(\"Epoch\")\n",
        "    ax_acc.set_ylabel(\"Accuracy\")\n",
        "    ax_acc.set_ylim(0, 1)\n",
        "    ax_acc.legend(loc=\"lower right\")\n",
        "    ax_acc.set_title(\"Accuracy\")\n",
        "\n",
        "    # ---------- Right panel – loss ------------------------------------\n",
        "    ax_loss.plot(epochs, history[\"train_loss\"], label=\"train loss\", color=\"C2\")\n",
        "    ax_loss.plot(epochs, history[\"val_loss\"],   label=\"val loss\",   color=\"C3\")\n",
        "    ax_loss.set_xlabel(\"Epoch\")\n",
        "    ax_loss.set_ylabel(\"Loss\")\n",
        "    ax_loss.set_ylim(bottom=0)        # let matplotlib decide the top limit\n",
        "    ax_loss.legend(loc=\"upper right\")\n",
        "    ax_loss.set_title(\"Loss\")\n",
        "\n",
        "    # ---------- Figure title -----------------------------------------\n",
        "    if title is None:\n",
        "        title = \"Training History\"\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Keep the suptitle separate from the sub‑plots\n",
        "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "oswoHyl_Z3z1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CREATE DATA FUNCTIONS**"
      ],
      "metadata": {
        "id": "4zb__kXIZ-dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Data Functions\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms, models\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from typing import Optional\n",
        "\n",
        "# Set Import Variables\n",
        "NUM_EPOCHS: int = 4\n",
        "BATCH_SIZE: int = 8  # Reduced for memory management\n",
        "IMG_SIZE: int = 224  # Standard size for ResNet\n",
        "\n",
        "# ------------------------------------------\n",
        "# Custom Dataset Class with Transforms\n",
        "# ------------------------------------------\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, images: np.ndarray, labels: np.ndarray, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to PIL Image for transforms (this is the key fix!)\n",
        "        from PIL import Image\n",
        "\n",
        "        # Ensure proper shape: if it's CHW format convert to HWC\n",
        "        if len(image.shape) == 3 and image.shape[0] == 3:\n",
        "            # Transpose from CHW to HWC\n",
        "            image_np = np.transpose(image, (1, 2, 0))\n",
        "        else:\n",
        "            image_np = image\n",
        "\n",
        "        # Handle different value ranges\n",
        "        if image_np.max() <= 1.0:\n",
        "            image_np = (image_np * 255).astype(np.uint8)\n",
        "        else:\n",
        "            image_np = image_np.astype(np.uint8)\n",
        "\n",
        "        pil_image = Image.fromarray(image_np, mode='RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            processed_image = self.transform(pil_image)\n",
        "        else:\n",
        "            # If no transform, convert back to tensor manually\n",
        "            processed_image = torch.from_numpy(image).float()\n",
        "            if len(processed_image.shape) == 3:  # CHW format already?\n",
        "                pass  # Already correct\n",
        "\n",
        "        return processed_image, label\n",
        "\n",
        "# ------------------------------------------\n",
        "# Helper: Build transforms\n",
        "# ------------------------------------------\n",
        "def get_transform(\n",
        "    img_size=IMG_SIZE,\n",
        "    is_train: bool = True,\n",
        "    crop_size=IMG_SIZE,\n",
        "    h_flip: bool = True,\n",
        "    augment: bool = False\n",
        ") -> transforms.Compose:\n",
        "    \"\"\"\n",
        "    Returns a torchvision transform chain.\n",
        "    \"\"\"\n",
        "    if is_train:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.RandomResizedCrop(crop_size) if augment else transforms.CenterCrop(crop_size),\n",
        "            transforms.RandomHorizontalFlip() if h_flip else transforms.Lambda(lambda x: x),\n",
        "            transforms.ToTensor(),  # This will convert to float and normalize [0,1]\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "    else:  # eval / test\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.CenterCrop(crop_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    return transform\n",
        "\n",
        "# ------------------------------------------\n",
        "# Helper: Build dataloaders\n",
        "# ------------------------------------------\n",
        "def build_dataloaders(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    val_split: float = 0.2,\n",
        "    seed: int = 42,\n",
        "    num_workers: int = 4,\n",
        ") -> tuple[DataLoader, DataLoader]:\n",
        "    \"\"\"\n",
        "    Returns training and validation DataLoaders.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure data is in proper format (CHW)\n",
        "    if len(X.shape) == 4 and X.shape[-1] != 3:\n",
        "        print(\"Converting from HWC to CHW format...\")\n",
        "        X = np.transpose(X, (0, 3, 1, 2))  # Convert (N,H,W,C) to (N,C,H,W)\n",
        "\n",
        "    # Apply transforms\n",
        "    transform_train = get_transform(is_train=True)\n",
        "    transform_eval = get_transform(is_train=False)\n",
        "\n",
        "    dataset = SimpleImageDataset(X, y, transform=transform_train)\n",
        "\n",
        "    total = len(dataset)\n",
        "    val_len = int(total * val_split)\n",
        "    train_len = total - val_len\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "    # Override transforms for validation\n",
        "    val_ds.dataset.transform = transform_eval\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# ------------------------------------------\n",
        "# Training loop\n",
        "# ------------------------------------------\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    model_name: str | None = None,          # NEW\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Train a single epoch.  The new ``model_name`` argument is *only* for\n",
        "    bookkeeping – it is stored on the model as ``model.name``.\n",
        "    \"\"\"\n",
        "    if model_name is not None:\n",
        "        # Store the name for later inspection\n",
        "        model.name = model_name\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for imgs, targets in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    return epoch_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# Measure validation loss during training\n",
        "# --------------------------------------------\n",
        "def validate(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> tuple[float, float]:\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in tqdm(loader, desc=\"Validation\", leave=False):\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            epoch_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == targets).sum().item()\n",
        "\n",
        "    val_loss = epoch_loss / len(loader.dataset)\n",
        "    val_acc = correct / len(loader.dataset)\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# ------------------------------------------\n",
        "# Get ResNet50 model\n",
        "# ------------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "def get_resnet50(\n",
        "    num_classes: int,\n",
        "    pretrained: bool = True,\n",
        "    device: torch.device | None = None,\n",
        "    name: str | None = None,\n",
        "    dropout_p: float = 0.0,\n",
        "    custom_layers: list[nn.Module] | None = None,\n",
        ") -> nn.Module:\n",
        "    \"\"\"\n",
        "    Return a *complete* ResNet‑50 model with optional layers after the backbone.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_classes : int\n",
        "        Number of classes for the final classifier.\n",
        "    pretrained : bool, default=True\n",
        "        If True, load ImageNet pre‑training.\n",
        "    device : torch.device | None, default=None\n",
        "        If None, pick CUDA if available, otherwise CPU.\n",
        "    name : str | None, default=None\n",
        "        A name that is attached to the returned model (`model.name = name`).\n",
        "    dropout_p : float, default=0.0\n",
        "        Drop‑out probability that will be inserted right before the final linear.\n",
        "    custom_layers : list[nn.Module] | None, default=None\n",
        "        Any extra modules you want to inject *after* the backbone and *before*\n",
        "        the classifier.  For example: `[nn.Dropout(0.3), nn.BatchNorm1d(2048)]`.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    nn.Module\n",
        "        A `torch.nn.Sequential` that implements:\n",
        "            backbone → (custom layers) → final Linear\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1. Load the plain ResNet‑50\n",
        "    backbone = models.resnet50(pretrained=pretrained)\n",
        "\n",
        "    # 2. Store the original fully‑connected in‑features (2048 for ResNet‑50)\n",
        "    in_features = backbone.fc.in_features\n",
        "\n",
        "    # 3. Replace the default fc with Identity → backbone now returns 2048‑D\n",
        "    backbone.fc = nn.Identity()\n",
        "\n",
        "    # 4. Build the list of modules that will be wrapped into nn.Sequential\n",
        "    layers: list[nn.Module] = [backbone]\n",
        "\n",
        "    # 5. Inject any user‑supplied layers\n",
        "    if custom_layers:\n",
        "        layers.extend(custom_layers)\n",
        "\n",
        "    # 6. Optional Drop‑out *after* the backbone\n",
        "    if dropout_p > 0.0:\n",
        "        layers.append(nn.Dropout(p=dropout_p))\n",
        "\n",
        "    # 7. Final classification head\n",
        "    layers.append(nn.Linear(in_features, num_classes))\n",
        "\n",
        "    # 8. Stack everything\n",
        "    model = nn.Sequential(*layers)\n",
        "\n",
        "    # 9. Move to the requested device\n",
        "    model.to(device)\n",
        "\n",
        "    # 10. Attach a human‑friendly name (if supplied)\n",
        "    if name is not None:\n",
        "        model.name = name\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Training routine\n",
        "# ------------------------------------------\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "#  Updated run_training() – now works with a lazy DicomImageDataset\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import copy, gc\n",
        "\n",
        "def run_training_lazy(\n",
        "    file_paths: list[str],\n",
        "    labels: list[int],\n",
        "    num_epochs: int = NUM_EPOCHS,\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "    lr: float = 1e-4,\n",
        "    weight_decay: float = 1e-4,\n",
        "    val_split: float = 0.2,\n",
        "    device: torch.device | None = None,\n",
        "    patience: int = 5,\n",
        "    early_stopping: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Very small wrapper that mirrors the logic of the original\n",
        "    `run_training()` but now accepts a *list of file paths* instead of\n",
        "    a huge NumPy array.  All heavy lifting (reading DICOM, resizing,\n",
        "    normalisation) is done in the `DicomImageDataset` class defined above.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 1. Create the two lazy datasets\n",
        "    # --------------------------------------------------------------------------\n",
        "    transform_train = get_transform(is_train=True)\n",
        "    transform_val   = get_transform(is_train=False)\n",
        "\n",
        "    full_dataset = DicomImageDataset(file_paths, labels, transform=transform_train)\n",
        "\n",
        "    # 2. Train / val split\n",
        "    train_size = int(len(full_dataset) * (1 - val_split))\n",
        "    val_size   = len(full_dataset) - train_size\n",
        "    train_ds, val_ds = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    # Override the validation transform\n",
        "    val_ds.dataset.transform = transform_val\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=4,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        pin_memory=True,\n",
        "        num_workers=4,\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 3. Build the model (or accept a custom one)\n",
        "    # --------------------------------------------------------------------------\n",
        "    if full_dataset.num_classes is None:\n",
        "        num_classes = max(labels) + 1\n",
        "    else:\n",
        "        num_classes = full_dataset.num_classes\n",
        "\n",
        "    model = get_resnet50(num_classes=num_classes, pretrained=True, device=device)\n",
        "    model.to(device)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 4. Loss / optimiser / scheduler\n",
        "    # --------------------------------------------------------------------------\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=num_epochs, eta_min=lr * 0.01\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 5. Early‑stopping bookkeeping\n",
        "    # --------------------------------------------------------------------------\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_epoch = 0\n",
        "    best_train_acc = best_val_acc = None\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    epochs_without_improve = 0\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 6. Epoch loop\n",
        "    # --------------------------------------------------------------------------\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        # -------- Training ------------------------------------------------------\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item() * xb.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct_train += (preds == yb).sum().item()\n",
        "\n",
        "        epoch_train_loss /= len(train_loader.dataset)\n",
        "        epoch_train_acc = correct_train / len(train_loader.dataset)\n",
        "\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc)\n",
        "\n",
        "        # -------- Validation ----------------------------------------------------\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "\n",
        "                epoch_val_loss += loss.item() * xb.size(0)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                correct_val += (preds == yb).sum().item()\n",
        "\n",
        "        epoch_val_loss /= len(val_loader.dataset)\n",
        "        epoch_val_acc = correct_val / len(val_loader.dataset)\n",
        "\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # -------- Early‑stopping ------------------------------------------------\n",
        "        if early_stopping:\n",
        "            if epoch_val_loss < best_val_loss - 1e-5:\n",
        "                best_val_loss = epoch_val_loss\n",
        "                best_epoch = epoch\n",
        "                best_train_acc = epoch_train_acc\n",
        "                best_val_acc = epoch_val_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                epochs_without_improve = 0\n",
        "            else:\n",
        "                epochs_without_improve += 1\n",
        "\n",
        "            if epochs_without_improve >= patience:\n",
        "                print(\"\\nEarly stopping triggered.\")\n",
        "                print(f\"Best epoch (before stopping): {best_epoch}\")\n",
        "                print(f\"  Train Acc: {best_train_acc:.4f} | Train Loss: {epoch_train_loss:.4f}\")\n",
        "                print(f\"  Valid Acc: {best_val_acc:.4f} | Valid Loss: {epoch_val_loss:.4f}\\n\")\n",
        "                model.load_state_dict(best_model_wts)\n",
        "                break\n",
        "\n",
        "        # -------- Progress ------------------------------------------------------\n",
        "        print(\n",
        "            f\"Epoch {epoch}/{num_epochs} | \"\n",
        "            f\"Train Acc:  {epoch_train_acc:.4f} | \"\n",
        "            f\"Train Loss: {epoch_train_loss:.4f} | \"\n",
        "            f\"Val Acc:    {epoch_val_acc:.4f} | \"\n",
        "            f\"Val Loss:   {epoch_val_loss:.4f}\"\n",
        "        )\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 7. Final report\n",
        "    # --------------------------------------------------------------------------\n",
        "    if not early_stopping or epochs_without_improve < patience:\n",
        "        best_epoch = epoch\n",
        "        best_train_acc = epoch_train_acc\n",
        "        best_val_acc = epoch_val_acc\n",
        "        best_val_loss = epoch_val_loss\n",
        "\n",
        "    print(\"\\nTraining finished.\")\n",
        "    print(f\"Best epoch: {best_epoch}\")\n",
        "    print(f\"  Train Acc: {best_train_acc:.4f} | Train Loss: {epoch_train_loss:.4f}\")\n",
        "    print(f\"  Val   Acc: {best_val_acc:.4f} | Val   Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 8. Return history\n",
        "    # --------------------------------------------------------------------------\n",
        "    return {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"train_acc\": train_accs,\n",
        "        \"val_loss\": val_losses,\n",
        "        \"val_acc\": val_accs,\n",
        "        \"best_epoch\": best_epoch,\n",
        "        \"best_train_acc\": best_train_acc,\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"best_train_loss\": epoch_train_loss,\n",
        "        \"best_val_loss\": epoch_val_loss,\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "C08yJuVxZ-zN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MoUS42V0q21M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QKktZjsuL8n",
        "outputId": "f53c8107-d5ad-4be1-a370-a3680a22d97e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g44pnV-VmJm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kSNGUTeQmb3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────  1.  Load the CSV / TSV that contains only patientId  ────────\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1.1  Load the label CSV (it contains 'patientId' + 'Target')\n",
        "# -------------------------------------------------------------\n",
        "LABELS_CSV = \"/content/pna_data/pna_train_labels.csv\"\n",
        "labels_df = pd.read_csv(LABELS_CSV)      # columns: ['patientId', 'Target']\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1.2  Merge `filtered_df` (which already has 'patientId' & 'Target')\n",
        "# -------------------------------------------------------------\n",
        "#   * Keep only the first occurrence of each patient (duplicates come from\n",
        "#     multiple bounding‑box rows)\n",
        "merged_df = (\n",
        "    filtered_df\n",
        "    .merge(labels_df, on=\"patientId\", how=\"left\")   # (optional – you already have Target)\n",
        "    .drop_duplicates(subset=\"patientId\")           # one image per patient\n",
        "    .dropna(subset=[\"Target\"])                     # safety: no missing labels\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1.3  Build absolute file‑paths for the DICOMs\n",
        "# -------------------------------------------------------------\n",
        "IMAGES_DIR = Path(\"/content/pna_data/pna_train_images\")\n",
        "merged_df[\"file_path\"] = merged_df[\"patientId\"].apply(\n",
        "    lambda pid: IMAGES_DIR / f\"{pid}.dcm\"\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1.4  Extract the two lists required by the lazy loader\n",
        "# -------------------------------------------------------------\n",
        "file_paths = merged_df[\"file_path\"].tolist()\n",
        "labels     = merged_df[\"Target\"].astype(int).tolist()\n",
        "\n",
        "# (Optional) limit the number of samples you actually train on\n",
        "# MAX_SAMPLES = 5000\n",
        "# file_paths, labels = file_paths[:MAX_SAMPLES], labels[:MAX_SAMPLES]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "NGaxny2OmcVO",
        "outputId": "3beddf85-2cff-42cf-ae12-ec4c6f5ae6e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtered_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1352720228.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#     multiple bounding‑box rows)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m merged_df = (\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mfiltered_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"patientId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (optional – you already have Target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"patientId\"\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# one image per patient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filtered_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TRAIN NEURAL NETWORK**"
      ],
      "metadata": {
        "id": "M1HSCbnQuG_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uqjhvG7NSsqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/pna_data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxtA5LFKXJRL",
        "outputId": "63ffa27b-1f2b-429e-b927-230504fc07be"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pna_detailed_class_info.csv  pna_train_images  pna_train_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KzJa51ihXtip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_paths_and_labels(filtered_df, max_samples=None):\n",
        "    file_paths = filtered_df[\"file_path\"].tolist()\n",
        "    labels = filtered_df[\"label\"].tolist()\n",
        "\n",
        "    if max_samples is not None:\n",
        "        file_paths = file_paths[:max_samples]\n",
        "        labels = labels[:max_samples]\n",
        "\n",
        "    return file_paths, labels"
      ],
      "metadata": {
        "id": "ZuaCFq9yXuJJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KcfWBfn_X1Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "H8o8X7IFX1iI",
        "outputId": "3172a95f-08de-4dc3-93ca-a86f3b2f539a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'file_paths' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-823621096.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'file_paths' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "qm4WeiW8YKzv",
        "outputId": "3956d1e7-f1a3-4256-a492-d8b4c6dd93fb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtered_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1863951626.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'filtered_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jrMdYVA5axZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "60_AKfYbdndK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────  1.  Prepare data from the already‑filtered dataframe  ────────\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- 1.1  Drop duplicate patients (multiple bounding‑box rows)  ----------\n",
        "merged_df = (\n",
        "    filtered_df\n",
        "    .drop_duplicates(subset=\"patientId\")          # keep only one row per patient\n",
        "    .dropna(subset=[\"Target\"])                    # safety: no missing labels\n",
        ")\n",
        "\n",
        "# ---------- 1.2  Build absolute file‑paths for the DICOMs  ----------\n",
        "IMAGES_DIR = Path(\"/content/pna_data/pna_train_images\")\n",
        "merged_df[\"file_path\"] = merged_df[\"patientId\"].apply(\n",
        "    lambda pid: IMAGES_DIR / f\"{pid}.dcm\"\n",
        ")\n",
        "\n",
        "# ---------- 1.3  Extract the two lists needed by `run_training_lazy`  ----------\n",
        "file_paths = merged_df[\"file_path\"].tolist()\n",
        "labels     = merged_df[\"Target\"].astype(int).tolist()\n",
        "\n",
        "# (Optional) limit the number of samples you actually train on\n",
        "# MAX_SAMPLES = 5000\n",
        "# file_paths, labels = file_paths[:MAX_SAMPLES], labels[:MAX_SAMPLES]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "K3EjVkiPeKXQ",
        "outputId": "fd360220-d6f3-4f7f-8a68-aad589928985"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtered_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3460434962.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ---------- 1.1  Drop duplicate patients (multiple bounding‑box rows)  ----------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m merged_df = (\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfiltered_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"patientId\"\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# keep only one row per patient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;31m# safety: no missing labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filtered_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X0KCVDl8eJFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oxxPW9BvfWCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────────────────────────────────────\n",
        "# 2.  Train the model (make sure labels are the *list* you built above)\n",
        "# ───────────────────────────────────────────────────\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2.1  Re‑create the lists just to be 100 % sure we’re passing the right objects\n",
        "# ------------------------------------------------------------------\n",
        "file_paths_list = merged_df[\"file_path\"].tolist()\n",
        "labels_list     = merged_df[\"Target\"].astype(int).tolist()\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2.2  Train\n",
        "# ------------------------------------------------------------------\n",
        "NUM_EPOCHS   = 20\n",
        "BATCH_SIZE   = 4\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY  = 1e-4\n",
        "VAL_SPLIT     = 0.2\n",
        "PATIENCE      = 7\n",
        "EARLY_STOPPING = True\n",
        "\n",
        "history = run_training_lazy(\n",
        "    file_paths   = file_paths_list,\n",
        "    labels       = labels_list,\n",
        "    num_epochs   = NUM_EPOCHS,\n",
        "    batch_size   = BATCH_SIZE,\n",
        "    lr           = LEARNING_RATE,\n",
        "    weight_decay = WEIGHT_DECAY,\n",
        "    val_split    = VAL_SPLIT,\n",
        "    patience     = PATIENCE,\n",
        "    early_stopping = EARLY_STOPPING\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "Sh6bwgDBfWm0",
        "outputId": "323a7311-04c1-4558-9215-a62cc7d693c1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'merged_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2004260371.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 2.1  Re‑create the lists just to be 100 % sure we’re passing the right objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfile_paths_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mlabels_list\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'merged_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NDBdNGDTfU-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train neural network\n",
        "import time\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Get model\n",
        "NUM_CLASSES = len(set(y))          # y is your numpy array of labels\n",
        "pna_model = get_resnet50(num_classes=NUM_CLASSES, name=\"pna_model\")\n",
        "\n",
        "print(f\"---Training is starting for {NUM_EPOCHS} epochs ----------\")\n",
        "start_time = time.time()\n",
        "\n",
        "history = run_training(\n",
        "    X, y,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    val_split=0.2,\n",
        "    patience=7,          # stop after 7 val‑loss plateaus\n",
        "    early_stopping=True  # default – set False to ignore early stopping\n",
        ")\n",
        "\n",
        "# Record end time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "tA2a6RsIuPKX",
        "outputId": "e2a0d13a-2414-42f8-f6fc-15c7560ff4a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-510265688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Get model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mNUM_CLASSES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# y is your numpy array of labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpna_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_resnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pna_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nJfZec6wvhDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model name: {model.name}\")      # → ResNet50+Dropout\n",
        "#print(model)                            # prints the full Sequential\n",
        "print(next(model.parameters()).shape)   # should be (2048,) after backbone\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqOxJPzGvhhS",
        "outputId": "5c31cbbd-e4d3-46a1-977c-82be69361f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model name: ResNet50+Dropout\n",
            "torch.Size([64, 3, 7, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wU9z0V7OgLE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_val(history, NUM_EPOCHS, BATCH_SIZE, title=\"Training and Validation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "MzC5I0z3il38",
        "outputId": "20bd6b27-ae51-4afe-a39e-a0f6df516948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (20,) and (13,)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-437215139.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_train_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training and Validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-645914444.py\u001b[0m in \u001b[0;36mplot_train_val\u001b[0;34m(history, num_epochs, batch_size, title)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# ---------- Left panel – accuracy --------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0max_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0max_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val acc\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0max_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \"\"\"\n\u001b[1;32m   1776\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1777\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    495\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (13,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}