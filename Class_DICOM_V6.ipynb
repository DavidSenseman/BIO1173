{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOM0kJlgCFhfJRPX1/yPK94",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_DICOM_V6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "moE_n1EYbFbb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJfLC-SQa0nP",
        "outputId": "3dbe82ef-f98f-4360-c87b-8e5eb647bac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydicom"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BkEkd9_3bNzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "import zipfile\n",
        "import sys\n",
        "import shutil\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Configuration – change only if you want a different URL / filename\n",
        "# ------------------------------------------------------------------\n",
        "URL = \"https://biologicslab.co/BIO1173/data/\"\n",
        "ZIP_FILENAME = \"pna_data.zip\"\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Download the zip file (streamed, so it works with large files)\n",
        "# ------------------------------------------------------------------\n",
        "def download_zip(url: str, dest: Path, chunk_size: int = 8192) -> None:\n",
        "    \"\"\"Download a file from `url` and write it to `dest`.\"\"\"\n",
        "    print(f\"Downloading {ZIP_FILENAME} to {dest}...\", end='')\n",
        "    with requests.get(url, stream=True, timeout=30) as r:\n",
        "        r.raise_for_status()           # will raise for 4xx/5xx\n",
        "        with dest.open(\"wb\") as f_out:\n",
        "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                if chunk:               # filter out keep‑alive new chunks\n",
        "                    f_out.write(chunk)\n",
        "    print(\"done\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Un‑zip the downloaded archive into a *named* directory\n",
        "# ------------------------------------------------------------------\n",
        "def unzip_file(zip_path: Path, extract_to: Path) -> None:\n",
        "    \"\"\"Extract all members of `zip_path` into `extract_to`.\"\"\"\n",
        "    print(f\"Unzipping {ZIP_FILENAME} to {extract_to}...\", end='')\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(extract_to)\n",
        "    print(\"done\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Optional – delete the zip after extraction\n",
        "# ------------------------------------------------------------------\n",
        "def clean_up_zip(zip_path: Path) -> None:\n",
        "    \"\"\"Delete the zip file – only if you no longer need it.\"\"\"\n",
        "    zip_path.unlink()\n",
        "    print(f\"Removed temporary archive: {zip_path}... done\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Main routine\n",
        "# ------------------------------------------------------------------\n",
        "def main() -> None:\n",
        "    cwd          = Path.cwd()            # current working directory\n",
        "    zip_path     = cwd / ZIP_FILENAME\n",
        "    extract_dir  = cwd / zip_path.stem   # e.g. /pna_data\n",
        "\n",
        "    # Ensure the extraction directory exists\n",
        "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download\n",
        "    download_zip(URL+ZIP_FILENAME, zip_path)\n",
        "\n",
        "    # Un‑zip\n",
        "    unzip_file(zip_path, extract_dir)\n",
        "\n",
        "    # Clean‑up the downloaded archive\n",
        "    clean_up_zip(zip_path)\n",
        "\n",
        "    print(f\"Files have been extracted to {extract_dir}\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viYtPwa4bcBx",
        "outputId": "a4516d65-5b0f-41c8-a67e-83e10d7f832b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading pna_data.zip to /content/pna_data.zip...done\n",
            "Unzipping pna_data.zip to /content/pna_data...done\n",
            "Removed temporary archive: /content/pna_data.zip... done\n",
            "Files have been extracted to /content/pna_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w1UUwA-7bhRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "\n",
        "# Global settings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Helper: read a single DICOM file\n",
        "# -------------------------------------------------------------\n",
        "def read_dicom_file(file_path: str):\n",
        "    \"\"\"Read a DICOM file and extract image data + basic metadata.\"\"\"\n",
        "    ds = pydicom.dcmread(file_path)\n",
        "\n",
        "    # Basic metadata\n",
        "    metadata = {\n",
        "        'filename': os.path.basename(file_path),\n",
        "        'patient_name': getattr(ds, 'PatientName', 'Unknown'),\n",
        "        'patient_id': getattr(ds, 'PatientID', 'Unknown'),\n",
        "        'study_date': getattr(ds, 'StudyDate', 'Unknown'),\n",
        "        'study_time': getattr(ds, 'StudyTime', 'Unknown'),\n",
        "        'modality': getattr(ds, 'Modality', 'Unknown'),\n",
        "        'manufacturer': getattr(ds, 'Manufacturer', 'Unknown'),\n",
        "        'institution_name': getattr(ds, 'InstitutionName', 'Unknown'),\n",
        "        'series_description': getattr(ds, 'SeriesDescription', 'Unknown'),\n",
        "        'bits_allocated': getattr(ds, 'BitsAllocated', 'Unknown'),\n",
        "        'rows': getattr(ds, 'Rows', 'Unknown'),\n",
        "        'columns': getattr(ds, 'Columns', 'Unknown'),\n",
        "        'pixel_spacing': getattr(ds, 'PixelSpacing', 'Unknown')\n",
        "    }\n",
        "\n",
        "    # Image data\n",
        "    if hasattr(ds, 'pixel_array'):\n",
        "        image_array = ds.pixel_array\n",
        "\n",
        "        # Normalise to 0‑255 if needed\n",
        "        if image_array.dtype != np.uint8:\n",
        "            image_array = ((image_array - image_array.min()) /\n",
        "                           (image_array.max() - image_array.min()) * 255).astype(np.uint8)\n",
        "\n",
        "        metadata['image_available'] = True\n",
        "        metadata['image_shape'] = image_array.shape\n",
        "    else:\n",
        "        metadata['image_available'] = False\n",
        "        metadata['image_shape'] = 'No image data'\n",
        "\n",
        "    return ds, metadata\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Helper: fast drop‑check\n",
        "# -------------------------------------------------------------\n",
        "def is_file_dropped(file_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Quick guard that tells us whether a DICOM file is already\n",
        "    missing / unreadable.\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(file_path):\n",
        "        return True\n",
        "\n",
        "    if os.path.getsize(file_path) == 0:\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        pydicom.dcmread(file_path, stop_before_pixels=True)\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Load & preprocess – merge CSVs, keep only valid DICOM rows\n",
        "# -------------------------------------------------------------\n",
        "def load_and_preprocess_data(\n",
        "    data_dir: str = '.',\n",
        "    log_dropped: bool = True\n",
        "):\n",
        "    # Load the two CSVs, merge on patient ID, and keep only rows\n",
        "    # that have an intact DICOM image.\n",
        "    info_df   = pd.read_csv(os.path.join(data_dir, 'pna_detailed_class_info.csv'))\n",
        "    labels_df = pd.read_csv(os.path.join(data_dir, 'pna_train_labels.csv'))\n",
        "\n",
        "    # Define patient ID variable\n",
        "    info_id_col   = 'patientId'\n",
        "    labels_id_col = 'patientId'\n",
        "\n",
        "    merged_df = pd.merge(info_df, labels_df, left_on=info_id_col,\n",
        "                         right_on=labels_id_col, how='inner')\n",
        "\n",
        "    dicom_dir = os.path.join(data_dir, 'pna_train_images')\n",
        "    valid_rows = []\n",
        "    dropped_ids = []\n",
        "\n",
        "    for idx, row in merged_df.iterrows():\n",
        "        patient_id = row[info_id_col]\n",
        "        dicom_file = os.path.join(dicom_dir, f\"{patient_id}.dcm\")\n",
        "\n",
        "        if is_file_dropped(dicom_file):\n",
        "            dropped_ids.append(patient_id)\n",
        "        else:\n",
        "            valid_rows.append(idx)\n",
        "\n",
        "    filtered_df = merged_df.loc[valid_rows].copy()\n",
        "    print(f\"Filtered DataFrame shape (with valid DICOM files): {filtered_df.shape}\")\n",
        "\n",
        "    return filtered_df, dropped_ids\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Build X, y arrays – skip any DICOM that fails\n",
        "# -------------------------------------------------------------\n",
        "def create_dataset(\n",
        "    filtered_df: pd.DataFrame,\n",
        "    data_dir: str = '.',\n",
        "    target_column: str = 'Target',\n",
        "    max_samples: int | None = None,\n",
        "    verbose: bool = True,\n",
        "    # optional: allow a reproducible random seed\n",
        "    random_state: int | None = None,\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Build X, y arrays from the given DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filtered_df : pd.DataFrame\n",
        "        DataFrame that contains (at least) the columns `patientId` and the target.\n",
        "    data_dir : str, default='.'\n",
        "        Base directory that contains the sub‑folder `pna_train_images/`.\n",
        "    target_column : str, default='Target'\n",
        "        Column name that holds the target label.\n",
        "    max_samples : int | None, default=None\n",
        "        If provided, a random subset of this size will be chosen from\n",
        "        ``filtered_df`` before loading.  If ``max_samples`` is larger\n",
        "        than the number of available rows, all rows are used.\n",
        "    verbose : bool, default=True\n",
        "        Show progress / debug messages.\n",
        "    random_state : int | None, default=None\n",
        "        Seed for reproducible shuffling.  Pass an integer if you need\n",
        "        deterministic behaviour; otherwise the selection is truly random.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : np.ndarray\n",
        "        Image data of shape (N, H, W, 3) with dtype uint8.\n",
        "    y : np.ndarray\n",
        "        Corresponding target labels of shape (N,) with dtype int.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    from tqdm import tqdm\n",
        "    import numpy as np\n",
        "\n",
        "    if filtered_df.empty:\n",
        "        if verbose:\n",
        "            print(\"Input DataFrame is empty – nothing to load.\")\n",
        "        return np.array([], dtype=np.uint8), np.array([], dtype=int)\n",
        "\n",
        "    # ---------- Randomly sub‑sample if requested ----------\n",
        "    if max_samples is not None:\n",
        "        n_available = len(filtered_df)\n",
        "        if max_samples < n_available:\n",
        "            # ``sample`` performs a random shuffle; replace=False guarantees\n",
        "            # unique rows.  ``random_state`` makes the selection reproducible.\n",
        "            filtered_df = filtered_df.sample(n=max_samples, random_state=random_state)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\n",
        "                    f\"Requested max_samples={max_samples} \"\n",
        "                    f\"but only {n_available} rows are available – \"\n",
        "                    \"using all rows.\"\n",
        "                )\n",
        "    # ---------- Build dataset ----------\n",
        "    print(\"Creating dataset\")\n",
        "\n",
        "    dicom_dir = os.path.join(data_dir, \"pna_train_images\")\n",
        "\n",
        "    iterator = tqdm(\n",
        "        filtered_df.iterrows(),\n",
        "        total=len(filtered_df),\n",
        "        desc=\"Loading DICOM Images\",\n",
        "        disable=not verbose,\n",
        "    )\n",
        "\n",
        "    images, labels = [], []\n",
        "\n",
        "    info_id_col, labels_id_col = 'patientId', 'patientId'\n",
        "\n",
        "    for i, (_, row) in enumerate(iterator, start=1):\n",
        "        patient_id = row[info_id_col]\n",
        "        dicom_file = os.path.join(dicom_dir, f\"{patient_id}.dcm\")\n",
        "\n",
        "        if not os.path.exists(dicom_file):\n",
        "            if verbose:\n",
        "                tqdm.write(f\"[{i}] Missing file: {patient_id}\")\n",
        "            continue\n",
        "\n",
        "        ds, metadata = read_dicom_file(dicom_file)\n",
        "\n",
        "        if metadata is None or not metadata.get(\"image_available\", False):\n",
        "            if verbose:\n",
        "                tqdm.write(f\"[{i}] No image in {patient_id}\")\n",
        "            continue\n",
        "\n",
        "        img = ds.pixel_array\n",
        "        if img.ndim == 2:\n",
        "            img = np.stack([img] * 3, axis=-1)\n",
        "\n",
        "        images.append(img.astype(np.uint8))\n",
        "        labels.append(int(row[target_column]))\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Assemble the final arrays *after* the loop finishes\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    X = np.stack(images, axis=0) if images else np.array([], dtype=np.uint8)\n",
        "    y = np.array(labels, dtype=int) if labels else np.array([], dtype=int)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"The number of Image files = {len(X)}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Main block – run the whole pipeline\n",
        "# -------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the data root\n",
        "    data_root = os.path.join('.', 'pna_data')"
      ],
      "metadata": {
        "id": "gxit5npSbh3Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JqvC503rbrwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "\n",
        "# Define number of images to use\n",
        "MAX_SAMPLES=500\n",
        "\n",
        "# Generate filtered_df\n",
        "filtered_df, _ = load_and_preprocess_data(data_dir=data_root)\n",
        "\n",
        "# Build X, y\n",
        "X, y = create_dataset(filtered_df, data_dir=data_root, target_column='Target', max_samples=MAX_SAMPLES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw0S6bqobsYI",
        "outputId": "f4ece140-1f7e-4a50-f676-a14f512fdbdf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered DataFrame shape (with valid DICOM files): (9337, 7)\n",
            "Creating dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading DICOM Images: 100%|██████████| 500/500 [00:04<00:00, 120.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of Image files = 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uXNGUU8kb0SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms, models\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set Import Variables\n",
        "NUM_EPOCHS: int = 4\n",
        "BATCH_SIZE: int = 8  # Reduced for memory management\n",
        "IMG_SIZE: int = 224  # Standard size for ResNet\n",
        "\n",
        "# ------------------------------------------\n",
        "# Custom Dataset Class with Transforms\n",
        "# ------------------------------------------\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, images: np.ndarray, labels: np.ndarray, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Ensure image is in CHW format (N,C,H,W)\n",
        "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
        "            image = np.transpose(image, (2, 0, 1))  # Convert HWC to CHW\n",
        "\n",
        "        image = torch.from_numpy(image).float()\n",
        "\n",
        "        if self.transform:\n",
        "            # Apply transforms\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# ------------------------------------------\n",
        "# Helper: Build transforms\n",
        "# ------------------------------------------\n",
        "def get_transform(\n",
        "    img_size=IMG_SIZE,\n",
        "    is_train: bool = True,\n",
        "    crop_size=IMG_SIZE,\n",
        "    h_flip: bool = True,\n",
        "    augment: bool = False\n",
        ") -> transforms.Compose:\n",
        "    \"\"\"\n",
        "    Returns a torchvision transform chain.\n",
        "    \"\"\"\n",
        "    if is_train:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.RandomResizedCrop(crop_size) if augment else transforms.CenterCrop(crop_size),\n",
        "            transforms.RandomHorizontalFlip() if h_flip else transforms.Lambda(lambda x: x),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "    else:  # eval / test\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.CenterCrop(crop_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    return transform\n",
        "\n",
        "# ------------------------------------------\n",
        "# Helper: Build dataloaders\n",
        "# ------------------------------------------\n",
        "def build_dataloaders(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    val_split: float = 0.2,\n",
        "    seed: int = 42,\n",
        "    num_workers: int = 4,\n",
        ") -> tuple[DataLoader, DataLoader]:\n",
        "    \"\"\"\n",
        "    Returns training and validation DataLoaders.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure data is in proper format (CHW)\n",
        "    if len(X.shape) == 4 and X.shape[-1] != 3:\n",
        "        print(\"Converting from HWC to CHW format...\")\n",
        "        X = np.transpose(X, (0, 3, 1, 2))  # Convert (N,H,W,C) to (N,C,H,W)\n",
        "\n",
        "    # Apply transforms\n",
        "    transform_train = get_transform(is_train=True)\n",
        "    transform_eval = get_transform(is_train=False)\n",
        "\n",
        "    dataset = ImageDataset(X, y, transform=transform_train)\n",
        "\n",
        "    total = len(dataset)\n",
        "    val_len = int(total * val_split)\n",
        "    train_len = total - val_len\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "    # Override transforms for validation\n",
        "    val_ds.dataset.transform = transform_eval\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# ------------------------------------------\n",
        "# Training loop\n",
        "# ------------------------------------------\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        ") -> float:\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for imgs, targets in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    return epoch_loss / len(loader.dataset)\n",
        "\n",
        "# --------------------------------------------\n",
        "# Measure validation loss during training\n",
        "# --------------------------------------------\n",
        "def validate(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> tuple[float, float]:\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in tqdm(loader, desc=\"Validation\", leave=False):\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            epoch_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == targets).sum().item()\n",
        "\n",
        "    val_loss = epoch_loss / len(loader.dataset)\n",
        "    val_acc = correct / len(loader.dataset)\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# ------------------------------------------\n",
        "# Get ResNet50 model\n",
        "# ------------------------------------------\n",
        "def get_resnet50(\n",
        "    num_classes: int,\n",
        "    pretrained: bool = True,\n",
        "    device: torch.device | None = None,\n",
        "    name: str | None = None\n",
        ") -> nn.Module:\n",
        "    \"\"\"Return a ResNet‑50 backbone.  Optionally attach a `name` attribute.\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    backbone = models.resnet50(pretrained=pretrained)\n",
        "    backbone.fc = nn.Linear(backbone.fc.in_features, num_classes)\n",
        "    backbone.to(device)\n",
        "\n",
        "    # Attach a name only if one was supplied\n",
        "    if name is not None:\n",
        "        backbone.name = name\n",
        "    return backbone\n",
        "\n",
        "# ------------------------------------------\n",
        "# Training routine\n",
        "# ------------------------------------------\n",
        "def run_training(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    lr: float = 1e-4,\n",
        "    weight_decay: float = 1e-4,\n",
        "    val_split: float = 0.2,\n",
        "    device: torch.device | None = None,\n",
        "    model: nn.Module | None = None\n",
        ") -> dict:\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # Device handling\n",
        "    # -------------------------------------------------------------\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # Prepare the dataset with proper transforms\n",
        "    # -------------------------------------------------------------\n",
        "\n",
        "    print(f\"Shape of X: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape}\")\n",
        "    print(f\"Sample data shape: {X[0].shape if len(X) > 0 else 'No data'}\")\n",
        "\n",
        "    # Ensure CHW format (N,C,H,W)\n",
        "    if len(X.shape) == 4 and X.shape[-1] != 3:\n",
        "        print(\"Converting from HWC to CHW format...\")\n",
        "        X = np.transpose(X, (0, 3, 1, 2))  # Convert (N,H,W,C) to (N,C,H,W)\n",
        "\n",
        "    # Ensure correct data types\n",
        "    X = X.astype(np.float32)\n",
        "\n",
        "    # Split into train / val\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=val_split, stratify=y, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Train set size: {len(X_train)}\")\n",
        "    print(f\"Validation set size: {len(X_val)}\")\n",
        "\n",
        "    # Apply transforms through custom dataset class\n",
        "    transform_train = get_transform(is_train=True)\n",
        "    transform_eval  = get_transform(is_train=False)\n",
        "\n",
        "    train_dataset = ImageDataset(X_train, y_train, transform=transform_train)\n",
        "    val_dataset   = ImageDataset(X_val,   y_val,   transform=transform_eval)\n",
        "\n",
        "    batch_size = BATCH_SIZE\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # Model handling - if no model provided, create one\n",
        "    # -------------------------------------------------------------\n",
        "    if model is None:\n",
        "        print(\"WARNING: No model. Generating ResNet50 model\")\n",
        "        num_classes = int(y.max().item() + 1)  # Get number of classes\n",
        "        model = get_resnet50(num_classes=num_classes, pretrained=True, device=device)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # Loss, Optimizer, Scheduler\n",
        "    # -------------------------------------------------------------\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=num_epochs, eta_min=lr * 0.01\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # Lists to store metrics\n",
        "    # -------------------------------------------------------------\n",
        "    train_losses, val_losses, val_accs = [], [], []\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # Epoch loop\n",
        "    # -------------------------------------------------------------\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "\n",
        "        # ---- Train ------------------------------------------------\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss   = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item() * xb.size(0)   # accumulate over batch\n",
        "\n",
        "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        # ---- Validation ------------------------------------------\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                loss   = criterion(logits, yb)\n",
        "\n",
        "                running_val_loss += loss.item() * xb.size(0)\n",
        "\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                correct += (preds == yb).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        epoch_val_acc  = correct / len(val_loader.dataset)\n",
        "\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "\n",
        "        # ---- Scheduler step ---------------------------------------\n",
        "        scheduler.step()\n",
        "\n",
        "        # ---- Print progress ----------------------------\n",
        "        print(f\"Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # Build and return the history dict\n",
        "    # -------------------------------------------------------------\n",
        "    history = {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"val_loss\":   val_losses,\n",
        "        \"val_acc\":    val_accs,\n",
        "    }\n",
        "    return history"
      ],
      "metadata": {
        "id": "xNF8ysZ1b0r4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ysyYJx60dGtj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d-9eogtPdHNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4glKHkcJcONW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "NUM_EPOCHS = 4\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "print(\"---Training is starting for {NUM_EPOCHS} epochs ----------\")\n",
        "history = run_training(\n",
        "    X, y,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    val_split=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "fxz2x-VTcO5-",
        "outputId": "aacc85e8-80b0-4fe9-be4a-055e61d5a0c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Training is starting for {NUM_EPOCHS} epochs ----------\n",
            "Shape of X: (500, 1024, 1024, 3)\n",
            "Shape of y: (500,)\n",
            "Sample data shape: (1024, 1024, 3)\n",
            "Train set size: 400\n",
            "Validation set size: 100\n",
            "WARNING: No model. Generating ResNet50 model\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 234MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4157575427.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---Training is starting for {NUM_EPOCHS} epochs ----------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m history = run_training(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-443508502.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(X, y, num_epochs, batch_size, lr, weight_decay, val_split, device, model)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mrunning_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-443508502.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Apply transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pic should be PIL Image or ndarray. Got {type(pic)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_numpy_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
          ]
        }
      ]
    }
  ]
}