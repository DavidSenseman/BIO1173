{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "## **Module 4: Chatbots and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Introduction to Large Language Models (LLMs)\n",
        "* **Part 4.2: Chatbots**\n",
        "* Part 4.3: Image Generation with StableDiffusion\n",
        "* Part 4.4: Agentic AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded.\n"
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Your GEMINI_API_KEY\n",
        "\n",
        "In order to run the code in this lesson you will need to have your secret `GEMINI_API_KEY` installed in your **Secrets** on this Colab notebook. Detailed steps for purchasing your `GEMINI_API_KEY` and installing it in your Colab notebook Secrets was provide in `Class_04_1`.\n",
        "\n",
        "Run the code in the next cell to see if your `GEMINI_API_KEY` is installed correctly. You make have to Grant Access for your notebook to use your API key."
      ],
      "metadata": {
        "id": "v7QopUS2wT9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify your API key setup\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Check if API key is properly loaded\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    print(\"API key loaded successfully!\")\n",
        "    print(f\"Key length: {len(GEMINI_API_KEY)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading API key: {e}\")\n",
        "    print(\"Please set-up your GEMINI_API_KEY key in your Colab Secrets\")"
      ],
      "metadata": {
        "id": "Gi0ydJubCZsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. You may see this message when you run this cell:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image08C.png)\n",
        "\n",
        "If you do see this popup just click on `Grant access`.\n",
        "\n",
        "\n",
        "2. If your `GEMINI_API_KEY` is correctly installed you should see something _similar_ to the following output.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image09C.png)\n",
        "\n",
        "3. However, if you see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image10C.png)\n",
        "\n",
        "You will need to correct the error before you can continue. Ask your Instructor or TA for help if you can resolve the error yourself."
      ],
      "metadata": {
        "id": "l5SoKbnFMT-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `LangChain` packages\n",
        "\n",
        "Run the code in the following cell to install the `langchain-google_genai` and related packages."
      ],
      "metadata": {
        "id": "DJbJtTtexCNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run these installations\n",
        "\n",
        "!pip install -q langchain-core\n",
        "!pip install -q pydub google-genai nest_asyncio langchain-community langchain-google-genai"
      ],
      "metadata": {
        "id": "qh8sAF16-mTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might not see any output or you might see the the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image07E.png)\n",
        "\n",
        "If you see this error message, don't worry about it."
      ],
      "metadata": {
        "id": "IqvEOnmrdGVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to ChatBots**\n",
        "\n",
        "Run the next cell to see short introduction to ChatBots. This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "UlTxZtcKMvNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = 'gmUHEvrpYoU'\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\"> </iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "6Wobn6A5Mv7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Speech Processing with Gemini**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/CourseImage.gif)\n",
        "\n",
        "In this lesson, we explore how to use both computer-generated voice and voice recognition to create a `ChatBot`. We'll be working with the **Google Gemini API** and the **LangChain Google integration** to achieve this. Specifically, we'll demonstrate how to input normal text and have it spoken by the computer, and conversely, how we can speak to the computer and have it respond. We'll ultimately integrate these functionalities to create a chatbot that handles both text-to-speech and speech-to-text interactions.\n",
        "\n",
        "While we'll use Google `Colab` for this demonstration, in production environments, you'd likely use a mobile app or a web-based JavaScript solution, as each platform handles voice differently. We'll focus on keeping things generic and simple in Colab for now.\n",
        "\n",
        "Voice applications are everywhere. For example, I can ask \"`Alexa`, what time is it?\" and multiple `Alexa` devices in my home will respond, although not always perfectly. I usually mute them during recording sessions. Applications like `Siri` or `Gemini` also offer voice interactions. For instance, you can now interact with the `Gemini` mobile app completely hands-free, or use the microphone input on the web interface.\n",
        "\n",
        "To illustrate, I asked `Gemini`, \"How are you doing?\" and it responded by offering some insightful thoughts about the rapid evolution of multimodal AI. It highlighted that models like Gemini aren't just processing text anymore‚Äîthey are natively designed to understand text, images, and **audio** simultaneously. It also suggested that students experiment with these new \"multimodal\" capabilities, as building hands-on projects is one of the best ways to understand the future of AI."
      ],
      "metadata": {
        "id": "TYZYyC4l_y8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part I: Speech to Text with Gemini**\n",
        "\n",
        "Here we delve into the realm of speech-to-text technology, focusing on the powerful multimodal capabilities offered by **Google's Gemini models**. Speech-to-text, also known as automatic speech recognition (ASR), is a technology that converts spoken language into written text.\n",
        "\n",
        "**Google's Gemini 1.5** models represent the cutting edge of this field. Unlike traditional models that require separate systems for audio and text, Gemini is **natively multimodal**. This means it can accept audio inputs directly, leveraging advanced machine learning techniques to achieve high accuracy and robustness across various accents, languages, and acoustic environments. We'll explore how these models can be integrated into applications to enable voice-based interactions, transcription services, and accessibility features. By harnessing Gemini's audio capabilities, we'll unlock new possibilities for human-computer interaction and demonstrate how to transform audio input into actionable text data with remarkable precision.\n",
        "\n",
        "Note: We will make use of the JavaScript technique described below to record audio directly within Google Colab, as Colab runs on a remote server and cannot access your local microphone by default.\n",
        "\n",
        "https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be"
      ],
      "metadata": {
        "id": "JU2315DH_7Cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Native Audio Understanding**\n",
        "\n",
        "Here we delve into the realm of multimodal audio processing, focusing on the powerful capabilities offered by Google's **Gemini 2.5** models. Unlike traditional Speech-to-Text (ASR) which simply converts sound waves into words, Gemini treats audio as a \"native\" modality‚Äîmeaning it processes the raw audio waveform directly alongside text.\n",
        "\n",
        "This approach allows Gemini to not only transcribe speech with high accuracy but also to:\n",
        "* **Understand Context:** Detect emotions (sarcasm, excitement) and non-verbal cues.\n",
        "* **Diarize:** Distinguish between multiple speakers automatically.\n",
        "* **Reason:** Summarize or answer questions about the audio content without needing a separate text-processing step.\n",
        "\n",
        "We will explore how `gemini-2.5-flash` can be used to transform raw audio input into actionable data with remarkable precision.\n",
        "\n",
        "**Note on Recording in Colab:**\n",
        "Because Google Colab runs on a remote server, it cannot access your local microphone directly. We will make use of a JavaScript bridge (adapted from [this technique](https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be)) to capture audio from your browser and stream it to the Python environment for processing."
      ],
      "metadata": {
        "id": "UMkpzxDuGOOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions to Record and Transcribe Audio\n",
        "\n",
        "The code cell below creates two important functions for the next part of this lesson `record_audio()` and `transcribe()`.\n",
        "\n",
        "### **`record_audio()`**\n",
        "This Python function records audio from the user's microphone, converts it to a WAV file, and saves it to the disk. It accomplishes this by generating a JavaScript snippet that runs within a browser environment (i.e. Colab Notebook using `output.eval_js`) to access the microphone, then converts the resulting audio blob to a base64 string, and passes it back to Python for processing. The Python part then decodes the base64 string, saves it as a `.webm` file, and uses `ffmpeg` to convert it to `.wav` format. If successful, it deletes the .webm file and returns the path to the `.wav` file.\n",
        "\n",
        "**How it Works**\n",
        "\n",
        "1. **Audio Capture:** Uses JavaScript (navigator.mediaDevices.getUserMedia) to request microphone access and record audio for a specified number of seconds (default 3).\n",
        "2. **Data Conversion:** The recorded audio is converted into a base64 encoded string within the JavaScript environment and passed back to the Python environment.\n",
        "3. **File Saving & Conversion:** Python decodes the base64 string, saves it as a `.webm` file, and uses `ffmpeg` to transcode it to .wav format (16kHz, mono).\n",
        "4. **Cleanup:** It deletes the .webm file after successful conversion and returns the path to the .wav file. If any step fails, it returns None.\n",
        "\n",
        "##### **Summary:**\n",
        "This code is useful for recording voice commands or short audio snippets within a web-based interface or a Jupyter Notebook environment. It bridges the gap between browser-based microphone access and server-side Python processing.\n",
        "__________________________________________________________________\n",
        "\n",
        "#### **`transcribe()`**\n",
        "\n",
        "The **transcribe()** function takes an audio file (typically a WAV file) and sends it to the Google Gemini API (specifically the \"gemini-2.5-flash\" model) to generate a text transcription of the audio.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. **Input Validation:** It first checks if the provided filename exists. If not, it returns `None`.\n",
        "2. **File Reading:** It opens the audio file in binary mode and reads the contents into memory.\n",
        "3. **API Call:** It constructs a request to the Gemini API, including the user's prompt (e.g., \"Transcribe this audio...\") and the raw audio bytes with a MIME type of audio/wav.\n",
        "4. **Processing:** The API processes the audio and returns a text response.\n",
        "5. **Cleanup:** By default, it deletes the original audio file after successful transcription. If an error occurs, it also attempts to delete the file (unless keep_file=True is specified).\n",
        "6. **Return Value:** It returns the transcription text if successful, or None if any step fails.\n",
        "\n",
        "##### **Summary:**\n",
        "\n",
        "This function is a utility for converting audio files to text. It acts as a wrapper around the Google Gemini API, handling file I/O and error management to ensure that the audio file is cleaned up after the task is complete. It's useful for applications that need to transcribe voice notes or convert speech to text.\n"
      ],
      "metadata": {
        "id": "tjuLS9sTIUwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create record_audio() and transcribe() functions\n",
        "\n",
        "# ============================================================================\n",
        "# RECORDING AND TRANSCRIPTION FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import base64\n",
        "import time\n",
        "import subprocess\n",
        "from IPython.display import display, Audio\n",
        "from google.colab import output, userdata\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# Initialize Gemini\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "# ============================================================================\n",
        "# RECORDING\n",
        "# ============================================================================\n",
        "\n",
        "def record_audio(sec=3):\n",
        "    \"\"\"Records audio.\"\"\"\n",
        "    complete_js = f\"\"\"\n",
        "    (async function() {{\n",
        "      const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "      const b2text = blob => new Promise(resolve => {{\n",
        "        const reader = new FileReader()\n",
        "        reader.onloadend = e => resolve(e.target.result)\n",
        "        reader.readAsDataURL(blob)\n",
        "      }})\n",
        "\n",
        "      try {{\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({{ audio: true }})\n",
        "        const recorder = new MediaRecorder(stream)\n",
        "        const chunks = []\n",
        "\n",
        "        recorder.ondataavailable = e => {{\n",
        "          if (e.data.size > 0) chunks.push(e.data)\n",
        "        }}\n",
        "\n",
        "        const recordingPromise = new Promise((resolve, reject) => {{\n",
        "          recorder.onstop = async () => {{\n",
        "            stream.getTracks().forEach(track => track.stop())\n",
        "            if (chunks.length === 0) reject('No audio data')\n",
        "            else {{\n",
        "              const blob = new Blob(chunks, {{ type: 'audio/webm' }})\n",
        "              if (blob.size === 0) reject('Empty blob')\n",
        "              else resolve(await b2text(blob))\n",
        "            }}\n",
        "          }}\n",
        "          recorder.onerror = e => reject('Error: ' + e.error)\n",
        "        }})\n",
        "\n",
        "        recorder.start()\n",
        "        await sleep({sec * 1000})\n",
        "        recorder.stop()\n",
        "        return await recordingPromise\n",
        "\n",
        "      }} catch (error) {{\n",
        "        return 'ERROR: ' + error.message\n",
        "      }}\n",
        "    }})()\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        s = output.eval_js(complete_js)\n",
        "        if not s or s.startswith('ERROR:') or ',' not in s:\n",
        "            return None\n",
        "\n",
        "        binary = base64.b64decode(s.split(',')[1])\n",
        "        filename = f'rec_{int(time.time())}.webm'\n",
        "\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(binary)\n",
        "\n",
        "        wav_filename = filename.replace('.webm', '.wav')\n",
        "        subprocess.run(['ffmpeg', '-i', filename, '-ar', '16000', '-ac', '1', '-y', wav_filename],\n",
        "                      stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "        if os.path.exists(wav_filename):\n",
        "            os.remove(filename)\n",
        "            return wav_filename\n",
        "        return filename\n",
        "\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# ============================================================================\n",
        "# TRANSCRIPTION\n",
        "# ============================================================================\n",
        "\n",
        "def transcribe(filename, prompt=\"Transcribe this audio. Return only the transcription.\", keep_file=False):\n",
        "    \"\"\"Transcribes audio.\"\"\"\n",
        "    if not filename or not os.path.exists(filename):\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        with open(filename, \"rb\") as f:\n",
        "            audio_bytes = f.read()\n",
        "\n",
        "        response = client.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=[prompt,\n",
        "                     types.Part.from_bytes(data=audio_bytes, mime_type=\"audio/wav\")]\n",
        "        )\n",
        "\n",
        "        if not keep_file:\n",
        "            os.remove(filename)\n",
        "        return response.text.strip() if response and response.text else None\n",
        "\n",
        "    except:\n",
        "        if not keep_file and os.path.exists(filename):\n",
        "            os.remove(filename)\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ record_audio() and transcribe() functions loaded!\")\n"
      ],
      "metadata": {
        "id": "SNDhX7esYOE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transcribing Audio with Gemini**\n",
        "\n",
        "#####  **Overview of the API**\n",
        "- **Models**: We use **Gemini 2.5 Flash**. It is a \"multimodal\" model, meaning it can natively understand text, images, and **audio** simultaneously.\n",
        "- **Input**: Accepts audio data directly (e.g., WAV, MP3, MP4) alongside text prompts.\n",
        "- **Output**: Returns text, JSON, or structured data based on your instructions.\n",
        "- **Capabilities**: Unlike traditional \"transcription-only\" models, you can ask Gemini to do things *while* it listens, such as \"Summarize this recording,\" \"Extract the patient's symptoms,\" or \"Translate this to Spanish.\"\n",
        "\n",
        "##### **Why It's Useful for Biomedical Investigators**\n",
        "\n",
        "1. **Transcribing Interviews & Focus Groups**\n",
        "   Automatically convert recorded conversations with patients, clinicians, or research participants into text for qualitative analysis.\n",
        "\n",
        "2. **Clinical Note Dictation**\n",
        "   Researchers can dictate observations or notes during fieldwork or lab work, streamlining documentation.\n",
        "\n",
        "3. **Meeting & Conference Transcripts**\n",
        "   Capture and archive discussions from research meetings, seminars, or collaborative calls.\n",
        "\n",
        "4. **Data Extraction from Audio**\n",
        "   Enables downstream NLP tasks like identifying social determinants of health (SDOH) or extracting biomedical entities directly from spoken content without needing a separate transcription step.\n",
        "\n",
        "5. **Multilingual Support**\n",
        "   Useful in global health research where interviews or data collection occur in multiple languages.\n"
      ],
      "metadata": {
        "id": "oq-ADN7rDKuC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jurTAwUCKRM-"
      },
      "source": [
        "## Example 1: Speech-to-Text\n",
        "\n",
        "This code in the cell below uses the `record_audio()` function to convert your voice into text and then uses the `transcribe()` function to print out what you said.\n",
        "\n",
        "Once you hit the run cell icon\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png)start counting out loud from `1` to `10`.\n",
        "\n",
        "**WARNING:** If you see this popu window:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image28F.png)\n",
        "\n",
        "you will proabably need to run this cell again to get it to work."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Speech-to-Text\n",
        "\n",
        "from IPython.display import Audio, display\n",
        "import time\n",
        "import sys\n",
        "\n",
        "# Configuration\n",
        "LLM_MODEL = \"gemini-2.5-flash\"\n",
        "RECORD_DURATION = 10  # Seconds\n",
        "TIMEOUT = 30  # Maximum seconds to wait for permission + recording\n",
        "\n",
        "try:\n",
        "    # 1. Capture Audio\n",
        "    print(f\"üé§ Start speaking!\")\n",
        "    print(f\"‚è±Ô∏è Please grant microphone permission if prompted (timeout in {TIMEOUT}s)\")\n",
        "    sys.stdout.flush()  # Force output to display immediately\n",
        "\n",
        "    start_time = time.time()\n",
        "    audio_filename = record_audio(sec=RECORD_DURATION)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    # Check if it took suspiciously long (likely hung on permission dialog)\n",
        "    if elapsed > (RECORD_DURATION + 15):\n",
        "        print(\"‚ö†Ô∏è Recording took too long - permission may have been delayed.\")\n",
        "        print(\"üí° Tip: Run this cell again. Permission should already be granted.\")\n",
        "\n",
        "    if audio_filename:\n",
        "        # 2. Transcribe using Native Audio Reasoning\n",
        "        print(f\"üì° Sending waveform to {LLM_MODEL}...\")\n",
        "\n",
        "        transcription = transcribe(\n",
        "            filename=audio_filename,\n",
        "            prompt=\"Transcribe accurately. Include speaker labels if multiple people are speaking.\",\n",
        "            keep_file=True\n",
        "        )\n",
        "\n",
        "        # 3. Output Results\n",
        "        print(\"\\n\" + \"=\"*30)\n",
        "        print(\"üìú TRANSCRIPTION\")\n",
        "        print(\"=\"*30)\n",
        "        print(transcription)\n",
        "        print(\"=\"*30)\n",
        "\n",
        "        # 4. Playback for verification\n",
        "        print(\"\\nüîä Playing back recorded audio...\")\n",
        "        display(Audio(audio_filename, autoplay=False))\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Recording failed. Please check your browser's microphone permissions.\")\n",
        "        print(\"üí° Make sure to click 'Allow' when the browser asks for microphone access.\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è Recording interrupted by user.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è An error occurred during the Speech-to-Text process: {e}\")\n",
        "    print(\"üí° Try running the cell again - microphone permission may need to be granted first.\")\n"
      ],
      "metadata": {
        "id": "Ymdyu7ldBiPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image15F.png)"
      ],
      "metadata": {
        "id": "k-nF8ycpXUkq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCA54j6KNooN"
      },
      "source": [
        "## **Exercise 1: Speech-to-Text**\n",
        "\n",
        "In the cell below, write to code to generate Speech-to-Text using the code in Example 2 as a template.\n",
        "\n",
        "For **Exercise 1**, once you hit the run cell icon\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png)start counting **_backwards_** from `10` to `1`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n"
      ],
      "metadata": {
        "id": "sOCUCa1JB1Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image16F.png)"
      ],
      "metadata": {
        "id": "uP2uir0WYLhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text to Speech with Google**\n",
        "\n",
        "In this section, we'll explore text-to-speech (TTS), focusing on Google's powerful speech synthesis tools. While Gemini is excellent at generating the *words*, we use Google's dedicated Text-to-Speech engine (`gTTS`) to convert that written text into natural-sounding speech.\n",
        "\n",
        "Google's TTS models are optimized for both real-time applications and high-fidelity audio storage. This technology represents a significant advancement in speech synthesis, using deep learning to produce clear, lifelike vocal outputs in a wide variety of languages and accents. By utilizing these tools, we'll explore their capabilities and understand how they revolutionize industries, from accessibility solutions to interactive voice assistants and beyond."
      ],
      "metadata": {
        "id": "gXT75DQGGEQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TEXT-TO-SPEECH FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "import struct\n",
        "import asyncio\n",
        "import base64\n",
        "\n",
        "async def speak(text, voice=\"Kore\", autoplay=True, save_to=None):\n",
        "    \"\"\"\n",
        "    TTS using Live API.\n",
        "\n",
        "    Parameters:\n",
        "        text: The text to speak\n",
        "        voice: Voice name (default \"Kore\")\n",
        "        autoplay: Whether to automatically play the audio (default True)\n",
        "        save_to: Optional filename to save the audio to (default None)\n",
        "    \"\"\"\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        config = types.LiveConnectConfig(\n",
        "            response_modalities=[\"AUDIO\"],\n",
        "            speech_config=types.SpeechConfig(\n",
        "                voice_config=types.VoiceConfig(\n",
        "                    prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name=voice)\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "        audio_chunks = bytearray()\n",
        "\n",
        "        async with client.aio.live.connect(model=\"gemini-2.5-flash-native-audio-latest\", config=config) as session:\n",
        "            # Send text to speak\n",
        "            await session.send_client_content(\n",
        "                turns=[types.Content(\n",
        "                    role=\"user\",\n",
        "                    parts=[types.Part(text=f\"Read this text verbatim without any analysis: {text}\")]\n",
        "                )],\n",
        "                turn_complete=True\n",
        "            )\n",
        "\n",
        "            # Collect audio chunks\n",
        "            async for response in session.receive():\n",
        "                if response.server_content and response.server_content.model_turn:\n",
        "                    for part in response.server_content.model_turn.parts:\n",
        "                        if part.inline_data:\n",
        "                            audio_chunks.extend(part.inline_data.data)\n",
        "\n",
        "                if response.server_content and response.server_content.turn_complete:\n",
        "                    break\n",
        "\n",
        "        # Process audio\n",
        "        if audio_chunks:\n",
        "            sample_rate = 24000\n",
        "            wav_header = struct.pack(\n",
        "                '<4sI4s4sIHHIIHH4sI',\n",
        "                b'RIFF', 36 + len(audio_chunks), b'WAVE', b'fmt ', 16, 1, 1,\n",
        "                sample_rate, sample_rate * 2, 2, 16, b'data', len(audio_chunks)\n",
        "            )\n",
        "            full_audio = wav_header + audio_chunks\n",
        "\n",
        "            # Save to file if requested\n",
        "            if save_to:\n",
        "                with open(save_to, 'wb') as f:\n",
        "                    f.write(full_audio)\n",
        "\n",
        "            # Play audio if autoplay is enabled\n",
        "            if autoplay:\n",
        "                from IPython.display import HTML\n",
        "                audio_b64 = base64.b64encode(full_audio).decode('utf-8')\n",
        "\n",
        "                html = f\"\"\"\n",
        "                <audio autoplay style=\"display:none;\">\n",
        "                    <source src=\"data:audio/wav;base64,{audio_b64}\" type=\"audio/wav\">\n",
        "                </audio>\n",
        "                \"\"\"\n",
        "\n",
        "                print(\"üîä Speaking...\")\n",
        "                display(HTML(html))\n",
        "\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è TTS error: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"‚úÖ speak() function loaded!\")\n"
      ],
      "metadata": {
        "id": "HrdUWYShhbWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Google's Voices**\n",
        "\n",
        "When using the Gemini Multimodal Live API to generate real-time conversational audio, you utilize Native Audio models (such as gemini-live-2.5-flash-native-audio). Unlike traditional Text-to-Speech which \"synthesizes\" text into sound after the fact, Gemini's native audio models generate speech directly as a core modality. This allows for Affective Dialog‚Äîwhere the voice automatically adapts its tone, emotion, and emphasis based on the context of the conversation.\n",
        "\n",
        "Google offers a suite of distinct voice personas, along with a library of over 30 HD voices. The primary personas include:\n",
        "\n",
        "* Puck ‚Äì The most popular general-purpose voice. Conversational, friendly, and approachable with a mid-range pitch. It has an \"upbeat\" and \"guy-next-door\" feel.\n",
        "\n",
        "* Charon ‚Äì A deep, calm, and authoritative male voice. It projects a sense of informative experience and steady confidence, perfect for formal narrations.\n",
        "\n",
        "* Kore ‚Äì A bright, energetic, and professional female voice. Excellent for high-engagement tasks like coaching or upbeat customer support where a \"firm\" but engaging tone is needed.\n",
        "\n",
        "* Fenrir ‚Äì A warm, steady, and approachable male voice. It sits between Puck and Charon, making it perfect for long-form listening or educational content.\n",
        "\n",
        "* Aoede ‚Äì A clear, thoughtful, and articulate female voice. Known for a \"breezy\" and intelligent tone that handles complex discussions gracefully."
      ],
      "metadata": {
        "id": "eamjCvM69qzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Demonstrate Different Voices\n",
        "\n",
        "The code in the cell below demonstates 4 of the different voices that are available in the `Gemini` text-to-speech API:\n",
        "\n",
        "* **Puck:** A clear, direct, and conversational male voice with a mid-range pitch. Puck is often described as having a \"guy next door\" feel‚Äîfriendly, trustworthy, and approachable. Because of its balanced tone, it is the default choice for most general-purpose assistants.\n",
        "\n",
        "* **Charon:** A deep, calm, and authoritative male voice. Charon projects a sense of experience and steady confidence. It is best suited for scenarios that require a more formal or serious tone, such as news delivery, instructional narrations, or professional corporate guides.\n",
        "\n",
        "* **Kore:** An energetic and youthful female voice with a bright, professional quality. Kore conveys high enthusiasm and confidence without being overly casual. This makes it an excellent choice for upbeat tutorials, engaging customer support, or any interaction where you want to keep the user‚Äôs energy high.\n",
        "\n",
        "* **Fenrir** is widely considered the most versatile of the male voices. It sits perfectly between the high energy of Puck and the deep authority of Charon.\n",
        "\n",
        "For example, here is a more detailed description of the **Fenrir** voice:\n",
        "\n",
        ">  **Persona:** Warm, approachable, and steady. Fenrir has a mid-range pitch that feels exceptionally natural and human. It lacks the \"broadcast\" quality of Charon and the \"youthful bounce\" of Puck, making it feel more like a calm colleague or a supportive mentor.\n",
        "\n",
        ">  **Tone:** Balanced and conversational. It is designed to be \"easy to listen to\" for long periods, which is why it is frequently used for e-learning, narrations, and long-form assistants.\n",
        "\n",
        ">  **Best For:** Explainer videos, podcasting, technical support, or any application where you want to project reliability and warmth without being too formal.\n",
        "\n",
        "Run the code cell to hear each of these three voices."
      ],
      "metadata": {
        "id": "mPOHlT667J9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Demonstrate Different Voices\n",
        "\n",
        "from IPython.display import Audio, display\n",
        "import os\n",
        "\n",
        "async def run_voice_demos():\n",
        "    # Primary voice profiles\n",
        "    demo_voices = [\"Puck\", \"Charon\", \"Kore\", \"Fenrir\"]\n",
        "\n",
        "    # This text is designed to showcase the tonal differences of each profile\n",
        "    sample_text = \"Hello! Welcome to BIO 1173, Introduction to Computaional Biology at UT San Antonio!\"\n",
        "\n",
        "    print(\"--- Starting Gemini Live Voice Demo ---\")\n",
        "\n",
        "    for voice in demo_voices:\n",
        "        # Step 1: Generate the audio file using the Live API\n",
        "        # This function handles the WebSocket connection and PCM-to-WAV conversion\n",
        "        filename = f\"sample_{voice.lower()}.wav\"\n",
        "        result = await speak(sample_text, voice=voice, autoplay=False, save_to=filename)\n",
        "\n",
        "        # Step 2: Validate and display the playback widget\n",
        "        if result and os.path.exists(filename):\n",
        "            print(f\"\\n[‚úî] Playing sample for {voice}:\")\n",
        "            display(Audio(filename, autoplay=False))\n",
        "        else:\n",
        "            print(f\"  [X] Skipping {voice}: Generation failed or file not found.\")\n",
        "\n",
        "    print(\"\\n--- Voice Demonstration Complete ---\")\n",
        "\n",
        "# Execute the voice demo in Colab\n",
        "await run_voice_demos()\n"
      ],
      "metadata": {
        "id": "-CM8Y8Hwymlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image02F.png)\n",
        "\n",
        "Press the Play icon to listen to each voice."
      ],
      "metadata": {
        "id": "CK8segNTIwos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Demonstrate Different Voices**\n",
        "\n",
        "In the cell below, write the code to demonstrate the following 4 Gemini voices:\n",
        "\n",
        "* **Aoede:** A clear, thoughtful, and articulate female voice, often described as sounding intelligent and engaging.\n",
        "\n",
        "* **Leda:** A calm and steady voice with a balanced tone, suitable for neutral assistants.\n",
        "\n",
        "* **Orus:** A direct and confident male voice, slightly more formal than Puck.\n",
        "\n",
        "* **Zephyr:** An upbeat and energetic voice, similar in spirit to Kore but with a different tonal profile."
      ],
      "metadata": {
        "id": "oTlOpIJRE7q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dIiJ5eqCZEGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image03F.png)\n",
        "\n",
        "Press the Play icon to listen to each voice."
      ],
      "metadata": {
        "id": "b2xgKTWSJYJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Transcribe Recorded Data\n",
        "\n",
        "The code in the cell shows how to record your speech, print out a transcription of what you said, and finally, read the transcription using the \"Fenrir\" voice.\n",
        "\n",
        "Once you hit the run cell icon\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png), read out loud Carl Sandburg‚Äôs poem ‚ÄúFog‚Äù --a short, imagistic piece that captures the quiet, mysterious arrival of fog. Don't forget to start by saying the title of the poem, \"FOG\". When you read the poem, make sure to pause after evey line.\n",
        "\n",
        "```text\n",
        "FOG\n",
        "\n",
        "The fog comes\n",
        "on little cat feet.\n",
        "\n",
        "It sits looking\n",
        "over harbor and city\n",
        "on silent haunches\n",
        "and then moves on.\n",
        "```"
      ],
      "metadata": {
        "id": "8oHJAsCA5qHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Transcribe and Speak Back\n",
        "\n",
        "import sys\n",
        "\n",
        "# Set voice and duration\n",
        "voice = \"Fenrir\"\n",
        "duration = 20\n",
        "\n",
        "try:\n",
        "    # 1. Capture Audio\n",
        "    print(f\"üé§ Starting {duration} second recording... Speak now!\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    audio_path = record_audio(sec=duration)\n",
        "\n",
        "    if audio_path:\n",
        "        # 2. Transcribe using the transcribe function\n",
        "        print(\"üì° Transcribing audio...\")\n",
        "        transcription = transcribe(audio_path, keep_file=False)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*30)\n",
        "        print(f\"üìú Captured: {transcription}\")\n",
        "        print(\"=\"*30 + \"\\n\")\n",
        "\n",
        "        # 3. Speak back using TTS\n",
        "        print(\"üîä Reading back transcript...\")\n",
        "        await speak(transcription, voice=voice)\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Recording failed. Please check mic permissions.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error in the process: {e}\")\n"
      ],
      "metadata": {
        "id": "WFtH35EBERn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image20F.png)"
      ],
      "metadata": {
        "id": "xNDmAFlBCv0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Transcribe Recorded Data**\n",
        "\n",
        "In the cell below, write the code to record your speech, print out a transcription of what you said, and finally, read the transcription using the \"Leda\" voice.\n",
        "\n",
        "After you start running the cell, start reading _The Red Wheelbarrow_ by William Carlos Williams. Like _Fog_, it‚Äôs a minimalist, imagist poem that captures a vivid moment with few words. Make sure to pause after couplet.\n",
        "\n",
        "```text\n",
        "The Red Wheelbarrow\n",
        "\n",
        "so much depends\n",
        "upon\n",
        "\n",
        "a red wheel\n",
        "barrow\n",
        "\n",
        "glazed with rain\n",
        "water\n",
        "\n",
        "beside the white\n",
        "chickens.\n",
        "```"
      ],
      "metadata": {
        "id": "ttTsJ-QADFd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n"
      ],
      "metadata": {
        "id": "Kca6HFtFcC5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image21F.png)\n",
        "\n",
        "The poem is broken into four tiny stanzas, each with a long line followed by a short one.\n",
        "\n",
        "This spacing:\n",
        "* slows the reader down\n",
        "* isolates each image\n",
        "* makes you notice the shape of the words\n",
        "\n",
        "Even the word ‚Äúwheelbarrow‚Äù is split in half, forcing you to see it differently."
      ],
      "metadata": {
        "id": "ZIAxHm1sDFd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: Chatbots**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image10A.png)\n",
        "\n",
        "The history of **chatbots** is a fascinating journey through the evolution of artificial intelligence and human-computer interaction. Here's a brief overview:\n",
        "\n",
        "* **1. The Early Days (1950s-1970s)**\n",
        "1950 - Alan Turing's \"Imitation Game\": Turing proposed a test (now known as the Turing Test) to determine if a machine could exhibit intelligent behavior indistinguishable from a human.\n",
        "1966 - ELIZA: Created by Joseph Weizenbaum at MIT, ELIZA was the first chatbot. It mimicked a Rogerian psychotherapist by rephrasing user input into questions. It was simple but groundbreaking.\n",
        "1972 - PARRY: Developed by Kenneth Colby, PARRY simulated a person with paranoid schizophrenia. It was more complex than ELIZA and could hold more realistic conversations.\n",
        "* **2. Rule-Based Systems (1980s-1990s)**\n",
        "Chatbots during this era used hand-coded rules and decision trees.\n",
        "They were mostly used in academic research, customer service, and early virtual assistants.\n",
        "Examples include Jabberwacky (late 1980s), which aimed to simulate natural human chat through learning.\n",
        "* **3. Rise of the Internet and AI (2000s)**\n",
        "SmarterChild (2001): A popular chatbot on AOL Instant Messenger and MSN Messenger. It could answer questions, play games, and chat casually.\n",
        "ALICE (Artificial Linguistic Internet Computer Entity): Created by Richard Wallace, it won the Loebner Prize (a Turing Test competition) multiple times.\n",
        "* **4. Machine Learning and NLP Boom (2010s)**\n",
        "2011 - Siri: Apple introduced Siri, a voice-activated assistant that brought chatbots into the mainstream.\n",
        "2014 - Alexa and Cortana: Amazon and Microsoft launched their own virtual assistants.\n",
        "2016 - Facebook Messenger Bots: Facebook opened its platform to developers, leading to a surge in chatbot development for businesses.\n",
        "* **5. Neural Networks and Transformers (Late 2010s-2020s)**\n",
        "2018 ‚Äì BERT (Google) and GPT (OpenAI): These transformer-based models revolutionized natural language understanding and generation.\n",
        "2020 ‚Äì GPT-3: A massive leap in chatbot capabilities, enabling more coherent, context-aware, and human-like conversations.\n",
        "2022 ‚Äì ChatGPT: OpenAI released ChatGPT based on GPT-3.5 and later GPT-4, making advanced conversational AI widely accessible.\n",
        "* **6. The Present and Future (2020s-Today)**\n",
        "Chatbots are now integrated into education, healthcare, customer service, entertainment, and more.\n",
        "Multimodal models (like GPT-4 and beyond) can understand text, images, and even audio.\n",
        "The focus is shifting toward personalization, emotional intelligence, and ethical AI."
      ],
      "metadata": {
        "id": "7qM-H5TrEoBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create a `Google Nest` Chatbot**\n",
        "\n",
        "For Example 4 we are going to create an emulation of a Google `Nest Mini`.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image15E.png)\n",
        "\n",
        "### **Smart assistants have a \"wake word\"**\n",
        "\n",
        "Smart assistants can be programmed to work using a variety of different languages. This table shows the most commonly used languages and their corresponding abbreviations.\n",
        "\n",
        "| Language | Abbreviation | Wake Words\n",
        "| :--- | :--- | :-----|\n",
        "| **English** | `en` | \"hey google\", \"ok google\", \"hi google\"\n",
        "| **Spanish** | `es` | \"ok google\", \"oye google\"\n",
        "| **French** | `fr` | \"ok google\", \"dis google\"\n",
        "| **German** | `de` | \"ok google\", \"hallo google\"\n",
        "| **Japanese** | `jp` (or `ja`) |\"ok google\", \"ne google\"\n",
        "\n",
        "#### **What is a \"Wake Word\"?**\n",
        "\n",
        "A **Wake Word** (or \"Hotword\") is a specific phrase that activates a voice assistant from a dormant, power-saving state into an active, listening state.\n",
        "\n",
        "### **How it Works**\n",
        "Voice assistants like the Google Nest Mini operate in two distinct modes to protect privacy and conserve resources:\n",
        "\n",
        "1.  **Passive Listening (On-Device):**\n",
        "    * The device continuously records short loops of audio (usually a few seconds).\n",
        "    * It analyzes this audio locally on a specialized low-power chip.\n",
        "    * It is looking **only** for the specific acoustic signature of the wake word (e.g., *\"Hey Google\"*).\n",
        "    * If the wake word is *not* detected, the audio is discarded immediately and never leaves the device.\n",
        "\n",
        "2.  **Active Listening (Cloud Processing):**\n",
        "    * Once the wake word is detected, the device \"wakes up\" (often indicated by LEDs lighting up or a \"blip\" sound).\n",
        "    * It begins recording your actual command (e.g., *\"What is the weather?\"*).\n",
        "    * This command is then sent to the cloud (Google's servers) for advanced processing and response generation.\n",
        "\n",
        "##### **In Our Simulator**\n",
        "\n",
        "The Python code in Example 4 mimics this behavior using a `while` loop:\n",
        "* **State 1 (Passive):** It records 2.5-second chunks and checks *only* if the text contains \"Hey Gemini\" or \"OK Gemini\".\n",
        "* **State 2 (Active):** If detected, it switches to a longer recording mode to capture your full request, sends it to the LLM, and then speaks the response.\n"
      ],
      "metadata": {
        "id": "LDXWQYPw9BjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Functions\n",
        "\n",
        "Run the code in the next cell to create a number of audio functions needed for this lesson."
      ],
      "metadata": {
        "id": "r2oT-o3PokZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# VOICE ASSISTANT USING LIVE API (STREAMING AUDIO)\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import base64\n",
        "import time\n",
        "import asyncio\n",
        "import struct\n",
        "from IPython.display import display, Audio\n",
        "from google.colab import output, userdata\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import subprocess\n",
        "\n",
        "# Initialize Gemini\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "client = genai.Client(api_key=API_KEY, http_options={'api_version': 'v1beta'})\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# WAKE WORD DETECTION\n",
        "# ============================================================================\n",
        "\n",
        "def check_wake_word(text, wake_words):\n",
        "    \"\"\"Check for wake word.\"\"\"\n",
        "    if not text:\n",
        "        return False\n",
        "\n",
        "    norm = text.lower().strip()\n",
        "    norm = norm.replace(\"okay\", \"ok\").replace(\",\", \"\").replace(\".\", \"\")\n",
        "    norm = norm.replace(\"!\", \"\").replace(\"?\", \"\").replace(\"[no audio]\", \"\")\n",
        "\n",
        "    print(f\"   üìù '{text}' ‚Üí '{norm}'\")\n",
        "\n",
        "    if len(norm) < 2:\n",
        "        return False\n",
        "\n",
        "    patterns = {\n",
        "        \"hey gemini\": [\"hey gemini\", \"hey jiminy\", \"hey jamila\", \"it gemini\"],\n",
        "        \"ok gemini\": [\"ok gemini\", \"okay gemini\"]\n",
        "    }\n",
        "\n",
        "    for wake_word in wake_words:\n",
        "        if wake_word.lower() in patterns:\n",
        "            for pattern in patterns[wake_word.lower()]:\n",
        "                if pattern in norm or all(w in norm for w in pattern.split()):\n",
        "                    print(f\"   ‚úÖ WAKE WORD: '{wake_word}'\")\n",
        "                    return True\n",
        "\n",
        "    print(f\"   ‚ùå No wake word\")\n",
        "    return False\n",
        "\n",
        "# ============================================================================\n",
        "# WAKE WORD LOOP\n",
        "# ============================================================================\n",
        "\n",
        "async def listen_for_wake_word(wake_words, duration=4):\n",
        "    \"\"\"Listen for wake words.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéß WAKE WORD DETECTION\")\n",
        "    print(f\"üì¢ Say: {' or '.join(wake_words)}\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    while True:\n",
        "        print(\"üé§ Listening...\")\n",
        "        audio_file = record_audio(sec=duration)\n",
        "\n",
        "        if audio_file:\n",
        "            transcription = transcribe(audio_file)\n",
        "            if transcription and check_wake_word(transcription, wake_words):\n",
        "                print(\"\\nüéØ Activated!\\n\")\n",
        "                return True\n",
        "\n",
        "        await asyncio.sleep(0.3)\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CONVERSATION LOOP\n",
        "# ============================================================================\n",
        "\n",
        "async def conversation_loop(voice, conv_duration, pause, return_to_wake):\n",
        "    \"\"\"Conversation mode.\"\"\"\n",
        "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "    # Create LLM\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model='gemini-2.5-flash',\n",
        "        temperature=0.3,\n",
        "        google_api_key=API_KEY\n",
        "    )\n",
        "\n",
        "    # Greet\n",
        "    greeting = \"Hello! How can I help you?\"\n",
        "    print(f\"ü§ñ {greeting}\\n\")\n",
        "    await speak(greeting, voice=voice)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"üí¨ CONVERSATION MODE\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Say 'bye' to exit\\n\")\n",
        "\n",
        "    while True:\n",
        "        await asyncio.sleep(pause)\n",
        "\n",
        "        print(\"üé§ Your turn...\")\n",
        "        audio_file = record_audio(sec=conv_duration)\n",
        "\n",
        "        if not audio_file:\n",
        "            continue\n",
        "\n",
        "        user_input = transcribe(audio_file)\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        print(f\"üë§ You: {user_input}\\n\")\n",
        "\n",
        "        # Check exit\n",
        "        norm = user_input.lower().strip()\n",
        "        if any(word in norm for word in [\"bye\", \"goodbye\", \"exit\", \"stop\"]):\n",
        "            farewell = \"Goodbye! Have a great day!\"\n",
        "            print(f\"ü§ñ {farewell}\\n\")\n",
        "            await speak(farewell, voice=voice)\n",
        "\n",
        "            if return_to_wake:\n",
        "                print(\"üîÑ Returning to wake word\\n\")\n",
        "                return False\n",
        "            else:\n",
        "                print(\"üõë Exiting\\n\")\n",
        "                return True\n",
        "\n",
        "        # Get response\n",
        "        try:\n",
        "            # Add instruction for brief responses directly in the prompt\n",
        "            prompt = f\"Answer very briefly in 1-2 sentences: {user_input}\"\n",
        "\n",
        "            response = llm.invoke(prompt)\n",
        "            ai_text = response.content\n",
        "\n",
        "            print(f\"ü§ñ Gemini: {ai_text}\\n\")\n",
        "            await speak(ai_text, voice=voice)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN\n",
        "# ============================================================================\n",
        "\n",
        "async def start_wake_word_assistant(\n",
        "    wake_words=[\"hey gemini\", \"ok gemini\"],\n",
        "    voice=\"Kore\",\n",
        "    wake_duration=4,\n",
        "    conversation_duration=6,\n",
        "    pause_between=2,\n",
        "    return_to_wake=False\n",
        "):\n",
        "    \"\"\"Main assistant using Live API for TTS.\"\"\"\n",
        "    while True:\n",
        "        wake = await listen_for_wake_word(wake_words, wake_duration)\n",
        "\n",
        "        if wake:\n",
        "            should_exit = await conversation_loop(\n",
        "                voice, conversation_duration, pause_between, return_to_wake\n",
        "            )\n",
        "\n",
        "            if should_exit:\n",
        "                print(\"‚úÖ Stopped\\n\")\n",
        "                break\n",
        "\n",
        "print(\"‚úÖ Live API voice assistant loaded!\")\n",
        "print(\"üîß Uses gemini-2.5-flash-native-audio-latest Live API for TTS\")\n",
        "print(\"üìù Usage: await start_wake_word_assistant(return_to_wake=False)\")\n"
      ],
      "metadata": {
        "id": "WyjuGR4tbf61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Communicate with Chatbot\n",
        "\n",
        "Like Siri or Hey Google, you need to get the Chatbot's attention by saying the \"wake word\". For this example, the two wake words are \"hey gemini\" and \"ok gemini\".\n",
        "\n",
        "Once the Chatbot has heard its wake word, it will response with:\n",
        "\n",
        "```text\n",
        "üé§ Listening...\n",
        "   üìù '##' ‚Üí '##'\n",
        "   ‚ùå No wake word\n",
        "üé§ Listening...\n",
        "   üìù 'Hey Gemini.' ‚Üí 'hey gemini'\n",
        "   ‚úÖ WAKE WORD: 'hey gemini'\n",
        "\n",
        "üéØ Activated!\n",
        "```\n",
        "\n",
        "When the Chatbot has been activated by the wake word, it will respond with:\n",
        "\n",
        "```text\n",
        "ü§ñ Hello! How can I help you?\n",
        "\n",
        "üîä Speaking...\n",
        "```\n",
        "Unlike a Nest Mini, our Chatbot communicates using both text printed to your computer screen: \"Hello! How can I help you?\" as well as by speaking the output.\n",
        "\n",
        "At this point our Chatbot then enters the communication phase with this text output:\n",
        "\n",
        "```text\n",
        "============================================================\n",
        "üí¨ CONVERSATION MODE\n",
        "============================================================\n",
        "Say 'bye' to exit\n",
        "\n",
        "üé§ Your turn...\n",
        "```\n",
        "When you see \"Your turn...\" you can ask the Chatbot any question you like. The Chatbot should continue to \"chat\" with you as long as you like. To terminate your chat session, just say \"bye\" or \"goodbye\".\n",
        "\n",
        "For Example 4, just ask our Chatbot the following question, \"What is the capital of France?\" Once you have the answer, terminate the session by saying \"bye\" or \"goodbye\".\n",
        "\n",
        "**Warning:** Don't expect the same level of responsiveness that you get with Siri or Hey Google. Running our Chatbot code on Colab can be rather slow, so you need to be patient and go slow. Wait until you see ![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image24F.png) to start speaking."
      ],
      "metadata": {
        "id": "J0Hn0XB3F3ZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Communicate with Chatbot\n",
        "\n",
        "# Listen for the wake word\n",
        "await start_wake_word_assistant(\n",
        "    wake_words=[\"hey gemini\", \"ok gemini\"],\n",
        "    voice=\"Kore\",\n",
        "    return_to_wake=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ze9wWR5HOB5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image19F.png)"
      ],
      "metadata": {
        "id": "GtwCQ50AL_jV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Conversation with Chatbot**\n",
        "\n",
        "In the cell below write the code to start a new conversation with the `Chatbot`. Ask your `Chatbot` for **answers to 5 different questions** of your own choosing. After the 5th question has been answered, terminate your conversation by saying the word **\"bye\"**.\n",
        "\n",
        "**WARNING:** Getting the Chatbot to work can be a bit tricky. Do your best and go slow. Wait until you see ![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image24F.png) to start speaking."
      ],
      "metadata": {
        "id": "L2Xo4bAbnJqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n"
      ],
      "metadata": {
        "id": "rGo1O4YYMdk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your output will depend upon the questions you asked your Chatbot."
      ],
      "metadata": {
        "id": "Ck0mAyt4iOVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Medical History Taker\n",
        "\n",
        "In Example 5, we are going to create a Chatbot that asks a patient a series of health questions and records the responses. For our voice we will use \"Kore\".\n",
        "\n",
        "**WARNING:** Getting the Chatbot to work can be a bit tricky. Do your best and go slow. Wait until you see ![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image27F.png) to start speaking.\n"
      ],
      "metadata": {
        "id": "BLO4KDctdTjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Medical History Chatbot\n",
        "\n",
        "# Define voice\n",
        "VOICE=\"Kore\"\n",
        "\n",
        "async def medical_history_chatbot(voice=VOICE):\n",
        "    \"\"\"A chatbot that collects basic medical history.\"\"\"\n",
        "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "    import sys\n",
        "\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model='gemini-2.5-flash',\n",
        "        temperature=0.3,\n",
        "        google_api_key=API_KEY\n",
        "    )\n",
        "\n",
        "    # Store patient responses\n",
        "    patient_history = []\n",
        "\n",
        "    # Questions to ask\n",
        "    questions = [\n",
        "        \"What is your name?\",\n",
        "        \"What symptoms are you experiencing today?\",\n",
        "        \"How long have you had these symptoms?\",\n",
        "        \"Are you currently taking any medications?\",\n",
        "        \"Do you have any known allergies?\"\n",
        "    ]\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"üè• MEDICAL HISTORY CHATBOT\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Greeting\n",
        "    greeting = \"Hello, I'm here to collect some basic health information. Please answer a few questions.\"\n",
        "    print(f\"ü§ñ {greeting}\\n\")\n",
        "    await speak(greeting, voice=voice)\n",
        "    await asyncio.sleep(4)  # Wait for speech to finish\n",
        "\n",
        "    # Ask each question\n",
        "    for i, question in enumerate(questions):\n",
        "        print(f\"ü§ñ Question {i+1}: {question}\\n\")\n",
        "        await speak(question, voice=voice)\n",
        "        await asyncio.sleep(3)  # Wait for question to be spoken\n",
        "\n",
        "        # Record patient response\n",
        "        print(\"üé§ Your answer... (speak now!)\")\n",
        "        sys.stdout.flush()\n",
        "        audio_file = record_audio(sec=10)\n",
        "\n",
        "        if audio_file:\n",
        "            response = transcribe(audio_file)\n",
        "            if response and \"[no speech]\" not in response.lower():\n",
        "                print(f\"üë§ Patient: {response}\\n\")\n",
        "                patient_history.append({\"question\": question, \"answer\": response})\n",
        "            else:\n",
        "                print(\"üë§ Patient: (no response recorded)\\n\")\n",
        "                patient_history.append({\"question\": question, \"answer\": \"No response\"})\n",
        "\n",
        "        await asyncio.sleep(1)\n",
        "\n",
        "    # Summarize\n",
        "    print(\"=\"*60)\n",
        "    print(\"üìã PATIENT HISTORY SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    for item in patient_history:\n",
        "        print(f\"Q: {item['question']}\")\n",
        "        print(f\"A: {item['answer']}\\n\")\n",
        "\n",
        "    # Generate AI summary\n",
        "    history_text = \"\\n\".join([f\"Q: {item['question']} A: {item['answer']}\" for item in patient_history])\n",
        "    summary_prompt = f\"Summarize this patient intake in 2-3 sentences for a medical chart:\\n{history_text}\"\n",
        "\n",
        "    summary_response = llm.invoke(summary_prompt)\n",
        "    print(\"ü§ñ AI Summary:\")\n",
        "    print(summary_response.content)\n",
        "    await speak(summary_response.content, voice=VOICE)\n",
        "    await asyncio.sleep(4)  # Wait for summary to be spoken\n",
        "\n",
        "    # Thank you message\n",
        "    thanks = \"Thank you for providing your medical history. A healthcare provider will review this information shortly.\"\n",
        "    print(f\"\\nü§ñ {thanks}\\n\")\n",
        "    await speak(thanks, voice=VOICE)\n",
        "\n",
        "    return patient_history\n",
        "\n",
        "# Run the medical history chatbot\n",
        "history = await medical_history_chatbot(voice=VOICE)\n"
      ],
      "metadata": {
        "id": "QKbctEaJjDEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image22F.png)\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image23F.png)"
      ],
      "metadata": {
        "id": "D_ikbc5tkcD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Medical History Taker**\n",
        "\n",
        "For **Exercise 5**,create a Chatbot that asks a patient a series of health questions and records the responses. Use the voice **Fenrir**.\n",
        "\n",
        "Select one the following 6 options for your medical questions:\n",
        "\n",
        "**Option 1: Mental Health Screening**\n",
        "```python\n",
        "questions = [\n",
        "    \"What is your name?\",\n",
        "    \"How would you describe your mood over the past two weeks?\",\n",
        "    \"Have you been experiencing any difficulty sleeping?\",\n",
        "    \"How would you rate your stress level on a scale of 1 to 10?\",\n",
        "    \"Do you have a support system of friends or family you can talk to?\"\n",
        "]\n",
        "```\n",
        "\n",
        "**Option 2: Pain Assessment**\n",
        "```python\n",
        "questions = [\n",
        "    \"What is your name?\",\n",
        "    \"Where exactly is your pain located?\",\n",
        "    \"On a scale of 1 to 10, how severe is your pain?\",\n",
        "    \"Is the pain constant or does it come and go?\",\n",
        "    \"Does anything make the pain better or worse?\"\n",
        "]\n",
        "```\n",
        "**Option 3: Lifestyle and Preventive Health**\n",
        "```python\n",
        "questions = [\n",
        "    \"What is your name?\",\n",
        "    \"How many hours of sleep do you typically get per night?\",\n",
        "    \"How often do you exercise each week?\",\n",
        "    \"Do you smoke or use tobacco products?\",\n",
        "    \"How many servings of fruits and vegetables do you eat daily?\"\n",
        "]\n",
        "```\n",
        "\n",
        "**Option 4: COVID-19 / Respiratory Screening**\n",
        "```python\n",
        "questions = [\n",
        "    \"What is your name?\",\n",
        "    \"Do you have a fever or feel feverish?\",\n",
        "    \"Are you experiencing a cough or shortness of breath?\",\n",
        "    \"Have you lost your sense of taste or smell?\",\n",
        "    \"Have you been in close contact with anyone who tested positive for COVID-19?\"\n",
        "]\n",
        "```\n",
        "**Option 5: Family Medical History**\n",
        "```python\n",
        "questions = [\n",
        "    \"What is your name?\",\n",
        "    \"Has anyone in your immediate family had heart disease?\",\n",
        "    \"Is there a history of diabetes in your family?\",\n",
        "    \"Has anyone in your family been diagnosed with cancer?\",\n",
        "    \"Are there any other hereditary conditions that run in your family?\"\n",
        "]\n",
        "```\n",
        "\n",
        "**Option 6: Nutrition Assessment**\n",
        "```python\n",
        "questions = [\n",
        "    \"What is your name?\",\n",
        "    \"How many meals do you typically eat per day?\",\n",
        "    \"How much water do you drink daily?\",\n",
        "    \"Do you have any food restrictions or dietary preferences?\",\n",
        "    \"How often do you eat fast food or processed foods?\"\n",
        "]\n",
        "```\n",
        "\n",
        "**WARNING:** Getting the Medical History Taker to work properly can be a bit tricky. Do your best and go slow. Wait until you see ![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image27F.png) to start speaking."
      ],
      "metadata": {
        "id": "-sXMDIa3kwtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n"
      ],
      "metadata": {
        "id": "Obmv3Q53kwtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your output will depend up which option you chose and your answer to those questions.\n"
      ],
      "metadata": {
        "id": "FyrNkXsykwtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your output will depend on your 5 different questions."
      ],
      "metadata": {
        "id": "xgiSPWPs16Qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Biology Quiz Bot\n",
        "\n",
        "For Example 6 we are going to build a chatbot that will quiz students on a specific topic and provides feedback. For Example 6, the specific topic is **basic biology**. In this example, the voice is set to \"Kore\".\n",
        "\n",
        "**WARNING:** Getting the Quiz Bot to work correctly can be a bit tricky. Do your best and go slow. Wait until you see ![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image26F.png) to start speaking."
      ],
      "metadata": {
        "id": "Y_EeiA9neZZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Biology Quiz Chatbot\n",
        "\n",
        "# Set the voice\n",
        "VOICE=\"Kore\"\n",
        "\n",
        "async def quiz_chatbot(voice=VOICE):\n",
        "    \"\"\"A chatbot that quizzes students on specific topic.\"\"\"\n",
        "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "    import sys\n",
        "\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model='gemini-2.5-flash',\n",
        "        temperature=0.3,\n",
        "        google_api_key=API_KEY\n",
        "    )\n",
        "\n",
        "    # Quiz questions and answers\n",
        "    quiz = [\n",
        "        {\"question\": \"What organelle is known as the powerhouse of the cell?\", \"answer\": \"mitochondria\"},\n",
        "        {\"question\": \"What molecule carries genetic information?\", \"answer\": \"dna\"},\n",
        "        {\"question\": \"What is the process by which plants convert sunlight into energy?\", \"answer\": \"photosynthesis\"},\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"üß¨ QUIZ CHATBOT\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    greeting = \"Welcome to the Quiz! I'll ask you 3 questions. Let's begin!\"\n",
        "    print(f\"ü§ñ {greeting}\\n\")\n",
        "    await speak(greeting, voice=voice)\n",
        "    await asyncio.sleep(3)  # Wait for speech to finish\n",
        "\n",
        "    for i, q in enumerate(quiz):\n",
        "        print(f\"ü§ñ Question {i+1}: {q['question']}\\n\")\n",
        "        await speak(q['question'], voice=voice)\n",
        "        await asyncio.sleep(4)  # Wait for question to be spoken\n",
        "\n",
        "        print(\"üé§ Your answer... (speak now!)\")\n",
        "        sys.stdout.flush()\n",
        "        audio_file = record_audio(sec=8)\n",
        "\n",
        "        if audio_file:\n",
        "            student_answer = transcribe(audio_file)\n",
        "            if student_answer:\n",
        "                print(f\"üë§ You said: {student_answer}\\n\")\n",
        "\n",
        "                # Simple check - see if the correct answer appears in the student's response\n",
        "                correct_answer = q['answer'].lower()\n",
        "                student_lower = student_answer.lower()\n",
        "\n",
        "                # Check if answer is correct (simple string match)\n",
        "                if correct_answer in student_lower:\n",
        "                    score += 1\n",
        "                    feedback = \"That's correct! Great job!\"\n",
        "                else:\n",
        "                    feedback = f\"That's not quite right. The answer is {q['answer']}.\"\n",
        "\n",
        "                print(f\"ü§ñ {feedback}\\n\")\n",
        "                await speak(feedback, voice=voice)\n",
        "                await asyncio.sleep(2)  # Wait for feedback to be spoken\n",
        "\n",
        "        await asyncio.sleep(1)\n",
        "\n",
        "    # Final score\n",
        "    final = f\"Quiz complete! You scored {score} out of {len(quiz)}.\"\n",
        "    print(f\"\\nü§ñ {final}\")\n",
        "    await speak(final, voice=voice)\n",
        "    await asyncio.sleep(3)  # Wait for score to be spoken\n",
        "\n",
        "    # Thank you message\n",
        "    thanks = \"Thank you for playing!\"\n",
        "    print(f\"ü§ñ {thanks}\\n\")\n",
        "    await speak(thanks, voice=voice)\n",
        "\n",
        "# Run the quiz\n",
        "await quiz_chatbot(voice=VOICE)\n"
      ],
      "metadata": {
        "id": "0Irlch7Zge6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image29F.png)"
      ],
      "metadata": {
        "id": "kqOXJRPBrSO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6: Quiz Bot**\n",
        "\n",
        "For **Exercise 6**, build a chatbot that will quiz students on a specific topic and provide feedback. Instead of using basic biology as the topic in Example 6, you decide a different quiz topic that you find interesting. You will need to generate 3 questions and answers for your topic.\n",
        "\n",
        "Change the voice to **Aoede**.\n",
        "\n",
        "**WARNING:** Getting the Quiz Bot to work correctly can be a bit tricky. Do your best and go slow. Wait until you see ![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image26F.png) to start speaking."
      ],
      "metadata": {
        "id": "nLaG45IytKse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6 here\n",
        "\n"
      ],
      "metadata": {
        "id": "BQS3lu2VtKse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your output will depend upon which topic you selected and your questions for that topic.\n"
      ],
      "metadata": {
        "id": "ZUxJJwektKsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-in**\n",
        "When you have completed and run all of the code cells, use the `File --> Print.. --> Microsoft Print to PDF` if you are running either Windows 10 or 11 to generate a PDF of your Colab notebook. If you have a Mac, use the `File --> Print.. --> Save as PDF`\n",
        "\n",
        "In either case, save your PDF as Copy of Class_04_2.lastname.pdf where lastname is your last name, and upload the file to Canvas.\n"
      ],
      "metadata": {
        "id": "W_DEyDQ5w06v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "\n",
        "##**Attention Is All You Need**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/8/8f/The-Transformer-model-architecture.png)\n",
        "\n",
        "**\"Attention Is All You Need\"** is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique's potential for other tasks like question answering and what is now known as multimodal Generative AI.\n",
        "\n",
        "The paper's title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper's authors, liked the sound of that word.\n",
        "\n",
        "An early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\n",
        "\n",
        "Some early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\n",
        "\n",
        "As of 2024, the paper has been cited more than 140,000 times.\n",
        "\n",
        "**Authors**\n",
        "\n",
        "The authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" to the paper; the listed order was randomized. The Wired article highlights the group's diversity:\n",
        "\n",
        "Six of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.\n",
        "\n",
        "**Methods Discussed & Introduced**\n",
        "\n",
        "The paper is most well known for the introduction of the Transformer architecture, which forms the underlying architecture for most forms of modern Large Language Models (LLMs). A key reason for why the architecture is preferred by most modern LLMs is the parallelizability of the architecture over its predecessors. This ensures that the operations necessary for training can be accelerated on a GPU allowing both faster training times and models of bigger sizes to be trained.\n",
        "\n",
        "The following mechanisms were introduced by the paper as part of the development of the transformer architecture.\n",
        "\n",
        "**Scaled dot-product Attention & Self-attention**\n",
        "\n",
        "The use of the scaled dot-product attention and self-attention mechanism instead of an RNN or LSTM (which rely on recurrence instead) allow for better performance as described in the following paragraph.\n",
        "\n",
        "Since the model relies on Query (Q), Key (K) and Value (V) matrices that come from the same source itself (i.e. the input sequence / context window), this eliminates the need for RNNs completely ensuring parallelizability for the architecture. This differs from the original form of the Attention mechanism introduced in 2014. Additionally, the paper also discusses the use of an additional scaling factor that was found to be most effective with respect to the dimension of the key vectors.\n",
        "\n",
        "In the specific context of translation which the paper focused on, the Query and Key matrices are usually represented in embeddings corresponding to the source language while the Value matrix corresponds to the target language.\n",
        "\n",
        "**Multi-head Attention**\n",
        "\n",
        "In the self-attention mechanism, queries (Q), keys (K), and values (V) are dynamically generated for each input sequence (limited typically by the size of the context window), allowing the model to focus on different parts of the input sequence at different steps. Multi-head attention enhances this process by introducing multiple parallel attention heads. Each attention head learns different linear projections of the Q, K, and V matrices. This allows the model to capture different aspects of the relationships between words in the sequence simultaneously, rather than focusing on a single aspect.\n",
        "\n",
        "By doing this, multi-head attention ensures that the input embeddings are updated from a more varied and diverse set of perspectives. After the attention outputs from all heads are calculated, they are concatenated and passed through a final linear transformation to generate the output.\n",
        "\n",
        "**Positional Encoding**\n",
        "\n",
        "Since the `Transformer model` is not a `seq2seq model` and does not rely on the sequence of the text in order to perform encoding and decoding, the paper relied on the use of sine and cosine wave functions to encode the position of the token into the embedding.\n",
        "\n",
        "**Historical context**\n",
        "\n",
        "For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\n",
        "\n",
        "A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers. However, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.\n",
        "\n",
        "Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.\n",
        "\n",
        "#### **Transformer Architecture**\n",
        "\n",
        "In the Transformer architecture, the self-attention mechanism processes an input sequence by creating a new representation for each token, enriched with context from all other tokens in the sequence. Unlike older recurrent neural networks (RNNs) that process words one by one, self-attention processes the entire sequence in parallel, making it highly efficient.\n",
        "The core idea is for each token to \"look\" at all other tokens to determine their relevance and then use that information to create a more informed, context-aware representation of itself.\n",
        "\n",
        "**The self-attention process**\n",
        "\n",
        "For a given input sequence, such as \"The animal didn't cross the street because it was too tired,\" the self-attention process happens in the following stages:\n",
        "\n",
        "1. **Create Query, Key, and Value vectors:** For every token in the sequence (e.g., \"it\"), the model creates three distinct vectors:\n",
        "* * **Query (Q):** Represents the current token, acting like a question used to find related tokens.\n",
        "* * **Key (K):** Represents the token being looked at, acting like a label for its information.\n",
        "* **Value (V):** Contains the content or contextual information of the token.\n",
        "\n",
        "2. **Calculate attention scores:** To determine how much focus \"it\" should place on other words, the model calculates a score for every other token in the sentence. This is done by taking the dot product of the current token's query vector with each of the other tokens' key vectors. A high dot-product score indicates a strong relationship between the two tokens.\n",
        "\n",
        "3. **Scale the scores:** The scores are scaled by dividing them by the square root of the key vector's dimension. This prevents the scores from growing too large, which helps to stabilize training.\n",
        "\n",
        "* **Normalize with Softmax:** The scaled scores are passed through a softmax function, which converts them into a probability distribution. This ensures that all the attention weights sum up to 1, making them easier to interpret.\n",
        "\n",
        "5. **Compute the weighted sum:** Each token's value vector is multiplied by its corresponding softmax score. The weighted value vectors are then summed to produce a new, context-rich output vector for the original token. In the sentence example, this process would give the word \"it\" a new representation that incorporates information from \"animal,\" correctly linking the two words.\n",
        "\n",
        "#### **Enhancing self-attention with multi-head attention**\n",
        "\n",
        "The Transformer architecture takes this mechanism one step further by using multi-head **attention**.\n",
        "\n",
        "* Instead of a single attention calculation, multi-head attention performs several self-attention calculations in parallel using different learned sets of Q, K, and V weight matrices.\n",
        "* Each \"head\" learns to focus on different types of relationships. For example, one head might attend to grammatical connections, while another might focus on semantic meaning.\n",
        "* The results from each head are then concatenated and passed through a final linear layer to produce the refined output. This gives the model a much richer, multi-contextual understanding of the input.\n",
        "\n",
        "#### **Preserving word order with positional encoding**\n",
        "\n",
        "Because the self-attention mechanism processes all tokens in parallel, it inherently loses information about word order. To address this, the Transformer injects positional information into the input embeddings using positional encoding. This is typically done with sinusoidal functions that create a unique vector for each position in the sequence, which is then added to the token's embedding. This process allows the model to capture the sequence's structure without sacrificing parallel processing efficiency."
      ],
      "metadata": {
        "id": "H3ykBQufwypg"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}