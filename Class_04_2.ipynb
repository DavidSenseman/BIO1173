{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "## **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Introduction to Large Language Models (LLMs)\n",
        "* **Part 4.2: Chatbots**\n",
        "* Part 4.3: Image Generation with StableDiffusion\n",
        "* Part 4.4: Image Generation with DALL-E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded.\n"
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Your OPENAI_API_KEY\n",
        "\n",
        "In order to run the code in this lesson you will need to have your secret `OEPNAI_API_KEY` installed in your **Secrets** on this Colab notebook. Detailed steps for purchasing your `OPENAI_API_KEY` and installing it in your Colab notebook Secrets was provide in `Class_04_1`.\n",
        "\n",
        "Run the code in the next cell to see if your `OPENAI_API_KEY` is installed correctly. You make have to Grant Access for your notebook to use your API key."
      ],
      "metadata": {
        "id": "v7QopUS2wT9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify your API key setup\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Check if API key is properly loaded\n",
        "try:\n",
        "    OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"API key loaded successfully!\")\n",
        "    print(f\"Key length: {len(OPENAI_KEY)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading API key: {e}\")\n",
        "    print(\"Please set your API key in Google Colab:\")\n",
        "    print(\"1. Go to Secrets in the left sidebar\")\n",
        "    print(\"2. Create a new secret named 'openai_api_key'\")\n",
        "    print(\"3. Paste your OpenAI API key\")"
      ],
      "metadata": {
        "id": "Gi0ydJubCZsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. You may see this message when you run this cell:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image08C.png)\n",
        "\n",
        "If you do see this popup just click on `Grant access`.\n",
        "\n",
        "\n",
        "2. If your `OPENAI_API_KEY` is correctly installed you should see something _similar_ to the following output.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image09C.png)\n",
        "\n",
        "3. However, if you see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image10C.png)\n",
        "\n",
        "You will need to correct the error before you can continue. Ask your Instructor or TA for help if you can resolve the error yourself."
      ],
      "metadata": {
        "id": "l5SoKbnFMT-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the LLM models that You have Access\n",
        "\n",
        "Run the code below to see a list of the Open AI models that your Open AI key gives you access to."
      ],
      "metadata": {
        "id": "rn1bQWsJKCIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check LLM models\n",
        "\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Get the API key\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n",
        "\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# List available models\n",
        "try:\n",
        "    models = client.models.list()\n",
        "    print(\"Available models:\")\n",
        "    for model in models.data:\n",
        "        print(f\"  {model.id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error listing models: {e}\")"
      ],
      "metadata": {
        "id": "OK6x87a5KCu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to complete this lesson, you will need to have access to the following 5 models:\n",
        "\n",
        "Available models:\n",
        "* **dall-e-3**\n",
        "* **gpt-4o-mini**\n",
        "* **gpt-5-mini**\n",
        "* **tts-1**\n",
        "* **whisper-1**\n",
        "\n",
        " If you don't see all 5 models listed, you will need to add it to your `Default` project on `OpenAI` before you can continue. Please see the PDF instructions on Canvas on how to add models to your Default project in your OpenAI API account."
      ],
      "metadata": {
        "id": "f4VshMcqQd8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `LangChain` packages\n",
        "\n",
        "Run the code in the following cell to install the `langchain-openai` and related packages."
      ],
      "metadata": {
        "id": "DJbJtTtexCNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install langchain-openai package\n",
        "\n",
        "!pip install -q langchain langchain_openai openai pydub > /dev/null"
      ],
      "metadata": {
        "id": "uLrdjGVzFgVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any output."
      ],
      "metadata": {
        "id": "IqvEOnmrdGVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to ChatBots**\n",
        "\n",
        "Run the next cell to see short introduction to ChatBots. This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "UlTxZtcKMvNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"gmUHEvrpYoU\"\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen>\n",
        "</iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "6Wobn6A5Mv7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Speech Processing**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/CourseImage.gif)\n",
        "\n",
        "In this lesson, we explore how to use both computer-generated voice and voice recognition to create a `ChatBot`. We'll be working with the `OpenAI API` to achieve this. Specifically, we'll demonstrate how to input normal text and have it spoken by the computer, and conversely, how we can speak to the computer and have it respond. We'll ultimately integrate these functionalities to create a chatbot that handles both text-to-speech and speech-to-text interactions.\n",
        "\n",
        "While we'll use Google `Colab` for this demonstration, in production environments, you'd likely use a mobile app or a web-based JavaScript solution, as each platform handles voice differently. We'll focus on keeping things generic and simple in Colab for now.\n",
        "\n",
        "Voice applications are everywhere. For example, I can ask \"`Alexa`, what time is it?\" and multiple `Alexa` devices in my home will respond, although not always perfectly. I usually mute them during recording sessions. Applications like `Siri` or even `ChatGPT` also offer voice interactions. For instance, when you click the voice option in `ChatGPT` on a computer, it starts listening for your input.\n",
        "\n",
        "To illustrate, I asked `ChatGPT`, \"How are you doing?\" and it responded by offering some insightful thoughts about generative AI. It highlighted that generative AI isn't just about creating new content but about learning patterns from vast amounts of data and applying them creatively across text, images, and code. It also suggested that students experiment with different approaches, as hands-on experience is one of the best ways to learn."
      ],
      "metadata": {
        "id": "opMlmCu1p6z8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbUFwvHyqQ8D"
      },
      "source": [
        "## **Part I: Speech to Text**\n",
        "\n",
        "Here we delve into the realm of speech-to-text technology, focusing on the powerful capabilities offered by OpenAI's models. Speech-to-text, also known as automatic speech recognition (ASR), is a technology that converts spoken language into written text. OpenAI's speech-to-text models represent the cutting edge of this field, leveraging advanced machine learning techniques to achieve high accuracy and robustness across various accents, languages, and acoustic environments. We'll explore how these models can be integrated into applications to enable voice-based interactions, transcription services, and accessibility features. By harnessing OpenAI's speech-to-text technology, we'll unlock new possibilities for human-computer interaction and demonstrate how to transform audio input into actionable text data with remarkable precision.\n",
        "\n",
        "\n",
        "Note we will make use of the technique described here to record audio in CoLab.\n",
        "\n",
        "https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGoIgiyQKGcK"
      },
      "source": [
        "## **Summary of the Audio Recording Setup in Google Colab**\n",
        "\n",
        "This code in the cell below sets up the ability to **record audio from the user's microphone** in a **Google Colab notebook** using JavaScript and Python. We need to use JavaScript since Google Colab doesn't allow direct audio recording from a user's microphone primarily due to browser security restrictions and Colab's architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use JavaScript to record audio in Colab notebook\n",
        "\n",
        "from IPython.display import Javascript, display, Audio\n",
        "from google.colab import output\n",
        "import base64\n",
        "import io\n",
        "from pydub import AudioSegment\n",
        "import time\n",
        "\n",
        "# Global variable to track recording state\n",
        "recording_complete = False\n",
        "recorded_data = None\n",
        "\n",
        "# Updated RECORD JavaScript with proper callback handling\n",
        "RECORD = \"\"\"\n",
        "const sleep = (time) => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = (blob) => new Promise((resolve) => {\n",
        "    const reader = new FileReader()\n",
        "    reader.onloadend = (e) => resolve(e.srcElement.result)\n",
        "    reader.readAsDataURL(blob)\n",
        "})\n",
        "\n",
        "var record = (time) => new Promise(async (resolve) => {\n",
        "    try {\n",
        "        stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "        recorder = new MediaRecorder(stream)\n",
        "        chunks = []\n",
        "        recorder.ondataavailable = (e) => chunks.push(e.data)\n",
        "        recorder.start()\n",
        "        await sleep(time)\n",
        "        recorder.onstop = async () => {\n",
        "            blob = new Blob(chunks, { type: 'audio/webm' })\n",
        "            text = await b2text(blob)\n",
        "            resolve(text)\n",
        "        }\n",
        "        recorder.stop()\n",
        "    } catch (error) {\n",
        "        console.error('Recording error:', error)\n",
        "        resolve(null)\n",
        "    }\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record(seconds=3):\n",
        "    \"\"\"\n",
        "    Record audio using browser microphone with proper synchronization\n",
        "    \"\"\"\n",
        "    global recording_complete, recorded_data\n",
        "\n",
        "    # Reset tracking variables\n",
        "    recording_complete = False\n",
        "    recorded_data = None\n",
        "\n",
        "    print(f\"Recording now for {seconds} seconds...\")\n",
        "\n",
        "    # Display the JavaScript code\n",
        "    display(Javascript(RECORD))\n",
        "\n",
        "    # Execute the recording and wait for completion via callback\n",
        "    def on_recording_complete(result):\n",
        "        global recording_complete, recorded_data\n",
        "        recorded_data = result\n",
        "        recording_complete = True\n",
        "\n",
        "    # Register a function to handle the result\n",
        "    output.register_callback('record_completed', on_recording_complete)\n",
        "\n",
        "    # Start recording using JavaScript\n",
        "    js_code = f\"\"\"\n",
        "    record({seconds * 1000}).then(result => {{\n",
        "        google.colab.kernel.invokeFunction('record_completed', [result], {{}})\n",
        "    }})\n",
        "    \"\"\"\n",
        "\n",
        "    display(Javascript(js_code))\n",
        "\n",
        "    # Wait for recording to complete (with timeout)\n",
        "    start_time = time.time()\n",
        "    while not recording_complete and time.time() - start_time < seconds + 5:\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    if not recording_complete:\n",
        "        raise TimeoutError(\"Recording timed out\")\n",
        "\n",
        "    if recorded_data is None:\n",
        "        raise RuntimeError(\"No audio data received\")\n",
        "\n",
        "    # Process the recorded data\n",
        "    try:\n",
        "        binary_data = base64.b64decode(recorded_data.split(',')[1])\n",
        "\n",
        "        with io.BytesIO(binary_data) as audio_file:\n",
        "            audio = AudioSegment.from_file(audio_file, format=\"webm\")\n",
        "\n",
        "        # Export as WAV file\n",
        "        audio.export(\"recorded_audio.wav\", format=\"wav\")\n",
        "        print(\"Recording complete. Audio saved as 'recorded_audio.wav'\")\n",
        "\n",
        "        return audio\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing audio: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Simple version that should work better in Colab\n",
        "def record_simple(seconds=3):\n",
        "    \"\"\"\n",
        "    Simplified recording function with proper timing\n",
        "    \"\"\"\n",
        "    print(f\"Recording now for {seconds} seconds...\")\n",
        "\n",
        "    # Display the JavaScript code\n",
        "    display(Javascript(RECORD))\n",
        "\n",
        "    # Execute recording and wait for result (this approach works better in Colab)\n",
        "    try:\n",
        "        duration_ms = int(seconds * 1000)\n",
        "        s = output.eval_js(f'record({duration_ms})')\n",
        "\n",
        "        if not s or not isinstance(s, str):\n",
        "            raise RuntimeError(\"Failed to capture audio\")\n",
        "\n",
        "        # Decode and process the audio\n",
        "        binary_data = base64.b64decode(s.split(',')[1])\n",
        "\n",
        "        with io.BytesIO(binary_data) as audio_file:\n",
        "            audio = AudioSegment.from_file(audio_file, format=\"webm\")\n",
        "\n",
        "        # Export as WAV file\n",
        "        audio.export(\"recorded_audio.wav\", format=\"wav\")\n",
        "        print(\"Recording complete. Audio saved as 'recorded_audio.wav'\")\n",
        "\n",
        "        return audio\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during recording: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "BHYKG1ZR4fsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any output."
      ],
      "metadata": {
        "id": "XsH60N9a5tTv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBwAnTyBKLxP"
      },
      "source": [
        "### Example 1: Record and Play Audio\n",
        "\n",
        "The code in the cell below uses the `record_simple()` function to record a `5 second` audio file and then plays it back.\n",
        "\n",
        "You may need to grant permission for the program to access the microphone on your laptop or computer.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image07A.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Record and Play Audio\n",
        "\n",
        "# Set recording durations\n",
        "record_duration = 5\n",
        "\n",
        "# Record audio\n",
        "audio = record_simple(record_duration)\n",
        "\n",
        "# Play back audio\n",
        "display(Audio(\"recorded_audio.wav\", autoplay=True))"
      ],
      "metadata": {
        "id": "LifXtVma65dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image02A.png)"
      ],
      "metadata": {
        "id": "6S5IbInwcIEg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LciCRMKjCF8Z"
      },
      "source": [
        "### **Exercise 1: Record and Play Audio**\n",
        "\n",
        "In the cell below, use the `record()` function to record an `8 second` audio file and then play it back. Once you hit the run cell icon\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png)speak into your microphone.\n",
        "\n",
        "You should not encounter the same problem as in Example 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyQLLdbrCF8Z"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image04A.png)"
      ],
      "metadata": {
        "id": "1uZaGQdjCF8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OpenAI Speech-to-Text API**\n",
        "\n",
        "### Overview of the API\n",
        "- **Models**: Includes `Whisper` and newer `GPT-4o-based` models.\n",
        "- **Input**: Accepts audio files (e.g., MP3, WAV, MP4).\n",
        "- **Output**: Returns transcriptions in formats like plain text, JSON, or subtitle formats (SRT, VTT).\n",
        "- **Languages**: Supports dozens of languages with high accuracy, especially in noisy or accented speech environments.\n",
        "\n",
        "### Why It’s Useful for Biomedical Investigators\n",
        "\n",
        "1. **Transcribing Interviews & Focus Groups**  \n",
        "   Automatically convert recorded conversations with patients, clinicians, or research participants into text for qualitative analysis.\n",
        "\n",
        "2. **Clinical Note Dictation**  \n",
        "   Researchers can dictate observations or notes during fieldwork or lab work, streamlining documentation.\n",
        "\n",
        "3. **Meeting & Conference Transcripts**  \n",
        "   Capture and archive discussions from research meetings, seminars, or collaborative calls.\n",
        "\n",
        "4. **Data Extraction from Audio**  \n",
        "   Enables downstream NLP tasks like identifying social determinants of health (SDOH) or extracting biomedical entities from spoken content.\n",
        "\n",
        "5. **Multilingual Support**  \n",
        "   Useful in global health research where interviews or data collection occur in multiple languages.\n",
        "\n",
        "\n",
        "This code in the cell below demonstrates how to use OpenAI's speech-to-text API to transcribe audio files. It defines a function `transcribe_audio` that takes a filename as input. The function opens the specified audio file in binary mode and uses the OpenAI client to create a transcription. The `client.audio.transcriptions.create() method` is called with two parameters: the model (\"whisper-1\") and the audio file. `Whisper` is OpenAI's state-of-the-art speech recognition model, known for its robustness across various languages and accents. The function returns the transcribed text. In the example usage, an audio file named \"recorded_audio.wav\" is transcribed, and the resulting text is printed. This code provides a simple yet powerful way to convert speech to text, which can be invaluable for tasks such as generating subtitles, creating searchable archives of audio content, or enabling voice commands in applications.\n"
      ],
      "metadata": {
        "id": "nD3zE5yVE0vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `transcribe_audio()` function\n",
        "\n",
        "Run the code in the next cell to create the `transcribe_audio(filename)` function. This Python function is designed to transcribe spoken words from an audio file into text using OpenAI's `Whisper` model via their API.\n",
        "\n",
        "Here's a breakdown of what it does:\n",
        "\n",
        "* Opens the audio file specified by filename in binary read mode (\"rb\").\n",
        "* Sends the file to the OpenAI Whisper model (whisper-1) using the client.audio.transcriptions.create() method.\n",
        "* Receives the transcription result from the model.\n",
        "* Returns the transcribed text from the audio."
      ],
      "metadata": {
        "id": "aoRtz5uizINp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create transcribe_audio() function\n",
        "\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def transcribe_audio(filename, model=\"whisper-1\", language=None, prompt=None):\n",
        "    \"\"\"\n",
        "    Transcribe audio file using OpenAI's Whisper API\n",
        "\n",
        "    Args:\n",
        "        filename (str): Path to the audio file\n",
        "        model (str): Whisper model to use (default: \"whisper-1\")\n",
        "        language (str): Language code (e.g., \"en\", \"es\") - optional\n",
        "        prompt (str): Optional text prompt for transcription - optional\n",
        "\n",
        "    Returns:\n",
        "        str: Transcribed text\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If audio file doesn't exist\n",
        "        ValueError: If API key is not set or invalid\n",
        "        Exception: For other transcription errors\n",
        "    \"\"\"\n",
        "\n",
        "    # Input validation\n",
        "    if not filename:\n",
        "        raise ValueError(\"Filename cannot be empty\")\n",
        "\n",
        "    if not os.path.exists(filename):\n",
        "        raise FileNotFoundError(f\"Audio file not found: {filename}\")\n",
        "\n",
        "    # Check if OpenAI API key is set\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n",
        "\n",
        "    try:\n",
        "        # Initialize client\n",
        "        client = OpenAI(api_key=api_key)\n",
        "\n",
        "        # Prepare transcription parameters\n",
        "        params = {\n",
        "            \"model\": model,\n",
        "            \"file\": open(filename, \"rb\"),\n",
        "            \"response_format\": \"text\"  # Return as text directly\n",
        "        }\n",
        "\n",
        "        # Add optional parameters if provided\n",
        "        if language:\n",
        "            params[\"language\"] = language\n",
        "        if prompt:\n",
        "            params[\"prompt\"] = prompt\n",
        "\n",
        "        # Perform transcription\n",
        "        with open(filename, \"rb\") as audio_file:\n",
        "            transcription = client.audio.transcriptions.create(\n",
        "                model=model,\n",
        "                file=audio_file,\n",
        "                response_format=\"text\"\n",
        "            )\n",
        "\n",
        "        return transcription\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Audio file not found: {filename}\")\n",
        "    except Exception as e:\n",
        "        if \"API key\" in str(e).lower() or \"authentication\" in str(e).lower():\n",
        "            raise ValueError(\"Invalid or missing OpenAI API key\")\n",
        "        elif \"model\" in str(e).lower() and \"not found\" in str(e).lower():\n",
        "            raise ValueError(f\"Model '{model}' not available\")\n",
        "        else:\n",
        "            raise Exception(f\"Transcription failed: {str(e)}\")"
      ],
      "metadata": {
        "id": "1JqHGnpm-0G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any output."
      ],
      "metadata": {
        "id": "sVaPNWChXWMK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jurTAwUCKRM-"
      },
      "source": [
        "## Example 2: Speech-to-Text\n",
        "\n",
        "This code in the cell below uses the `transcribe_audio()` function to convert your voice into text.\n",
        "\n",
        "Once you hit the run cell icon\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png)start counting out loud from `1` to `10`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Speech-to-Text\n",
        "\n",
        "import openai\n",
        "import os\n",
        "from IPython.display import Audio\n",
        "\n",
        "# Set LLM model\n",
        "LLM_MODEL = \"whisper-1\"\n",
        "\n",
        "# Set recording duration\n",
        "record_duration = 10\n",
        "\n",
        "# Record audio using your improved recording function\n",
        "audio = record_simple(record_duration)\n",
        "\n",
        "# Set api key\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# Transcribe the audio file using whisper-1 model (best for speech-to-text)\n",
        "try:\n",
        "    with open(\"recorded_audio.wav\", \"rb\") as audio_file:\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=LLM_MODEL,\n",
        "            file=audio_file,\n",
        "            response_format=\"text\"\n",
        "        )\n",
        "\n",
        "        print(\"Transcription:\")\n",
        "        print(transcription)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during transcription: {str(e)}\")\n",
        "\n",
        "# Display the audio file\n",
        "display(Audio(\"recorded_audio.wav\", autoplay=True))\n"
      ],
      "metadata": {
        "id": "BPociJwXBIhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image38C.png)"
      ],
      "metadata": {
        "id": "k-nF8ycpXUkq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCA54j6KNooN"
      },
      "source": [
        "## **Exercise 2: Speech-to-Text**\n",
        "\n",
        "In the cell below, write to code to generate Speech-to-Text using the code in Example 2 as an template.\n",
        "\n",
        "For **Exercise 2**, once you hit the run cell icon\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png)start counting **_backwards_** from `10` to `1`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n"
      ],
      "metadata": {
        "id": "_GanAyHOXtEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image39C.png)"
      ],
      "metadata": {
        "id": "uP2uir0WYLhF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2MPPX0c1pHi"
      },
      "source": [
        "## **Part 2: Text to Speech**\n",
        "\n",
        "In Part 2, we'll explore the fascinating world of text-to-speech (TTS) Large Language Models (LLMs), focusing on OpenAI's cutting-edge offerings. We'll primarily utilize OpenAI's `TTS-1` model, a powerful and versatile tool designed for converting written text into natural-sounding speech.\n",
        "\n",
        "`TTS-1` is optimized for real-time applications, making it ideal for scenarios that require low-latency audio generation. This model represents a significant advancement in speech synthesis technology, leveraging deep learning techniques to produce high-quality, lifelike vocal outputs. By delving into TTS-1, we'll explore its capabilities, examine its practical applications, and understand how it's revolutionizing various industries, from accessibility solutions to interactive voice responses and beyond.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDOMjRUP1fdv"
      },
      "source": [
        "## **OpenAI's Voices**\n",
        "\n",
        "When using `OpenAI's` text-to-speech API to generate spoken audio from text, you can select one of several voices. The `openai.audio.speech.create()` function is called with three parameters: the model (\"tts-1\"), the voice (e.g. \"alloy\"), and the input text. `OpenAI` offers several voice options, including:\n",
        "\n",
        "* **alloy** - neutral\n",
        "* **echo** - young\n",
        "* **fable** - male\n",
        "* **onyx** - deep male\n",
        "* **nova** - female\n",
        "* **shimmer** - warm female\n",
        "\n",
        "Each voice has its unique characteristics, allowing users to choose the most suitable one for their application. Additionally, `OpenAI` provides a high-definition model called `tts-1-hd` for enhanced audio quality, though it may have higher latency. The function returns a response object, from which the audio content is extracted and stored in the `audio_data variable` for further processing or playback."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Demonstrate Different Voices\n",
        "\n",
        "The code in the cell below demonstates 3 of the different voices that are available in the `OpenAI` text-to-speech API:\n",
        "\n",
        "* **alloy**\n",
        "* **echo**\n",
        "* **fable**\n",
        "\n",
        "Run the code cell to hear each of these three voices."
      ],
      "metadata": {
        "id": "mPOHlT667J9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Demonstrate different voices\n",
        "\n",
        "import io\n",
        "from openai import OpenAI\n",
        "from IPython.display import Audio, display\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "def demonstrate_voices():\n",
        "    \"\"\"\n",
        "    Demonstrate different text-to-speech voices with improved error handling\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize OpenAI client\n",
        "    # Set api key\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    if not api_key:\n",
        "        raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n",
        "\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    # Define voices\n",
        "    voices = [\"alloy\", \"echo\", \"fable\"]\n",
        "    audio_segments = []\n",
        "\n",
        "    print(\"Generating speech with different voices...\")\n",
        "\n",
        "    try:\n",
        "        # Loop through voices\n",
        "        for i, voice in enumerate(voices):\n",
        "            text = f\"Hello, Welcome to BIO 1 1 7 3...Introduction to Computational Biology..., I am the {voice} voice.\"\n",
        "\n",
        "            print(f\"Generating audio with '{voice}' voice...\")\n",
        "\n",
        "            response = client.audio.speech.create(\n",
        "                model=\"tts-1\",\n",
        "                voice=voice,\n",
        "                input=text,\n",
        "                speed=1.0  # Optional: adjust speech speed\n",
        "            )\n",
        "\n",
        "            audio_segments.append(response.content)\n",
        "            print(f\"✓ Completed voice: {voice}\")\n",
        "\n",
        "        if not audio_segments:\n",
        "            raise ValueError(\"No audio segments were generated\")\n",
        "\n",
        "        # Combine audio segments with proper handling\n",
        "        print(\"Combining audio segments...\")\n",
        "        combined_audio = AudioSegment.empty()\n",
        "\n",
        "        for i, segment in enumerate(audio_segments):\n",
        "            try:\n",
        "                # Convert each segment to AudioSegment\n",
        "                audio_segment = AudioSegment.from_file(io.BytesIO(segment), format=\"mp3\")\n",
        "                combined_audio += audio_segment\n",
        "\n",
        "                # Add a small pause between voices (optional)\n",
        "                if i < len(audio_segments) - 1:  # Not the last segment\n",
        "                    silence = AudioSegment.silent(duration=500)  # 500ms silence\n",
        "                    combined_audio += silence\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not process segment {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if len(combined_audio) == 0:\n",
        "            raise ValueError(\"No valid audio was created\")\n",
        "\n",
        "        # Convert the combined audio to a byte stream\n",
        "        buffer = io.BytesIO()\n",
        "        combined_audio.export(buffer, format=\"mp3\")\n",
        "        buffer.seek(0)\n",
        "\n",
        "        # Play the audio in Colab\n",
        "        print(\"\\nPlaying combined audio with all voices:\")\n",
        "        display(Audio(buffer.read(), autoplay=True))\n",
        "\n",
        "        # Optional: Save to file\n",
        "        output_filename = f\"combined_voices_{int(time.time())}.mp3\"\n",
        "        with open(output_filename, \"wb\") as f:\n",
        "            buffer.seek(0)\n",
        "            f.write(buffer.read())\n",
        "        print(f\"\\nAudio saved as: {output_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during voice demonstration: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Run the demonstration\n",
        "try:\n",
        "    demonstrate_voices()\n",
        "except ValueError as ve:\n",
        "    print(f\"Setup Error: {ve}\")\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error: {e}\")"
      ],
      "metadata": {
        "id": "tgmKRFmLZSO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image41C.png)\n",
        "\n",
        "You should have heard 3 different voices speaking."
      ],
      "metadata": {
        "id": "9GhWUHcZ5ULL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Demonstrate Different Voices**\n",
        "\n",
        "In the cell below, write the code to demonstate the other 3 voices that are available:\n",
        "\n",
        "* **onyx**\n",
        "* **nova**\n",
        "* **shimmmer**\n",
        "\n",
        "You should use the code in Example 3 as a template."
      ],
      "metadata": {
        "id": "vY_hEkxq90M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n"
      ],
      "metadata": {
        "id": "lBQ74jlDfDZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image42C.png)\n",
        "\n",
        "You should have heard 3 different voices speaking."
      ],
      "metadata": {
        "id": "xq9abR-C7eYE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZPN9RExHnaQ"
      },
      "source": [
        "### Create `generate_text()` function\n",
        "\n",
        "Run the code in the next cell to create the `generate text()` function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create generate_text() function\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "def generate_text(text, voice=\"alloy\", model=\"tts-1\", speed=1.0, response_format=\"mp3\"):\n",
        "    \"\"\"\n",
        "    Generate audio from text using OpenAI's text-to-speech API\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to convert to speech\n",
        "        voice (str): The voice to use (default: \"alloy\")\n",
        "        model (str): The TTS model to use (default: \"tts-1\")\n",
        "        speed (float): Speech speed (0.25 to 4.0, default: 1.0)\n",
        "        response_format (str): Output format (\"mp3\", \"opus\", \"aac\", \"flac\") (default: \"mp3\")\n",
        "\n",
        "    Returns:\n",
        "        bytes: Audio data as bytes\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If input parameters are invalid\n",
        "        Exception: For API errors\n",
        "    \"\"\"\n",
        "\n",
        "    # Input validation\n",
        "    if not text or not isinstance(text, str):\n",
        "        raise ValueError(\"Text must be a non-empty string\")\n",
        "\n",
        "    if not voice or not isinstance(voice, str):\n",
        "        raise ValueError(\"Voice must be a valid string\")\n",
        "\n",
        "    if speed < 0.25 or speed > 4.0:\n",
        "        raise ValueError(\"Speed must be between 0.25 and 4.0\")\n",
        "\n",
        "    # Initialize client globally\n",
        "    global client\n",
        "    if 'client' not in globals():\n",
        "        # Set api key\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "        if not api_key:\n",
        "            raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n",
        "        client = OpenAI(api_key=api_key)\n",
        "\n",
        "    try:\n",
        "        response = client.audio.speech.create(\n",
        "            model=model,\n",
        "            voice=voice,\n",
        "            input=text,\n",
        "            speed=speed,\n",
        "            response_format=response_format\n",
        "        )\n",
        "\n",
        "        return response.content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating speech: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "CJ9Z940iqYHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `speak_text()` function\n",
        "\n",
        "Run the next cell to generate the `speak_text()` function."
      ],
      "metadata": {
        "id": "Mh42_Bb7vvoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create speak_text() function\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the api_key\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Generate text\n",
        "def speak_text(text, voice=\"alloy\", model=\"tts-1\", speed=1.0,\n",
        "                       autoplay=True, save_to_file=None, play_after_save=False):\n",
        "    \"\"\"\n",
        "    Advanced text-to-speech function with additional features\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to convert to speech\n",
        "        voice (str): The voice to use\n",
        "        model (str): The TTS model to use\n",
        "        speed (float): Speech speed\n",
        "        autoplay (bool): Whether to automatically play the audio\n",
        "        save_to_file (str): Optional filename to save the audio file\n",
        "        play_after_save (bool): Whether to play after saving\n",
        "\n",
        "    Returns:\n",
        "        tuple: (audio_data, file_path) - audio data and saved file path\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate audio\n",
        "    audio_data = generate_text(text, voice, model, speed)\n",
        "\n",
        "    # Save if requested\n",
        "    file_path = None\n",
        "    if save_to_file:\n",
        "        try:\n",
        "            with open(save_to_file, \"wb\") as f:\n",
        "                f.write(audio_data)\n",
        "            file_path = save_to_file\n",
        "            print(f\"Audio saved to: {file_path}\")\n",
        "\n",
        "            if play_after_save:\n",
        "                display(Audio(audio_data, autoplay=True))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not save file: {e}\")\n",
        "\n",
        "    # Play audio\n",
        "    if autoplay:\n",
        "        display(Audio(audio_data, autoplay=True))\n",
        "\n",
        "    return audio_data, file_path\n",
        "\n"
      ],
      "metadata": {
        "id": "0zHs4_U8fyBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Transcribe Recorded Data\n",
        "\n",
        "The code in the cell shows how to record your speech, print out a transcription of what you said, and finally, read the transcription using the \"alloy\" voice.\n",
        "\n",
        "Once you hit the run cell icon\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png), read out loud Carl Sandburg’s poem “Fog” --a short, imagistic piece that captures the quiet, mysterious arrival of fog. Don't forget to start by saying the title of the poem, \"FOG\".\n",
        "\n",
        "```text\n",
        "FOG\n",
        "\n",
        "The fog comes\n",
        "on little cat feet.\n",
        "\n",
        "It sits looking\n",
        "over harbor and city\n",
        "on silent haunches\n",
        "and then moves on.\n",
        "```"
      ],
      "metadata": {
        "id": "8oHJAsCA5qHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Transcribe recorded audio\n",
        "\n",
        "# Define voice\n",
        "voice = \"alloy\"\n",
        "\n",
        "# Define recording duration\n",
        "duration = 20\n",
        "\n",
        "try:\n",
        "    # Record audio using your improved recording function\n",
        "    audio = record_simple(duration)\n",
        "\n",
        "    # Transcribe audio\n",
        "    transcription = transcribe_audio(\"recorded_audio.wav\")\n",
        "    print(\"Transcription:\")\n",
        "    print(transcription)\n",
        "\n",
        "    # Speak transcription\n",
        "    speak_text(transcription, voice)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in the process: {e}\")"
      ],
      "metadata": {
        "id": "tcDQFFljACvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image14A.png)"
      ],
      "metadata": {
        "id": "xNDmAFlBCv0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Transcribe Recorded Data**\n",
        "\n",
        "In the cell below, write the code to record your speech, print out a transcription of what you said, and finally, read the transcription using the \"onyx\" voice.\n",
        "\n",
        "After you start running the cell, start reading _The Red Wheelbarrow_ by William Carlos Williams. Like _Fog_, it’s a minimalist, imagist poem that captures a vivid moment with few words:\n",
        "\n",
        "```text\n",
        "The Red Wheelbarrow\n",
        "\n",
        "so much depends\n",
        "upon\n",
        "\n",
        "a red wheel\n",
        "barrow\n",
        "\n",
        "glazed with rain\n",
        "water\n",
        "\n",
        "beside the white\n",
        "chickens.\n",
        "```"
      ],
      "metadata": {
        "id": "ttTsJ-QADFd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n"
      ],
      "metadata": {
        "id": "DD1z6Q7ZDFd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image09A.png)"
      ],
      "metadata": {
        "id": "ZIAxHm1sDFd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: Chatbots**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image10A.png)\n",
        "\n",
        "The history of **chatbots** is a fascinating journey through the evolution of artificial intelligence and human-computer interaction. Here's a brief overview:\n",
        "\n",
        "* **1. The Early Days (1950s-1970s)**\n",
        "1950 - Alan Turing's \"Imitation Game\": Turing proposed a test (now known as the Turing Test) to determine if a machine could exhibit intelligent behavior indistinguishable from a human.\n",
        "1966 - ELIZA: Created by Joseph Weizenbaum at MIT, ELIZA was the first chatbot. It mimicked a Rogerian psychotherapist by rephrasing user input into questions. It was simple but groundbreaking.\n",
        "1972 - PARRY: Developed by Kenneth Colby, PARRY simulated a person with paranoid schizophrenia. It was more complex than ELIZA and could hold more realistic conversations.\n",
        "* **2. Rule-Based Systems (1980s-1990s)**\n",
        "Chatbots during this era used hand-coded rules and decision trees.\n",
        "They were mostly used in academic research, customer service, and early virtual assistants.\n",
        "Examples include Jabberwacky (late 1980s), which aimed to simulate natural human chat through learning.\n",
        "* **3. Rise of the Internet and AI (2000s)**\n",
        "SmarterChild (2001): A popular chatbot on AOL Instant Messenger and MSN Messenger. It could answer questions, play games, and chat casually.\n",
        "ALICE (Artificial Linguistic Internet Computer Entity): Created by Richard Wallace, it won the Loebner Prize (a Turing Test competition) multiple times.\n",
        "* **4. Machine Learning and NLP Boom (2010s)**\n",
        "2011 - Siri: Apple introduced Siri, a voice-activated assistant that brought chatbots into the mainstream.\n",
        "2014 - Alexa and Cortana: Amazon and Microsoft launched their own virtual assistants.\n",
        "2016 - Facebook Messenger Bots: Facebook opened its platform to developers, leading to a surge in chatbot development for businesses.\n",
        "* **5. Neural Networks and Transformers (Late 2010s-2020s)**\n",
        "2018 – BERT (Google) and GPT (OpenAI): These transformer-based models revolutionized natural language understanding and generation.\n",
        "2020 – GPT-3: A massive leap in chatbot capabilities, enabling more coherent, context-aware, and human-like conversations.\n",
        "2022 – ChatGPT: OpenAI released ChatGPT based on GPT-3.5 and later GPT-4, making advanced conversational AI widely accessible.\n",
        "* **6. The Present and Future (2020s-Today)**\n",
        "Chatbots are now integrated into education, healthcare, customer service, entertainment, and more.\n",
        "Multimodal models (like GPT-4 and beyond) can understand text, images, and even audio.\n",
        "The focus is shifting toward personalization, emotional intelligence, and ethical AI."
      ],
      "metadata": {
        "id": "7qM-H5TrEoBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `Chatbot` Class\n",
        "\n",
        "The code in the next cell creates class called `Chatbot`. In Python, functions and classes are both fundamental building blocks, but they serve different purposes. A Python **`function`** is a reusable block of code that performs a specific task. On the other hand, a Python **`class`** is a blueprint for creating objects. It groups related data and behaviors together.\n",
        "\n",
        "**Key Features of a `Class`**\n",
        "\n",
        "* Encapsulates data (attributes) and functions (methods) that operate on that data.\n",
        "* Supports object-oriented programming (OOP).\n",
        "* Allows for inheritance, encapsulation, and polymorphism."
      ],
      "metadata": {
        "id": "nDnv5hKGDQwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Chatbot Class\n",
        "\n",
        "import os\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts.chat import PromptTemplate\n",
        "from IPython.display import display_markdown\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the API key and set it as an environment variable\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n",
        "\n",
        "DEFAULT_TEMPLATE = \"\"\"You are a helpful assistant. DO not use markdown, just regular text.\n",
        "limit your response to just a few sentences. If the user says something that indicates\n",
        "that they wish to end the chat return just \"bye\" (no quotes), so I can end the loop.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "AI:\"\"\"\n",
        "\n",
        "MODEL = 'gpt-5-mini'\n",
        "\n",
        "class ChatBot:\n",
        "    def __init__(self, template=DEFAULT_TEMPLATE):\n",
        "        \"\"\"\n",
        "        Initializes the ChatBot with a language model and conversation template.\n",
        "        The API key is retrieved from the environment variable.\n",
        "        \"\"\"\n",
        "        self.llm_chat = ChatOpenAI(model=MODEL)\n",
        "        self.template = template\n",
        "        self.prompt_template = PromptTemplate(input_variables=[\"history\", \"input\"], template=self.template)\n",
        "\n",
        "        # Initialize memory using ConversationBufferMemory (recommended alternative to deprecated class)\n",
        "        self.memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
        "\n",
        "        # Build the chain manually without relying on deprecated `ConversationChain`\n",
        "        def format_history(inputs):\n",
        "            history = self.memory.load_memory_variables({})[\"history\"]\n",
        "            formatted_history = \"\\n\".join([str(h) for h in history])\n",
        "            inputs['history'] = formatted_history\n",
        "            return inputs\n",
        "\n",
        "        self.chain = (\n",
        "            RunnablePassthrough() |\n",
        "            self.prompt_template |\n",
        "            self.llm_chat\n",
        "        )\n",
        "\n",
        "    def chat(self, prompt):\n",
        "        print(f\"Human: {prompt}\")\n",
        "\n",
        "        # Add the user message to memory first\n",
        "        self.memory.chat_memory.add_user_message(prompt)\n",
        "\n",
        "        # Prepare input with history and new user message\n",
        "        inputs = {\"input\": prompt}\n",
        "        formatted_inputs = self.format_history(inputs)\n",
        "\n",
        "        response = self.chain.invoke(formatted_inputs)\n",
        "        display_markdown(response.content, raw=True)  # Use .content if using ChatOpenAI\n",
        "        return response.content\n",
        "\n",
        "    def format_history(self, inputs):\n",
        "        history = self.memory.load_memory_variables({})[\"history\"]\n",
        "        formatted_history = \"\\n\".join([str(h) for h in history])\n",
        "        inputs['history'] = formatted_history\n",
        "        return inputs\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.memory.clear()\n"
      ],
      "metadata": {
        "id": "QBFxYTzE4bIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any output."
      ],
      "metadata": {
        "id": "YWoEZ72849uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Communicate with GPT-5-mini\n",
        "\n",
        "Now that we have create our `ChatBot` class, we can use it to communicate with a LLM. The code in the next cell shows how to converse with `gpt-5-mini`."
      ],
      "metadata": {
        "id": "K3faMMovj2Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Communicate with GPT-5-mini\n",
        "\n",
        "c = ChatBot()\n",
        "response = c.chat(\"Hello, my name is Rowdy the Roadrunner.\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "uSgYb2Jyf3bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image45C.png)\n",
        "\n",
        "Ignore any warnings that you might receive."
      ],
      "metadata": {
        "id": "q1sQ7f_De19l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Communicate with LLM**\n",
        "\n",
        "In the next cell write the code needed to introduce yourself to the `gpt-5-mini` LLM. In other words use your first name instead of `Rowdy the Roadrunner`."
      ],
      "metadata": {
        "id": "q3D2M9AAfK0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jbA9DYGlfK0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image46C.png)\n",
        "\n",
        "except your first name should appear instead of `David` (unless your first is `David`)."
      ],
      "metadata": {
        "id": "yFACGf3EgCyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Integrate OpenAI's Whisper and TTS (Text-to-Speech) with JavaScript**\n",
        "\n",
        "The Python code in the cell below integrates **OpenAI's Whisper and TTS (Text-to-Speech)** models with **Google Colab's JavaScript capabilities** to create an interactive voice interface.\n",
        "\n",
        "**Why Use JavaScript for Microphone Input in Colab?**\n",
        "\n",
        "Google Colab runs Python code on a remote server, not on your local machine.\n",
        "\n",
        "This means:\n",
        "\n",
        "* Python in Colab **cannot directly access your hardware**, like your microphone or webcam.\n",
        "\n",
        "* However, **JavaScript runs in your browser**, which can access local devices (with permission).\n",
        "\n",
        "**What JavaScript Enables**\n",
        "\n",
        "By embedding JavaScript in a Colab cell, you can:\n",
        "\n",
        "* Prompt the user for microphone access.\n",
        "* Record audio using the browser's MediaRecorder API.\n",
        "* Convert the audio to a base64 string.\n",
        "* Send that string back to Python for processing."
      ],
      "metadata": {
        "id": "SscgWjcA5OmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrate OpenAI's Whisper and TTS (Text-to-Speech) with JavaScript\n",
        "\n",
        "from openai import OpenAI\n",
        "from IPython.display import Javascript, Audio, display, HTML\n",
        "from google.colab import output\n",
        "# import base64\n",
        "from base64 import b64decode\n",
        "import io\n",
        "import time\n",
        "import uuid\n",
        "from pydub import AudioSegment\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "    const reader = new FileReader()\n",
        "    reader.onloadend = e => resolve(e.srcElement.result)\n",
        "    reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "    stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "    recorder = new MediaRecorder(stream)\n",
        "    chunks = []\n",
        "    recorder.ondataavailable = e => chunks.push(e.data)\n",
        "    recorder.start()\n",
        "    await sleep(time)\n",
        "    recorder.onstop = async ()=>{\n",
        "        blob = new Blob(chunks)\n",
        "        text = await b2text(blob)\n",
        "        resolve(text)\n",
        "    }\n",
        "    recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def generate_text(text, voice=\"nova\", model=\"tts-1\", speed=1.0):\n",
        "    response = client.audio.speech.create(\n",
        "        model=model,\n",
        "        voice=voice,\n",
        "        input=text,\n",
        "        speed=speed\n",
        "    )\n",
        "    return response.content\n",
        "\n",
        "def speak_text(text, autoplay=True):\n",
        "    audio_data = generate_text(text)\n",
        "\n",
        "    # Encode audio to base64 for embedding in HTML\n",
        "    audio_b64 = base64.b64encode(audio_data).decode('utf-8')\n",
        "\n",
        "    # Generate a unique ID for this audio element\n",
        "    audio_id = f\"audio_{uuid.uuid4().hex}\"\n",
        "\n",
        "    # Display the audio with the unique ID\n",
        "    audio_html = f'''\n",
        "    <audio id=\"{audio_id}\" src=\"data:audio/mpeg;base64,{audio_b64}\" autoplay=\"{str(autoplay).lower()}\" style=\"display: none;\">\n",
        "    </audio>\n",
        "    '''\n",
        "    display(HTML(audio_html))\n",
        "\n",
        "    # Create a hidden div to store the audio status\n",
        "    status_div = f'<div id=\"{audio_id}_status\" style=\"display: none;\">playing</div>'\n",
        "    display(HTML(status_div))\n",
        "\n",
        "    # JavaScript to handle audio playback and status\n",
        "    js_code = f\"\"\"\n",
        "    var audioElement = document.getElementById('{audio_id}');\n",
        "    if (audioElement) {{\n",
        "        audioElement.onended = function() {{\n",
        "            document.getElementById('{audio_id}_status').textContent = 'finished';\n",
        "        }};\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the JavaScript\n",
        "    display(HTML(f\"<script>{js_code}</script>\"))\n",
        "\n",
        "    # Wait for the audio to finish\n",
        "    while True:\n",
        "        try:\n",
        "            status = output.eval_js(f\"document.getElementById('{audio_id}_status').textContent\")\n",
        "            if status == 'finished':\n",
        "                break\n",
        "        except:\n",
        "            time.sleep(0.1)\n",
        "        time.sleep(0.1)\n",
        "\n",
        "def old_speak_text(text, autoplay=True):\n",
        "    audio_data = generate_text(text)\n",
        "\n",
        "    # Generate a unique ID for this audio element\n",
        "    audio_id = f\"audio_{uuid.uuid4().hex}\"\n",
        "\n",
        "    # Display the audio with the unique ID\n",
        "    # display(Audio(audio_data, autoplay=autoplay, element_id=audio_id))\n",
        "    audio_html = f'''\n",
        "    <audio id=\"{audio_id}\" src=\"data:audio/mpeg;base64,{audio_b64}\" autoplay=\"{str(autoplay).lower()}\" style=\"display: none;\">\n",
        "    </audio>\n",
        "    '''\n",
        "    display(HTML(audio_html))\n",
        "\n",
        "    # Create a hidden div to store the audio status\n",
        "    status_div = f'<div id=\"{audio_id}_status\" style=\"display: none;\">playing</div>'\n",
        "    display(HTML(status_div))\n",
        "\n",
        "    # JavaScript to handle audio playback and status\n",
        "    js_code = f\"\"\"\n",
        "    var audioElement = document.getElementById('{audio_id}');\n",
        "    if (audioElement) {{\n",
        "        audioElement.onended = function() {{\n",
        "            document.getElementById('{audio_id}_status').textContent = 'finished';\n",
        "        }};\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the JavaScript\n",
        "    display(HTML(f\"<script>{js_code}</script>\"))\n",
        "\n",
        "    # Wait for the audio to finish\n",
        "    while True:\n",
        "        try:\n",
        "            status = output.eval_js(f\"document.getElementById('{audio_id}_status').textContent\")\n",
        "            if status == 'finished':\n",
        "                break\n",
        "        except:\n",
        "            time.sleep(0.1)\n",
        "        time.sleep(0.1)\n",
        "\n",
        "def record(seconds=3):\n",
        "    print(f\"ChatBot is listening....\")   # {seconds} seconds.\")\n",
        "    display(Javascript(RECORD))\n",
        "    try:\n",
        "        display(Javascript(RECORD))\n",
        "        s = output.eval_js('record(%d)' % (seconds * 1000))\n",
        "    except Exception as e:\n",
        "        print(f\"Recording error: {e}\")\n",
        "        return None\n",
        "\n",
        "    binary = b64decode(s.split(',')[1])\n",
        "\n",
        "    # Convert to AudioSegment\n",
        "    audio = AudioSegment.from_file(io.BytesIO(binary), format=\"webm\")\n",
        "\n",
        "    # Export as WAV\n",
        "    audio.export(\"recorded_audio.wav\", format=\"wav\")\n",
        "    print(\"ChatBot is processing data...\")\n",
        "    return \"recorded_audio.wav\"\n",
        "\n",
        "def transcribe_audio(filename):\n",
        "    with open(filename, \"rb\") as audio_file:\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file\n",
        "        )\n",
        "    return transcription.text\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=userdata.get('OPENAI_API_KEY'),\n",
        "    model=\"gpt-5-mini\",\n",
        "    temperature=0.3,\n",
        "    n=1\n",
        ")\n",
        "\n",
        "def old_start_chatbot():\n",
        "    \"\"\"Trigger function to start the ChatBot session\"\"\"\n",
        "    print(\"ChatBot is ready! How can I help you? (say 'bye' to exit).\")\n",
        "    response = None\n",
        "\n",
        "    while response != \"bye\":\n",
        "        try:\n",
        "            filename = record(5)\n",
        "            if filename is None:\n",
        "                break\n",
        "\n",
        "            transcription = transcribe_audio(filename)\n",
        "            print(f\"Human: {transcription}\")\n",
        "\n",
        "            # Simple chat interaction (remove ChatBot class for now):\n",
        "            ai_response = llm.invoke([HumanMessage(content=transcription)]).content\n",
        "            response = ai_response.strip().lower()  # Convert to lowercase for comparison\n",
        "            print(f\"AI: {response}\")\n",
        "            speak_text(response)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Chat ended by user\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            break\n",
        "\n",
        "    print(\"ChatBot session ended.\")\n",
        "\n",
        "def start_chatbot():\n",
        "    \"\"\"Trigger function to start the ChatBot session\"\"\"\n",
        "    print(\"ChatBot is ready! How can I help you? (say 'bye' to exit).\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            filename = record(5)\n",
        "            if filename is None:\n",
        "                break\n",
        "\n",
        "            transcription = transcribe_audio(filename)\n",
        "            print(f\"Human: {transcription}\")\n",
        "\n",
        "            # Exit condition based on human input\n",
        "            if \"bye\" in transcription.lower():\n",
        "                print(\"AI: Goodbye — take care!\")\n",
        "                speak_text(\"Goodbye — take care!\")\n",
        "                break\n",
        "\n",
        "            # Simple chat interaction:\n",
        "            ai_response = llm.invoke([HumanMessage(content=transcription)]).content\n",
        "            response = ai_response.strip().lower()\n",
        "            print(f\"AI: {response}\")\n",
        "            speak_text(response)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nChat ended by user\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            break\n",
        "\n",
        "    print(\"ChatBot session ended.\")"
      ],
      "metadata": {
        "id": "g7nXLWjLc7v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Conversation with Chatbot\n",
        "\n",
        "We now continue a conversation with our `Chatbot` until the user requests it to end.\n",
        "\n",
        "For Example 6, firt ask the LLM **\"What is the capital of Texas?\"** and wait for the LLM to stop processing your input. Then tell the LLM **\"bye\"** to end your conversation."
      ],
      "metadata": {
        "id": "J0Hn0XB3F3ZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Communicate with llm\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import sys\n",
        "\n",
        "# First, make sure you've run the modified ChatBot code cell above\n",
        "# (The one that contains start_chatbot() function)\n",
        "\n",
        "# Set up your OpenAI key and model\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "MODEL = 'gpt-5-mini'  # Using your specified model\n",
        "\n",
        "# Initialize the LLM with your API key\n",
        "llm = ChatOpenAI(\n",
        "    model=MODEL,\n",
        "    temperature=0.3,\n",
        "    openai_api_key=OPENAI_KEY\n",
        ")\n",
        "\n",
        "# To start the ChatBot conversation, simply call:\n",
        "start_chatbot()"
      ],
      "metadata": {
        "id": "NunKg4Kvwi8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6: Conversation with Chatbot**\n",
        "\n",
        "In the cell below write the code to start a new conversation with the `Chatbot`. Ask your `Chatbot` for **answers to 5 different questions** of your own choosing. After the 5th question has been answered, terminate your conversation by saying the word **\"bye\"**."
      ],
      "metadata": {
        "id": "L2Xo4bAbnJqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6 here\n",
        "\n"
      ],
      "metadata": {
        "id": "ErVz6rRpnJqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output you should see depends upon your 5 questions.\n"
      ],
      "metadata": {
        "id": "O_VGE0wanJqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 4: Text-to-Image**\n",
        "\n",
        "**Text-to-Image programs** are AI-powered tools that generate images based on textual descriptions. These systems use deep learning models, particularly **generative models** like **diffusion models** or **GANs (Generative Adversarial Networks)**, to interpret and visualize the content described in natural language.\n",
        "\n",
        "**How They Work**\n",
        "\n",
        "1. **Input**: A user provides a text prompt (e.g., \"a futuristic city at sunset\").\n",
        "2. **Processing**: The model analyzes the prompt using natural language understanding and maps it to visual concepts.\n",
        "3. **Generation**: The model synthesizes an image that matches the description, often refining it through multiple steps (as in diffusion models).\n",
        "\n",
        "**Popular Examples**\n",
        "- **DALL·E** (by OpenAI)\n",
        "- **Midjourney**\n",
        "- **Stable Diffusion**\n",
        "- **Adobe Firefly**\n",
        "\n",
        "**Applications**\n",
        "\n",
        "- **Art and Design**: Creating concept art, illustrations, and visual assets.\n",
        "- **Education**: Visualizing historical scenes or scientific concepts.\n",
        "- **Marketing**: Generating visuals for campaigns and branding.\n",
        "- **Entertainment**: Storyboarding and character design.\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "- May misinterpret vague or complex prompts.\n",
        "- Can reflect biases present in training data.\n",
        "- Image quality varies depending on model and prompt specificity.\n",
        "\n"
      ],
      "metadata": {
        "id": "9YeDB-lKTIdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7: Text-to-Image\n",
        "\n",
        "The code in the next cell uses the `dall-e-3` Text-to-Image program to generate a picture of a Welsh Corgi Pembroke puppy using the following prompt\n",
        "\n",
        "```text\n",
        "# Define your image prompt\n",
        "PROMPT=\"a Welsh Corgi Pembroke puppy\"\n",
        "TITLE=\"Welsh Corgi Puppy\"\n",
        "```"
      ],
      "metadata": {
        "id": "zOI7oqfSW0RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: Text-to-image\n",
        "\n",
        "import openai\n",
        "import os\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define your image prompt\n",
        "PROMPT=\"a Welsh Corgi Pembroke puppy\"\n",
        "TITLE=\"Welsh Corgi Puppy\"\n",
        "\n",
        "# Get the secret from the environment\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# Generate a single image\n",
        "response = client.images.generate(\n",
        "    model=\"dall-e-3\",\n",
        "    prompt=PROMPT,\n",
        "    size=\"1024x1024\",\n",
        "    quality=\"standard\",\n",
        "    n=1\n",
        ")\n",
        "\n",
        "# Get the image URL\n",
        "image_url = response.data[0].url\n",
        "\n",
        "# Fetch and display the image\n",
        "img_response = requests.get(image_url)\n",
        "img = Image.open(BytesIO(img_response.content))\n",
        "\n",
        "# Save the image\n",
        "img.save(\"dalle3_image.jpg\", \"JPEG\")\n",
        "\n",
        "# Show the image\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title(TITLE)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DCElZ0fLV-of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image20A.png)\n",
        "\n",
        "There is a degree of randomization in Text-to-Image programs so that each image generated is different."
      ],
      "metadata": {
        "id": "r4uLcEaDqmmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7: Text-to-Image**\n",
        "\n",
        "In the next cell write the code to use the `dall-e-3` Text-to-Image program to generate a picture. You are free to generate any picture that you want. Don't forget to change the **image title** to match your subject.\n",
        "\n",
        "### **NOTICE**\n",
        "\n",
        "Test-to-Image programs have restrictions on the kinds of images that can be generate. If you try to generate an pornographic or another censored image type you will receive the following error message:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image21A.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "cBlM6f0fqvAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 7 here\n",
        "\n"
      ],
      "metadata": {
        "id": "vuo_boXrqvAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output you see will depend upon your prompt. Make sure that the image title matches the subject matter of your image."
      ],
      "metadata": {
        "id": "UduXbTt7qvAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why DALL·E 3 Generates Different Images from the Same Prompt\n",
        "\n",
        "**DALL·E 3**, like other generative AI models, can produce **varied outputs** even when the input prompt is identical. This behavior is intentional and rooted in how the model is designed.\n",
        "\n",
        "### **Key Reasons for Variation**\n",
        "\n",
        "#### 1. **Stochastic Sampling**\n",
        "- DALL·E 3 uses **randomness** during the image generation process.\n",
        "- Even with the same prompt, the model samples from a distribution of possible outputs, leading to different results.\n",
        "\n",
        "#### 2. **Latent Space Diversity**\n",
        "- The model operates in a **latent space** where many visual interpretations of a prompt can exist.\n",
        "- For example, \"a cat on a windowsill\" could vary in breed, lighting, style, background, and pose.\n",
        "\n",
        "#### 3. **Prompt Interpretation**\n",
        "- Natural language is inherently **ambiguous**.\n",
        "- The model may emphasize different aspects of the prompt each time (e.g., focusing more on \"windowsill\" vs. \"cat\").\n",
        "\n",
        "#### 4. **Model Temperature Settings**\n",
        "- Some platforms allow adjusting the **temperature** (a parameter controlling randomness).\n",
        "- Higher temperature = more creative and varied outputs.\n",
        "\n",
        "#### 5. **Fine-Tuning and Updates**\n",
        "- If the model has been updated or fine-tuned, even subtle changes can affect output consistency.\n",
        "\n",
        "### Can You Get Consistent Results?\n",
        "\n",
        "Yes, but with limitations:\n",
        "- Use **prompt engineering** to be extremely specific.\n",
        "- Some platforms allow setting a **seed value** to control randomness (though DALL·E 3 may not expose this directly).\n",
        "\n"
      ],
      "metadata": {
        "id": "zHe_eX1PuUFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 8: Create Multiple Images\n",
        "\n",
        "To illustrate the degree of variation between images generated by exactly the same prompt, the code in the cell below generates 3 images using the same prompt that was used in Example 7."
      ],
      "metadata": {
        "id": "3VHigre3tQbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 8: Create multiple images\n",
        "\n",
        "import openai\n",
        "import os\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define your image prompt and title\n",
        "PROMPT = PROMPT=\"a Welsh Corgi Pembroke puppy\"\n",
        "TITLE = \"Welsh Corgi Puppy\"\n",
        "\n",
        "# Get the secret from the environment\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# Generate and save 3 images\n",
        "for i in range(3):\n",
        "    response = client.images.generate(\n",
        "        model=\"dall-e-3\",\n",
        "        prompt=PROMPT,\n",
        "        size=\"1024x1024\",\n",
        "        quality=\"standard\",\n",
        "        n=1\n",
        "    )\n",
        "\n",
        "    image_url = response.data[0].url\n",
        "    img_response = requests.get(image_url)\n",
        "    img = Image.open(BytesIO(img_response.content))\n",
        "    img.save(f\"dalle3_image_{i+1}.jpg\", \"JPEG\")\n",
        "\n",
        "# Display images side-by-side\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "for i, ax in enumerate(axes):\n",
        "    img = Image.open(f\"dalle3_image_{i+1}.jpg\")\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"{TITLE} #{i+1}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BJfQeCnncolh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image22A.png)"
      ],
      "metadata": {
        "id": "1p9gvKAudem2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 8: Create Multiple Images**\n",
        "\n",
        "In the cell below write the code to create three images using the same prompt that you used above in **Exercise 8**."
      ],
      "metadata": {
        "id": "u8_Ty2nTvvbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 8 here\n",
        "\n"
      ],
      "metadata": {
        "id": "ChIKWpHGvvbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output you see will depend upon your prompt. Make sure that the image title matches the subject matter of your image."
      ],
      "metadata": {
        "id": "W1T9Fu2RvvbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-in**\n",
        "When you have completed and run all of the code cells, use the `File --> Print.. --> Microsoft Print to PDF` if you are running either Windows 10 or 11 to generate a PDF of your Colab notebook. If you have a Mac, use the `File --> Print.. --> Save as PDF`\n",
        "\n",
        "In either case, save your PDF as Copy of Class_04_2.lastname.pdf where lastname is your last name, and upload the file to Canvas.\n",
        "\n",
        "**NOTE TO WINDOWS USERS:** You grade will be **reduced by 10% if your PDF is missing pages** when being graded in Canvas and the grader has take the additional steps to download your PDF, print it out using Microsoft Print to PDF and then resubmit to Canvas for grading."
      ],
      "metadata": {
        "id": "W_DEyDQ5w06v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lizard Tail**\n",
        "\n",
        "\n",
        "##**Attention Is All You Need**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/8/8f/The-Transformer-model-architecture.png)\n",
        "\n",
        "**\"Attention Is All You Need\"** is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique's potential for other tasks like question answering and what is now known as multimodal Generative AI.\n",
        "\n",
        "The paper's title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper's authors, liked the sound of that word.\n",
        "\n",
        "An early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\n",
        "\n",
        "Some early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\n",
        "\n",
        "As of 2024, the paper has been cited more than 140,000 times.\n",
        "\n",
        "**Authors**\n",
        "\n",
        "The authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" to the paper; the listed order was randomized. The Wired article highlights the group's diversity:\n",
        "\n",
        "Six of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.\n",
        "\n",
        "**Methods Discussed & Introduced**\n",
        "\n",
        "The paper is most well known for the introduction of the Transformer architecture, which forms the underlying architecture for most forms of modern Large Language Models (LLMs). A key reason for why the architecture is preferred by most modern LLMs is the parallelizability of the architecture over its predecessors. This ensures that the operations necessary for training can be accelerated on a GPU allowing both faster training times and models of bigger sizes to be trained.\n",
        "\n",
        "The following mechanisms were introduced by the paper as part of the development of the transformer architecture.\n",
        "\n",
        "**Scaled dot-product Attention & Self-attention**\n",
        "\n",
        "The use of the scaled dot-product attention and self-attention mechanism instead of an RNN or LSTM (which rely on recurrence instead) allow for better performance as described in the following paragraph.\n",
        "\n",
        "Since the model relies on Query (Q), Key (K) and Value (V) matrices that come from the same source itself (i.e. the input sequence / context window), this eliminates the need for RNNs completely ensuring parallelizability for the architecture. This differs from the original form of the Attention mechanism introduced in 2014. Additionally, the paper also discusses the use of an additional scaling factor that was found to be most effective with respect to the dimension of the key vectors.\n",
        "\n",
        "In the specific context of translation which the paper focused on, the Query and Key matrices are usually represented in embeddings corresponding to the source language while the Value matrix corresponds to the target language.\n",
        "\n",
        "**Multi-head Attention**\n",
        "\n",
        "In the self-attention mechanism, queries (Q), keys (K), and values (V) are dynamically generated for each input sequence (limited typically by the size of the context window), allowing the model to focus on different parts of the input sequence at different steps. Multi-head attention enhances this process by introducing multiple parallel attention heads. Each attention head learns different linear projections of the Q, K, and V matrices. This allows the model to capture different aspects of the relationships between words in the sequence simultaneously, rather than focusing on a single aspect.\n",
        "\n",
        "By doing this, multi-head attention ensures that the input embeddings are updated from a more varied and diverse set of perspectives. After the attention outputs from all heads are calculated, they are concatenated and passed through a final linear transformation to generate the output.\n",
        "\n",
        "**Positional Encoding**\n",
        "\n",
        "Since the `Transformer model` is not a `seq2seq model` and does not rely on the sequence of the text in order to perform encoding and decoding, the paper relied on the use of sine and cosine wave functions to encode the position of the token into the embedding.\n",
        "\n",
        "**Historical context**\n",
        "\n",
        "For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\n",
        "\n",
        "A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers. However, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.\n",
        "\n",
        "Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.\n",
        "\n",
        "#### **Transformer Architecture**\n",
        "\n",
        "In the Transformer architecture, the self-attention mechanism processes an input sequence by creating a new representation for each token, enriched with context from all other tokens in the sequence. Unlike older recurrent neural networks (RNNs) that process words one by one, self-attention processes the entire sequence in parallel, making it highly efficient.\n",
        "The core idea is for each token to \"look\" at all other tokens to determine their relevance and then use that information to create a more informed, context-aware representation of itself.\n",
        "\n",
        "**The self-attention process**\n",
        "\n",
        "For a given input sequence, such as \"The animal didn't cross the street because it was too tired,\" the self-attention process happens in the following stages:\n",
        "\n",
        "1. **Create Query, Key, and Value vectors:** For every token in the sequence (e.g., \"it\"), the model creates three distinct vectors:\n",
        "* * **Query (Q):** Represents the current token, acting like a question used to find related tokens.\n",
        "* * **Key (K):** Represents the token being looked at, acting like a label for its information.\n",
        "* **Value (V):** Contains the content or contextual information of the token.\n",
        "\n",
        "2. **Calculate attention scores:** To determine how much focus \"it\" should place on other words, the model calculates a score for every other token in the sentence. This is done by taking the dot product of the current token's query vector with each of the other tokens' key vectors. A high dot-product score indicates a strong relationship between the two tokens.\n",
        "\n",
        "3. **Scale the scores:** The scores are scaled by dividing them by the square root of the key vector's dimension. This prevents the scores from growing too large, which helps to stabilize training.\n",
        "\n",
        "* **Normalize with Softmax:** The scaled scores are passed through a softmax function, which converts them into a probability distribution. This ensures that all the attention weights sum up to 1, making them easier to interpret.\n",
        "\n",
        "5. **Compute the weighted sum:** Each token's value vector is multiplied by its corresponding softmax score. The weighted value vectors are then summed to produce a new, context-rich output vector for the original token. In the sentence example, this process would give the word \"it\" a new representation that incorporates information from \"animal,\" correctly linking the two words.\n",
        "\n",
        "#### **Enhancing self-attention with multi-head attention**\n",
        "\n",
        "The Transformer architecture takes this mechanism one step further by using multi-head **attention**.\n",
        "\n",
        "* Instead of a single attention calculation, multi-head attention performs several self-attention calculations in parallel using different learned sets of Q, K, and V weight matrices.\n",
        "* Each \"head\" learns to focus on different types of relationships. For example, one head might attend to grammatical connections, while another might focus on semantic meaning.\n",
        "* The results from each head are then concatenated and passed through a final linear layer to produce the refined output. This gives the model a much richer, multi-contextual understanding of the input.\n",
        "\n",
        "#### **Preserving word order with positional encoding**\n",
        "\n",
        "Because the self-attention mechanism processes all tokens in parallel, it inherently loses information about word order. To address this, the Transformer injects positional information into the input embeddings using positional encoding. This is typically done with sinusoidal functions that create a unique vector for each position in the sequence, which is then added to the token's embedding. This process allows the model to capture the sequence's structure without sacrificing parallel processing efficiency."
      ],
      "metadata": {
        "id": "H3ykBQufwypg"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}