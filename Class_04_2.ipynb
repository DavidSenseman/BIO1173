{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqDIpQLjdgBT"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PlD6wjRdgBT"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYIHucyqdgBU"
      },
      "source": [
        "##### **Module 4: Training for Tabular Data**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Encoding a Feature Vector for Keras Deep Learning\n",
        "* **Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC**\n",
        "* Part 4.3: Keras Regression for Deep Neural Networks with RMSE\n",
        "* Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Neural Network Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4EKp4pkdgBU"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to ```/content/drive``` and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izIVk7n6dgBU"
      },
      "source": [
        "## **Datasets for Class_04_2**\n",
        "\n",
        "For Class_04_2 we will be using the Pima Indians Diabetes dataset for the Examples, and the Wisconsin Breast Cancer dataset for the **Exercises**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZf61pI_dgBU"
      },
      "source": [
        "### **Pima Indians Diabetes Dataset**\n",
        "\n",
        "[Pima Indians Diabetes dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/Pima.jpg)\n",
        "\n",
        "The **_Pima Indians_** are known for their high prevalence of Type 2 diabetes, making them an important population for studying and understanding the genetic and environmental factors that contribute to the development of diabetes. They have one of the highest rates of diabetes in the world, with some estimates suggesting that up to 50% of Pima adults have the disease.\n",
        "\n",
        "Studying the Pima Indians has provided valuable insights into the genetic and lifestyle factors that contribute to the development of diabetes, as well as potential strategies for prevention and treatment. Researchers have identified specific genetic variants associated with diabetes in the Pima population, as well as lifestyle factors such as diet and exercise habits that may influence the risk of developing the disease.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/PimaParadox.png)\n",
        "\n",
        "[THE PIMA PARADOX](https://www.newyorker.com/magazine/1998/02/02/the-pima-paradox)\n",
        "\n",
        "The article by Malcom Gladwell, \"THE PIMA PARADOX\", that appeared in the January 25, 1998 edition of the New Yorker Magazine, provides a highly readable introduction to this topic.\n",
        "\n",
        "**Pima Indians Diabetes Dataset**\n",
        "\n",
        "The datasets consist of several medical predictor (independent) variables and one target (dependent) variable, `Outcome`. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
        "\n",
        "Predictor (Independent) Variables:\n",
        "* **Pregnancies:** Number of times pregnant\n",
        "* **Glucose:** Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "* **BloodPressure:** Diastolic blood pressure (mm Hg)\n",
        "* **SkinThickness:** Triceps skin fold thickness (mm)\n",
        "* **Insulin:** 2-Hour serum insulin (mu U/ml)\n",
        "* **BMI:** Body mass index (weight in kg/(height in m)<sup>2</sup>)\n",
        "* **DiabetesPedigree:** Diabetes pedigree function\n",
        "* **Age:** Age (years)\n",
        "\n",
        "Target (Dependent) Variable\n",
        "* **Outcome:** Class variable (0 or 1) 268 of 768 are 1, the others are 0\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "Bc2rwAAMdgBV"
      },
      "source": [
        "### **Breast Cancer Wisconsin (Diagnostic) Data Set**\n",
        "\n",
        "[Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data)\n",
        "\n",
        "This dataset was described in the previous lesson (Class_04_1).\n",
        "\n",
        "The list of features computed from digitized images of breast mass cell nuclei obtained from by FNA in the Breast Cancer Wisconsing datasete are as follows:\n",
        "\n",
        "**Attribute Information:**\n",
        "\n",
        "* **ID number**\n",
        "* **Diagnosis:** (M = malignant, B = benign)\n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus:\n",
        "\n",
        "*  **radius:** (mean of distances from center to points on the perimeter)\n",
        "* **texture:** (standard deviation of gray-scale values)\n",
        "* **perimeter:**\n",
        "* **area:**\n",
        "* **smoothness:** (local variation in radius lengths)\n",
        "* **compactness:** (perimeter<sup>2</sup> / area - 1.0)\n",
        "* **concavity:** (severity of concave portions of the contour)\n",
        "* **concave points:** (number of concave portions of the contour)\n",
        "* **symmetry:**\n",
        "* **fractal dimension:** (\"coastline approximation\" - 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis of Tabular Data with Neural Networks**\n",
        "\n",
        "The output of modern neural networks can be of many different forms. However, classically, neural network output has typically been one of the following:\n",
        "\n",
        "* **Binary Classification** - Classification between two possibilities (positive and negative). Common in medical testing, does the person has the disease (positive) or not (negative).\n",
        "* **Classification** - Classification between more than 2 possibilites, for example the Iris dataset (3-way classification).\n",
        "* **Regression** - Numeric prediction.  How many MPG does a car get? (covered in next video)\n",
        "\n",
        "In this lesson we will look at two types of classification: (1) Binary Classification and (2) Multiclass Classification. In the next lesson we will look at Regression.\n"
      ],
      "metadata": {
        "id": "8o53ujbLdBMU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF1dep09K1AL"
      },
      "source": [
        "# **Binary Classification**\n",
        "\n",
        "**Binary classification** occurs when a neural network must choose between two options: true/false, yes/no, correct/incorrect, or healthy/diseased. To see how to use binary classification, we will consider a classification system for quantitative cytological measurements of cells extracted from female breast tumors. This system will to classirisY if a breast tumor is cancerous (malignant) or not (benign). This classification system must decide how to respond to a new patient presenting with a breast tumor.  \n",
        "\n",
        "When you have only two classes that you can consider, the objective function's score is the number of false-positive predictions versus the number of false negatives. False negatives and false positives are both types of errors, and it is essential to understand the difference. For breast cancer data example, diagnosing a breast tumor as cancerous (malignant) would be positive. A false positive occurs when the model decides the tumor is cancerous when in fact it is benign. A false negative happens when the model decides the tumor is benign when in fact it is cancerous.\n",
        "\n",
        "Because only two options exist, we can choose the mistake that is the more serious type of error, a false positive or a false negative. This depends entirely on the situation.\n",
        "\n",
        "#### **Breast Cancer Diagnosis**\n",
        "\n",
        "In the context of diagnosing breast cancer tumors, a false positive result is generally considered worse than a false negative result in the short-term, but not in the long-term.\n",
        "\n",
        "* **False positive result:** means that a patient is incorrectly told they have cancer when they do not. This can lead to unnecessary anxiety, invasive follow-up tests, and potentially harmful treatments such as surgery, chemotherapy, and radiation therapy.\n",
        "\n",
        "* **False negative result:** means that a patient is incorrectly told they do not have cancer when they actually do. This may delay necessary treatment and allow the cancer to progress without intervention, potentially leading to poorer outcomes in the long run.\n",
        "\n",
        "\n",
        "#### **COVID-19 Diagnosis**\n",
        "\n",
        "On the other hand, consider the situation where a model predicts whether a subject has COVID-19. In the context of diagnosing COVID-19, a false negative result is generally considered worse than a false positive result.\n",
        "\n",
        "* **False negative result:** means that a person is incorrectly told they do not have COVID-19 when they actually do. This can lead to the person unknowingly spreading the virus to others, potentially causing further infections and contributing to community transmission.\n",
        "\n",
        "* **False positive result:** means that a person is incorrectly told they have COVID-19 when they do not. While this can lead to unnecessary isolation and anxiety for the individual, it typically does not have as severe consequences for public health as a false negative.\n",
        "\n",
        "While it is not always clear which type of error is more important, it is obvious that having a solid understanding of how neural network models work, and how to assess their predictive accuracy is of paramount importance for their appropriate deployment in a clinical setting and how to explain a model's predictions to patients and their family members."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qorDQRU-dgBV"
      },
      "source": [
        "### Example 1: Read Datafile and Create DataFrame\n",
        "\n",
        "For the intial examples in this lesson, we will be using the \"Pima\" dataset that contains clinical information for 768 women of the Pima Indian nation.\n",
        "\n",
        "The data file, `pima.csv` is located on the course HTTPS server, located in the following filepath: `/BIO1173/data/`. The code below shows how to read this data file and store the information in a new DataFrame called `pidDF` (Pima Indian Diabetes DataFrame).   \n",
        "\n",
        "_Code Description:_\n",
        "\n",
        "Here is the code chunk that reads the datafile:\n",
        "\n",
        "~~~text\n",
        "# Read file and create DataFrame\n",
        "pidDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/pima.csv\",\n",
        "#    index_col=0,\n",
        "    sep=',',\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "~~~\n",
        "\n",
        "Please note that one of the lines of code has been \"commented out\":\n",
        "\n",
        "~~~text\n",
        "#    index_col=0,\n",
        "~~~\n",
        "\n",
        "Sometimes you need this line of code commented, other times, you need to remove the comment.\n",
        "\n",
        "Removing the comment changes which column in the datafile Pandas should consider to be the first column. For this datafile, you need to keep this argument commented out.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmjrLkDwK1AM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Example 1: Read datafile and create DataFrame\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read file and create DataFrame\n",
        "pidDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/pima.csv\",\n",
        "#    index_col=0,\n",
        "    sep=',',\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', 6)\n",
        "pd.set_option('display.max_rows', 6)\n",
        "\n",
        "# Display DataFrame\n",
        "display(pidDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW39as6RdgBV"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04_2_Exm1A.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnOTGw0sdgBV"
      },
      "source": [
        "### **Exercise 1: Read Datafile and Create DataFrame**\n",
        "\n",
        "In the cell below, read the datafile `bcwbreast.csv` containing the _Breast Cancer Wisconsin_ dataset that is located on the course HTTPS server. Create a new DataFrame called `bcwDF` to hold the data.  \n",
        "\n",
        "\n",
        "_Code Hints:_\n",
        "\n",
        "To read this file correctly, you will need to remove the `#` at the start of the\n",
        "of the line of code that reads: `index_col=0`. If you don't uncomment this line, your DataFrame `bcwDF` will have the wrong column names.\n",
        "\n",
        "----------------------------\n",
        "\n",
        "The Wisconsin Breast Cancer dataset provides data for 569 patients on 30 features of the cell nuclei obtained from a digitized image of a fine needle aspirate (FNA) of a breast mass. For each patient the cancer was diagnosed as malignant (`M`) or benign (`B`) in the column labeled `diagnosis`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Z3mWpet8dgBV"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVh2DZy0dgBW"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image03.png)\n",
        "\n",
        "If your output looks exactly like this, you are good to go. However, if your output looks like the one shown below, you have a big problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OTF_Kt7C_qk"
      },
      "source": [
        "**WARNING:** If your output looks like this:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image09.png)\n",
        "\n",
        "It means that you did not read the instructions for this exercise carefully. You must fix your error before preceding. Otherwise your code will not work correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic_JG5KpK1AN"
      },
      "source": [
        "## ROC Curves\n",
        "\n",
        "\n",
        "An ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a classification model at different threshold settings. It plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) for different threshold values. The curve allows for comparison of different models and helps in determining the optimal threshold for a given model. A higher area under the ROC curve indicates better performance of the model.\n",
        "\n",
        "Understand how to interpret an ROC curve can be extremely important in medical testing where the result is often **_binary_**, (1) the patient has the disease, (2) the patient does _not_ have the disease. This diagnosis can lead to two types of errors, known as false positives and false negatives:\n",
        "\n",
        "* **False Positive** - Your test (neural network) indicated that the patient had the disease; however, the patient did not.\n",
        "* **False Negative** - Your test (neural network) indicated that the patient did not have the disease; however, the patient did have the disease.\n",
        "\n",
        "And there are two ways that your diagnosis can be correct:\n",
        "\n",
        "* **True Positive** - Your test (neural network) correctly identified that the patient had the disease.\n",
        "* **True Negative** - Your test (neural network) correctly identified that the patient did not have the disease.\n",
        "\n",
        "**Type of Error**\n",
        "![__](https://biologicslab.co/BIO1173/images/class_4_errors.png)\n",
        "\n",
        "Neural networks classirisY in terms of the probability of it being positive. However, at what possibility do you give a positive result? Is the cutoff 50%? 90%? Where you set this cutoff is called the **_threshold_**. Anything above the cutoff is positive; anything below is negative. Setting this cutoff allows the model to be more sensitive or specific.\n",
        "\n",
        "* **Sensitivity** of a test refers to its ability to correctly identirisY individuals who have the condition or disease (true positive rate). A highly sensitive test will correctly identirisY all or most of the individuals with the condition, resulting in few false negative results.\n",
        "\n",
        "* **Specificity** of a test, on the other hand, refers to its ability to correctly identirisY individuals who do not have the condition or disease (true negative rate). A highly specific test will correctly identirisY all or most of the individuals without the condition, resulting in few false positive results.\n",
        "\n",
        "The key point is this:\n",
        "\n",
        "* **_Sensitivity_** focuses on the ability of a test to correctly identirisY individuals with the condition\n",
        "* **_Specificity_** focuses on the ability to correctly identirisY individuals without the condition.\n",
        "\n",
        "More info on Sensitivity vs. Specificity: [Khan Academy](https://www.youtube.com/watch?v=Z5TtopYX1Gc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncOBRMogdgBW"
      },
      "source": [
        "### XY plot illustrating Sensitivity vs. Specificity\n",
        "\n",
        "The code in the cell below generates an XY plot illustrating the inherent trade-off between test sensitivity and test specificity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QfjMVDf2K1AO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "3b83f729-9a84-4a75-bc39-a78f7ae213fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Generate XY Plot\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import math\n",
        "\n",
        "mu1 = -2\n",
        "mu2 = 2\n",
        "variance = 1\n",
        "sigma = math.sqrt(variance)\n",
        "x1 = np.linspace(mu1 - 5*sigma, mu1 + 4*sigma, 100)\n",
        "x2 = np.linspace(mu2 - 5*sigma, mu2 + 4*sigma, 100)\n",
        "plt.plot(x1, stats.norm.pdf(x1, mu1, sigma)/1,color=\"green\",\n",
        "         linestyle='dashed')\n",
        "plt.plot(x2, stats.norm.pdf(x2, mu2, sigma)/1,color=\"red\")\n",
        "plt.axvline(x=-2,color=\"black\")\n",
        "plt.axvline(x=0,color=\"black\")\n",
        "plt.axvline(x=+2,color=\"black\")\n",
        "plt.text(-2.7,0.55,\"Sensitive\")\n",
        "plt.text(-0.7,0.55,\"Balanced\")\n",
        "plt.text(1.7,0.55,\"Specific\")\n",
        "plt.ylim([0,0.53])\n",
        "plt.xlim([-5,5])\n",
        "plt.legend(['Negative','Positive'])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following plot:\n",
        "\n",
        "![____](https://biologicslab.co/BIO1173/images/class_04/Class_04_2_image01.png)\n"
      ],
      "metadata": {
        "id": "YV8e3DUGiAuX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQxAmjVSdgBW"
      },
      "source": [
        "**Why not simply make a great test that is both highly sensitve and highly specific?**\n",
        "\n",
        "A test _cannot_ be both highly sensitive and highly specific simultaneously due to inherent trade-offs in test characteristics. In general, increasing sensitivity tends to decrease specificity, and vice versa. This is because the parameters that contribute to sensitivity (true positive rate) and specificity (true negative rate) are often inversely related.\n",
        "\n",
        "* **Sensitivity** focuses on minimizing false negatives, which means it aims to correctly identirisY all individuals with the condition. To achieve high sensitivity, the test may be designed to detect even low levels of the condition, making it more likely to generate false positives (lower specificity).\n",
        "\n",
        "* **Specificity** aims to minimize false positives by correctly identirisYing individuals without the condition. To achieve high specificity, the test may be designed to only detect the presence of the condition with a high degree of certainty, potentially leading to missed detections of milder cases (lower sensitivity).\n",
        "\n",
        "Therefore, to maximize sensitivity, a test may detect even slight signals of the condition, increasing the likelihood of false positives and reducing specificity. Conversely, a test designed for high specificity may require stronger, more specific indicators of the condition, potentially missing milder cases and reducing sensitivity. Achieving a balance between sensitivity and specificity is crucial in developing effective diagnostic tests.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5tUnrYLdgBW"
      },
      "source": [
        "### Example 2: Preprocess Data for Neural Network Training\n",
        "\n",
        "As usual, we will have to pre-process the Pima data in the DataFrame `pidDF` before it can be used to train a neural network. All of the data in `pidDF` is numeric, so it won't be necessary to One-Hot encode any categorical variables (strings).\n",
        "\n",
        "However, there are rather large differences in magnitude between different categories. For example, the average (mean) `Glucose` value = 120.894531, while the mean for the category `DiabetesPedigree` is only 0.471876.\n",
        "\n",
        "Its not that neural networks can't work with large and small numerical values at the same time, it's just that they **train better** when all of the data is similar in size. This can be achieve using data **_standarization_**. In the cell below we use a very common technique for standarization, converting each number to it's Z-score value.\n",
        "\n",
        "We beging by making a list of columns that we want to include in our standarization. In this example, we want to use all of the columns, except the column `Outcome`. Here is the code chunk to do this:\n",
        "~~~text\n",
        "# Generate column list for preprocessing\n",
        "pidX_columns = pidDF.columns.drop('Outcome')\n",
        "~~~\n",
        "\n",
        "Next we use the `for` loop to \"loop through\" each column in the column list, and convert the numerical values in that column to its z-score equivalent:\n",
        "\n",
        "~~~text\n",
        "# Replace values with their Z-scores\n",
        "for col in pidX_columns:\n",
        "    pidDF[col] = zscore(pidDF[col])\n",
        "~~~\n",
        "In this dataset, that's all the preprocessing that needs to be done. We can then generate our X feature vector and our Y feature vector in the usual way (e.g. see Class_04_1).\n",
        "\n",
        "Finally, we print out the first 4 values of our X and Y feature vectors to make sure they are in the correct format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m55_Ygs9K1AP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Example 2: Pre-process data\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "\n",
        "\n",
        "# Generate column list for preprocessing\n",
        "pidX_columns = pidDF.columns.drop('Outcome')\n",
        "\n",
        "# Replace values with their Z-scores\n",
        "for col in pidX_columns:\n",
        "    pidDF[col] = zscore(pidDF[col])\n",
        "\n",
        "# Generate X feature vector\n",
        "pidX = pidDF[pidX_columns].values\n",
        "pidX = np.asarray(pidX).astype(np.float32)\n",
        "\n",
        "# Generate Y as Numpy array\n",
        "pidY = pidDF['Outcome']\n",
        "pidY = np.asarray(pidY).astype(np.float32)\n",
        "\n",
        "# Print out X and Y\n",
        "np.set_printoptions(suppress=True,precision=4)\n",
        "print(\"The first 4 X-values are:\")\n",
        "print(pidX[0:4])\n",
        "print(\"\\nTheir corresponding Y-values are:\")\n",
        "print(pidY[0:4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvOF8olIdgBW"
      },
      "source": [
        "If your code is correct you should see the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFhihV4vdgBW"
      },
      "source": [
        "~~~text\n",
        "The first 4 X-values are:\n",
        "[[ 0.6399  0.8483  0.1496  0.9073 -0.6929  0.204   0.4685  1.426 ]\n",
        " [-0.8449 -1.1234 -0.1605  0.5309 -0.6929 -0.6844 -0.3651 -0.1907]\n",
        " [ 1.2339  1.9437 -0.2639 -1.2882 -0.6929 -1.1033  0.6044 -0.1056]\n",
        " [-0.8449 -0.9982 -0.1605  0.1545  0.1233 -0.494  -0.9208 -1.0415]]\n",
        "\n",
        "Their corresponding Y-values are:\n",
        "[1. 0. 1. 0.]\n",
        "~~~\n",
        "\n",
        "By inspection, you can see the X-values have been converted in their Z-score. Notice that z-scores are small values, near `0`, that can be both positive or negative. A z-score that is positive, it means that its original value was greater than column's average (mean). A z-score that is negative means that its original value was less than column's average (mean). If by chance, a you find a z-score = 0.00000, that means the original value just happen to exactly the same value as the mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICz77SbCdgBX"
      },
      "source": [
        "---------------------------------\n",
        "\n",
        "## **Z-scores**\n",
        "\n",
        "Z-scores are a method of standardizing values by expressing them in terms of standard deviations from the mean. To calculate a z-score, you subtract the mean of the dataset from a specific value and then divide by the standard deviation. This normalization process allows for comparison of values across different datasets and variables, as it puts everything on a common scale.\n",
        "\n",
        "Z-scores are important because they allow for the transformation of raw data into a standardized form, making it easier to interpret and analyze. They can help identirisY outliers in a dataset, determine the relative position of a data point within a distribution, and assess the significance of a particular value compared to the overall dataset. Z-scores are commonly used in various fields such as statistics, finance, and psychology to compare and evaluate data in a meaningful way.\n",
        "\n",
        "---------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3bM6wFidgBX"
      },
      "source": [
        "### **Exercise 2: Preprocess Data for Neural Network Training**\n",
        "\n",
        "In the cell below you are write the code to preprocess the Breast Cancer Wisconsin data in the DataFrame `bcwDF`. Start by generating a list of columns to be used as X, `bcwX_columns`. To do this, begin by dropping the two columns using this line of code:\n",
        "\n",
        "~~~text\n",
        "bcwX_columns = bcwDF.columns.drop('diagnosis').drop('id')`\n",
        "~~~\n",
        "\n",
        "The column `id` should be dropped since it doesn't provide any useful information for making predictions about breast cancer while the column `diagnosis` needs to be dropped since it is the target column (Y).\n",
        "\n",
        "As shown in Example 2, replace the X values in `bcwX_columns` with their z-scores and then generate a Numpy array called `bcwX`.\n",
        "\n",
        "Unlike the Pima data preprocessed in Example 2, the target column `diagnosis` contains the categorical values 'M' for malignant and 'B' for benign. You will need to map the letter 'M' to the integer `1` and the letter `B` to the integer `0`.  \n",
        "\n",
        "You can use the following line of code to combine both the mapping and the creation of the Y feature vector with a single line of code:\n",
        "~~~text\n",
        "# Generate Y feature vector\n",
        "bcwY = bcwDF['diagnosis'].map({'M':1,\"B\":0}).values`\n",
        "~~~\n",
        "\n",
        "Finally, print out the first 4 values in your X and Y feature vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP6gd_DzdgBX"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo4owJu5dgBX"
      },
      "source": [
        "If your code is correct you should see the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-kVfUXgdgBX"
      },
      "source": [
        "~~~text\n",
        "The first 4 X-values are:\n",
        "[[ 1.0971 -2.0733  1.2699  0.9844  1.5685  3.2835  2.6529  2.5325  2.2175\n",
        "   2.2557  2.4897 -0.5653  2.833   2.4876 -0.214   1.3169  0.724   0.6608\n",
        "   1.1488  0.9071  1.8867 -1.3593  2.3036  2.0012  1.3077  2.6167  2.1095\n",
        "   2.2961  2.7506  1.937 ]\n",
        " [ 1.8298 -0.3536  1.686   1.9087 -0.827  -0.4871 -0.0238  0.5481  0.0014\n",
        "  -0.8687  0.4993 -0.8762  0.2633  0.7424 -0.6054 -0.6929 -0.4408  0.2602\n",
        "  -0.8055 -0.0994  1.8059 -0.3692  1.5351  1.8905 -0.3756 -0.4304 -0.1467\n",
        "   1.0871 -0.2439  0.2812]\n",
        " [ 1.5799  0.4562  1.5665  1.5589  0.9422  1.0529  1.3635  2.0372  0.9397\n",
        "  -0.398   1.2287 -0.7801  0.8509  1.1813 -0.297   0.815   0.2131  1.4248\n",
        "   0.237   0.2936  1.5119 -0.024   1.3475  1.4563  0.5274  1.0829  0.855\n",
        "   1.955   1.1523  0.2014]\n",
        " [-0.7689  0.2537 -0.5927 -0.7645  3.2836  3.4029  1.9159  1.4517  2.8674\n",
        "   4.9109  0.3264 -0.1104  0.2866 -0.2884  0.6897  2.7443  0.8195  1.115\n",
        "   4.7327  2.0475 -0.2815  0.134  -0.2499 -0.55    3.3943  3.8934  1.9896\n",
        "   2.1758  6.046   4.935 ]]\n",
        "\n",
        "Their corresponding Y-values are:\n",
        "[1. 1. 1. 1.]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as4rUCd1K1AP"
      },
      "source": [
        "### Define functions for plotting\n",
        "\n",
        "The code cell below defines two functions that we will used later. The first function, `plot_confusion_matrix()` plots a confusion matrix. The second function, `plot_roc()` plots a ROC chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "25ys9moDK1AP"
      },
      "outputs": [],
      "source": [
        "# Define functions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "# Plot a confusion matrix.\n",
        "# cm is the confusion matrix, names are the names of the classes.\n",
        "def plot_confusion_matrix(cm, names, title='Confusion matrix',\n",
        "                            cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(names))\n",
        "    plt.xticks(tick_marks, names, rotation=45)\n",
        "    plt.yticks(tick_marks, names)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "# Plot an ROC. pred - the predictions, y - the expected output.\n",
        "def plot_roc(pred,y):\n",
        "    fpr, tpr, _ = roc_curve(y, pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m02dTEbldgBX"
      },
      "source": [
        "There should be no output when you run the code cell above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQsttEMKdgBX"
      },
      "source": [
        "### Example 3: Construct, Compile and Train Binary Classification Neural Network\n",
        "\n",
        "The code in the cell below, uses the Keras/Tensorflow libraries to construct, compile and train a binary classificantion neural network called `pidModel` with 3 hidden layers.\n",
        "\n",
        "The model `pidModel` is designed to predict whether a women has Type II diabetes based on a several clinical measures. The model is trained (fitted) to the clinical data stored in the feature vectors `pidX` and `pidY`. A monitor function, `pidMonitor`, is also created. This monitor function to allow for Early Stopping when the validation loss fails to improve after waiting for 10 epochs. Training (fitting) is set for 1000 epochs, but the training will stop before reaching 1000 due to Early Stopping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tchQMWrIK1AQ"
      },
      "outputs": [],
      "source": [
        "# Example 3: Construct, Compile and Train\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import Precision, Recall\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train/test---------------------------------------------------------\n",
        "pidX_train, pidX_test, pidY_train, pidY_test = train_test_split(\n",
        "    pidX, pidY, test_size=0.25, random_state=42)\n",
        "\n",
        "# Construct model---------------------------------------------------------------\n",
        "pidModel = Sequential()\n",
        "pidModel.add(Input(shape=(pidX.shape[1],)))\n",
        "pidModel.add(Dense(100, activation='relu', kernel_initializer='random_normal'))\n",
        "pidModel.add(Dense(50,activation='relu',kernel_initializer='random_normal'))\n",
        "pidModel.add(Dense(25,activation='relu',kernel_initializer='random_normal'))\n",
        "pidModel.add(Dense(1,activation='sigmoid',kernel_initializer='random_normal'))\n",
        "\n",
        "# Compile model------------------------------------------------------------------\n",
        "pidModel.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=Adam(learning_rate=0.001),  # SpecirisY learning rate\n",
        "    metrics=['accuracy', Precision(), Recall()]  # Add additional metrics\n",
        ")\n",
        "\n",
        "# Create monitor------------------------------------------------------------------\n",
        "pidMonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3,\n",
        "    patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
        "\n",
        "# Train model---------------------------------------------------------------------\n",
        "pidModel.fit(pidX_train,pidY_train,validation_data=(pidX_test,pidY_test),\n",
        "          callbacks=[pidMonitor],verbose=2,epochs=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZRdcE3gdgBY"
      },
      "source": [
        "If your code is correct you should see something similiar to the output below:\n",
        "~~~text\n",
        "Epoch 1/1000\n",
        "18/18 - 2s - 123ms/step - accuracy: 0.6458 - loss: 0.6905 - precision: 0.4684 - recall: 0.1859 - val_accuracy: 0.6771 - val_loss: 0.6873 - val_precision: 0.8182 - val_recall: 0.1304\n",
        "Epoch 2/1000\n",
        "18/18 - 0s - 5ms/step - accuracy: 0.7361 - loss: 0.6767 - precision: 0.8133 - recall: 0.3065 - val_accuracy: 0.7396 - val_loss: 0.6620 - val_precision: 0.7111 - val_recall: 0.4638\n",
        "Epoch 3/1000\n",
        "18/18 - 0s - 8ms/step - accuracy: 0.7674 - loss: 0.6137 - precision: 0.7241 - recall: 0.5276 - val_accuracy: 0.7344 - val_loss: 0.5768 - val_precision: 0.6500 - val_recall: 0.5652\n",
        "Epoch 4/1000\n",
        "18/18 - 0s - 7ms/step - accuracy: 0.7760 - loss: 0.5007 - precision: 0.7303 - recall: 0.5578 - val_accuracy: 0.7344 - val_loss: 0.5265 - val_precision: 0.6286 - val_recall: 0.6377\n",
        "Epoch 5/1000\n",
        "18/18 - 0s - 4ms/step - accuracy: 0.7865 - loss: 0.4554 - precision: 0.7021 - recall: 0.6633 - val_accuracy: 0.7240 - val_loss: 0.5378 - val_precision: 0.6111 - val_recall: 0.6377\n",
        "Epoch 6/1000\n",
        "18/18 - 0s - 8ms/step - accuracy: 0.7865 - loss: 0.4413 - precision: 0.7262 - recall: 0.6131 - val_accuracy: 0.7552 - val_loss: 0.5288 - val_precision: 0.6618 - val_recall: 0.6522\n",
        "Epoch 7/1000\n",
        "18/18 - 0s - 5ms/step - accuracy: 0.7795 - loss: 0.4353 - precision: 0.7195 - recall: 0.5930 - val_accuracy: 0.7396 - val_loss: 0.5378 - val_precision: 0.6338 - val_recall: 0.6522\n",
        "Epoch 8/1000\n",
        "18/18 - 0s - 7ms/step - accuracy: 0.7865 - loss: 0.4290 - precision: 0.7235 - recall: 0.6181 - val_accuracy: 0.7396 - val_loss: 0.5389 - val_precision: 0.6338 - val_recall: 0.6522\n",
        "Epoch 9/1000\n",
        "18/18 - 0s - 8ms/step - accuracy: 0.7847 - loss: 0.4260 - precision: 0.7119 - recall: 0.6332 - val_accuracy: 0.7396 - val_loss: 0.5416 - val_precision: 0.6338 - val_recall: 0.6522\n",
        "Epoch 9: early stopping\n",
        "Restoring model weights from the end of the best epoch: 4.\n",
        "<keras.src.callbacks.history.History at 0x7acd9803df30>\n",
        "~~~\n",
        "\n",
        "In this instance, the model `pidModel` reached a minimum validation loss (`val_loss: 0.54164`) after only 9 epochs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dkdb4s5dgBY"
      },
      "source": [
        "### **Exercise 3: Construct, Compile and Train Binary Classification Neural Network**\n",
        "\n",
        "In the cell below, use the Keras libraries to construct, compile and train a binary classificantion neural network called `bcwModel` with 3 hidden layers using Example 3 as a template.\n",
        "\n",
        "Your model should be designed to predict whether a breast tumor is cancerous (malignant) or benign after fitting it to the clinical data stored in the feature vectors `bcwX` and `bcwY`.\n",
        "\n",
        "Create a monitor function called `bcwMonitor` to allow for EarlyStopping when validation loss fails to improve after waiting for 10 epochs. Set the training (fitting) for 1000 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NRJnqakdgBY"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 here:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFhKH-AjdgBY"
      },
      "source": [
        "If your code is correct you should see something similiar to the following output:\n",
        "\n",
        "~~~text\n",
        "Epoch 10/1000\n",
        "14/14 - 0s - 10ms/step - accuracy: 0.9836 - loss: 0.0549 - precision_2: 0.9809 - recall_2: 0.9747 - val_accuracy: 0.9860 - val_loss: 0.0484 - val_precision_2: 0.9815 - val_recall_2: 0.9815\n",
        "Epoch 11/1000\n",
        "14/14 - 0s - 10ms/step - accuracy: 0.9859 - loss: 0.0503 - precision_2: 0.9872 - recall_2: 0.9747 - val_accuracy: 0.9860 - val_loss: 0.0479 - val_precision_2: 0.9815 - val_recall_2: 0.9815\n",
        "Epoch 12/1000\n",
        "14/14 - 0s - 7ms/step - accuracy: 0.9859 - loss: 0.0460 - precision_2: 0.9810 - recall_2: 0.9810 - val_accuracy: 0.9860 - val_loss: 0.0478 - val_precision_2: 0.9815 - val_recall_2: 0.9815\n",
        "Epoch 13/1000\n",
        "14/14 - 0s - 11ms/step - accuracy: 0.9859 - loss: 0.0427 - precision_2: 0.9810 - recall_2: 0.9810 - val_accuracy: 0.9860 - val_loss: 0.0519 - val_precision_2: 0.9815 - val_recall_2: 0.9815\n",
        "Epoch 14/1000\n",
        "14/14 - 0s - 6ms/step - accuracy: 0.9906 - loss: 0.0375 - precision_2: 0.9936 - recall_2: 0.9810 - val_accuracy: 0.9860 - val_loss: 0.0500 - val_precision_2: 0.9815 - val_recall_2: 0.9815\n",
        "Epoch 15/1000\n",
        "14/14 - 0s - 6ms/step - accuracy: 0.9906 - loss: 0.0359 - precision_2: 1.0000 - recall_2: 0.9747 - val_accuracy: 0.9860 - val_loss: 0.0523 - val_precision_2: 0.9815 - val_recall_2: 0.9815\n",
        "Epoch 15: early stopping\n",
        "Restoring model weights from the end of the best epoch: 10.\n",
        "<keras.src.callbacks.history.History at 0x7e22734a70a0>\n",
        "~~~\n",
        "In this instance, your model `bcwModel` reached a minimum validation loss (` val_loss: 0.0484`) after only 1o epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv0LnKwhdgBY"
      },
      "source": [
        "-----------------------------------------\n",
        "\n",
        "## **ROC Curves**\n",
        "\n",
        "**Receiver Operating Characteristic (ROC) Curve** is a graphical representation that illustrates the diagnostic ability of a binary classification system as its discrimination threshold is varied. The ROC curve plots the true positive rate (Sensitivity) against the false positive rate (1-Specificity) for various threshold values, showing the trade-off between sensitivity and specificity.\n",
        "\n",
        "The ROC curve is important in evaluating the performance of diagnostic tests or classification models as it provides a comprehensive assessment of their ability to discriminate between positive and negative cases. Some key reasons why ROC curves are important include:\n",
        "\n",
        "* **Quantitative Measure:** ROC curves provide a quantitative measure of the diagnostic accuracy of a test by summarizing its performance across all possible thresholds.\n",
        "\n",
        "* **Comparison:** ROC curves allow for a direct comparison of different tests or models based on their area under the curve (AUC), where a higher AUC indicates better overall performance.\n",
        "\n",
        "* **Threshold Selection:** ROC curves help in selecting the optimal threshold for classification that balances sensitivity and specificity based on the specific requirements of the application.\n",
        "\n",
        "* **Visual Representation:** The visual nature of ROC curves facilitates a clear understanding of the trade-offs between sensitivity and specificity, aiding in decision-making and interpretation of test results.\n",
        "\n",
        "--------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDyGw6MrdgBb"
      },
      "source": [
        "### Example 4: Plot ROC curve for the Model\n",
        "\n",
        "The code in the cell below uses the model, `pidModel`, to **predict** whether each of the 192 women who were randomly placed in the `pidX_test` dataset, had Type II diabetes based on their clinical values.\n",
        "\n",
        "It then sends these predictions all with the women's actual diagnosis, stored in `bcwY_test`, to the plotting function, `plot_roc()` defined earlier in this lesson.\n",
        "\n",
        "Using the predicted and the actual values, the plotting function generates an ROC curve for the `pidModel` model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_95P1kTBK1AQ"
      },
      "outputs": [],
      "source": [
        "# Example 4: Plot ROC curve\n",
        "\n",
        "# Use model to generate predictions\n",
        "pidPred = pidModel.predict(pidX_test)\n",
        "\n",
        "# Plot model predictions against actual values\n",
        "plot_roc(pidPred,pidY_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO9kI1lmdgBb"
      },
      "source": [
        "Here is an annotated version of a similar ROC plot from the `pidModel` model.\n",
        "\n",
        "![ROC Curve ](https://biologicslab.co/BIO1173/images/class_04_2_ROC1B.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNDL-NK0dgBb"
      },
      "source": [
        "### ROC Curve: Receiver Operating Characteristic Curve\n",
        "\n",
        "ROC curve stands for \"Receiver Operating Characteristic Curve\". The ROC Curve above shows the performance of the classification model `piModel` at all classification thresholds for predicting whether a Pima women has Type II diabetes.\n",
        "\n",
        "This curve plots two parameters:\n",
        "\n",
        "* True Positive Rate (Sensitivity)\n",
        "* False Positive Rate\n",
        "\n",
        "**True Positive Rate (TPR)** is a synonym for _sensitivity_ and is defined as follows:\n",
        "\n",
        "$TPR=\\frac{TP}{TP+FN}$\n",
        "\n",
        "In other words, the sensitivity of the model (True Positive Rate) increases to a perfect score (1.0) as the False Negative rate decreases to zero.\n",
        "\n",
        "**False Positive Rate (FPR)** is defined as follows:\n",
        "\n",
        "$FPR=\\frac{FP}{FP+TN}$\n",
        "\n",
        "In other words, the False Positive Rate increases to a perfect score (1.0) as the True Negative rate decreases to zero.\n",
        "\n",
        "An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives.\n",
        "\n",
        "### AUC: Area Under the ROC Curve\n",
        "\n",
        "AUC stands for \"Area under the ROC Curve.\" In the plot above, the area colored light blue is the AUC. AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).\n",
        "\n",
        "AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.\n",
        "\n",
        "AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0. In this example, the AUC of the model `piModel` was 0.80.\n",
        "\n",
        "AUC is desirable for the following two reasons:\n",
        "\n",
        "* AUC is **scale-invariant**. It measures how well predictions are ranked, rather than their absolute values.\n",
        "\n",
        "* AUC is **classification-threshold-invariant**. It measures the quality of the model's predictions irrespective of what classification threshold is chosen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXIHTvrNdgBb"
      },
      "source": [
        "### **Exercise 4: Plot ROC curve for the Model**\n",
        "\n",
        "In the cell below write the code to generate a ROC plot of your Breast Cancer classsification model, `bcwModel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "rBouqiC1dgBb"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJsEN4zrdgBc"
      },
      "source": [
        "If code is correct you should see the following ROC curve:\n",
        "\n",
        "![ROCCurve](https://biologicslab.co/BIO1173/images/class_04_2_ROC2a.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5u0lv7KdgBc"
      },
      "source": [
        "Compared to the Pima classification model, `pidModel`, the ROC curve shows that your Breast Cancer model, `bcwModel` is much more accurate since its AUC (area under the curve) = 1.0."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Compute Accuracy Score for Binary Classification\n",
        "\n",
        "Computing the **accuracy score** for a binary classification neural network involves evaluating how well your model's predictions match the actual labels in your test dataset.\n",
        "\n",
        "The accuracy score is a metric that represents the proportion of correct predictions made by your model out of all predictions. In binary classification, it measures how often the classifier correctly predicts the positive or negative class.\n",
        "\n",
        "\n",
        "$\n",
        "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
        "$\n",
        "\n",
        "\n",
        "The code in the cell below shows how to compute the accuracy score for a binary classification neural network.\n"
      ],
      "metadata": {
        "id": "x3Sc3R9RboOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Compute accuracy score\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Obtain predictions\n",
        "predictions = pidModel.predict(pidX_test)\n",
        "predictions = (predictions > 0.5).astype(int)  # Convert probabilities to binary\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(predictions, pidY_test)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "eKoiowFFbv7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following output:\n",
        "\n",
        "~~~text\n",
        "6/6  0s 5ms/step\n",
        "Accuracy: 73.96%\n",
        "~~~"
      ],
      "metadata": {
        "id": "yYJZPyErgb5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Compute Accuracy Score for Binary Classification**\n",
        "\n",
        "In the cell below write the code to compute the accuracy score for your binary classification neural network (`bcModel`).\n"
      ],
      "metadata": {
        "id": "ZAzcurWRgyPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n"
      ],
      "metadata": {
        "id": "avrb43z-hXK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see something similar to the following output:\n",
        "\n",
        "~~~text\n",
        "6/6  0s 5ms/step\n",
        "Accuracy: 73.96%\n",
        "~~~"
      ],
      "metadata": {
        "id": "gGJJMf7EgyPN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yElFCdNQK1AR"
      },
      "source": [
        "# **Multiclass Classification**\n",
        "\n",
        "If you want to predict more than one outcome, you will need more than one output neuron. Because a single neuron can predict two results, (e.g. `0` for \"no\", `1` for \"yes\") neural networks with only two output neurons are somewhat rare.\n",
        "\n",
        "However, with three or more outcomes, there will be three or more output neurons. The following sections will examine several metrics for evaluating classification error in multiclass situtations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHZgyIuMdgBc"
      },
      "source": [
        "### Example 6: Read Datafile and Create DataFrame\n",
        "\n",
        "For the next section in this lesson, we will be using the Heart Disease dataset for the Examples. The data file, `heart_disease.csv` is located on the course HTTPS server, located in the following filepath: `/BIO1173/data/`. The code below shows how to read this data file and store the information in a new DataFrame called `ecgDF`. We will generate a new DataFrame to prevent problems with the previous manipulations of this dataset.\n",
        "\n",
        "Since we want to explore **Multiclass Classification**, we will use `RestingECG` as our target column (Y). We will want to include the `HeartDisease` column as part of the independent variables (X).\n",
        "\n",
        "The column `RestingECG` has the 3 following categorical values: `Normal`, `ST` (ST segment elevation), `LVH` (left ventricular hypertropy). Consequently, our classification neural network model will have **_3_** output neurons--one for each category.\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "BuRRSbPadgBc"
      },
      "outputs": [],
      "source": [
        "# Example 6: Read the data set\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Read the data file and create DataFrame\n",
        "ecgDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/heart_disease.csv\",\n",
        "    #    index_col=0,\n",
        "    sep=',',\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', 6)\n",
        "pd.set_option('display.max_rows', 6)\n",
        "\n",
        "# Display DataFrame\n",
        "display(ecgDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y40AylRTdgBc"
      },
      "source": [
        "If code is correct you should see the following table:\n",
        "\n",
        "![ROC Curve](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tig1Uf29dgBc"
      },
      "source": [
        "### **Exercise 6: Read Datafile and Create DataFrame**\n",
        "\n",
        "For the next section, you will be using the Iris Flower dataset for your Exercises. The data file, `iris.csv` is located on the course HTTPS server. You can use this code chunk to read the datafile and create a new DataFrame called `irisDF` (Iris Flower DataFrame).\n",
        "\n",
        "~~~text\n",
        "# Read the data file and create DataFrame\n",
        "irisDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/iris.csv\",\n",
        "#    index_col=0,\n",
        "    sep=',',\n",
        "    na_values=['NA','?'])\n",
        "~~~\n",
        "\n",
        "Set the display options to show all columns and 6 rows of the `irisDF` DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ouHrfh3NdgBd"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 6 here\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfhuFt6ldgBd"
      },
      "source": [
        "If code is correct you should see the following table:\n",
        "\n",
        "![ROCCurve](https://biologicslab.co/BIO1173/images/class_04_2_Exe5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGA-9toLdgBd"
      },
      "source": [
        "### Example 7: Compute Number of Samples in Each Output Class\n",
        "\n",
        "It is generally useful to have some idea how many samples are in each output class. Ideally, for training a multiclass classification neural network, the number of samples in each class should be approximately equal.\n",
        "\n",
        "The code below uses the grouby.count to print out next section in this lesson, we will again be using the Heart Disease dataset for the Examples. The data file, heart_disease.csv is located on the course HTTPS server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-x6T31NxdgBd"
      },
      "outputs": [],
      "source": [
        "# Example 7: Groupby\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create groupby\n",
        "ecgGroups = ecgDF.groupby(['RestingECG'])['RestingECG'].count()\n",
        "\n",
        "# Print results\n",
        "print(ecgGroups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLE3AGavdgBd"
      },
      "source": [
        "If your code is correct you should see the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xarFRL7dgBd"
      },
      "source": [
        "~~~text\n",
        "RestingECG\n",
        "LVH       188\n",
        "Normal    552\n",
        "ST        178\n",
        "Name: RestingECG, dtype: int64\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlO3zg-GdgBd"
      },
      "source": [
        "As you can see from the output above, the number of samples in each of the 3 output classes, `LVH`, `Normal` and `ST` are _not_ ideal. The number of samples in the class `Normal` is almost twice the size of the other two classes, `LVH` and `ST`. Let's see if this becomes a problem later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOMQv9EjdgBd"
      },
      "source": [
        "### **Exercise 6: Compute Number of Samples in Each Output Class**\n",
        "\n",
        "In the cell below write the code to print out the number of samples in each class of the column `species` in your DataFrame `irisDF`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RgKt0Po5dgBd"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 6 here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgsKT952dgBd"
      },
      "source": [
        "\n",
        "If your code is correct you should see the following output:\n",
        "~~~text\n",
        "species\n",
        "Iris-setosa        50\n",
        "Iris-versicolor    50\n",
        "Iris-virginica     50\n",
        "Name: species, dtype: int64\n",
        "~~~\n",
        "As you can see from the output above, the number of samples in each of the 3 output classes, `Iris-setosa`, `Iris-versicolor` and `Iris-virginica` are exactly equal (i.e. `50`), which is ideal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFiTYpVldgBd"
      },
      "source": [
        "### Example 8: Check for Missing Data\n",
        "\n",
        "The code in the cell below checks the DataFrame `ecgDF` for missing values. The number of rows is set to `15` to make sure that the results from all of the columns are displayed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Lc-QNZ0dgBe"
      },
      "outputs": [],
      "source": [
        "# Example 8: Check for missing data\n",
        "\n",
        "# Find the locations of missing data\n",
        "missing_locations = ecgDF.isnull().any()\n",
        "\n",
        "# Set max rows to 15\n",
        "pd.set_option('display.max_rows', 15)\n",
        "\n",
        "# Display the locations of missing data\n",
        "print(missing_locations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMkCHIFgdgBe"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "~~~text\n",
        "Age               False\n",
        "Sex               False\n",
        "ChestPainType     False\n",
        "RestingBP         False\n",
        "Cholesterol       False\n",
        "FastingBS         False\n",
        "RestingECG        False\n",
        "MaxHR             False\n",
        "ExerciseAngina    False\n",
        "Oldpeak           False\n",
        "ST_Slope          False\n",
        "HeartDisease      False\n",
        "dtype: bool\n",
        "~~~\n",
        "The results show that there are no missing data values in the DataFrame `ecgDF`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spxQyDb0dgBe"
      },
      "source": [
        "### **Exercise 8: Check for Missing Data**\n",
        "\n",
        "In the cell below write the code to check for missing values in the DataFrame `irisDF`. Set the number of rows to display to `5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjbxw6S8dgBe"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 8 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gygyjHcIdgBe"
      },
      "source": [
        "If your code is correct you should see the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxOHtGCTdgBe"
      },
      "source": [
        "~~~text\n",
        "sepal_length    False\n",
        "sepal_width     False\n",
        "petal_length    False\n",
        "petal_width     False\n",
        "species         False\n",
        "dtype: bool\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVOLTlyudgBe"
      },
      "source": [
        "Again, the results show that there are no missing data values in the DataFrame `irisDF`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8iUvVbydgBe"
      },
      "source": [
        "### Example 9: Preprocess Data for Neural Network Training\n",
        "\n",
        "The code in the cell below preprocesses the Heart Disease data to make it ready for machine learning.\n",
        "\n",
        "Two columns, `Sex`, and `ExerciseAngina` have _binary_ categorical variables. The column `Sex` has the letters `M` and `F`, while the column `ExerciseAngina` has the letters `Y` and `N`. In the cell below, these categorical values will be **mapped** to the integers `1` and `0`.  \n",
        "\n",
        "Two other columns have categorical values than need to be converted into numerical values. The column `ST_Slope`, has the categorical values `Down`, `Up` and `Flat` while the column `ChestPainTypes` has four categorical values, `ATA`, `NAP`, `ASY`, `TA`. One-Hot encoding will be applied to these two columns before they are used to generate the X-values.\n",
        "\n",
        "The `ecgDF` DataFrame has 4 columns with numeric data: `RestingBP`, `Cholesterol`, `FastingBS` and `MaxHR`. Since the values in these columns vary substantially in magnitude, there will be **standardized** by replacing their values by their z-scores.\n",
        "\n",
        "Finally, the target column for this example, `RestingECG` has three categorical values: `LVH`, `Normal`, and `ST`. Since this is is going to be our target column, it will be One-Hot encoded separately when generating the Y feature vector.\n",
        "\n",
        "Remember, the with classification neural networks, it is **always** necessary to One-Hot encode the target column when generating the Y values, whether or not the target column contains string or numerical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD57aaWKK1AR"
      },
      "outputs": [],
      "source": [
        "# Example 9: Preprocess data\n",
        "\n",
        "# Map `Sex` to int\n",
        "mapping = {'M': 1, 'F': 0}\n",
        "ecgDF['Sex'] = ecgDF['Sex'].map(mapping)\n",
        "\n",
        "# Map 'ExerciseAngina' to int\n",
        "mapping = {'Y': 1, 'N': 0}\n",
        "ecgDF['ExerciseAngina'] = ecgDF['ExerciseAngina'].map(mapping)\n",
        "\n",
        "# One Hot encode ChestPainType\n",
        "ecgDF = pd.concat([ecgDF,pd.get_dummies(ecgDF['ChestPainType'],prefix=\"PainType\", dtype=float)],axis=1)\n",
        "ecgDF.drop('ChestPainType', axis=1, inplace=True)\n",
        "\n",
        "# One Hot encode ST_Slope\n",
        "ecgDF = pd.concat([ecgDF,pd.get_dummies(ecgDF['ST_Slope'],prefix=\"ST_Slope\", dtype=float)],axis=1)\n",
        "ecgDF.drop('ST_Slope', axis=1, inplace=True)\n",
        "\n",
        "# Standardize ranges in numeric columns\n",
        "ecgDF['RestingBP'] = zscore(ecgDF['RestingBP'])\n",
        "ecgDF['Cholesterol'] = zscore(ecgDF['Cholesterol'])\n",
        "ecgDF['FastingBS'] = zscore(ecgDF['RestingBP'])\n",
        "ecgDF['MaxHR'] = zscore(ecgDF['MaxHR'])\n",
        "\n",
        "\n",
        "# Generate X from all columns EXCEPT the target colunm\n",
        "ecgX_columns = ecgDF.columns.drop('RestingECG')\n",
        "ecgX = ecgDF[ecgX_columns].values\n",
        "ecgX = np.asarray(ecgX).astype(np.float32)\n",
        "\n",
        "# One-Hot encode the target column and generate Y\n",
        "dummies = pd.get_dummies(ecgDF['RestingECG'], dtype=float) # Classification\n",
        "ECGclasses = dummies.columns\n",
        "ecgY = dummies.values\n",
        "ecgY = np.asarray(ecgY).astype(np.float32)\n",
        "\n",
        "# Print the ECGtypes\n",
        "print(*ECGclasses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88BB8ZOkdgBf"
      },
      "source": [
        "If your code is correct you should see the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "zok5Ha8kdgBf"
      },
      "source": [
        "~~~text\n",
        "LVH Normal ST\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE1yJzeLdgBf"
      },
      "source": [
        "The output represents the 3 output classes (Y) that our neural network will be designed to predict after being trained on the X data stored in the variable `ecgX`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoCT28VKdgBg"
      },
      "source": [
        "--------------------------\n",
        "\n",
        "## **Should binary numerical categories be converted to Z-scores?**\n",
        "\n",
        "Since we converted the numerical data into Z-scores, should we also convert the data in the columns that were mapped to `0` and `1` into Z-scores?\n",
        "\n",
        "No, binary numerical categories should **not** be converted into Z-scores when fitting a neural network. Z-scores are used for standardizing continuous numerical data by calculating the number of standard deviations a data point is from the mean. Binary numerical categories are already in a format that is suitable for inputting into a neural network, as they typically represent two distinct classes or states. Converting them into Z-scores would not add any meaningful information and could potentially introduce unnecessary noise into the data.\n",
        "\n",
        "--------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6EO4abQdgBg"
      },
      "source": [
        "### **Exercise 9: Preprocess Data for Neural Network Training**\n",
        "\n",
        "In the cell below, preprocess the data in your DataFrame `irisDF` to make it ready for processing by a neural network. Compared to Example 8 above, your code will require fewer steps.\n",
        "\n",
        "Things you **don't** have to do for preprocessing:\n",
        "\n",
        "1. Since there are no missing data in `irisDF` you don't need to correct for missing data.\n",
        "2. Since the all of the columns (except the target column) are numeric, you don't have to map any categorical values to integers.\n",
        "\n",
        "Things you **will** need to do for preprocessing:\n",
        "\n",
        "1. Create a list of columns to used in creating your X values called `irisX_columns`, by dropping the target column `species`.\n",
        "2. Use your list to generate an X feature vector called `irisX`.\n",
        "3. Convert `irisX` to float32 using the command: `irisX = np.asarray(irisX).astype(np.float32)`\n",
        "4. Generate your Y feature vector by One-Hot encode the target column, `species` using this code chunk:\n",
        "\n",
        "~~~text\n",
        "# Generate Y feature vector\n",
        "dummies = pd.get_dummies(irisDF['species'], dtype=float) # Classification\n",
        "IRIS_species = dummies.columns\n",
        "irisY = dummies.values\n",
        "irisY = np.asarray(irisY).astype(np.float32)\n",
        "~~~\n",
        "Print out the contents of the variable `IRIS_species` using the starred notation demonstrated in Example 8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0jcqZfidgBg"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 9 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "casJ9cJXdgBg"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "~~~text\n",
        "Iris-setosa Iris-versicolor Iris-virginica\n",
        "~~~\n",
        "These are the three classes that your neural network will be designed to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KHbtF94dgBh"
      },
      "source": [
        "### Example 10: Construct, Compile and Train Multiclass Classification Neural Network\n",
        "\n",
        "The code in the cell below starts by splitting the X-values stored in the X feature vector, `ecgX`, into `ecgX_train` and `ecgX_test` sets with 75% being used for training.\n",
        "\n",
        "Similarily, the Y-values stores in `ecgY` are split into `ecgY_train` and `ecgY_test` sets. Keep in mind that the Y-values in the training and testing sets match the X-values in the training and testing sets.\n",
        "\n",
        "The cell below then builds a linear (\"sequential\") muticlass classsification neural network called `ecgModel` with 3 hidden layers. The number of inputs to the 1st hidden layer is specified by the argument `input_dim=ecgX.shape[1]`. Similarly, the argument `ecgY.shape[1]`, is used in the output layer to specirisY the number output neurons. Since there are 3 classes in `ecgY` (`LVH`, `Normal`, `ST`) the output layer will have 3 neurons--one neuron for each class.\n",
        "\n",
        "Since this model is doing classification, we will compile it using the loss function, `categorical_crossentropy` with the `Adam` optimizer.\n",
        "\n",
        "To allow for early stopping, an EarlyStopping monitor, `ecgMoniter` is created.  The monitor is specified to wait `50` epochs for the validation loss to start improving after reaching a local minimum, before termining the training. This is a fairly long time to wait, but it is needed in this example due to the relatively low number of data samples.\n",
        "\n",
        "Finally, the model `ecgModel` is trained (fitted) to the training and test (validation) data sets for 1000 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aggbZoEUK1AR"
      },
      "outputs": [],
      "source": [
        "# Example 10: Construct, compile and train Multiclass Classification neural network\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import Precision, Recall\n",
        "\n",
        "# Split into train/test--------------------------------------------------------\n",
        "ecgX_train, ecgX_test, ecgY_train, ecgY_test = train_test_split(\n",
        "    ecgX, ecgY, test_size=0.25, random_state=10)\n",
        "\n",
        "\n",
        "# Construct model---------------------------------------------------------------\n",
        "ecgModel = Sequential()\n",
        "ecgModel.add(Input(shape=(ecgX.shape[1],)))\n",
        "ecgModel.add(Dense(100, activation='relu',\n",
        "                kernel_initializer='random_normal'))  # Hidden 1\n",
        "ecgModel.add(Dense(50,activation='relu',\n",
        "                   kernel_initializer='random_normal')) # Hidden 2\n",
        "ecgModel.add(Dense(25,activation='relu',\n",
        "                   kernel_initializer='random_normal')) # Hidden 3\n",
        "ecgModel.add(Dense(ecgY.shape[1],activation='softmax',\n",
        "                kernel_initializer='random_normal')) # Output\n",
        "\n",
        "# Compile model------------------------------------------------------------------\n",
        "ecgModel.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics =['accuracy'])\n",
        "\n",
        "# Create EarlyStopping monitor---------------------------------------------------\n",
        "ecgMonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=50,\n",
        "                        verbose=1, mode='auto', restore_best_weights=True)\n",
        "\n",
        "# Train model--------------------------------------------------------------------\n",
        "ecgModel.fit(ecgX_train,ecgY_train,validation_data=(ecgX_test,ecgY_test),\n",
        "          callbacks=[ecgMonitor],verbose=2,epochs=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_3er0EydgBh"
      },
      "source": [
        "If you code is correct you should see something similiar to the output below.\n",
        "\n",
        "~~~text\n",
        "Epoch 1/1000\n",
        "22/22 - 2s - 98ms/step - accuracy: 0.5247 - loss: 1.0418 - val_accuracy: 0.5609 - val_loss: 1.0121\n",
        "Epoch 2/1000\n",
        "22/22 - 0s - 16ms/step - accuracy: 0.6148 - loss: 0.9552 - val_accuracy: 0.5609 - val_loss: 1.0294\n",
        "Epoch 3/1000\n",
        "22/22 - 0s - 14ms/step - accuracy: 0.6148 - loss: 0.9583 - val_accuracy: 0.5609 - val_loss: 1.0167\n",
        "Epoch 4/1000\n",
        "22/22 - 0s - 13ms/step - accuracy: 0.6148 - loss: 0.9548 - val_accuracy: 0.5609 - val_loss: 1.0108\n",
        "Epoch 5/1000\n",
        "22/22 - 0s - 14ms/step - accuracy: 0.6148 - loss: 0.9574 - val_accuracy: 0.5609 - val_loss: 1.0141\n",
        "\n",
        ".................\n",
        "\n",
        "Epoch 79/1000\n",
        "22/22 - 0s - 6ms/step - accuracy: 0.6308 - loss: 0.8164 - val_accuracy: 0.5565 - val_loss: 0.9527\n",
        "Epoch 80/1000\n",
        "22/22 - 0s - 8ms/step - accuracy: 0.6352 - loss: 0.8153 - val_accuracy: 0.5696 - val_loss: 0.9686\n",
        "Epoch 81/1000\n",
        "22/22 - 0s - 13ms/step - accuracy: 0.6337 - loss: 0.8243 - val_accuracy: 0.5783 - val_loss: 0.9780\n",
        "Epoch 82/1000\n",
        "22/22 - 0s - 9ms/step - accuracy: 0.6279 - loss: 0.8294 - val_accuracy: 0.5609 - val_loss: 0.9549\n",
        "Epoch 83/1000\n",
        "22/22 - 0s - 11ms/step - accuracy: 0.6352 - loss: 0.8141 - val_accuracy: 0.5609 - val_loss: 0.9607\n",
        "Epoch 84/1000\n",
        "22/22 - 0s - 8ms/step - accuracy: 0.6308 - loss: 0.8096 - val_accuracy: 0.5609 - val_loss: 0.9645\n",
        "Epoch 84: early stopping\n",
        "Restoring model weights from the end of the best epoch: 34.\n",
        "<keras.src.callbacks.history.History at 0x7a0258edabc0>\n",
        "~~~\n",
        "\n",
        "In this particular run, the model `egcModel` reached the lowest validation loss (`val_loss: 0.9301`) after 34 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXyaGrxdgBh"
      },
      "source": [
        "### **Exercise 10: Construct, Compile and Train Multiclass Classification Neural Network**\n",
        "\n",
        "In the cell below construct, compile and train a multiclass classification neural network model called `irisModel` using the code in Example 8 as a template.\n",
        "\n",
        "Start by splitting the X-values stored in `irisX` into `irisX_train` and `irisX_test` sets with 75% being used for training. Similarily, split the Y-values stores in `irisY` are split into `irisY_train` and `irisY_test` sets.\n",
        "\n",
        "Build a linear (\"sequential\") muticlass classsification neural network called `irisModel` with 3 hidden layers. SpecirisY the number of inputs by the argument `irisX.shape[1]` and specirisY the number of neurons in the output layer by the argument `irisY.shape[1]`. Since there are 3 classes in `irisY` (`Iris-setosa`, `Iris-versicolor`, `Iris-virginica`) the output layer will have 3 neurons--one neuron for each class.\n",
        "\n",
        "Once the model is constructed, compile it using the loss function, `categorical_crossentropy` with the `Adam` optimizer.\n",
        "\n",
        "Create an EarlyStopping monitor called, `irisMoniter` and specirisY the monitor to wait `20` epochs for the validation loss to start improving after reaching a local minimum, before termining the training.\n",
        "\n",
        "Finally, fit your model `irisModel` to the training and test (validation) data sets for 1000 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "aVFB9B8rdgBh"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 10 here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyNI6YtEdgBh"
      },
      "source": [
        "If you code is correct you should see something similiar to the output below.\n",
        "\n",
        "~~~text\n",
        "Epoch 1/1000\n",
        "4/4 - 2s - 493ms/step - accuracy: 0.3393 - loss: 1.0969 - val_accuracy: 0.3158 - val_loss: 1.0940\n",
        "Epoch 2/1000\n",
        "4/4 - 0s - 20ms/step - accuracy: 0.3393 - loss: 1.0933 - val_accuracy: 0.3158 - val_loss: 1.0907\n",
        "Epoch 3/1000\n",
        "4/4 - 0s - 18ms/step - accuracy: 0.3393 - loss: 1.0898 - val_accuracy: 0.3158 - val_loss: 1.0866\n",
        "Epoch 4/1000\n",
        "4/4 - 0s - 36ms/step - accuracy: 0.3393 - loss: 1.0848 - val_accuracy: 0.3158 - val_loss: 1.0808\n",
        "Epoch 5/1000\n",
        "4/4 - 0s - 29ms/step - accuracy: 0.4286 - loss: 1.0777 - val_accuracy: 0.5789 - val_loss: 1.0725\n",
        "\n",
        "........................\n",
        "\n",
        "Epoch 449/1000\n",
        "4/4 - 0s - 31ms/step - accuracy: 0.9821 - loss: 0.0666 - val_accuracy: 0.9737 - val_loss: 0.0346\n",
        "Epoch 450/1000\n",
        "4/4 - 0s - 39ms/step - accuracy: 0.9732 - loss: 0.0531 - val_accuracy: 0.9474 - val_loss: 0.0512\n",
        "Epoch 451/1000\n",
        "4/4 - 0s - 29ms/step - accuracy: 0.9643 - loss: 0.0734 - val_accuracy: 0.9737 - val_loss: 0.0430\n",
        "Epoch 452/1000\n",
        "4/4 - 0s - 26ms/step - accuracy: 0.9821 - loss: 0.0626 - val_accuracy: 0.9737 - val_loss: 0.0335\n",
        "Epoch 453/1000\n",
        "4/4 - 0s - 35ms/step - accuracy: 0.9732 - loss: 0.0566 - val_accuracy: 0.9737 - val_loss: 0.0334\n",
        "Epoch 453: early stopping\n",
        "Restoring model weights from the end of the best epoch: 403.\n",
        "<keras.src.callbacks.history.History at 0x7a025a8eb1c0>\n",
        "~~~\n",
        "\n",
        "In this particular run, the model irisModel reached the lowest validation loss (`val_loss: 0.0506`) after 403 epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr9U9rgvK1AR"
      },
      "source": [
        "## **Classification Accuracy for Multiclass Classification**\n",
        "\n",
        "Accuracy is the number of rows where the neural network correctly predicted the target class.  Accuracy is _only_ used for classification, not regression.\n",
        "\n",
        "$$ accuracy = \\frac{c}{N} $$\n",
        "\n",
        "Where $c$ is the number correct and $N$ is the size of the evaluated set (training or validation). Higher accuracy numbers are desired.\n",
        "\n",
        "As we just saw, by default, Keras will return the percent probability for each class. We can change these prediction probabilities into the actual prediction values with **argmax**. For each prediction, argmax \"looks\" at the numerical value in each of the output neurons and assigns a value of one to the neuron with the highest values. In other words, this is an example of a \"winner take all\" strategy. The output neuron that \"wins\" represents the model's prediction for that particular combination of X values for one subject assigned to the training (validation) set.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g99EGcgFdgBh"
      },
      "source": [
        "### Example 11: Convert Prediction Probabilites into Actual Prediction Values\n",
        "\n",
        "The code in the cell below, uses `argmax` method to convert prediction probabilites from the `ecgModel` into actual prediction values. The `ecgModel` has three output neurons, the first one representing `RestingECG` class `LVH` (left ventricular hypertropy), the second one representing the class `Normal` and the third one representing the class `ST` (ST interval elevation). For each of the 230 subjects randomly assigned to the `ecgX_test` validation dataset, the model `ecgModel` will process the X-values for each subject and predicts the **_probability_** that the subject belongs to each class of `RestingECG` stores these prediction probabilites in a variable called `ecgProb`.\n",
        "\n",
        "What is actually stored in `ecgProb` is a 3-element numpy array for each of the 230 subjects in `ecgX_test` with the prediction probability values of the 3 output neurons. For example, here are the prediction probabilites values in `ecgProb` for the first subject in the `ecgX_test` data set:\n",
        "~~~text\n",
        "[[0.24944986 0.501199   0.24935108]]\n",
        "~~~\n",
        "\n",
        "The first value, `0.24944986` is the model's probability prediction that the subject had the `LVH` class of `RestingECG`, `0.501199` is the model's prediction probability that this subject had the `Normal` class and `0.24935108` the prediction probability that the subject had the `ST` class. As might be expected, the sum of these three values is 1.0.\n",
        "\n",
        "These prediction probabilites are converted into actual predictions using this line of code:\n",
        "~~~text\n",
        "ecgPred = np.argmax(ecgP1,axis=1)\n",
        "~~~\n",
        "\n",
        "The function `np.argmax()` sequentially processes each 3-element array in `ecgProb` and generates a new array called `ecgPred`. The array `ecgPred` will have a single integer value: `0`, `1` or `2` , for each subject in `x_test`, depending on prediction probabilities. If first value is the highest, `np.argmax()` will place a `0` in `ecgPred`, if the second value is the highest, the number `1` will be placed in `ecgPred`, and so forth. Subject 1, in the example above, would be assigned the value `1` since the second output neuron had the highest probability (`0.501199`). In other words, `np.argmax` assigns the model's actual prediction, `0`, `1` or `2`, to the output neuron with the highest probability prediction.\n",
        " subject.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 11: Convert Prediction Probabilites into Actual Prediction Values\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the predict function outside of any loops to prevent retracing\n",
        "@tf.function\n",
        "def predict_probabilities(model, inputs):\n",
        "    return model(inputs)\n",
        "\n",
        "# Use model to predict probabilities for subjects in x_test\n",
        "ecgProb = predict_probabilities(ecgModel, ecgX_test)\n",
        "\n",
        "# Print prediction probabilities (first 6 samples)\n",
        "print(f\"Prediction Probabilities (first 6):\\n{ecgProb[0:6]}\")\n",
        "\n",
        "# Use argmax to convert probabilities to prediction values\n",
        "ecgPred = tf.argmax(ecgProb, axis=1)\n",
        "\n",
        "# Print out prediction values (first 6 samples)\n",
        "print(f\"Prediction Values (first 6):\\n{ecgPred.numpy()[0:6]}\")\n"
      ],
      "metadata": {
        "id": "E0ZzSPNRY4gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3HVoe7QdgBi"
      },
      "source": [
        "If your code is correct you should see something similiar to the output below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgcEe9XPdgBi"
      },
      "source": [
        "~~~text\n",
        "Prediction Probabilities (first 6):\n",
        "[[0.1829 0.57   0.2471]\n",
        " [0.1563 0.582  0.2616]\n",
        " [0.1329 0.6982 0.169 ]\n",
        " [0.3349 0.5634 0.1018]\n",
        " [0.2448 0.6693 0.0859]\n",
        " [0.2089 0.5905 0.2006]]\n",
        "Prediction Values (first 6):\n",
        "[1 1 1 1 1 1]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pJM5FPjdgBi"
      },
      "source": [
        "Clearly, our model `ecgModel` is **_not_** very certain when it comes to predicting the `RestingECG` class. Essentially the model is saying that there is a 50% chance most subjects will have a `RestingECG` that is `Normal`, and a 50% chance that the subject's `RestingECG` will not be normal. This is basically the same accuracy as \"coin-flipping\".\n",
        "\n",
        "There is no guarantee that a neural network model you build will be especially accurate when fitted to given dataset! In this case, the relatively low number of test subjects in the dataset (_n_=918) is probably a main reason for the low accuracy. It is also possible that the class of `RestingECG` is not strongly dependent on the clinical variables used in the training set, `ecgX_train`.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJPRx5ssdgBi"
      },
      "source": [
        "### **Exercise 11: Convert Prediction Probabilites into Actual Prediction Values**\n",
        "\n",
        "In the cell below write the code to generate an Numpy array called `ifProb` that contains the prediction probabilities for all of the Iris flowers in the validation set, `irisX_train`. Print out the first 6 values in `ifProb`. The use the function `np.argmax()` to convert these prediction probabilites into actual predictions. Store the model's actual predictions in a new Numpy array called `irisPred`. Print out the first 6 values in `irisPred`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHKqsGs-dgBi"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 11 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZUeRWupdgBi"
      },
      "source": [
        "If your code is correct you should see something similar to the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCoNHt9ZdgBi"
      },
      "source": [
        "~~~text\n",
        "Prediction Probabilities (first 6):\n",
        "[[0.     0.9938 0.0062]\n",
        " [0.     0.0025 0.9975]\n",
        " [1.     0.     0.    ]\n",
        " [0.     0.997  0.003 ]\n",
        " [1.     0.     0.    ]\n",
        " [0.0004 0.9984 0.0013]]\n",
        "Prediction Values (first 6):\n",
        "[1 2 0 1 0 1]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQsCnwjidgBi"
      },
      "source": [
        "Compared the model `ecgModel`, the Iris Flower model, `irisModel` is clearly more certain about which `species` class to assign to the flowers in the validation data set `irisX_test`. In the example above, the model `irisModel` predicted that the first flower in the validation data set had the following probabilites:\n",
        "\n",
        "~~~text\n",
        "0.     0.9938 0.0062\n",
        "~~~\n",
        "\n",
        "In other words, `irisModel` predicts that there is an essentially `0%` chance that the first flower in the validation test set is `Iris setosa`, a `99%` chance that the flower species is `Iris versicolor` and less than a `1%` chance that the flower species is `Iris virginica`.\n",
        "\n",
        "The Numpy function, `np.argmax()` places a `1` in `irisPred` representing `Iris versicolor` as the model's actual prediction for the first flower.\n",
        "\n",
        "Even though the Iris flower dataset is much smaller (_n_ =150) than the Heart Disease dataset (_n_ =918), why do you think the neural network constructed in **Exercise 9** so much more accurate?\n",
        "\n",
        "The answer lies in the different nature of the two datasets. It turns out that the sepal and petal dimensions of Iris flowers are **very** species specific. It simply not that hard for a neural network to figure out the correct relationships (i.e. connection weights) between the neurons in the hidden layers so that a particular set of sepal and petal dimensions (X values) end up going to correct output neuron.\n",
        "\n",
        "On the other hand, humans often exhibit a wide range of variations in their characteristics. This is especially true for the measurements found in medical/clinical datasets. **_If_** there is a correlation between clinical values, it may take a very large dataset for a neural network to correctly identirisY these correlations.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_a5WcpJK1AS"
      },
      "source": [
        "### Example 12: Compute the Percent Accuracy\n",
        "\n",
        "Now that we have the actual `RestingECG` type predicted for each subject in the validation test set (`ecgX_test), we can calculate the percent accuracy (how many were correctly classified). The code for doing this is shown in the next cell.\n",
        "\n",
        "In the first step, a Numpy array called `ecg_compare` is created. This array contains the actual `RestingEGC` value for each subject in the validation set, where `0` represents `LVH`, `1` represents `Normal` and `2` represents `ST`.\n",
        "\n",
        "In the next step, the function `metric.accuracy_score()` from the `sklean` library computes the accuracy score by comparing the **actual** RestingECG values in `ecg_compare` with the **_predicted_** RestingECG values stored in `ecgPred` and returns the accuracy in the variable called `score`.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 12: Compute percent accuracy\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "# Generate array containing actual class type\n",
        "ecgY_compare = np.argmax(ecgY_test, axis=1)\n",
        "print(f'First 6 values in Y_compare: {ecgY_compare[0:6]}')\n",
        "\n",
        "# Make sure ecgPred is also a NumPy array on the CPU\n",
        "if not isinstance(ecgPred, np.ndarray):\n",
        "    ecgPred = ecgPred.numpy()\n",
        "\n",
        "# Compute the percentage score\n",
        "score = metrics.accuracy_score(ecgY_compare, ecgPred)\n",
        "\n",
        "# Print out the score\n",
        "print(\"Accuracy score: {}\".format(score))\n"
      ],
      "metadata": {
        "id": "KSdlf2jHmyG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QzN18pIdgBj"
      },
      "source": [
        "If your code is correct you should see something like the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdnbftm-dgBj"
      },
      "source": [
        "~~~text\n",
        "First 6 values in Y_compare: [1 2 1 2 0 1]\n",
        "Accuracy score: 0.5652173913043478\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1CTJt7xdgBj"
      },
      "source": [
        "In this run, the model `ecgModel` was only able to predict the correct class of `RestingECG` about 56% of the time. Not especially impressive, but it's about what we would have expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiG6NOchdgBj"
      },
      "source": [
        "### **Exercise 12: Compute the Percent Accuracy**\n",
        "\n",
        "In the cell below, compute the accuracy score for your model `irisModel` using Example 11 as a template. Store the actual species name (class value) in an array called `irisY_compare` and print out the first 6 values. Use the function `metrics.accuracy_score()` to compute the accuracy of your model and print it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NuxoNIodgBj"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 11 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY3p9BN1dgBj"
      },
      "source": [
        "If your code is correct you should see something like the following output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbsF8pQtdgBj"
      },
      "source": [
        "~~~text\n",
        "First 6 values in Y_compare: [1 0 2 1 1 0]\n",
        "Accuracy score: 0.9736842105263158\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86YdXfKKdgBj"
      },
      "source": [
        "As might be expected, your model `irisModel` is able predict the correct species of Iris flower with more than 95% accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zlG17MKK1AS"
      },
      "source": [
        "## **Calculate Classification Log Loss**\n",
        "\n",
        "**_Log loss_**, also known as logarithmic loss or cross-entropy loss, is a common evaluation metric used in machine learning, particularly for binary classification problems. It measures the performance of a classification model by penalizing false classifications.\n",
        "\n",
        "In binary classification, where the target variable has two classes (usually labeled as 0 and 1), log loss is calculated as the negative logarithm of the predicted probability assigned to the correct class. The formula for log loss for a single observation is:\n",
        "\n",
        "$\\text{Log Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]$\n",
        "\n",
        "where:\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;$N$ is the number of observations, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;$y_i$ is the actual class label (0 or 1) for the $i$th observation, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;$p_i$ is the predicted probability that the $i$th observation belongs to class 1.\n",
        "\n",
        "A lower log loss indicates better performance, with 0 representing a perfect model and higher values indicating poorer performance. Log loss is a useful metric for evaluating the accuracy of probabilistic predictions generated by a classification model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elcUmNQwdgBj"
      },
      "source": [
        "### Example 13: Calculate Log Loss\n",
        "\n",
        "The code in the cell below calculates the log loss for the model `ecgModel`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 13: Calculate Log Loss\n",
        "\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "# Generate predictions\n",
        "ecgProb = ecgModel.predict(ecgX_test)\n",
        "\n",
        "# Ensure the predicted probabilities sum to one for each sample\n",
        "print(\"Sum of probabilities for the first prediction:\", np.sum(ecgProb[0]))\n",
        "\n",
        "# Print prediction probabilities (first 6 samples)\n",
        "print(\"Numpy array of predictions\")\n",
        "display(ecgProb[0:5])\n",
        "\n",
        "# As percent probability\n",
        "print(\"As percent probability\")\n",
        "print(ecgProb[0] * 100)\n",
        "\n",
        "# Calculate log loss score\n",
        "score = metrics.log_loss(ecgY_test, ecgProb)\n",
        "print(\"Log loss score: {}\".format(score))\n",
        "\n",
        "# Convert raw probabilities to chosen class (highest probability)\n",
        "ecgPred = np.argmax(ecgProb, axis=1)\n",
        "\n",
        "# Print out prediction values (first 6 samples)\n",
        "print(ecgPred[0:6])"
      ],
      "metadata": {
        "id": "if2bDYvN09Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkLNRLlSdgBk"
      },
      "source": [
        "If your code is correct you should see something similar to the output below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f3OPlAvdgBk"
      },
      "source": [
        "~~~text\n",
        "8/8  0s 2ms/step\n",
        "Sum of probabilities for the first prediction: 0.99999994\n",
        "Numpy array of predictions\n",
        "array([[0.2209, 0.5206, 0.2585],\n",
        "       [0.1106, 0.6174, 0.272 ],\n",
        "       [0.1573, 0.6488, 0.1939],\n",
        "       [0.3576, 0.5334, 0.109 ],\n",
        "       [0.2702, 0.6347, 0.0951]], dtype=float32)\n",
        "As percent probability\n",
        "[22.0891 52.0633 25.8476]\n",
        "Log loss score: 0.9267466264226367\n",
        "[1 1 1 1 1 1]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAisJZrsdgBk"
      },
      "source": [
        "The log loss score `0.93` looks pretty high. Let's see how it compares to your model `irisModel`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1plDFC_ZdgBk"
      },
      "source": [
        "### **Exercise 13: Calculate Log Loss**\n",
        "\n",
        "In the cell below calculate the log loss for the model `irisModel`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 12 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X3sMHu4l0lhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj-duSPUdgBk"
      },
      "source": [
        "If your code is correct you should see something similar to the output below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY5UkW9qdgBk"
      },
      "source": [
        "~~~text\n",
        "2/2  0s 6ms/step\n",
        "Sum of probabilities for the first prediction: 1.0\n",
        "Numpy array of predictions\n",
        "array([[0.0001, 0.9941, 0.0058],\n",
        "       [0.    , 0.0031, 0.9969],\n",
        "       [1.    , 0.    , 0.    ],\n",
        "       [0.0001, 0.997 , 0.0029],\n",
        "       [1.    , 0.    , 0.    ]], dtype=float32)\n",
        "As percent probability\n",
        "[ 0.0097 99.4073  0.583 ]\n",
        "Log loss score: 0.03508699879655408\n",
        "[1 2 0 1 0 1]\n",
        "Log loss score: 0.03375224627976317\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t9i4b5_dgBk"
      },
      "source": [
        "The log loss score for your Iris flower model, 0.03 is much lower, and therefore significantly better than the 0.92 log loss score for the model `ecgModel`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU3NLdorK1AS"
      },
      "source": [
        "[Log loss](https://www.kaggle.com/wiki/LogarithmicLoss) is calculated as follows:\n",
        "\n",
        "$$ \\mbox{log loss} = -\\frac{1}{N}\\sum_{i=1}^N {( {y}_i\\log(\\hat{y}_i) + (1 - {y}_i)\\log(1 - \\hat{y}_i))} $$\n",
        "\n",
        "\n",
        "You should use this equation only as an objective function for classifications that have two outcomes. The variable y-hat is the neural networks prediction, and the variable y is the known correct answer.  In this case, y will always be 0 or 1.  The training data have no probabilities. The neural network classifies it either into one class (1) or the other (0).  \n",
        "\n",
        "The variable N represents the number of elements in the training set the number of questions in the test.  We divide by N because this process is customary for an average.  We also begin the equation with a negative because the log function is always negative over the domain 0 to 1.  This negation allows a positive score for the training to minimize.\n",
        "\n",
        "You will notice two terms are separated by the addition (+).  Each contains a log function.  Because y will be either 0 or 1, then one of these two terms will cancel out to 0.  If y is 0, then the first term will reduce to 0.  If y is 1, then the second term will be 0.  \n",
        "\n",
        "If your prediction for the first class of a two-class prediction is y-hat, then your prediction for the second class is 1 minus y-hat.  Essentially, if your prediction for class A is 70% (0.7), then your prediction for class B is 30% (0.3).  Your score will increase by the log of your prediction for the correct class.  If the neural network had predicted 1.0 for class A, and the correct answer was A, your score would increase by log (1), which is 0. For log loss, we seek a low score, so a correct answer results in 0.  Some of these log values for a neural network's probability estimate for the correct class:\n",
        "\n",
        "* -log(1.0) = 0\n",
        "* -log(0.95) = 0.02\n",
        "* -log(0.9) = 0.05\n",
        "* -log(0.8) = 0.1\n",
        "* -log(0.5) = 0.3\n",
        "* -log(0.1) = 1\n",
        "* -log(0.01) = 2\n",
        "* -log(1.0e-12) = 12\n",
        "* -log(0.0) = negative infinity\n",
        "\n",
        "As you can see, giving a low confidence to the correct answer affects the score the most.  Because log (0) is negative infinity, we typically impose a minimum value.  Of course, the above log values are for a single training set element.  We will average the log values for the entire training set.\n",
        "\n",
        "The log function is useful to penalizing wrong answers.  The following code demonstrates the utility of the log function:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib.pyplot import figure, show\n",
        "from numpy import arange, log\n",
        "\n",
        "# Start t from a small positive value to avoid log(0)\n",
        "t = arange(1e-10, 1.0, 0.00001)  # data scientists\n",
        "\n",
        "fig = figure(1, figsize=(10, 8))\n",
        "ax1 = fig.add_subplot(211)\n",
        "ax1.plot(t, log(t))\n",
        "ax1.grid(True)\n",
        "ax1.set_ylim((-8, 1.5))\n",
        "ax1.set_xlim((-0.1, 1))\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_title('log(x)')\n",
        "show()\n"
      ],
      "metadata": {
        "id": "4yTjv4fct11H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnyNhfdnvDNT"
      },
      "source": [
        "If code is correct you should see the following plot:\n",
        "\n",
        "![ROCCurve](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaRtddpcK1AT"
      },
      "source": [
        "## **Confusion Matrix**\n",
        "\n",
        "A **Confusion Matrix** is a performance measurement tool used in machine learning for evaluating the accuracy of a classification model. It is a table that allows visualization of the performance of a model by comparing the actual values of the target variable with the predicted values. The confusion matrix is particularly useful for evaluating the performance of a classification model on a dataset with known class labels.\n",
        "\n",
        "The confusion matrix is organized into a grid with four possible outcomes:\n",
        "\n",
        "* **True Positives (TP):** The number of correct predictions that the model has made for the positive class.\n",
        "* **True Negatives (TN):** The number of correct predictions that the model has made for the negative class.\n",
        "* **False Positives (FP):** The number of incorrect predictions that the model has made, predicting a positive class when the actual class is negative (Type I error).\n",
        "* **False Negatives (FN):** The number of incorrect predictions that the model has made, predicting a negative class when the actual class is positive (Type II error).\n",
        "\n",
        "The confusion matrix enables the calculation of various evaluation metrics such as accuracy, precision, recall, F1 score, and specificity. These metrics help in assessing the model's performance, identirisYing potential areas for improvement, and making informed decisions about the model's effectiveness for the given classification task.\n",
        "\n",
        "In other words, a confusion matrix shows which predicted classes are often confused for the other classes. The vertical axis (y) represents the true labels and the horizontal axis (x) represents the predicted labels. When the true label and predicted label are the same, the highest values occur down the diagonal extending from the upper left to the lower right. The other values, outside the diagonal, represent incorrect predictions. For example, in the confusion matrix below, the value in row 2, column 1 shows how often the predicted value A occurred when it should have been B."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0jq8N1ddgBl"
      },
      "source": [
        "### Example 14: Generate Confusion Matrix\n",
        "\n",
        "The code in the cell below generates a Confusion Matrix for the model `ecgModel`. It uses the function `plot_confusion_matrix()` that was defined earlier in this lesson. To generate the Confusion Matrix, only two arguments need to be passed to the plotting function, the actual values of the `RestingEGC` of the patients in the validation set (i.e. `ecgY_compare`), and the model's predicted values for these patients (i.e. `ecgPred`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pvqhwRcK1AT",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Example 14: Generate confusion matrix\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(ecgY_compare, ecgPred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Normalize the confusion matrix by row (i.e by the number of samples\n",
        "# in each class)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "print('Normalized confusion matrix')\n",
        "print(cm_normalized)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm_normalized, ECGclasses,\n",
        "        title='Normalized confusion matrix')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXqEFTl9dgBl"
      },
      "source": [
        "If your code is correct you should see a figure similar to the one below:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04_2_CM1.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8QeTdZrdgBl"
      },
      "source": [
        "In this particular Confusion Matrix shown above for the model `ecgModel`, indicates that it predicted the `RestingEGC` was normal for nearly **_every_** subject in the validation (test) set, irregardless of their actual type. The model was able to correctly identirisY patients with LVH only 8% of the time but missidentified the remaining 90%  as having a normal `RestingECG` the rest of the time. The model was never able to identirisY any of the patients in the validation data set who had a `RestingECG` of the type `ST`.\n",
        "\n",
        "Based on our results so far, we should expect your model `irisModel` to generate a much different looking Confusion Matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBovRPzfdgBl"
      },
      "source": [
        "### **Exercise 14: Generate Confusion Matrix**\n",
        "\n",
        "In the cell below write the code to generate a Confusion Matrix for your Iris flower model, `irisModel`, using Example 13 as a template. Use the variable `IRIS_species` instead of `ECGclasses` when you give the command `plot_confusion_mactrix()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWXJXD2rdgBl"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 14 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7zGRz9ndgBl"
      },
      "source": [
        "If your code is correct you should see the figure below:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04_2_CM2a.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv0wSPFkdgBl"
      },
      "source": [
        "### **Analysis of the Confusion Matrix**\n",
        "\n",
        "Let's analyze the Confusion Matrix above to see what is says about your Iris Flower model, `irisModel`.\n",
        "\n",
        "Based on the Confusion Matrix your `irisModel` was able to predict, with 100% accuracy, the correct species of flowers obtained from both _`Iris setosa`_ (top left) and _`Iris virginica`_ (bottom right). The top left and bottom right squares in the Confusion Matrix are colored with the darkest possible shade of blue. According to the color key shown at the right of this image, this shade of dark blue represents a value of `1.0`, which means a perfect score.\n",
        "\n",
        "When it came to flowers from the species, _`Iris versicolor`_, however, your model was accurate only about 90% of the time. Furthermore, based on the Confusion Matrix, when your model made a mistake, it invariably classified an _`Iris versicolor`_ flower as being from the species _`Iris virginica`_, but it _never_ confused the flower as being from the species _`Iris setosa`_. (Note light blue color panel in the center of the right side).\n",
        "\n",
        "If we wanted to improve our model, we should focus on ways to improve the model's ability to discrimate between flowers from these two species.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ORPxeeFdgBl"
      },
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_04_2.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Poly-A Tail**\n",
        "\n",
        "## **Attention Is All You Need**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/8/8f/The-Transformer-model-architecture.png)\n",
        "\n",
        "**\"Attention Is All You Need\"** is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique's potential for other tasks like question answering and what is now known as multimodal Generative AI.\n",
        "\n",
        "The paper's title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper's authors, liked the sound of that word.\n",
        "\n",
        "An early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\n",
        "\n",
        "Some early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\n",
        "\n",
        "As of 2024, the paper has been cited more than 140,000 times.\n",
        "\n",
        "**Authors**\n",
        "\n",
        "The authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" to the paper; the listed order was randomized. The Wired article highlights the group's diversity:\n",
        "\n",
        "Six of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.\n",
        "\n",
        "**Methods discussed & introduced**\n",
        "\n",
        "The paper is most well known for the introduction of the Transformer architecture, which forms the underlying architecture for most forms of modern Large Language Models (LLMs). A key reason for why the architecture is preferred by most modern LLMs is the parallelizability of the architecture over its predecessors. This ensures that the operations necessary for training can be accelerated on a GPU allowing both faster training times and models of bigger sizes to be trained.\n",
        "\n",
        "The following mechanisms were introduced by the paper as part of the development of the transformer architecture.\n",
        "\n",
        "**Scaled dot-product attention & self-attention**\n",
        "\n",
        "The use of the scaled dot-product attention and self-attention mechanism instead of an RNN or LSTM (which rely on recurrence instead) allow for better performance as described in the following paragraph.\n",
        "\n",
        "Since the model relies on Query (Q), Key (K) and Value (V) matrices that come from the same source itself (i.e. the input sequence / context window), this eliminates the need for RNNs completely ensuring parallelizability for the architecture. This differs from the original form of the Attention mechanism introduced in 2014. Additionally, the paper also discusses the use of an additional scaling factor that was found to be most effective with respect to the dimension of the key vectors.\n",
        "\n",
        "In the specific context of translation which the paper focused on, the Query and Key matrices are usually represented in embeddings corresponding to the source language while the Value matrix corresponds to the target language.\n",
        "\n",
        "**Multi-head attention**\n",
        "\n",
        "In the self-attention mechanism, queries (Q), keys (K), and values (V) are dynamically generated for each input sequence (limited typically by the size of the context window), allowing the model to focus on different parts of the input sequence at different steps. Multi-head attention enhances this process by introducing multiple parallel attention heads. Each attention head learns different linear projections of the Q, K, and V matrices. This allows the model to capture different aspects of the relationships between words in the sequence simultaneously, rather than focusing on a single aspect.\n",
        "\n",
        "By doing this, multi-head attention ensures that the input embeddings are updated from a more varied and diverse set of perspectives. After the attention outputs from all heads are calculated, they are concatenated and passed through a final linear transformation to generate the output.\n",
        "\n",
        "**Positional encoding**\n",
        "\n",
        "Since the Transformer model is not a seq2seq model and does not rely on the sequence of the text in order to perform encoding and decoding, the paper relied on the use of sine and cosine wave functions to encode the position of the token into the embedding.\n",
        "\n",
        "** Historical context**\n",
        "\n",
        "For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\n",
        "\n",
        "A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers. However, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.\n",
        "\n",
        "Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer."
      ],
      "metadata": {
        "id": "RYHW6OYFnjoU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zuuvXsWDpNuS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}