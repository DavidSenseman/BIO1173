{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "## **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* Part 4.1: Introduction to Large Language Models (LLMs)\n",
        "* **Part 4.2: Chatbots**\n",
        "* Part 4.3: Image Generation with StableDiffusion\n",
        "* Part 4.4: Image Generation with DALL-E\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded.\n"
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Your OPENAI_API_KEY\n",
        "\n",
        "In order to run the code in this lesson you will need to have your secret `OEPNAI_API_KEY` installed in your **Secrets** on this Colab notebook. Detailed steps for purchasing your `OPENAI_API_KEY` and installing it in your Colab notebook Secrets was provide in `Class_04_1`.\n",
        "\n",
        "Run the code in the next cell to see if your `OPENAI_API_KEY` is installed correctly. You make have to Grant Access for your notebook to use your API key."
      ],
      "metadata": {
        "id": "v7QopUS2wT9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for OpenAI API Key\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Create local API variable\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Retrieve the OpenAI API key and store it in a variable\n",
        "#OPENAI_KEY = userdata.get('OPENAI_KEY')\n",
        "\n",
        "# Ensure that the API key is correctly set\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\")\n",
        "else:\n",
        "  print(f\"Your secret OPENAI_API_KEY =\", OPENAI_KEY)"
      ],
      "metadata": {
        "id": "Gi0ydJubCZsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you OPENAI_API_KEY is correctly installed, you should see\n",
        "~~~type\n",
        "Your secret OPENAI_API_KEY = sk-proj-DHILXTfLOz.....\n",
        "~~~\n",
        "\n",
        "However, if the output says\n",
        "```type\n",
        "\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\n",
        "```\n",
        "You must correct this error before you can continue with this lesson. Again, details instructions were given in Class_04_1."
      ],
      "metadata": {
        "id": "P0NXL1s6cKqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `LangChain` packages\n",
        "\n",
        "Run the code in the following cell to install the `langchain-openai` and related packages."
      ],
      "metadata": {
        "id": "DJbJtTtexCNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install langchain-openai package\n",
        "\n",
        "!pip install -q langchain langchain_openai openai pydub > /dev/null"
      ],
      "metadata": {
        "id": "uLrdjGVzFgVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any output."
      ],
      "metadata": {
        "id": "IqvEOnmrdGVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Speech Processing**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/CourseImage.gif)\n",
        "\n",
        "In this lesson, we explore how to use both computer-generated voice and voice recognition to create a `ChatBot`. We'll be working with the `OpenAI API` to achieve this. Specifically, we'll demonstrate how to input normal text and have it spoken by the computer, and conversely, how we can speak to the computer and have it respond. We'll ultimately integrate these functionalities to create a chatbot that handles both text-to-speech and speech-to-text interactions.\n",
        "\n",
        "While we'll use Google `Colab` for this demonstration, in production environments, you'd likely use a mobile app or a web-based JavaScript solution, as each platform handles voice differently. We'll focus on keeping things generic and simple in Colab for now.\n",
        "\n",
        "Voice applications are everywhere. For example, I can ask \"`Alexa`, what time is it?\" and multiple `Alexa` devices in my home will respond, although not always perfectly. I usually mute them during recording sessions. Applications like `Siri` or even `ChatGPT` also offer voice interactions. For instance, when you click the voice option in `ChatGPT` on a computer, it starts listening for your input.\n",
        "\n",
        "To illustrate, I asked `ChatGPT`, \"How are you doing?\" and it responded by offering some insightful thoughts about generative AI. It highlighted that generative AI isn't just about creating new content but about learning patterns from vast amounts of data and applying them creatively across text, images, and code. It also suggested that students experiment with different approaches, as hands-on experience is one of the best ways to learn."
      ],
      "metadata": {
        "id": "opMlmCu1p6z8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbUFwvHyqQ8D"
      },
      "source": [
        "## **Part I: Speech to Text**\n",
        "\n",
        "Here we delve into the realm of speech-to-text technology, focusing on the powerful capabilities offered by OpenAI's models. Speech-to-text, also known as automatic speech recognition (ASR), is a technology that converts spoken language into written text. OpenAI's speech-to-text models represent the cutting edge of this field, leveraging advanced machine learning techniques to achieve high accuracy and robustness across various accents, languages, and acoustic environments. We'll explore how these models can be integrated into applications to enable voice-based interactions, transcription services, and accessibility features. By harnessing OpenAI's speech-to-text technology, we'll unlock new possibilities for human-computer interaction and demonstrate how to transform audio input into actionable text data with remarkable precision.\n",
        "\n",
        "\n",
        "Note we will make use of the technique described here to record audio in CoLab.\n",
        "\n",
        "https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGoIgiyQKGcK"
      },
      "source": [
        "## **Summary of the Audio Recording Setup in Google Colab**\n",
        "\n",
        "This code in the cell below sets up the ability to **record audio from the user's microphone** in a **Google Colab notebook** using JavaScript and Python. We need to use JavaScript since Google Colab doesn't allow direct audio recording from a user's microphone primarily due to browser security restrictions and Colab's architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import io\n",
        "from IPython.display import Audio\n",
        "\n",
        "from pydub import AudioSegment\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "    const reader = new FileReader()\n",
        "    reader.onloadend = e => resolve(e.srcElement.result)\n",
        "    reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "    stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "    recorder = new MediaRecorder(stream)\n",
        "    chunks = []\n",
        "    recorder.ondataavailable = e => chunks.push(e.data)\n",
        "    recorder.start()\n",
        "    await sleep(time)\n",
        "    recorder.onstop = async ()=>{\n",
        "        blob = new Blob(chunks)\n",
        "        text = await b2text(blob)\n",
        "        resolve(text)\n",
        "    }\n",
        "    recorder.stop()\n",
        "})\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CDuHp3tPNYTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create `record()` function**\n",
        "\n",
        "Here is a step-by-step explaination of the `record()` function created in the next code cell.\n",
        "\n",
        "**1. Start Recording**\n",
        "The function prints a message indicating the start of recording for the specified number of seconds.\n",
        "\n",
        "**2. Inject JavaScript**\n",
        "The RECORD JavaScript code is injected into the notebook using display(Javascript(RECORD)). This code handles microphone access and audio recording in the browser.\n",
        "\n",
        "**3. Execute JavaScript and Retrieve Audio**\n",
        "The `output.eval_js()` function runs the `JavaScript record()` function for the specified duration (converted to milliseconds). It returns a Base64-encoded string representing the recorded audio.\n",
        "\n",
        "**4. Decode Audio Data**\n",
        "The Base64 string is split to isolate the encoded audio portion, which is then decoded into binary format using b64decode.\n",
        "\n",
        "**5. Convert to AudioSegment**\n",
        "The binary audio data is wrapped in a BytesIO stream and passed to AudioSegment.from_file() to create an audio object. The input format is specified as \"webm\".\n",
        "\n",
        "**6. Export as WAV File**\n",
        "The audio is exported and saved locally as \"recorded_audio.wav\" in WAV format.\n",
        "\n",
        "**7. Return Audio Object**\n",
        "The function prints a confirmation message and returns the AudioSegment object for further use (e.g., playback, analysis, or visualization).\n",
        "\n"
      ],
      "metadata": {
        "id": "xEFmTXEkYSCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create record() function\n",
        "\n",
        "def record(seconds=3):\n",
        "    print(f\"Recording now for {seconds} seconds...\")\n",
        "    display(Javascript(RECORD))\n",
        "    s = output.eval_js(f'record({seconds * 1000})')\n",
        "    binary = b64decode(s.split(',')[1])\n",
        "\n",
        "    # Convert to AudioSegment\n",
        "    audio = AudioSegment.from_file(io.BytesIO(binary), format=\"webm\")\n",
        "\n",
        "    # Export as WAV\n",
        "    audio.export(\"recorded_audio.wav\", format=\"wav\")\n",
        "    print(\"Recording complete. Audio saved as 'recorded_audio.wav'\")\n",
        "    return audio"
      ],
      "metadata": {
        "id": "FU1Wtv0DMY4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBwAnTyBKLxP"
      },
      "source": [
        "### Example 1: Record and Play Audio\n",
        "\n",
        "The code in the cell below uses the `record()` function to record a `5 second` audio file and then plays it back.\n",
        "\n",
        "You may need to grant permission for the program to access the microphone on your laptop or computer.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image07A.png)\n",
        "\n",
        "### **IMPORTANT--Read the Following**\n",
        "\n",
        "You will very likely encounted a problem running the code in the next cell. After you hit the run button, the code will likely `hang up` and continue to run indefinitely.\n",
        "\n",
        "In order to sucessfully run Example 1, you have to stop the code by clicking on the spinning run cell icon ![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png). This will generate an error message. Once the error message is displayed, just hit the run button again, and the code should execute corrrectly.\n",
        "\n",
        "Once you hit the run cell icon\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png)\n",
        " a second time, speak into your microphone to record your voice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeO-I6RIOIzA"
      },
      "outputs": [],
      "source": [
        "# Example 1: Record and play audio\n",
        "\n",
        "# Set recording duration\n",
        "record_duration=5\n",
        "\n",
        "# Record audio\n",
        "audio = record(record_duration)\n",
        "display(Audio(\"recorded_audio.wav\", autoplay=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image02A.png)"
      ],
      "metadata": {
        "id": "6S5IbInwcIEg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LciCRMKjCF8Z"
      },
      "source": [
        "### **Exercise 1: Record and Play Audio**\n",
        "\n",
        "In the cell below, use the `record()` function to record an `8 second` audio file and then play it back. Once you hit the run cell icon\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png)speak into your microphone.\n",
        "\n",
        "You should not encounter the same problem as in Example 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyQLLdbrCF8Z"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1 here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image04A.png)"
      ],
      "metadata": {
        "id": "1uZaGQdjCF8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OpenAI Speech-to-Text API**\n",
        "\n",
        "### Overview of the API\n",
        "- **Models**: Includes `Whisper` and newer `GPT-4o-based` models.\n",
        "- **Input**: Accepts audio files (e.g., MP3, WAV, MP4).\n",
        "- **Output**: Returns transcriptions in formats like plain text, JSON, or subtitle formats (SRT, VTT).\n",
        "- **Languages**: Supports dozens of languages with high accuracy, especially in noisy or accented speech environments.\n",
        "\n",
        "### Why It’s Useful for Biomedical Investigators\n",
        "\n",
        "1. **Transcribing Interviews & Focus Groups**  \n",
        "   Automatically convert recorded conversations with patients, clinicians, or research participants into text for qualitative analysis.\n",
        "\n",
        "2. **Clinical Note Dictation**  \n",
        "   Researchers can dictate observations or notes during fieldwork or lab work, streamlining documentation.\n",
        "\n",
        "3. **Meeting & Conference Transcripts**  \n",
        "   Capture and archive discussions from research meetings, seminars, or collaborative calls.\n",
        "\n",
        "4. **Data Extraction from Audio**  \n",
        "   Enables downstream NLP tasks like identifying social determinants of health (SDOH) or extracting biomedical entities from spoken content.\n",
        "\n",
        "5. **Multilingual Support**  \n",
        "   Useful in global health research where interviews or data collection occur in multiple languages.\n",
        "\n",
        "\n",
        "This code in the cell below demonstrates how to use OpenAI's speech-to-text API to transcribe audio files. It defines a function `transcribe_audio` that takes a filename as input. The function opens the specified audio file in binary mode and uses the OpenAI client to create a transcription. The `client.audio.transcriptions.create() method` is called with two parameters: the model (\"whisper-1\") and the audio file. `Whisper` is OpenAI's state-of-the-art speech recognition model, known for its robustness across various languages and accents. The function returns the transcribed text. In the example usage, an audio file named \"recorded_audio.wav\" is transcribed, and the resulting text is printed. This code provides a simple yet powerful way to convert speech to text, which can be invaluable for tasks such as generating subtitles, creating searchable archives of audio content, or enabling voice commands in applications.\n"
      ],
      "metadata": {
        "id": "nD3zE5yVE0vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `transcribe_audio()` function\n",
        "\n",
        "Run the code in the next cell to create the `transcribe_audio(filename)` function. This Python function is designed to transcribe spoken words from an audio file into text using OpenAI's `Whisper` model via their API.\n",
        "\n",
        "Here's a breakdown of what it does:\n",
        "\n",
        "* Opens the audio file specified by filename in binary read mode (\"rb\").\n",
        "* Sends the file to the OpenAI Whisper model (whisper-1) using the client.audio.transcriptions.create() method.\n",
        "* Receives the transcription result from the model.\n",
        "* Returns the transcribed text from the audio."
      ],
      "metadata": {
        "id": "aoRtz5uizINp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function\n",
        "\n",
        "def transcribe_audio(filename):\n",
        "    with open(filename, \"rb\") as audio_file:\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file\n",
        "        )\n",
        "    return transcription.text"
      ],
      "metadata": {
        "id": "7a_uR6zOMN_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jurTAwUCKRM-"
      },
      "source": [
        "## Example 2: Speech-to-Text\n",
        "\n",
        "This code in the cell below uses the `transcribe_audio()` function to convert your voice into text.\n",
        "\n",
        "Once you hit the run cell icon\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png)start counting out loud from `1` to `10`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Speech-to-Text\n",
        "\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Set recording duration\n",
        "record_duration=5\n",
        "\n",
        "# Record audio\n",
        "audio = record(record_duration)\n",
        "\n",
        "# Get the secret from the environment\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# Transcribe the audio file\n",
        "transcription = transcribe_audio(\"recorded_audio.wav\")\n",
        "print(\"Transcription:\")\n",
        "print(transcription)\n"
      ],
      "metadata": {
        "id": "reVITraVKsiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image11A.png)"
      ],
      "metadata": {
        "id": "upnn3_LkM4vI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCA54j6KNooN"
      },
      "source": [
        "## **Exercise 2: Speech-to-Text**\n",
        "\n",
        "In the cell below, write to code to generate Speech-to-Text using the code in Example 2 as an template.\n",
        "\n",
        "For **Exercise 2**, once you hit the run cell icon\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png)start counting **_backwards_** from `10` to `1`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n"
      ],
      "metadata": {
        "id": "SmzMDOmqNooO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image12A.png)"
      ],
      "metadata": {
        "id": "HwJujPEYNooO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2MPPX0c1pHi"
      },
      "source": [
        "## **Part 2: Text to Speech**\n",
        "\n",
        "In Part 2, we'll explore the fascinating world of text-to-speech (TTS) Large Language Models (LLMs), focusing on OpenAI's cutting-edge offerings. We'll primarily utilize OpenAI's `TTS-1` model, a powerful and versatile tool designed for converting written text into natural-sounding speech.\n",
        "\n",
        "`TTS-1` is optimized for real-time applications, making it ideal for scenarios that require low-latency audio generation. This model represents a significant advancement in speech synthesis technology, leveraging deep learning techniques to produce high-quality, lifelike vocal outputs. By delving into TTS-1, we'll explore its capabilities, examine its practical applications, and understand how it's revolutionizing various industries, from accessibility solutions to interactive voice responses and beyond.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDOMjRUP1fdv"
      },
      "source": [
        "## **OpenAI's Voices**\n",
        "\n",
        "When using `OpenAI's` text-to-speech API to generate spoken audio from text, you can select one of several voices. The `openai.audio.speech.create()` function is called with three parameters: the model (\"tts-1\"), the voice (e.g. \"alloy\"), and the input text. `OpenAI` offers several voice options, including:\n",
        "\n",
        "* **alloy** - neutral\n",
        "* **echo** - young\n",
        "* **fable** - male\n",
        "* **onyx** - deep male\n",
        "* **nova** - female\n",
        "* **shimmer** - warm female\n",
        "\n",
        "Each voice has its unique characteristics, allowing users to choose the most suitable one for their application. Additionally, `OpenAI` provides a high-definition model called `tts-1-hd` for enhanced audio quality, though it may have higher latency. The function returns a response object, from which the audio content is extracted and stored in the `audio_data variable` for further processing or playback."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Demonstrate Different Voices\n",
        "\n",
        "The code in the cell below demonstates 3 of the different voices that are available in the `OpenAI` text-to-speech API:\n",
        "\n",
        "* **alloy**\n",
        "* **echo**\n",
        "* **fable**\n",
        "\n",
        "Run the code cell to hear each of these three voices."
      ],
      "metadata": {
        "id": "mPOHlT667J9s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eUftPM_EHRc"
      },
      "outputs": [],
      "source": [
        "# Example 3: Demonstate different voices\n",
        "\n",
        "import io\n",
        "from openai import OpenAI\n",
        "from IPython.display import Audio, display\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# Define voices\n",
        "voices = [\"alloy\", \"echo\", \"fable\"]\n",
        "audio_segments = []\n",
        "\n",
        "# Loop through voices\n",
        "for voice in voices:\n",
        "    text = f\"Hello, Welcome to BIO 1 1 7 3...Introduction to Computational Biology..., I am the {voice} voice.\"\n",
        "    response = client.audio.speech.create(\n",
        "        model=\"tts-1\",\n",
        "        voice=voice,\n",
        "        input=text\n",
        "    )\n",
        "    audio_segments.append(response.content)\n",
        "\n",
        "# Combine audio segments\n",
        "from pydub import AudioSegment\n",
        "\n",
        "combined_audio = AudioSegment.empty()\n",
        "for segment in audio_segments:\n",
        "    audio = AudioSegment.from_mp3(io.BytesIO(segment))\n",
        "    combined_audio += audio\n",
        "\n",
        "# Convert the combined audio to a byte stream\n",
        "buffer = io.BytesIO()\n",
        "combined_audio.export(buffer, format=\"mp3\")\n",
        "buffer.seek(0)\n",
        "\n",
        "# Play the audio in Colab\n",
        "print(\"Playing audio:\")\n",
        "display(Audio(buffer.read(), autoplay=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image13A.png)\n",
        "\n",
        "You should have heard 3 different voices speaking."
      ],
      "metadata": {
        "id": "9GhWUHcZ5ULL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Demonstrate Different Voices**\n",
        "\n",
        "In the cell below, write the code to demonstate the other 3 voices that are available:\n",
        "\n",
        "* **onyx**\n",
        "* **nova**\n",
        "* **shimmmer**\n",
        "\n",
        "You should use the code in Example 3 as a template."
      ],
      "metadata": {
        "id": "vY_hEkxq90M8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSyBMbdy90M9"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image13A.png)\n",
        "\n",
        "You should have heard 3 different voices speaking."
      ],
      "metadata": {
        "id": "xq9abR-C7eYE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZPN9RExHnaQ"
      },
      "source": [
        "### Create Functions\n",
        "\n",
        "The code in the next cell to create two functions (1) `generate text()` and `speak_text()`.\n",
        "\n",
        "### **generate_text(text, voice)**\n",
        "\n",
        "This function uses OpenAI's Text-to-Speech (TTS) API to generate spoken audio from a given text.\n",
        "\n",
        "**1. Inputs:**\n",
        "\n",
        "* * **text:** The string you want to convert to speech.\n",
        "* * **voice:** The voice model to use (e.g., \"alloy\", \"echo\", \"fable\", etc.).\n",
        "\n",
        "**2. API Call:**\n",
        "\n",
        "* * It sends the text and voice to the OpenAI TTS model (tts-1) using client.audio.speech.create.\n",
        "\n",
        "**3. Returns:**\n",
        "\n",
        "The raw audio data (response.content) that contains the spoken version of the input text.\n",
        "\n",
        "### **speak_text(text, voice)**\n",
        "\n",
        "This function builds on `generate_text` to play the generated speech audio.\n",
        "\n",
        "**1.** Calls `generate_text(text, voice)` to get the audio data.\n",
        "\n",
        "**2.** Uses `IPython.display.Audio` to play the audio directly in a Jupyter notebook or similar environment.\n",
        "* * **autoplay=True** ensures the audio starts playing immediately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7hqIzt93rhe"
      },
      "outputs": [],
      "source": [
        "# Create functions\n",
        "\n",
        "def generate_text(text,voice):\n",
        "    response = client.audio.speech.create(\n",
        "        model=\"tts-1\",\n",
        "        voice=voice,\n",
        "        input=text\n",
        "    )\n",
        "    audio_data = response.content\n",
        "    return audio_data  # Return the audio data directly\n",
        "\n",
        "def speak_text(text,voice):\n",
        "    audio_data = generate_text(text,voice)\n",
        "    display(Audio(audio_data, autoplay=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Transcribe Recorded Data\n",
        "\n",
        "The code in the cell shows how to record your speech, print out a transcription of what you said, and finally, read the transcription using the \"alloy\" voice.\n",
        "\n",
        "Once you hit the run cell icon\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image10A.png) read out loud Carl Sandburg’s poem “Fog” --a short, imagistic piece that captures the quiet, mysterious arrival of fog.\n",
        "\n",
        "```text\n",
        "FOG\n",
        "\n",
        "The fog comes\n",
        "on little cat feet.\n",
        "\n",
        "It sits looking\n",
        "over harbor and city\n",
        "on silent haunches\n",
        "and then moves on.\n",
        "```"
      ],
      "metadata": {
        "id": "8oHJAsCA5qHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Transcribe recorded audio\n",
        "\n",
        "# Define voice\n",
        "voice=\"alloy\"\n",
        "\n",
        "# Deine recording duration\n",
        "duration=20\n",
        "\n",
        "# Record audio\n",
        "audio_data = record(duration)\n",
        "\n",
        "# Transcribe audio\n",
        "transcription = transcribe_audio(\"recorded_audio.wav\")\n",
        "print(\"Transcription:\")\n",
        "print(transcription)\n",
        "\n",
        "# Speak transciption\n",
        "speak_text(transcription,voice)"
      ],
      "metadata": {
        "id": "aWV9LUMC4Nj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image14A.png)\n",
        "\n",
        "And hear recording of you reading the poem."
      ],
      "metadata": {
        "id": "xNDmAFlBCv0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Transcribe Recorded Data**\n",
        "\n",
        "In the cell below, write the code to record your speech, print out a transcription of what you said, and finally, read the transcription using the \"alloy\" voice.\n",
        "\n",
        "After you start running the cell, start reading _The Red Wheelbarrow_ by William Carlos Williams. Like _Fog_, it’s a minimalist, imagist poem that captures a vivid moment with few words:\n",
        "\n",
        "```text\n",
        "The Red Wheelbarrow\n",
        "\n",
        "so much depends\n",
        "upon\n",
        "\n",
        "a red wheel\n",
        "barrow\n",
        "\n",
        "glazed with rain\n",
        "water\n",
        "\n",
        "beside the white\n",
        "chickens.\n",
        "```"
      ],
      "metadata": {
        "id": "ttTsJ-QADFd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4 here\n"
      ],
      "metadata": {
        "id": "DD1z6Q7ZDFd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image09A.png)\n",
        "\n",
        "And hear you reading the poem."
      ],
      "metadata": {
        "id": "ZIAxHm1sDFd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Audio File\n",
        "\n",
        "The code in the next cell shows how to convert an audio recording to an MP3 audio file."
      ],
      "metadata": {
        "id": "rOTFMEWS_NwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert audio to MP3 format\n",
        "\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Assuming audio_data is your AudioSegment object\n",
        "audio_data.export(\"audio.mp3\", format=\"mp3\")\n"
      ],
      "metadata": {
        "id": "C2RaibPRAu_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image15A.png)"
      ],
      "metadata": {
        "id": "CtnL_Fw8BYBn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZD654hWHWqT"
      },
      "source": [
        "### Download Audio File\n",
        "\n",
        "Once the audio file has been converted into the MP3 format, the code in the cell below shows how to download the file to your laptop or computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8SH6KRCWBYeQ",
        "outputId": "391695e0-55a9-4018-93da-8b93e066668b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fe747aad-1eb5-4640-b1e7-8847189945f4\", \"audio.mp3\", 160365)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download audio file\n",
        "\n",
        "from google.colab import files\n",
        "files.download('audio.mp3')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is corect you should have seen a pop-up window on your laptop or computer\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image16A.png)"
      ],
      "metadata": {
        "id": "R4_uXpdqCblr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: Chatbots**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_3_image10A.png)\n",
        "\n",
        "The history of **chatbots** is a fascinating journey through the evolution of artificial intelligence and human-computer interaction. Here's a brief overview:\n",
        "\n",
        "* **1. The Early Days (1950s-1970s)**\n",
        "1950 - Alan Turing's \"Imitation Game\": Turing proposed a test (now known as the Turing Test) to determine if a machine could exhibit intelligent behavior indistinguishable from a human.\n",
        "1966 - ELIZA: Created by Joseph Weizenbaum at MIT, ELIZA was the first chatbot. It mimicked a Rogerian psychotherapist by rephrasing user input into questions. It was simple but groundbreaking.\n",
        "1972 - PARRY: Developed by Kenneth Colby, PARRY simulated a person with paranoid schizophrenia. It was more complex than ELIZA and could hold more realistic conversations.\n",
        "* **2. Rule-Based Systems (1980s-1990s)**\n",
        "Chatbots during this era used hand-coded rules and decision trees.\n",
        "They were mostly used in academic research, customer service, and early virtual assistants.\n",
        "Examples include Jabberwacky (late 1980s), which aimed to simulate natural human chat through learning.\n",
        "* **3. Rise of the Internet and AI (2000s)**\n",
        "SmarterChild (2001): A popular chatbot on AOL Instant Messenger and MSN Messenger. It could answer questions, play games, and chat casually.\n",
        "ALICE (Artificial Linguistic Internet Computer Entity): Created by Richard Wallace, it won the Loebner Prize (a Turing Test competition) multiple times.\n",
        "* **4. Machine Learning and NLP Boom (2010s)**\n",
        "2011 - Siri: Apple introduced Siri, a voice-activated assistant that brought chatbots into the mainstream.\n",
        "2014 - Alexa and Cortana: Amazon and Microsoft launched their own virtual assistants.\n",
        "2016 - Facebook Messenger Bots: Facebook opened its platform to developers, leading to a surge in chatbot development for businesses.\n",
        "* **5. Neural Networks and Transformers (Late 2010s-2020s)**\n",
        "2018 – BERT (Google) and GPT (OpenAI): These transformer-based models revolutionized natural language understanding and generation.\n",
        "2020 – GPT-3: A massive leap in chatbot capabilities, enabling more coherent, context-aware, and human-like conversations.\n",
        "2022 – ChatGPT: OpenAI released ChatGPT based on GPT-3.5 and later GPT-4, making advanced conversational AI widely accessible.\n",
        "* **6. The Present and Future (2020s-Today)**\n",
        "Chatbots are now integrated into education, healthcare, customer service, entertainment, and more.\n",
        "Multimodal models (like GPT-4 and beyond) can understand text, images, and even audio.\n",
        "The focus is shifting toward personalization, emotional intelligence, and ethical AI."
      ],
      "metadata": {
        "id": "7qM-H5TrEoBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `Chatbot` Class\n",
        "\n",
        "The code in the next cell creates class called `Chatbot`. In Python, functions and classes are both fundamental building blocks, but they serve different purposes. A Python **`function`** is a reusable block of code that performs a specific task. On the other hand, a Python **`class`** is a blueprint for creating objects. It groups related data and behaviors together.\n",
        "\n",
        "**Key Features of a `Class`**\n",
        "\n",
        "* Encapsulates data (attributes) and functions (methods) that operate on that data.\n",
        "* Supports object-oriented programming (OOP).\n",
        "* Allows for inheritance, encapsulation, and polymorphism."
      ],
      "metadata": {
        "id": "nDnv5hKGDQwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Chatbot Class\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts.chat import PromptTemplate\n",
        "from IPython.display import display_markdown\n",
        "import pickle\n",
        "\n",
        "DEFAULT_TEMPLATE = \"\"\"You are a helpful assistant. DO not use markdown, just regular text.\n",
        "limit your response to just a few sentences. If the user says something that indicates\n",
        "that they wish to end the chat return just \"bye\" (no quotes), so I can end the loop.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "AI:\"\"\"\n",
        "\n",
        "MODEL = 'gpt-4o-mini'\n",
        "\n",
        "class ChatBot:\n",
        "    def __init__(self, llm_chat, llm_summary, template):\n",
        "        \"\"\"\n",
        "        Initializes the ChatBot with language models and a template for conversation.\n",
        "\n",
        "        :param llm_chat: A large language model for handling chat responses.\n",
        "        :param llm_summary: A large language model for summarizing conversations.\n",
        "        :param template: A string template defining the conversation structure.\n",
        "        \"\"\"\n",
        "        self.llm_chat = llm_chat\n",
        "        self.llm_summary = llm_summary\n",
        "        self.template = template\n",
        "        self.prompt_template = PromptTemplate(input_variables=[\"history\", \"input\"], template=self.template)\n",
        "\n",
        "        # Initialize memory and conversation chain\n",
        "        self.memory = ConversationSummaryMemory(llm=self.llm_summary)\n",
        "        self.conversation = ConversationChain(\n",
        "            prompt=self.prompt_template,\n",
        "            llm=self.llm_chat,\n",
        "            memory=self.memory,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        self.history = []\n",
        "\n",
        "    def converse(self, prompt):\n",
        "        \"\"\"\n",
        "        Processes a conversation prompt and updates the internal history and memory.\n",
        "\n",
        "        :param prompt: The input prompt from the user.\n",
        "        :return: The generated response from the language model.\n",
        "        \"\"\"\n",
        "        self.history.append([self.memory.buffer, prompt])\n",
        "        output = self.conversation.invoke(prompt)\n",
        "        return output['response']\n",
        "\n",
        "    def chat(self, prompt):\n",
        "        \"\"\"\n",
        "        Handles the full cycle of receiving a prompt, processing it, and displaying the result.\n",
        "\n",
        "        :param prompt: The input prompt from the user.\n",
        "        \"\"\"\n",
        "        print(f\"Human: {prompt}\")\n",
        "        output = self.converse(prompt)\n",
        "        display_markdown(output, raw=True)\n",
        "\n",
        "    def print_memory(self):\n",
        "        \"\"\"\n",
        "        Displays the current state of the conversation memory.\n",
        "        \"\"\"\n",
        "        print(\"**Memory:\")\n",
        "        print(self.memory.buffer)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        \"\"\"\n",
        "        Clears the conversation memory.\n",
        "        \"\"\"\n",
        "        self.memory.clear()\n",
        "\n",
        "    def undo(self):\n",
        "        \"\"\"\n",
        "        Reverts the conversation memory to the state before the last interaction.\n",
        "        \"\"\"\n",
        "        if len(self.history) > 0:\n",
        "            self.memory.buffer = self.history.pop()[0]\n",
        "        else:\n",
        "            print(\"Nothing to undo.\")\n",
        "\n",
        "    def regenerate(self):\n",
        "        \"\"\"\n",
        "        Re-executes the last undone interaction, effectively redoing an undo operation.\n",
        "        \"\"\"\n",
        "        if len(self.history) > 0:\n",
        "            self.memory.buffer, prompt = self.history.pop()\n",
        "            self.chat(prompt)\n",
        "        else:\n",
        "            print(\"Nothing to regenerate.\")\n",
        "\n",
        "    def save_history(self, file_path):\n",
        "        \"\"\"\n",
        "        Saves the conversation history to a file using pickle.\n",
        "\n",
        "        :param file_path: The file path where the history should be saved.\n",
        "        \"\"\"\n",
        "        with open(file_path, 'wb') as f:\n",
        "            pickle.dump(self.history, f)\n",
        "\n",
        "    def load_history(self, file_path):\n",
        "        \"\"\"\n",
        "        Loads the conversation history from a file using pickle.\n",
        "\n",
        "        :param file_path: The file path from which to load the history.\n",
        "        \"\"\"\n",
        "        with open(file_path, 'rb') as f:\n",
        "            self.history = pickle.load(f)\n",
        "            # Optionally reset the memory based on the last saved state\n",
        "            if self.history:\n",
        "                self.memory.buffer = self.history[-1][0]"
      ],
      "metadata": {
        "id": "Osk7h7DzEzY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see an output."
      ],
      "metadata": {
        "id": "pqNFyGTNEoUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Communicate with LLM\n",
        "\n",
        "Now that we have create our `ChatBot` class, we can use it to communicate with a LLM. The code in the next cell shows how to converse with the `gpt-4p-mini` LLM."
      ],
      "metadata": {
        "id": "aY-NP5NXEvD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Communicate with llm\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "MODEL = 'gpt-4o-mini'\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=MODEL,\n",
        "    temperature=0.3,\n",
        "    openai_api_key=OPENAI_KEY\n",
        ")\n",
        "\n",
        "c = ChatBot(llm, llm, DEFAULT_TEMPLATE)\n",
        "\n",
        "response = c.converse(\"Hello, my name is Rowdy the Roadrunner.\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "_81oadtqeOZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image17A.png)\n",
        "\n",
        "Ignore any warnings that you might receive."
      ],
      "metadata": {
        "id": "q1sQ7f_De19l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Communicate with LLM**\n",
        "\n",
        "In the next cell write the code needed to introduce yourself to the `gpt-4p-mini` LLM. In other words use your first name instead of `Rowdy the Roadrunner`. shows how to converse with"
      ],
      "metadata": {
        "id": "q3D2M9AAfK0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6 here\n"
      ],
      "metadata": {
        "id": "jbA9DYGlfK0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image18A.png)\n",
        "\n",
        "except your first name should appear instead of `David` (unless your first is `David`)."
      ],
      "metadata": {
        "id": "yFACGf3EgCyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integrate OpenAI's Whisper and TTS (Text-to-Speech) with JavaScript\n",
        "\n",
        "The Python code in the cell below integrates **OpenAI's Whisper and TTS (Text-to-Speech)** models with **Google Colab's JavaScript capabilities** to create an interactive voice interface.\n",
        "\n",
        "**Why Use JavaScript for Microphone Input in Colab?**\n",
        "\n",
        "Google Colab runs Python code on a remote server, not on your local machine.\n",
        "\n",
        "This means:\n",
        "\n",
        "* Python in Colab **cannot directly access your hardware**, like your microphone or webcam.\n",
        "\n",
        "* However, **JavaScript runs in your browser**, which can access local devices (with permission).\n",
        "\n",
        "**What JavaScript Enables**\n",
        "\n",
        "By embedding JavaScript in a Colab cell, you can:\n",
        "\n",
        "* Prompt the user for microphone access.\n",
        "* Record audio using the browser's MediaRecorder API.\n",
        "* Convert the audio to a base64 string.\n",
        "* Send that string back to Python for processing."
      ],
      "metadata": {
        "id": "oJoS0_lsgsRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrate TTS with JavaScript\n",
        "\n",
        "from openai import OpenAI\n",
        "from IPython.display import Javascript, Audio, display, HTML\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import io\n",
        "import time\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "    const reader = new FileReader()\n",
        "    reader.onloadend = e => resolve(e.srcElement.result)\n",
        "    reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "    stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "    recorder = new MediaRecorder(stream)\n",
        "    chunks = []\n",
        "    recorder.ondataavailable = e => chunks.push(e.data)\n",
        "    recorder.start()\n",
        "    await sleep(time)\n",
        "    recorder.onstop = async ()=>{\n",
        "        blob = new Blob(chunks)\n",
        "        text = await b2text(blob)\n",
        "        resolve(text)\n",
        "    }\n",
        "    recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def generate_text(text):\n",
        "    response = client.audio.speech.create(\n",
        "        model=\"tts-1\",\n",
        "        voice=\"nova\",\n",
        "        input=text\n",
        "    )\n",
        "    audio_data = response.content\n",
        "    return audio_data  # Return the audio data directly\n",
        "\n",
        "def speak_text(text):\n",
        "    audio_data = generate_text(text)\n",
        "\n",
        "    # Generate a unique ID for this audio element\n",
        "    audio_id = f\"audio_{uuid.uuid4().hex}\"\n",
        "\n",
        "    # Display the audio with the unique ID\n",
        "    display(Audio(audio_data, autoplay=True, element_id=audio_id))\n",
        "\n",
        "    # Create a hidden div to store the audio status\n",
        "    status_div = f'<div id=\"{audio_id}_status\" style=\"display: none;\">playing</div>'\n",
        "    display(HTML(status_div))\n",
        "\n",
        "    # JavaScript to handle audio playback and status\n",
        "    js_code = f\"\"\"\n",
        "    var audioElement = document.getElementById('{audio_id}');\n",
        "    if (audioElement) {{\n",
        "        audioElement.onended = function() {{\n",
        "            document.getElementById('{audio_id}_status').textContent = 'finished';\n",
        "        }};\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the JavaScript\n",
        "    display(HTML(f\"<script>{js_code}</script>\"))\n",
        "\n",
        "    # Wait for the audio to finish\n",
        "    while True:\n",
        "        status = eval_js(f\"document.getElementById('{audio_id}_status').textContent\")\n",
        "        if status == 'finished':\n",
        "            break\n",
        "        time.sleep(0.1)\n",
        "\n",
        "def eval_js(js_code):\n",
        "    from google.colab import output\n",
        "    return output.eval_js(js_code)\n",
        "\n",
        "def record(seconds=3):\n",
        "    print(f\"Recording now for {seconds} seconds.\")\n",
        "    display(Javascript(RECORD))\n",
        "    s = output.eval_js('record(%d)' % (seconds * 1000))\n",
        "    binary = b64decode(s.split(',')[1])\n",
        "\n",
        "    # Convert to AudioSegment\n",
        "    audio = AudioSegment.from_file(io.BytesIO(binary), format=\"webm\")\n",
        "\n",
        "    # Export as WAV\n",
        "    audio.export(\"recorded_audio.wav\", format=\"wav\")\n",
        "    print(\"Recording done.\")\n",
        "    return audio\n",
        "\n",
        "def transcribe_audio(filename):\n",
        "    with open(filename, \"rb\") as audio_file:\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file\n",
        "        )\n",
        "    return transcription.text"
      ],
      "metadata": {
        "id": "kjppJ7ltF3zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Conversation with Chatbot\n",
        "\n",
        "We now continue a conversation with our `Chatbot` until the user requests it to end.\n",
        "\n",
        "For Example 6, firt ask the LLM **\"What is the capital of Texas?\"** and wait for the LLM to stop processing your input. Then tell the LLM **\"bye\"** to end your conversation."
      ],
      "metadata": {
        "id": "J0Hn0XB3F3ZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Conversation with Chatbot\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "from pydub import AudioSegment\n",
        "\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "MODEL = 'gpt-4o-mini'\n",
        "\n",
        "# Initialize the OpenAI LLM with your API key\n",
        "llm = ChatOpenAI(\n",
        "  openai_api_key=OPENAI_KEY,\n",
        "  model=MODEL,\n",
        "  temperature= 0.3,\n",
        "  n= 1)\n",
        "\n",
        "c = ChatBot(llm, llm, DEFAULT_TEMPLATE)\n",
        "\n",
        "# Transcribe the recorded audio\n",
        "response = None\n",
        "while response != \"bye\":\n",
        "    audio = record(5)\n",
        "    transcription = transcribe_audio(\"recorded_audio.wav\")\n",
        "    print(f\"Human: {transcription}\")\n",
        "    response = c.converse(transcription)\n",
        "    print(f\"AI: {response}\")\n",
        "    speak_text(response)"
      ],
      "metadata": {
        "id": "j103jtglGDio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image19A.png)"
      ],
      "metadata": {
        "id": "S2OpYLwDmTjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6: Conversation with Chatbot**\n",
        "\n",
        "In the cell below write the code to start a new conversation with the `Chatbot`. Ask your `Chatbot` for **answers to 5 different questions** of your own choosing. After the 5th question has been answered, terminate your conversation by saying the word **\"bye\"**."
      ],
      "metadata": {
        "id": "L2Xo4bAbnJqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6 here\n"
      ],
      "metadata": {
        "id": "ErVz6rRpnJqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output you should see depends upon your 5 questions.\n"
      ],
      "metadata": {
        "id": "O_VGE0wanJqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 4: Text-to-Image**\n",
        "\n",
        "**Text-to-Image programs** are AI-powered tools that generate images based on textual descriptions. These systems use deep learning models, particularly **generative models** like **diffusion models** or **GANs (Generative Adversarial Networks)**, to interpret and visualize the content described in natural language.\n",
        "\n",
        "**How They Work**\n",
        "\n",
        "1. **Input**: A user provides a text prompt (e.g., \"a futuristic city at sunset\").\n",
        "2. **Processing**: The model analyzes the prompt using natural language understanding and maps it to visual concepts.\n",
        "3. **Generation**: The model synthesizes an image that matches the description, often refining it through multiple steps (as in diffusion models).\n",
        "\n",
        "**Popular Examples**\n",
        "- **DALL·E** (by OpenAI)\n",
        "- **Midjourney**\n",
        "- **Stable Diffusion**\n",
        "- **Adobe Firefly**\n",
        "\n",
        "**Applications**\n",
        "\n",
        "- **Art and Design**: Creating concept art, illustrations, and visual assets.\n",
        "- **Education**: Visualizing historical scenes or scientific concepts.\n",
        "- **Marketing**: Generating visuals for campaigns and branding.\n",
        "- **Entertainment**: Storyboarding and character design.\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "- May misinterpret vague or complex prompts.\n",
        "- Can reflect biases present in training data.\n",
        "- Image quality varies depending on model and prompt specificity.\n",
        "\n"
      ],
      "metadata": {
        "id": "9YeDB-lKTIdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7: Text-to-Image\n",
        "\n",
        "The code in the next cell uses the `dall-e-3` Text-to-Image program to generate a picture of a Welsh Corgi Pembroke puppy using the following prompt\n",
        "\n",
        "```text\n",
        "# Define your image prompt\n",
        "PROMPT=\"a Welsh Corgi Pembroke puppy\"\n",
        "TITLE=\"Welsh Corgi Puppy\"\n",
        "```"
      ],
      "metadata": {
        "id": "zOI7oqfSW0RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: Text-to-image\n",
        "\n",
        "import openai\n",
        "import os\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define your image prompt\n",
        "PROMPT=\"a Welsh Corgi Pembroke puppy\"\n",
        "TITLE=\"Welsh Corgi Puppy\"\n",
        "\n",
        "# Get the secret from the environment\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# Generate a single image\n",
        "response = client.images.generate(\n",
        "    model=\"dall-e-3\",\n",
        "    prompt=PROMPT,\n",
        "    size=\"1024x1024\",\n",
        "    quality=\"standard\",\n",
        "    n=1\n",
        ")\n",
        "\n",
        "# Get the image URL\n",
        "image_url = response.data[0].url\n",
        "\n",
        "# Fetch and display the image\n",
        "img_response = requests.get(image_url)\n",
        "img = Image.open(BytesIO(img_response.content))\n",
        "\n",
        "# Save the image\n",
        "img.save(\"dalle3_image.jpg\", \"JPEG\")\n",
        "\n",
        "# Show the image\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title(TITLE)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DCElZ0fLV-of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image20A.png)\n",
        "\n",
        "There is a degree of randomization in Text-to-Image programs so that each image generated is different."
      ],
      "metadata": {
        "id": "r4uLcEaDqmmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7: Text-to-Image**\n",
        "\n",
        "In the next cell write the code to use the `dall-e-3` Text-to-Image program to generate a picture. You are free to generate any picture that you want. Don't forget to change the **image title** to match your subject.\n",
        "\n",
        "### **NOTICE**\n",
        "\n",
        "Test-to-Image programs have restrictions on the kinds of images that can be generate. If you try to generate a pornographic or another censored image type you will receive the following error message:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image21A.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "cBlM6f0fqvAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 7 here\n"
      ],
      "metadata": {
        "id": "vuo_boXrqvAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output you see will depend upon your prompt. Make sure that the image title matches the subject matter of your image."
      ],
      "metadata": {
        "id": "UduXbTt7qvAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why DALL·E 3 Generates Different Images from the Same Prompt\n",
        "\n",
        "DALL·E 3, like other generative AI models, can produce **varied outputs** even when the input prompt is identical. This behavior is intentional and rooted in how the model is designed.\n",
        "\n",
        "## Key Reasons for Variation\n",
        "\n",
        "### 1. **Stochastic Sampling**\n",
        "- DALL·E 3 uses **randomness** during the image generation process.\n",
        "- Even with the same prompt, the model samples from a distribution of possible outputs, leading to different results.\n",
        "\n",
        "### 2. **Latent Space Diversity**\n",
        "- The model operates in a **latent space** where many visual interpretations of a prompt can exist.\n",
        "- For example, \"a cat on a windowsill\" could vary in breed, lighting, style, background, and pose.\n",
        "\n",
        "### 3. **Prompt Interpretation**\n",
        "- Natural language is inherently **ambiguous**.\n",
        "- The model may emphasize different aspects of the prompt each time (e.g., focusing more on \"windowsill\" vs. \"cat\").\n",
        "\n",
        "### 4. **Model Temperature Settings**\n",
        "- Some platforms allow adjusting the **temperature** (a parameter controlling randomness).\n",
        "- Higher temperature = more creative and varied outputs.\n",
        "\n",
        "### 5. **Fine-Tuning and Updates**\n",
        "- If the model has been updated or fine-tuned, even subtle changes can affect output consistency.\n",
        "\n",
        "## Can You Get Consistent Results?\n",
        "\n",
        "Yes, but with limitations:\n",
        "- Use **prompt engineering** to be extremely specific.\n",
        "- Some platforms allow setting a **seed value** to control randomness (though DALL·E 3 may not expose this directly).\n",
        "\n"
      ],
      "metadata": {
        "id": "zHe_eX1PuUFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 8: Create Multiple Images\n",
        "\n",
        "To illustrate the degree of variation between images generated by exactly the same prompt, the code in the cell below generates 3 images using the same prompt that was used in Example 7."
      ],
      "metadata": {
        "id": "3VHigre3tQbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 8: Create multiple images\n",
        "\n",
        "import openai\n",
        "import os\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define your image prompt and title\n",
        "PROMPT = PROMPT=\"a Welsh Corgi Pembroke puppy\"\n",
        "TITLE = \"Welsh Corgi Puppy\"\n",
        "\n",
        "# Get the secret from the environment\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# Generate and save 3 images\n",
        "for i in range(3):\n",
        "    response = client.images.generate(\n",
        "        model=\"dall-e-3\",\n",
        "        prompt=PROMPT,\n",
        "        size=\"1024x1024\",\n",
        "        quality=\"standard\",\n",
        "        n=1\n",
        "    )\n",
        "\n",
        "    image_url = response.data[0].url\n",
        "    img_response = requests.get(image_url)\n",
        "    img = Image.open(BytesIO(img_response.content))\n",
        "    img.save(f\"dalle3_image_{i+1}.jpg\", \"JPEG\")\n",
        "\n",
        "# Display images side-by-side\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "for i, ax in enumerate(axes):\n",
        "    img = Image.open(f\"dalle3_image_{i+1}.jpg\")\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"{TITLE} #{i+1}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BJfQeCnncolh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_04/class_04_2_image22A.png)"
      ],
      "metadata": {
        "id": "1p9gvKAudem2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 8: Create Multiple Images**\n",
        "\n",
        "In the cell below write the code to create three images using the same prompt that you used above in **Exercise 8**."
      ],
      "metadata": {
        "id": "u8_Ty2nTvvbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 8 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ChIKWpHGvvbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output you see will depend upon your prompt. Make sure that the image title matches the subject matter of your image."
      ],
      "metadata": {
        "id": "W1T9Fu2RvvbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-in**\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as Copy of Class_04_2._lastname_.pdf where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "W_DEyDQ5w06v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "\n",
        "##**Attention Is All You Need**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/8/8f/The-Transformer-model-architecture.png)\n",
        "\n",
        "**\"Attention Is All You Need\"** is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique's potential for other tasks like question answering and what is now known as multimodal Generative AI.\n",
        "\n",
        "The paper's title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper's authors, liked the sound of that word.\n",
        "\n",
        "An early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\n",
        "\n",
        "Some early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\n",
        "\n",
        "As of 2024, the paper has been cited more than 140,000 times.\n",
        "\n",
        "**Authors**\n",
        "\n",
        "The authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" to the paper; the listed order was randomized. The Wired article highlights the group's diversity:\n",
        "\n",
        "Six of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.\n",
        "\n",
        "**Methods Discussed & Introduced**\n",
        "\n",
        "The paper is most well known for the introduction of the Transformer architecture, which forms the underlying architecture for most forms of modern Large Language Models (LLMs). A key reason for why the architecture is preferred by most modern LLMs is the parallelizability of the architecture over its predecessors. This ensures that the operations necessary for training can be accelerated on a GPU allowing both faster training times and models of bigger sizes to be trained.\n",
        "\n",
        "The following mechanisms were introduced by the paper as part of the development of the transformer architecture.\n",
        "\n",
        "**Scaled dot-product Attention & Self-attention**\n",
        "\n",
        "The use of the scaled dot-product attention and self-attention mechanism instead of an RNN or LSTM (which rely on recurrence instead) allow for better performance as described in the following paragraph.\n",
        "\n",
        "Since the model relies on Query (Q), Key (K) and Value (V) matrices that come from the same source itself (i.e. the input sequence / context window), this eliminates the need for RNNs completely ensuring parallelizability for the architecture. This differs from the original form of the Attention mechanism introduced in 2014. Additionally, the paper also discusses the use of an additional scaling factor that was found to be most effective with respect to the dimension of the key vectors.\n",
        "\n",
        "In the specific context of translation which the paper focused on, the Query and Key matrices are usually represented in embeddings corresponding to the source language while the Value matrix corresponds to the target language.\n",
        "\n",
        "**Multi-head Attention**\n",
        "\n",
        "In the self-attention mechanism, queries (Q), keys (K), and values (V) are dynamically generated for each input sequence (limited typically by the size of the context window), allowing the model to focus on different parts of the input sequence at different steps. Multi-head attention enhances this process by introducing multiple parallel attention heads. Each attention head learns different linear projections of the Q, K, and V matrices. This allows the model to capture different aspects of the relationships between words in the sequence simultaneously, rather than focusing on a single aspect.\n",
        "\n",
        "By doing this, multi-head attention ensures that the input embeddings are updated from a more varied and diverse set of perspectives. After the attention outputs from all heads are calculated, they are concatenated and passed through a final linear transformation to generate the output.\n",
        "\n",
        "**Positional Encoding**\n",
        "\n",
        "Since the `Transformer model` is not a `seq2seq model` and does not rely on the sequence of the text in order to perform encoding and decoding, the paper relied on the use of sine and cosine wave functions to encode the position of the token into the embedding.\n",
        "\n",
        "**Historical context**\n",
        "\n",
        "For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\n",
        "\n",
        "A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers. However, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.\n",
        "\n",
        "Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.\n",
        "\n",
        "# 📘 Overview of \"Attention Is All You Need\"\n",
        "\n",
        "## 🧠 Authors\n",
        "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin\n",
        "\n",
        "## 📅 Published\n",
        "2017, at the Neural Information Processing Systems (NeurIPS) conference.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Key Contributions\n",
        "\n",
        "### 1. **Introduction of the Transformer Architecture**\n",
        "- The paper proposed a novel neural network architecture called the **Transformer**, which relies entirely on **attention mechanisms**.\n",
        "- It **eliminated recurrence and convolutions**, which were standard in sequence modeling tasks like machine translation.\n",
        "\n",
        "### 2. **Self-Attention Mechanism**\n",
        "- Introduced **scaled dot-product attention** and **multi-head attention**, allowing the model to focus on different parts of the input sequence simultaneously.\n",
        "- This mechanism enables better handling of long-range dependencies in data.\n",
        "\n",
        "### 3. **Parallelization and Efficiency**\n",
        "- Unlike RNNs and LSTMs, Transformers allow for **parallel processing** of input sequences, significantly speeding up training and inference.\n",
        "- This made it highly scalable and suitable for large datasets and models.\n",
        "\n",
        "### 4. **Positional Encoding**\n",
        "- Since Transformers lack recurrence, they use **positional encodings** to retain information about the order of tokens in a sequence.\n",
        "\n",
        "---\n",
        "\n",
        "## 📈 Impact on Neural Network Research\n",
        "\n",
        "- **Foundation for Modern NLP Models**: The Transformer architecture became the backbone of models like **BERT**, **GPT**, **T5**, and **XLNet**.\n",
        "- **State-of-the-Art Performance**: It achieved superior results on tasks like machine translation, text classification, and question answering.\n",
        "- **Cross-Domain Influence**: Transformers have been adapted for **vision (ViT)**, **audio**, **reinforcement learning**, and **protein folding**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Why It's a Landmark Paper\n",
        "\n",
        "- **Paradigm Shift**: It shifted the focus from recurrence-based models to attention-based models.\n",
        "- **Scalability**: Enabled training of massive models with billions of parameters.\n",
        "- **Versatility**: Its architecture is modular and adaptable to various domains beyond NLP.\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Citation\n",
        "> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. *Advances in Neural Information Processing Systems*, 30.\n",
        "\n"
      ],
      "metadata": {
        "id": "H3ykBQufwypg"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}