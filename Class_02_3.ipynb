{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM56i4wt4yZSYRPsdlIgbRP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_02_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ],
      "metadata": {
        "id": "QGXRJ8-TG2mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "uvvKzBf7G5eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Module 2: Neural Networks with PyTorch**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 2 Material\n",
        "\n",
        "* Part 2.1: Introduction to Neural Networks with PyTorch\n",
        "* Part 2.2: Encoding Feature Vectors\n",
        "* **Part 2.3: Controlling Overfitting**\n",
        "* Part 2.4: Saving and Loading a PyTorch Neural Network"
      ],
      "metadata": {
        "id": "xLU60GttHArg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ],
      "metadata": {
        "id": "4Xti6nC3HLiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXdTjc1EGXos"
      },
      "outputs": [],
      "source": [
        "# @title You must run this cell first!\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google Colab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "```text\n",
        "Mounted at /content/drive\n",
        "Note: Using Google Colab\n",
        "studentbio1173@gmail.com\n",
        "```\n",
        "Make sure you GMAIL is visible or you will lose 20 pts on your submission."
      ],
      "metadata": {
        "id": "4U4gC8qEHRLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Functions\n",
        "\n",
        "Run the cell below to create a function that will be needed later in this lesson."
      ],
      "metadata": {
        "id": "n3fJxCKKHYbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Functions\n",
        "\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "\n",
        "def set_seeds(seed_value):\n",
        "    \"\"\"\n",
        "    Sets the seed for reproducibility across Python, NumPy, and PyTorch.\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    # 1. Base Python\n",
        "    random.seed(seed_value)\n",
        "\n",
        "    # 2. NumPy\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    # 3. PyTorch (CPU and CUDA)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # For multi-GPU setups\n",
        "\n",
        "    # 4. CuDNN Determinism (Crucial for GPU consistency)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # 5. Optional: Ensure all operations are deterministic\n",
        "    # Note: Some specialized operations might throw an error if no\n",
        "    # deterministic implementation exists.\n",
        "    # torch.use_deterministic_algorithms(True)\n",
        "\n",
        "    print(f\"✅ Seeds set to: {seed_value}\")\n",
        "\n",
        "print(\"Functions for this lesson have been created successfully.\")"
      ],
      "metadata": {
        "id": "mHV76pNoHf4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "```text\n",
        "Functions for this lesson have been created successfully.\n",
        "```"
      ],
      "metadata": {
        "id": "EwbytKCPHvQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Sets for this Lesson**\n",
        "\n",
        "For this lesson, the **Obesity Data Set** will be used for the Examples and the **Heart Disease Data Set** will be used for the **Exercises**. Information about the Heart Disease Data set was presented in the previous lesson. Here is the information about the Obesity Data set.\n",
        "\n",
        "### **Obesity Data Set**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/obesity.jpg)\n",
        "\n",
        "[Obesity Data Set](https://archive.ics.uci.edu/ml/datasets/)\n",
        "\n",
        "**Description:**\n",
        "\n",
        "The **Obesity Data Set** includes data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. The data contains 17 attributes and 2111 records, the records are labeled with the class variable NObesity (Obesity Level), that allows classification of the data using the values of `Insufficient Weight`, `Normal Weight`, `Overweight Level I`, `Overweight Level II`, `Obesity Type I`, `Obesity Type II` and `Obesity Type III`.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "* **Gender-** Female/Male\n",
        "* **Age-** Numeric value\n",
        "* **Height-** Numeric value in meters\n",
        "* **Weight-** Numeric value in kilograms\n",
        "* **family_history_with_overweight-** Has a family member suffered or suffers from overweight - Yes/No\n",
        "* **FAVC-** Do you eat high caloric food frequently - Yes/No\n",
        "* **FCVC-** Do you usually eat vegetables in your meals - Never/Sometimes/Always\n",
        "* **NCP-** How many main meals do you have daily - Between 1 y 2/Three/More than three\n",
        "* **CAEC-** Do you eat any food between meals? - No/Sometimes/Frequently/Always\n",
        "* **SMOKE-** Do you smoke? - Yes/No\n",
        "* **CH2O-** How much water do you drink daily? - Less than a liter/Between 1 and 2 L/More than 2 L\n",
        "* **SCC-** Do you monitor the calories you eat daily - Yes/No\n",
        "* **FAF-** How often do you have physical activity? - I do not have/1 or 2 days/2 or 4 days/4 or 5 days\n",
        "* **TUE-** How much time do you use technological devices such as cell phone, videogames, television, computer and others - 0–2 hours/3–5 hours/More than 5 hours\n",
        "* **CALC-** How often do you drink alcohol? - I do not drink/Sometimes/Frequently/Always\n",
        "* **MTRANS-** Which transportation do you usually use? Automobile/Motorbike/Bike/Public Transportation/Walking\n",
        "* **NObeyesdad-** Obesity levels: 'Insufficient_Weight', 'Obesity_Type_III', 'Normal_Weight', 'Obesity_Type_II', 'Overweight_Level_I', 'Obesity_Type_I', 'Overweight_Level_II'\n",
        "\n",
        "For our classification neural network (`eg_model`), the response variable (`Y`) will be the column `NObeyesdad`. This column contains 7 categorical variables shown here sorted from lowest to highest obesity:\n",
        "\n",
        "* Insufficient_Weight\n",
        "* Normal_Weight\n",
        "* Overweight_Level_I\n",
        "* Overweight_Level_II\n",
        "* Obesity_Type_I  \n",
        "* Obesity_Type_II\n",
        "* Obesity_Type_III\n"
      ],
      "metadata": {
        "id": "YFOohVHIFS3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------\n",
        "\n",
        "# **The Problem of `Overfitting`**\n",
        "\n",
        "When a neural network learns a task, it adjusts its parameters so that the error on the training data decreases.\n",
        "\n",
        "However, it can be quite easy for the model to becomes **too good** at reproducing the training sample by **memorizing idiosyncratic noise** instead of capturing the true underlying relationship. This situation is called **overfitting**.\n",
        "\n",
        "#### **How `Overfitting` Happens**\n",
        "\n",
        "* **Too many trainable parameters:**\n",
        "A very deep or wide network can represent a vast space of functions.\n",
        "With few training examples, the model can fit every detail, including random fluctuations.\n",
        "\n",
        "* **Insufficient or unrepresentative data:**\n",
        "If the training set does not cover the variability of the problem domain, the model learns patterns that only exist in the training data.\n",
        "\n",
        "* **Training for too many epochs:**\n",
        "Continuing the optimisation past the point where validation error stops decreasing lets the network fine‑tune to noise.\n",
        "\n",
        "* **Lack of regularisation:**\n",
        "No constraints on weights, activations or hidden‑layer outputs allow the model to swing wildly to minimise the training loss.\n",
        "\n",
        "#### **Why `Overfitting` is Bad**\n",
        "\n",
        "* **Poor generalisation:**\n",
        "The model's predictions on new data (test set, real-world inputs) are much worse than its performance on the training set.\n",
        "\n",
        "* **Misleading confidence:**\n",
        "An overfitted network often reports low loss or high accuracy, giving a false sense of reliability.\n",
        "\n",
        "* **Wasted resources:**\n",
        "Training longer or with more complex architectures is unnecessary when the model will not perform better on unseen data.\n",
        "\n",
        "* **Deployment risks:**\n",
        "In safety-critical applications (self-driving cars, medical diagnosis), an overfitted model can produce dangerous errors."
      ],
      "metadata": {
        "id": "ZS0GhueQiew0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Controlling `Overfitting`**\n",
        "\n",
        "This lesson focuses on how you can control overfitting during the training of neural networks. Here are the 4 common strategies that are typically used in PyTorch to deal with the issues related to overfitting:\n",
        "\n",
        "\n",
        "* **`L2 Regularization`**\n",
        "* **`Dropout` Layers**\n",
        "* **`Batch Processing`**\n",
        "* **`Early Stopping`**\n",
        "\n",
        "To illustrate the strengths and weaknesses of each strategy, this lesson will use four Examples and four companion **Exercises**.\n",
        "\n",
        "1. `Example 1`/**`Exercise 1`**: A classification neural network will be trained _without_ any measures to prevent overfitting. This will serve as a **baseline** against which to judge the remaining Examples and **Exercises**.\n",
        "2. `Example 2`/**`Exercise 2`**: Exactly the same neural network will be trained using **`L2 Regularization`** to control overfitting.\n",
        "3. `Example 3`/**`Exercise 3`**: Instead of L2 Regularization, the technique of **`Dropout Layers`** will be used to limit overfitting.\n",
        "4. `Example 4`/**`Exercise 4`**: The technique of **`Batch Processing`** will be used to limit overfitting.\n",
        "4. `Example 5`/**`Exercise 5`**: Finally, the technique of adding **`Early Stopping`** will be used to limit overfitting.\n",
        "\n",
        "It should be noted that there is no technical reason to limit yourself to a single technique to limit overfitting -- two (or more) strategies are often used in combination depending upon the particular situation."
      ],
      "metadata": {
        "id": "YvVbIsf8fKXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1A: No Overfitting Prevention\n",
        "\n",
        "In earlier lessons the training of neural networks was divided into a series of smaller steps to help students master the underlying programming concepts. The code in the cell below, however, provides a all the code in a single block that is needed to:\n",
        "1. Read and preparing the tabular data\n",
        "2. Build and compile a classification neural network using PyTorch\n",
        "3. Train the neural network.\n",
        "4. Visualize the training.\n",
        "\n",
        "While a detailed explanation of the different code chucks will not be provided, here are a few key points.\n",
        "\n",
        "1. The following code chunk reads the `Obesity Data Set` from the course web server and stores it in a DataFrame called `eg_df`.\n",
        "```text\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "eg_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "```\n",
        "You will of course have to modify this line of code to read the Heart Disease data set for **`Exercise 1`**.\n",
        "\n",
        "You should note that the variable **`VERSBOSE`** has been set to `0`. This means there will be no output during training. This has been done to help keep the length of you Colab PDF a more reasonable value. However, in the last example/exercise pair, the verbose variable is set to `2` to print out the values generated during training."
      ],
      "metadata": {
        "id": "oQ4ZLvruwovE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1A: No Overfitting Prevention\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# ------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# ------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "PATIENCE      = 10\n",
        "VERBOSE       = 0     # 0 means no output during training\n",
        "LEARNING_RATE = 0.05\n",
        "BATCH_SIZE    = 32\n",
        "\n",
        "# Print title\n",
        "print(\"Example 1A: No Overfitting Prevention\")\n",
        "\n",
        "# Set Seed\n",
        "seed_value = 42\n",
        "set_seeds(seed_value)\n",
        "\n",
        "# Set device to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "eg_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# ------------------------------------------------------------\n",
        "eg_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "eg_df[eg_target_col] = le.fit_transform(eg_df[eg_target_col])\n",
        "\n",
        "eg_X = eg_df.drop(columns=[eg_target_col])\n",
        "eg_y = eg_df[eg_target_col].astype('int32')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# ------------------------------------------------------------\n",
        "# First split into train+val and test\n",
        "eg_X_temp, eg_X_test, eg_y_temp, eg_y_test = train_test_split(\n",
        "    eg_X, eg_y, test_size=0.2, random_state=42, stratify=eg_y)\n",
        "\n",
        "# Then split train+val into train and val\n",
        "eg_X_train, eg_X_val, eg_y_train, eg_y_val = train_test_split(\n",
        "    eg_X_temp, eg_y_temp, test_size=0.1, random_state=42, stratify=eg_y_temp)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# ------------------------------------------------------------\n",
        "categorical_cols = [c for c in eg_X.columns if eg_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in eg_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Convert data to correct numeric type (Numpy arrays)\n",
        "eg_X_train_proc = preprocessor.fit_transform(eg_X_train).astype(np.float32)\n",
        "eg_X_val_proc   = preprocessor.transform(eg_X_val).astype(np.float32)\n",
        "eg_X_test_proc  = preprocessor.transform(eg_X_test).astype(np.float32)\n",
        "\n",
        "eg_y_train = eg_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "eg_y_val   = eg_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "eg_y_test  = eg_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6️⃣  Convert to PyTorch Tensors & DataLoaders\n",
        "# ------------------------------------------------------------\n",
        "# Convert Features to Float Tensors\n",
        "eg_X_train_t = torch.tensor(eg_X_train_proc).to(device)\n",
        "eg_X_val_t   = torch.tensor(eg_X_val_proc).to(device)\n",
        "eg_X_test_t  = torch.tensor(eg_X_test_proc).to(device)\n",
        "\n",
        "# Convert Targets to Long Tensors (Required for CrossEntropyLoss)\n",
        "eg_y_train_t = torch.tensor(eg_y_train, dtype=torch.long).to(device)\n",
        "eg_y_val_t   = torch.tensor(eg_y_val, dtype=torch.long).to(device)\n",
        "eg_y_test_t  = torch.tensor(eg_y_test, dtype=torch.long).to(device)\n",
        "\n",
        "# Create DataLoaders for batching\n",
        "eg_train_dataset = TensorDataset(eg_X_train_t, eg_y_train_t)\n",
        "eg_val_dataset   = TensorDataset(eg_X_val_t, eg_y_val_t)\n",
        "\n",
        "eg_train_loader = DataLoader(eg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "eg_val_loader   = DataLoader(eg_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7️⃣  Build Model & Optimizer\n",
        "# ------------------------------------------------------------\n",
        "n_classes = len(np.unique(eg_y_train))\n",
        "input_dim = eg_X_train_proc.shape[1]\n",
        "\n",
        "# Define PyTorch Model\n",
        "class ObesityNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "        # Note: No Softmax here. CrossEntropyLoss expects raw logits.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "eg_model = ObesityNet(input_dim, n_classes).to(device)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(eg_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8️⃣  Train model (Manual Loop)\n",
        "# ------------------------------------------------------------\n",
        "print(f\"------Training Starting for {EPOCHS} epochs --------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Dictionary to store history\n",
        "eg_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Training Phase ---\n",
        "    eg_model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in eg_train_loader:\n",
        "        optimizer.zero_grad()               # Reset gradients\n",
        "        outputs = eg_model(X_batch)         # Forward pass\n",
        "        loss = loss_fn(outputs, y_batch)    # Calculate loss\n",
        "        loss.backward()                     # Backward pass\n",
        "        optimizer.step()                    # Update weights\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(eg_train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    eg_model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad(): # No gradient needed for validation\n",
        "        for X_batch, y_batch in eg_val_loader:\n",
        "            outputs = eg_model(X_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(eg_val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    # Store metrics\n",
        "    eg_history['accuracy'].append(train_acc)\n",
        "    eg_history['val_accuracy'].append(val_acc)\n",
        "    eg_history['loss'].append(avg_train_loss)\n",
        "    eg_history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "    # Verbose print\n",
        "    if VERBOSE > 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "            f\"loss: {avg_train_loss:.4f} - \"\n",
        "            f\"acc: {train_acc:.4f} - \"\n",
        "            f\"val_loss: {avg_val_loss:.4f} - \"\n",
        "            f\"val_acc: {val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished\")\n",
        "print(\"Best val accuracy:\", max(eg_history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
      ],
      "metadata": {
        "id": "zQWzxBXeZkvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "```text\n",
        "Example 1A: No Overfitting Prevention\n",
        "✅ Seeds set to: 42\n",
        "Using device: cpu\n",
        "------Training Starting for 100 epochs --------------\n",
        "\n",
        "Training finished\n",
        "Best val accuracy: 0.9644970414201184\n",
        "Elapsed time: 0:00:08.11\n",
        "```\n",
        "Our `eg_model` neural network appears to done a great job since the best validation accuracy (`val accuracy`) is above 95%. However, let's examine the model's training accuracy more carefully in `Example 1B` below.\n"
      ],
      "metadata": {
        "id": "hkO0__mdwpWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1B: Visualize Effects of No Overfitting Prevention\n",
        "\n",
        "After training we can assess its efficacy by visualizing two training curves —specifically **train loss vs. validation loss** and **train accuracy vs. validation accuracy**.\n",
        "\n",
        "The code in the cell below uses the `matplotlib.pyplot` graphics library to generate the two views of the training."
      ],
      "metadata": {
        "id": "SVC1D2YoyowA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1B: No Overfitting Prevention\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print title\n",
        "print(\"Example 1B: Visualize Effects of No Overfitting Prevention\")\n",
        "# Create a figure with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 1: Loss (Left Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax1.plot(eg_history['loss'], label='Train Loss')\n",
        "ax1.plot(eg_history['val_loss'], label='Val Loss')\n",
        "ax1.set_title('Model Loss (No Overfitting Prevention)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 2: Accuracy (Right Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax2.plot(eg_history['accuracy'], label='Train Acc')\n",
        "ax2.plot(eg_history['val_accuracy'], label='Val Acc')\n",
        "ax2.set_title('Model Accuracy (No Overfitting Prevention)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FSti1s6igOJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image01A.png)\n",
        "\n",
        "If you don't set the random seed, there can be considerable variability in these plots from one run to the next. Here is an analysis of the two plots shown above.\n",
        "\n",
        "#### **Analysis of Model Performance**\n",
        "\n",
        "Based on the training plots, the model is exhibiting a textbook case of **overfitting**.\n",
        "\n",
        "##### **Model Loss (Left Plot)**\n",
        "* **Train Loss (Blue):** This line drops consistently and rapidly approaches 0. This indicates the model is successfully learning the training data and, by Epoch 55, has essentially \"memorized\" it.\n",
        "* **Validation Loss (Orange):** This represents how the model performs on data it hasn't seen before. While it decreases initially, it is highly volatile (spiky) and eventually plateaus around 0.25.\n",
        "* **The Gap:** The large gap between the Training Loss (near 0) and Validation Loss (~0.25) signifies that the model is fitting noise in the training data rather than generalizing the underlying patterns.\n",
        "\n",
        "##### **Model Accuracy (Right Plot)**\n",
        "* **Train Accuracy (Blue):** The model achieves perfect accuracy (1.0 or 100%) on the training data around Epoch 50 and stays there.\n",
        "* **Validation Accuracy (Orange):** The validation accuracy improves quickly but hits a ceiling around 96-97%. It never reaches the perfect score of the training set.\n",
        "* **Observation:** While 96% accuracy is objectively good, the fact that training is at 100% confirms the model has sufficient capacity to learn the task but lacks the constraints to ignore irrelevant details (noise).\n",
        "\n",
        "##### **Diagnosis**\n",
        "Here is what happened mechanistically:\n",
        "\n",
        "1.  **Memorization:** The neural network is likely too complex (too many parameters/layers) relative to the amount of data available. It has enough \"memory\" to simply recall specific training examples rather than learning rules.\n",
        "2.  **Volatility:** The spikes in the orange validation loss line (specifically between epochs 10 and 50) suggest the model is unstable. Small updates to the weights are causing large swings in error when applied to unseen data."
      ],
      "metadata": {
        "id": "KGCgzxUnzj2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1A: No Overfitting Prevention**\n",
        "\n",
        "In the cell below write the code to to build, compile and train a classification neural network called `ex_model`. As usual, start by copying all of `Example 1` into the cell below.\n",
        "\n",
        "Since you will be using the Heart Disease dataset for all of the **`Exercises`**, use this code chunk to read your dataset from the course file server and create a DataFrame called `ex_df`.\n",
        "```text\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "ex_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/heart_disease.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "```\n",
        "\n",
        "Your objective is to predict the kind of `Resting ECG` a patient will likely display given his/her other clinical measurements. Therefore the data in the column `RestingECG` will be your `Y-values` and the data in the other columns will be your `X-values`. Since your target column has 3 classes: `Normal`, `ST` and `LVH`, you will be training your neural network (`ex_model`) to predict which of type of Resting ECG a particular patient will likely have given his/her other clinical measurements (`X-values`).\n",
        "\n",
        "You will therefore need to specify your target column as follows:\n",
        "```text\n",
        "ex_target_col = \"RestingECG\"\n",
        "```\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Change the prefix `eg_` to `ex_` everywhere in the code copied from Example 1.\n",
        "\n",
        "Make sure to change the value of the `seed_value` to 1604\n"
      ],
      "metadata": {
        "id": "mPS3lEus1KOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 1A: No Overfitting Prevention\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dAd0jgO9cg9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "```text\n",
        "Exercise 1A: No Overfitting Prevention\n",
        "✅ Seeds set to: 1604\n",
        "Using device: cpu\n",
        "------Training Starting for 100 epochs --------------\n",
        "\n",
        "Training finished\n",
        "Best val accuracy: 0.6621621621621622\n",
        "Elapsed time: 0:00:02.92\n",
        "```\n"
      ],
      "metadata": {
        "id": "zrLIG-k-1KOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1B: Visualize Effects of No Overfitting Prevention**\n",
        "\n",
        "In the cell below write the code to visualize your training by creating a  **train loss vs. validation loss** plot and **train accuracy vs. validation accuracy** plot.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Your `history` object should be called `ex_history` instead of `eg_history`."
      ],
      "metadata": {
        "id": "yNwREBwM8ZPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 1B: Visualize Effects of No Overfitting Prevention\n",
        "\n"
      ],
      "metadata": {
        "id": "qBtO4qEo8ZPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image02A.png)\n",
        "\n",
        "### **Analysis of Model Performance (Severe Overfitting)**\n",
        "\n",
        "Based on these training plots, the model is exhibiting a case of **severe overfitting**, arguably worse than the previous example because the validation performance is actively degrading rather than just stalling.\n",
        "\n",
        "##### **Model Loss (Left Plot)**\n",
        "* **Train Loss (Blue):** The training loss decreases steadily from ~1.0 down to ~0.5. This confirms the model is learning to minimize error on the training set.\n",
        "* **Validation Loss (Orange):** This is the critical warning sign. Instead of decreasing or plateauing, the validation loss **increases** dramatically, rising from ~1.0 to over 3.0.\n",
        "* **The Gap:** The lines diverge almost immediately (around Epoch 5). The fact that validation loss is exploding upwards indicates the model is becoming increasingly confident in its wrong predictions on unseen data.\n",
        "\n",
        "##### **Model Accuracy (Right Plot)**\n",
        "* **Train Accuracy (Blue):** The model steadily improves its accuracy on the training data, climbing from 60% to nearly 77%.\n",
        "* **Validation Accuracy (Orange):** The accuracy on validation data degrades over time. It starts near 60% but drops to fluctuate between 45% and 55%—essentially performing no better (or worse) than random guessing as training continues.\n",
        "* **Observation:** The model is \"learning\" patterns that are specific only to the training data and detrimental to generalization. As it optimizes for the training set, it actively gets worse at the general task.\n",
        "\n",
        "##### **Diagnosis**\n",
        "Here is what is happening mechanistically:\n",
        "\n",
        "1.  **Immediate Divergence:** Unlike the previous graph where the model generalized for a while before overfitting, this model begins overfitting almost instantly.\n",
        "2.  **Memorization of Noise:** The rising orange loss line suggests the model is fitting the \"noise\" or random fluctuations in the training data so aggressively that it is losing the underlying signal.\n",
        "3.  **Potential Causes:** This specific shape (diverging lines) often hints that the model capacity is far too high for the dataset size, or the learning rate might be too high, causing the model to overshoot optimal weights for the validation set."
      ],
      "metadata": {
        "id": "NbsfMz779PFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **L2 Regularization (Weight Decay)**\n",
        "\n",
        "**L2 Regularization** is a standard technique used to reduce overfitting by preventing a neural network's weights from becoming too large. In the PyTorch ecosystem, this is commonly referred to as **Weight Decay**.\n",
        "\n",
        "##### **What is L2 Regularization?**\n",
        "* **The Concept:** It adds a penalty to the model's loss function proportional to the square of the magnitude of the weights.\n",
        "* **The Goal:** By penalizing large weights, the model is forced to learn simpler patterns rather than complex, high-frequency noise (memorization).\n",
        "* **The Result:** The model becomes less sensitive to small changes in input data, leading to a smoother decision boundary and better generalization.\n",
        "\n",
        "##### **How to Implement it in PyTorch**\n",
        "Unlike some frameworks where you add a penalty term manually to your loss function, PyTorch builds L2 regularization directly into the **optimizer**.\n",
        "\n",
        "You simply set the `weight_decay` parameter when initializing your optimizer.\n",
        "\n",
        "* **Parameter:** `weight_decay` (float)\n",
        "* **Typical Values:** Between `1e-5` (0.00001) and `1e-3` (0.001). A value of `0` means no regularization.\n",
        "\n",
        "##### **Code Example**\n",
        "Here is how you would add L2 regularization to an Adam optimizer:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Define your model (standard step)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 50),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(50, 1)\n",
        ")\n",
        "\n",
        "# 2. Define the Optimizer with L2 Regularization\n",
        "# 'weight_decay=1e-4' applies the L2 penalty\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# 3. The training loop remains exactly the same\n",
        "# PyTorch automatically adds the regularization term during the optimizer.step()"
      ],
      "metadata": {
        "id": "4-pad6a9cnPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2A: L2 Regularization\n",
        "\n",
        "The following chages were made to add L2 Regularization to the code shown in Example 1.\n",
        "\n",
        "##### **New Hyperparameter (Section 1)**\n",
        "* **Example 1A:** Only defines standard training parameters (`EPOCHS`, `LEARNING_RATE`, etc.).\n",
        "* **Example 2A:** Adds a specific variable for the regularization strength:\n",
        "```text\n",
        "  L2_REG = 0.001  # <--- New Parameter: Weight Decay Strength\n",
        "```\n",
        "\n",
        "##### **Optimizer Configuration (Section 7)**\n",
        "This is the most critical functional difference. PyTorch implements L2 Regularization directly inside the optimizer via the weight_decay argument.\n",
        "\n",
        "Example 1A (No Regularization): The optimizer is initialized with only the model parameters and learning rate.\n",
        "\n",
        "```text\n",
        "optimizer = optim.Adam(eg_model.parameters(), lr=LEARNING_RATE)\n",
        "```\n",
        "\n",
        "Example 2A (With L2 Regularization): The optimizer includes the weight_decay argument, passing in the L2_REG value defined earlier.\n",
        "\n",
        "```text\n",
        "optimizer = optim.Adam(eg_model.parameters(),\n",
        "                       lr=LEARNING_RATE,\n",
        "                       weight_decay=L2_REG) # <--- Applies L2 Penalty\n",
        "```\n"
      ],
      "metadata": {
        "id": "DMG8y4CE38W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2A: L2 Regularization (Weight Decay)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# -------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# -------------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "VERBOSE       = 0\n",
        "LEARNING_RATE = 0.05\n",
        "BATCH_SIZE    = 32\n",
        "L2_REG        = 0.001  # <--- New Parameter: Weight Decay Strength\n",
        "\n",
        "# Print title\n",
        "print(\"Example 2A: L2 Regularization (Weight Decay)\")\n",
        "\n",
        "# Set Seed\n",
        "seed_value = 1066\n",
        "set_seeds(seed_value)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# -------------------------------------------------------------------\n",
        "eg_df = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "    na_values=['NA','?']\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# -------------------------------------------------------------------\n",
        "eg_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "eg_df[eg_target_col] = le.fit_transform(eg_df[eg_target_col])\n",
        "\n",
        "eg_X = eg_df.drop(columns=[eg_target_col])\n",
        "eg_y = eg_df[eg_target_col].astype('int32')\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# -------------------------------------------------------------------\n",
        "eg_X_temp, eg_X_test, eg_y_temp, eg_y_test = train_test_split(\n",
        "    eg_X, eg_y, test_size=0.2, random_state=42, stratify=eg_y\n",
        ")\n",
        "\n",
        "eg_X_train, eg_X_val, eg_y_train, eg_y_val = train_test_split(\n",
        "    eg_X_temp, eg_y_temp, test_size=0.1, random_state=42, stratify=eg_y_temp\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# -------------------------------------------------------------------\n",
        "categorical_cols = [c for c in eg_X.columns if eg_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in eg_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Fit & transform training data\n",
        "eg_X_train_proc = preprocessor.fit_transform(eg_X_train).astype(np.float32)\n",
        "eg_X_val_proc   = preprocessor.transform(eg_X_val).astype(np.float32)\n",
        "eg_X_test_proc  = preprocessor.transform(eg_X_test).astype(np.float32)\n",
        "\n",
        "eg_y_train = eg_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "eg_y_val   = eg_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "eg_y_test  = eg_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6️⃣  Convert to PyTorch Tensors & DataLoaders\n",
        "# -------------------------------------------------------------------\n",
        "eg_X_train_t = torch.tensor(eg_X_train_proc).to(device)\n",
        "eg_X_val_t   = torch.tensor(eg_X_val_proc).to(device)\n",
        "eg_X_test_t  = torch.tensor(eg_X_test_proc).to(device)\n",
        "\n",
        "eg_y_train_t = torch.tensor(eg_y_train, dtype=torch.long).to(device)\n",
        "eg_y_val_t   = torch.tensor(eg_y_val, dtype=torch.long).to(device)\n",
        "eg_y_test_t  = torch.tensor(eg_y_test, dtype=torch.long).to(device)\n",
        "\n",
        "eg_train_dataset = TensorDataset(eg_X_train_t, eg_y_train_t)\n",
        "eg_val_dataset   = TensorDataset(eg_X_val_t, eg_y_val_t)\n",
        "\n",
        "eg_train_loader = DataLoader(eg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "eg_val_loader   = DataLoader(eg_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 7️⃣  Build Model (Standard)\n",
        "# -------------------------------------------------------------------\n",
        "n_classes = len(np.unique(eg_y_train))\n",
        "input_dim = eg_X_train_proc.shape[1]\n",
        "\n",
        "class ObesityNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        # No Dropout layer here, we are testing L2 specifically\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "eg_model = ObesityNet(input_dim, n_classes).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 'weight_decay' applies L2 penalty to the parameters\n",
        "optimizer = optim.Adam(eg_model.parameters(),\n",
        "                       lr=LEARNING_RATE,\n",
        "                       weight_decay=L2_REG)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 8️⃣  Train model\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"------Training Starting for {EPOCHS} epochs --------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "eg_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Training Phase ---\n",
        "    eg_model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in eg_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = eg_model(X_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(eg_train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    eg_model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in eg_val_loader:\n",
        "            outputs = eg_model(X_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(eg_val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    # Store metrics\n",
        "    eg_history['accuracy'].append(train_acc)\n",
        "    eg_history['val_accuracy'].append(val_acc)\n",
        "    eg_history['loss'].append(avg_train_loss)\n",
        "    eg_history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "    # Print if verbose\n",
        "    if VERBOSE > 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "            f\"loss: {avg_train_loss:.4f} - \"\n",
        "            f\"acc: {train_acc:.4f} - \"\n",
        "            f\"val_loss: {avg_val_loss:.4f} - \"\n",
        "            f\"val_acc: {val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished\")\n",
        "print(\"Best val accuracy:\", max(eg_history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
      ],
      "metadata": {
        "id": "u7Z2UPQamW1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "```text\n",
        "Example 2A: L2 Regularization (Weight Decay)\n",
        "✅ Seeds set to: 1066\n",
        "Using device: cpu\n",
        "------Training Starting for 100 epochs --------------\n",
        "\n",
        "Training finished\n",
        "Best val accuracy: 0.9704142011834319\n",
        "Elapsed time: 0:00:07.69\n",
        "```"
      ],
      "metadata": {
        "id": "CH7l-JmUbVaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2B: Visualize Effects of `L2 Regularization`.\n",
        "\n",
        "The code in the cell below is **exactly** the same as that used in `Example 1B` except the axis titles have been changed to specify the type of overfitting protection that was used."
      ],
      "metadata": {
        "id": "7rtE2hc_arZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2B: Visualize Effects of L2 Regularization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print title\n",
        "print(\"Example 2B: Visualize Effects of L2 Regularization\")\n",
        "\n",
        "# Create a figure with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 1: Loss (Left Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax1.plot(eg_history['loss'], label='Train Loss')\n",
        "ax1.plot(eg_history['val_loss'], label='Val Loss')\n",
        "ax1.set_title('Model Loss (L2 Regularization)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 2: Accuracy (Right Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax2.plot(eg_history['accuracy'], label='Train Acc')\n",
        "ax2.plot(eg_history['val_accuracy'], label='Val Acc')\n",
        "ax2.set_title('Model Accuracy (L2 Regularization)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XRnko8uZm68r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to these graphs.\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image03A.png)\n",
        "\n",
        "### **Comparative Analysis: L2 Regularization vs. No Prevention**\n",
        "\n",
        "These two sets of graphs provide a stark \"Before and After\" demonstration of how **L2 Regularization** (Weight Decay) stabilizes model training.\n",
        "\n",
        "##### **1. Top Graphs: Model with L2 Regularization**\n",
        "* **Model Loss (Left):** The most critical observation here is the **tight coupling** between the Training Loss (Blue) and Validation Loss (Orange). Unlike the chaotic divergence seen previously, the Validation line roughly follows the Training line downwards. They settle near each other (~0.1 for Train, ~0.2 for Val).\n",
        "* **Model Accuracy (Right):** The Validation Accuracy tracks the Training Accuracy very closely, reaching high performance (~95%).\n",
        "* **Diagnosis:** **Successful Generalization.** The L2 penalty effectively constrained the model weights. Instead of memorizing noise, the model was forced to learn robust patterns that applied equally well to unseen data.\n",
        "\n",
        "##### **2. Bottom Graphs: Model without Overfitting Prevention**\n",
        "* **Model Loss (Left):** This represents a catastrophic failure. While the Training Loss drops, the Validation Loss **explodes upwards** (from 1.0 to 3.5). The lines diverge immediately.\n",
        "* **Model Accuracy (Right):** The Training Accuracy climbs to ~77%, but the Validation Accuracy collapses to ~50% (essentially random guessing).\n",
        "* **Diagnosis:** **Severe Overfitting / Instability.** Without the constraint of L2 regularization, the model likely learned large, unstable weights that worked for the training data but produced massive errors on the validation data.\n",
        "\n",
        "##### **Key Takeaway**\n",
        "The addition of `weight_decay` in the optimizer turned a failing model (Bottom) into a highly accurate one (Top).\n",
        "* **Without L2:** The model became \"over-confident\" on training data, leading to massive errors on validation data.\n",
        "* **With L2:** The model remained \"disciplined,\" sacrificing a tiny bit of training perfection for massive gains in validation stability and accuracy."
      ],
      "metadata": {
        "id": "Z8tPwcphcsl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2A: L2 Regularization**\n",
        "\n",
        "Follow these step-by-step instructions to convert your Exercise 1A code (without L2 regularization) into Exercise 2A code (with L2 regularization).\n",
        "\n",
        "#### **Step 1: Set seed Parameter**\n",
        "\n",
        "Set the `seed_value` = 1948\n",
        "---\n",
        "\n",
        "#### **Step 2: Add L2_REG Parameter**\n",
        "\n",
        "**Location:** Section 1️⃣ Parameters (around line 18)\n",
        "\n",
        "**Add this new line after BATCH_SIZE:**\n",
        "\n",
        "    L2_REG = 0.01  # L2 regularization strength (weight decay)\n",
        "\n",
        "**Why:** This parameter controls the strength of L2 regularization. Common values range from 0.0001 to 0.1. A value of 0.01 is a good starting point.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 3: Update Optimizer with Weight Decay**\n",
        "\n",
        "**Location:** Section 7️⃣ Build Model & Optimizer (around line 122)\n",
        "\n",
        "**Original code:**\n",
        "\n",
        "    optimizer = optim.Adam(ex_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "**New code:**\n",
        "\n",
        "    optimizer = optim.Adam(ex_model.parameters(),\n",
        "                           lr=LEARNING_RATE,\n",
        "                           weight_decay=L2_REG)  # Apply L2 Penalty\n",
        "\n",
        "**Why:** The weight_decay parameter implements L2 regularization in PyTorch. It adds a penalty to the loss function proportional to the magnitude of the weights, which helps prevent overfitting by discouraging large weight values.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Summary of Changes**\n",
        "\n",
        "1. Set seed value to 1948\n",
        "2. Added L2_REG parameter (set to 0.01)\n",
        "3. Modified optimizer to include weight_decay=L2_REG\n",
        "\n",
        "---\n",
        "\n",
        "### **What L2 Regularization Does**\n",
        "\n",
        "**L2 regularization** (also called weight decay) is a technique to prevent overfitting by:\n",
        "- Adding a penalty term to the loss function based on the magnitude of model weights\n",
        "- Encouraging the model to keep weights small\n",
        "- Preventing the model from relying too heavily on any single feature\n",
        "- Improving generalization to new data\n",
        "\n",
        "The regularization penalty is calculated as: **lambda × sum(weight²)** where lambda is the L2_REG value.\n",
        "\n",
        "**Effect on training:**\n",
        "- Training accuracy may be slightly lower\n",
        "- Validation accuracy often improves\n",
        "- The model becomes more robust and generalizes better\n",
        "\n",
        "---\n",
        "\n",
        "#### **Experimenting with L2 Regularization**\n",
        "\n",
        "You can experiment with different L2_REG values to find the optimal amount of regularization:\n",
        "\n",
        "- **L2_REG = 0.0:** No regularization (same as Exercise 1A)\n",
        "- **L2_REG = 0.001:** Light regularization\n",
        "- **L2_REG = 0.01:** Moderate regularization (recommended starting point)\n",
        "- **L2_REG = 0.1:** Strong regularization\n",
        "- **L2_REG = 1.0:** Very strong regularization (may underfit)\n",
        "\n",
        "**Too little regularization:** Model may overfit (high training accuracy, low validation accuracy)\n",
        "\n",
        "**Too much regularization:** Model may underfit (low training and validation accuracy)\n",
        "\n",
        "---\n",
        "\n",
        "#### **Quick Checklist**\n",
        "\n",
        "Before running your code, verify you have made these changes:\n",
        "\n",
        "- Step 1: Added L2_REG parameter in the Parameters section\n",
        "- Step 2: Added weight_decay=L2_REG to the optimizer\n",
        "\n",
        "Once you've completed both steps, run your code and compare the results to Exercise 1A. You should observe how L2 regularization affects the training and validation accuracy!\n",
        "\n",
        "---\n",
        "\n",
        "#### **Expected Behavior**\n",
        "\n",
        "When you run this code with L2 regularization:\n",
        "- The model weights will be penalized for being too large\n",
        "- Training may take slightly longer to converge\n",
        "- Validation accuracy may improve compared to Exercise 1A\n",
        "- The gap between training and validation accuracy may decrease\n",
        "- The model should generalize better to unseen data\n"
      ],
      "metadata": {
        "id": "i3_f9nxJzj5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 2A: L2 Regularization\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LHiecUkXejd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "```text\n",
        "Exercise 2A: L2 Regularization\n",
        "✅ Seeds set to: 1948\n",
        "Using device: cpu\n",
        "------Training Starting for 100 epochs --------------\n",
        "\n",
        "Training finished\n",
        "Best val accuracy: 0.6486486486486487\n",
        "Elapsed time: 0:00:04.99\n",
        "```"
      ],
      "metadata": {
        "id": "5JZbjZlZejd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2B: Visualize Effects of L2 Regularization**\n",
        "\n",
        "To visualize the effects of L2 Regularization, `copy-and-paste` the code from **`Exercise 1B`** into the cell below.\n",
        "\n",
        "**Code Hints:**\n",
        "1. Change the axis title for the left graph to read:\n",
        "```text\n",
        "ax1.set_title('Model Loss (L2 Regularization)')\n",
        "```\n",
        "2. Change the axis title for the right graph to read:\n",
        "```text\n",
        "ax2.set_title('Model Accuracy (L2 Regularization)')\n",
        "```"
      ],
      "metadata": {
        "id": "G-ggkrEwejd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 2B: Visualize Effects of L2 Regularization\n",
        "\n"
      ],
      "metadata": {
        "id": "Cz-CJxPDpU5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image04A.png)\n"
      ],
      "metadata": {
        "id": "0jixmIxOM_ha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparative Analysis: Exercise 2A vs. Exercise 1A (Heart Disease Dataset)**\n",
        "\n",
        "This analysis compares the performance of the neural network on the **Heart Disease** dataset (Target: `RestingECG`) with and without L2 Regularization.\n",
        "\n",
        "##### **1. The Baseline: Exercise 1A (Image 02E - No Prevention)**\n",
        "* **Behavior:** This graph represents the \"worst-case scenario\" for the `RestingECG` classification task.\n",
        "* **Loss:** The Training Loss (Blue) drops smoothly, but the Validation Loss (Orange) explodes violently, rising from **~1.0 to over 3.0**.\n",
        "* **Accuracy:** While Training Accuracy reaches ~77%, the Validation Accuracy degrades to ~50% (random guessing).\n",
        "* **Diagnosis:** The model completely memorized the training data and failed to learn any generalizable features about the `RestingECG` patterns.\n",
        "\n",
        "##### **2. The Attempted Fix: Exercise 2A (Image 04E - L2 Regularization)**\n",
        "* **Behavior:** This model introduced **L2 Regularization** (Weight Decay) to penalize large weights.\n",
        "* **Loss Comparison:**\n",
        "    * **The Improvement:** The regularization *did* have an effect. The peak Validation Loss dropped from **>3.0** (in Exercise 1A) to **~1.4**. The \"explosion\" was dampened.\n",
        "    * **The Failure:** Despite the lower peak, the lines still diverge significantly. The Validation Loss is still increasing, not decreasing.\n",
        "* **Accuracy Comparison:**\n",
        "    * The Validation Accuracy remains highly volatile and low (fluctuating between 45% and 60%). It did not significantly improve over the baseline.\n",
        "* **Diagnosis:** **Insufficient Regularization.** The L2 penalty used in this exercise was likely too weak (e.g., a low `weight_decay` value). It curbed the extreme numeric instability seen in Exercise 1A but was not strong enough to force the model to learn a robust solution for the `RestingECG` target.\n",
        "\n",
        "##### **Summary**\n",
        "| Exercise | Technique | Val Loss Peak | Outcome |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Exercise 1A** | None | **~3.5** | **Catastrophic Overfitting** |\n",
        "| **Exercise 2A** | L2 Regularization | **~1.4** | **Dampened Overfitting** (Partial effect, but still failing to generalize) |"
      ],
      "metadata": {
        "id": "OrbI4dBzejd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dropout to Decrease Overfitting**\n",
        "\n",
        "Hinton, Srivastava, Krizhevsky, Sutskever, & Salakhutdinov (2012) introduced the **_dropout regularization_** algorithm.\n",
        "\n",
        "Although `dropout` works differently than `L2 Regularization`, it accomplishes the same goal—the prevention of **overfitting**. However, the algorithm does the task by actually **_removing_** neurons and connections—at least temporarily. Unlike `L2`, no weight penalty is added. `Dropout` does not directly seek to train small weights.\n",
        "\n",
        "`Dropout` works by causing hidden neurons of the neural network to be unavailable during part of the training. Dropping part of the neural network causes the remaining portion to be trained to still achieve a good score even without the dropped neurons. This technique decreases co-adaptation between neurons, which results in less overfitting.\n",
        "\n",
        "**Implementation in PyTorch**\n",
        "\n",
        "In PyTorch, `dropout` is implemented as a specific module (`nn.Dropout`) that is inserted into the model architecture, typically immediately after a dense (linear) layer or an activation function. The `nn.Dropout` module does not contain learnable weights itself; instead, it applies a \"mask\" to the data flowing through it.\n",
        "\n",
        "A critical aspect of using Dropout in PyTorch is managing the model's **state**:\n",
        "\n",
        "1.  **Training Mode (`model.train()`):** When the model is in training mode, the dropout layer is **active**. It randomly zeroes out (drops) a percentage of the elements in the input tensor based on the probability `p` you define.\n",
        "2.  **Evaluation Mode (`model.eval()`):** When the model is in evaluation mode (used for validation or testing), the dropout layer is **inactive**. It allows all data to pass through unchanged, ensuring that the full capacity of the network is used for making predictions.\n",
        "\n",
        "**How it Works Mechanically**\n",
        "\n",
        "PyTorch implements the `dropout layer` by temporarily masking neurons rather than permanently removing them. In other words, a dropout layer does **_not_** lose any of its neurons during the training process, and the model will still have the same number of neurons after training.\n",
        "\n",
        "This figure shows how a dropout layer might be situated with other layers.\n",
        "![Dropout Regularization](https://biologicslab.co/BIO1173/images/class_02/class_9_dropout.png \"Dropout Regularization\")\n",
        "\n",
        "**Dropout Regularization**\n",
        "\n",
        "The discarded neurons and their connections are shown as dashed lines. The input layer has two input neurons as well as a bias neuron. The second layer is a dense layer with three neurons and a bias neuron. The third layer shows the effect of dropout, where regular neurons are \"masked\" (ignored) even though they still exist in the architecture.\n",
        "\n",
        "While PyTorch drops these neurons during a training step, it neither calculates gradients for them nor updates their incoming weights. However, the final neural network will use _all_ of these neurons for the output when you switch to `model.eval()`.\n",
        "\n",
        "The specific neurons that are dropped change constantly. Although we might choose a probability of 50% for dropout, the computer will not necessarily drop exactly half the neurons every time. It is as if we flipped a coin for each of the dropout candidate neurons to choose if that neuron was dropped out. In PyTorch, a new random mask is generated for every **batch** of data during the forward pass.\n",
        "\n",
        "*Note: The bias neuron is typically never dropped; only the regular neurons connected to a dropout layer are candidates for removal.*"
      ],
      "metadata": {
        "id": "lryoaO4ErTKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Why does Dropout work?**\n",
        "\n",
        "A common question is why simply removing neurons would decrease overfitting. The answer lies in two main mechanisms: preventing **co-adaptation** (codependency) and simulating **ensemble learning**.\n",
        "\n",
        "### **1. Reducing Co-adaptation**\n",
        "Dropout reduces the chance of **codependency** developing between neurons. In a standard network, neurons can learn to fix up the mistakes of other neurons, creating complex co-adaptations that do not generalize to unseen data.\n",
        "\n",
        "When dropout is applied, a neuron can no longer rely on the presence of specific other neurons because they might be deactivated at any moment. This forces each neuron to be more robust and independent, learning features that are useful in a wide variety of contexts rather than just in specific combinations. As the attached video explains, this forces the network to learn a **redundant representation** of the data—ensuring that if one pathway is \"squashed,\" the information still flows through another.\n",
        "\n",
        "### **2. The Ensemble Effect (Bootstrapping)**\n",
        "Dropout also functions as a highly efficient form of **Ensemble Learning**.\n",
        "\n",
        "**Ensembling** is a standard machine learning technique where multiple models are combined to produce a better result than any individual model could achieve alone. (The term originates from musical ensembles, where the final sound is a harmonious combination of many instruments).\n",
        "\n",
        "One common ensemble method involves **Bootstrapping** (or Bagging), where a practitioner trains several independent neural networks on the same task. Because of random initialization and data sampling, each network makes different errors. When you average their outputs, the errors cancel out, resulting in a stronger, more generalized prediction.\n",
        "\n",
        "**How Dropout Mimics This:**\n",
        "Dropout mathematically approximates training a massive ensemble of different neural networks with shared weights.\n",
        "* **During Training:** Every time a batch of data is processed, a random set of neurons is dropped. This effectively creates a slightly different, \"thinned\" neural network for that specific step. Over many iterations, you are training exponentially many different variations of the network.\n",
        "* **During Testing:** When we stop dropping neurons (evaluation mode), we are essentially using a single \"averaged\" network that represents the consensus of all those temporary, thinned networks.\n",
        "\n",
        "Unlike traditional ensembling, which requires training and storing many separate models (computationally expensive), dropout allows you to achieve the same robustness within a **single model**.\n",
        "\n",
        "### **Video Explanation**\n",
        "This short `YouTube` video by Udacity provides a clear visualization of how dropout forces the network to learn redundant representations:"
      ],
      "metadata": {
        "id": "HJoxQ1Ehr-Jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"NhZVe50QwPM\"\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\">\n",
        "</iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "SpW2AdB6jGCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3A: Dropout\n",
        "\n",
        "The code in the cell below is an **exact** copy of the code in `Example 1A` with the following changes to the Model Architecture and Parameters to implement Dropout.\n",
        "\n",
        "##### **1. New Hyperparameter (Section 1)**\n",
        "Example 3A introduces a probability parameter that defines how aggressive the dropout should be.\n",
        "\n",
        "* **Code Change:**\n",
        "```text\n",
        "    DROPOUT_RATE = 0.5   # <--- New Parameter\n",
        "```\n",
        "* **Purpose:** This tells the model to randomly zero out 50% of the neurons in the hidden layer during every training step.\n",
        "\n",
        "##### **2. Model Architecture (Section 7)**\n",
        "This is where the actual structure of the neural network changes.\n",
        "\n",
        "* **In `__init__`:** A specific Dropout layer is defined using the standard PyTorch module `nn.Dropout`.\n",
        "* **In `forward`:** The dropout layer is applied to the data stream, typically immediately after the activation function (ReLU).\n",
        "\n",
        "**Difference in Code:**\n",
        "```text\n",
        "class ObesityNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        # --- ADDED IN EXAMPLE 3A ---\n",
        "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
        "        # ---------------------------\n",
        "        \n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        \n",
        "        # --- ADDED IN EXAMPLE 3A ---\n",
        "        x = self.dropout(x)  # Apply mask here\n",
        "        # ---------------------------\n",
        "        \n",
        "        x = self.output(x)\n",
        "        return x\n",
        "```\n"
      ],
      "metadata": {
        "id": "S1YlQAbQqaYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3A: Dropout\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# -------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# -------------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "PATIENCE      = 10\n",
        "VERBOSE       = 0\n",
        "LEARNING_RATE = 0.05\n",
        "BATCH_SIZE    = 32\n",
        "DROPOUT_RATE  = 0.5   # <--- New Parameter: Probability of zeroing a neuron\n",
        "\n",
        "# Print title\n",
        "print(\"Example 3A: Dropout\")\n",
        "\n",
        "# Set Seed\n",
        "seed_value = 1234\n",
        "set_seeds(seed_value)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# -------------------------------------------------------------------\n",
        "eg_df = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "    na_values=['NA','?']\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# -------------------------------------------------------------------\n",
        "eg_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "eg_df[eg_target_col] = le.fit_transform(eg_df[eg_target_col])\n",
        "\n",
        "eg_X = eg_df.drop(columns=[eg_target_col])\n",
        "eg_y = eg_df[eg_target_col].astype('int32')\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# -------------------------------------------------------------------\n",
        "eg_X_temp, eg_X_test, eg_y_temp, eg_y_test = train_test_split(\n",
        "    eg_X, eg_y, test_size=0.2, random_state=42, stratify=eg_y\n",
        ")\n",
        "\n",
        "eg_X_train, eg_X_val, eg_y_train, eg_y_val = train_test_split(\n",
        "    eg_X_temp, eg_y_temp, test_size=0.1, random_state=42, stratify=eg_y_temp\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# -------------------------------------------------------------------\n",
        "categorical_cols = [c for c in eg_X.columns if eg_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in eg_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Fit & transform training data\n",
        "eg_X_train_proc = preprocessor.fit_transform(eg_X_train).astype(np.float32)\n",
        "eg_X_val_proc   = preprocessor.transform(eg_X_val).astype(np.float32)\n",
        "eg_X_test_proc  = preprocessor.transform(eg_X_test).astype(np.float32)\n",
        "\n",
        "eg_y_train = eg_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "eg_y_val   = eg_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "eg_y_test  = eg_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6️⃣  Convert to PyTorch Tensors & DataLoaders\n",
        "# -------------------------------------------------------------------\n",
        "eg_X_train_t = torch.tensor(eg_X_train_proc).to(device)\n",
        "eg_X_val_t   = torch.tensor(eg_X_val_proc).to(device)\n",
        "eg_X_test_t  = torch.tensor(eg_X_test_proc).to(device)\n",
        "\n",
        "eg_y_train_t = torch.tensor(eg_y_train, dtype=torch.long).to(device)\n",
        "eg_y_val_t   = torch.tensor(eg_y_val, dtype=torch.long).to(device)\n",
        "eg_y_test_t  = torch.tensor(eg_y_test, dtype=torch.long).to(device)\n",
        "\n",
        "eg_train_dataset = TensorDataset(eg_X_train_t, eg_y_train_t)\n",
        "eg_val_dataset   = TensorDataset(eg_X_val_t, eg_y_val_t)\n",
        "\n",
        "eg_train_loader = DataLoader(eg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "eg_val_loader   = DataLoader(eg_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 7️⃣  Build Model with Dropout (Standard PyTorch Practice)\n",
        "# -------------------------------------------------------------------\n",
        "n_classes = len(np.unique(eg_y_train))\n",
        "input_dim = eg_X_train_proc.shape[1]\n",
        "\n",
        "class ObesityNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        # Dropout Layer: Randomly zeroes some elements of the input tensor\n",
        "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)  # Apply Dropout after activation\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "eg_model = ObesityNet(input_dim, n_classes).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(eg_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 8️⃣  Train model (Standard Loop - No Manual L1 Math needed)\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"------Training Starting for {EPOCHS} epochs --------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "eg_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Training Phase ---\n",
        "    eg_model.train() # Important: Enables Dropout\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in eg_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = eg_model(X_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(eg_train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    eg_model.eval() # Important: Disables Dropout (uses full network)\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in eg_val_loader:\n",
        "            outputs = eg_model(X_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(eg_val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    # Store metrics\n",
        "    eg_history['accuracy'].append(train_acc)\n",
        "    eg_history['val_accuracy'].append(val_acc)\n",
        "    eg_history['loss'].append(avg_train_loss)\n",
        "    eg_history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "    # Print if verbose\n",
        "    if VERBOSE > 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "            f\"loss: {avg_train_loss:.4f} - \"\n",
        "            f\"acc: {train_acc:.4f} - \"\n",
        "            f\"val_loss: {avg_val_loss:.4f} - \"\n",
        "            f\"val_acc: {val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished\")\n",
        "print(\"Best val accuracy:\", max(eg_history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
      ],
      "metadata": {
        "id": "is5sgK40jkN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the last part of the output:\n",
        "```text\n",
        "Example 3A: Dropout\n",
        "✅ Seeds set to: 1234\n",
        "Using device: cpu\n",
        "------Training Starting for 100 epochs --------------\n",
        "\n",
        "Training finished\n",
        "Best val accuracy: 0.9526627218934911\n",
        "Elapsed time: 0:00:11.54\n",
        "```"
      ],
      "metadata": {
        "id": "Sn99dbOer2U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3B: Visualize Effects of Dropout\n",
        "\n",
        "The code in the cell below is **exactly** the same as that used in `Example 1B` except the axis titles have been changed to specify the type of overfitting protection that was used."
      ],
      "metadata": {
        "id": "XKfbOra3J4QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3B: Visualize Effects of Dropout\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print title\n",
        "print(\"Example 3B: Visualize Effects of Dropout\")\n",
        "\n",
        "# Create a figure with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 1: Loss (Left Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax1.plot(eg_history['loss'], label='Train Loss')\n",
        "ax1.plot(eg_history['val_loss'], label='Val Loss')\n",
        "ax1.set_title('Model Loss (Dropout)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 2: Accuracy (Right Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax2.plot(eg_history['accuracy'], label='Train Acc')\n",
        "ax2.plot(eg_history['val_accuracy'], label='Val Acc')\n",
        "ax2.set_title('Model Accuracy (Dropout)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pR2KRCVHj64e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image05A.png)\n"
      ],
      "metadata": {
        "id": "4IQ4Er4OOP0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparative Analysis: Dropout (Example 3A) vs. No Prevention (Example 1A)**\n",
        "\n",
        "These graphs illustrate the unique behavioral signature of **Dropout**. Unlike L2 Regularization, which simply \"squeezes\" the training and validation lines together, Dropout often inverts them.\n",
        "\n",
        "##### **1. Model Loss (Left Plot)**\n",
        "* **Example 1A (Baseline):**\n",
        "    * **Behavior:** The Training Loss dropped to nearly **0**, while the Validation Loss plateaued or rose.\n",
        "    * **The Gap:** A clear sign of overfitting where the model performed significantly better on training data than validation data.\n",
        "* **Example 3A (Dropout - Image 06E):**\n",
        "    * **Behavior:** The Training Loss (Blue) is actually **higher** than the Validation Loss (Orange).\n",
        "    * **The Inversion:** This \"inverted gap\" is a hallmark of strong Dropout (rate = 0.5).\n",
        "    * **Why this happens:**\n",
        "        * **During Training:** The model is \"handicapped\"—50% of its brain is turned off at random. It struggles to fit the data, resulting in higher loss (~0.55).\n",
        "        * **During Validation:** The \"handicap\" is removed (all neurons are active). The full power of the network is unleashed, resulting in lower loss (~0.30) and better performance.\n",
        "\n",
        "##### **2. Model Accuracy (Right Plot)**\n",
        "* **Example 1A (Baseline):**\n",
        "    * **Behavior:** The model reached **100% (1.0)** accuracy on the Training set.\n",
        "    * **Diagnosis:** Pure memorization.\n",
        "* **Example 3A (Dropout - Image 06E):**\n",
        "    * **Behavior:** The Training Accuracy struggles to reach **82%**, while the Validation Accuracy soars to **~92%**.\n",
        "    * **Diagnosis:** The model is prevented from memorizing the training data. It is forced to learn robust features that work well even when parts of the network are missing.\n",
        "\n",
        "##### **Key Takeaway**\n",
        "Dropout successfully solved the overfitting problem but introduced a trade-off:\n",
        "* **Stability:** The validation performance is extremely stable and higher than the training performance.\n",
        "* **Peak Performance:** You might notice the peak validation accuracy (~92%) is slightly lower than the unconstrained baseline (~96-97%). This is the **cost of regularization**—we sacrifice a small amount of theoretical peak accuracy to ensure the model isn't just hallucinating patterns (overfitting)."
      ],
      "metadata": {
        "id": "8vDGQnP-L5yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3A: Dropout**\n",
        "\n",
        "Follow these step-by-step instructions to convert your Exercise 1A code (without dropout) into Exercise 3A code (with dropout regularization).\n",
        "\n",
        "---\n",
        "\n",
        "##### **Step 1: Add DROPOUT_RATE Parameter**\n",
        "\n",
        "**Location:** Section 1️⃣ Parameters (around line 18)\n",
        "\n",
        "**Add this new line after BATCH_SIZE:**\n",
        "\n",
        "    DROPOUT_RATE = 0.5   # Dropout rate (probability of dropping a neuron)\n",
        "\n",
        "**Why:** This parameter controls the dropout rate. A value of 0.5 means that during training, each neuron has a 50% chance of being temporarily \"dropped out\" (set to zero). Common values range from 0.2 to 0.5.\n",
        "\n",
        "---\n",
        "\n",
        "##### **Step 2: Modify the Model Class**\n",
        "\n",
        "**Location:** Section 7️⃣ Build Model & Optimizer (around line 109)\n",
        "\n",
        "**Original code:**\n",
        "\n",
        "    class HeartDiseaseNet(nn.Module):\n",
        "        def __init__(self, input_dim, output_dim):\n",
        "            super(HeartDiseaseNet, self).__init__()\n",
        "            self.layer1 = nn.Linear(input_dim, 32)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.output = nn.Linear(32, output_dim)\n",
        "            # Note: No Softmax here. CrossEntropyLoss expects raw logits.\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.relu(self.layer1(x))\n",
        "            x = self.output(x)\n",
        "            return x\n",
        "\n",
        "**New code:**\n",
        "\n",
        "    class HeartDiseaseNet(nn.Module):\n",
        "        def __init__(self, input_dim, output_dim):\n",
        "            super(HeartDiseaseNet, self).__init__()\n",
        "            self.layer1 = nn.Linear(input_dim, 32)\n",
        "            self.relu = nn.ReLU()\n",
        "            # Dropout Layer\n",
        "            self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
        "            self.output = nn.Linear(32, output_dim)\n",
        "            # Note: No Softmax here. CrossEntropyLoss expects raw logits.\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.relu(self.layer1(x))\n",
        "            x = self.dropout(x)  # Apply Dropout after activation\n",
        "            x = self.output(x)\n",
        "            return x\n",
        "\n",
        "**Why:**\n",
        "- The dropout layer is added after the ReLU activation\n",
        "- During training, dropout randomly sets a fraction of neurons to zero\n",
        "- During evaluation, dropout is automatically disabled\n",
        "\n",
        "---\n",
        "\n",
        "##### **Summary of Changes**\n",
        "\n",
        "1. Added DROPOUT_RATE parameter (set to 0.5)\n",
        "2. Added nn.Dropout layer in the __init__ method\n",
        "3. Applied dropout in the forward method after ReLU activation\n",
        "\n",
        "---\n",
        "\n",
        "##### **What Dropout Does**\n",
        "\n",
        "**Dropout** is a powerful regularization technique that prevents overfitting by:\n",
        "- Randomly \"dropping out\" (setting to zero) a fraction of neurons during training\n",
        "- Forcing the network to learn redundant representations\n",
        "- Preventing neurons from co-adapting too much\n",
        "- Creating an ensemble effect where different subnetworks are trained\n",
        "\n",
        "**Important behavior:**\n",
        "- **During training:** Dropout is ACTIVE - neurons are randomly dropped\n",
        "- **During evaluation/testing:** Dropout is INACTIVE - all neurons are used\n",
        "\n",
        "PyTorch automatically handles this through the model.train() and model.eval() modes.\n",
        "\n",
        "**Effect on training:**\n",
        "- Training accuracy will be lower due to neurons being dropped\n",
        "- Validation accuracy often improves\n",
        "- The model becomes more robust and generalizes better\n",
        "- Training may take slightly longer to converge\n",
        "\n",
        "---\n",
        "\n",
        "##### **How Dropout Works**\n",
        "\n",
        "When DROPOUT_RATE = 0.5:\n",
        "1. During training, each neuron in the dropout layer has a 50% chance of being set to zero\n",
        "2. The remaining active neurons are scaled up by 1/(1-0.5) = 2 to maintain the expected output\n",
        "3. During validation/testing, all neurons are active (no dropout)\n",
        "4. This creates a regularization effect similar to training an ensemble of networks\n",
        "\n",
        "---\n",
        "\n",
        "##### **Experimenting with Dropout**\n",
        "\n",
        "You can experiment with different DROPOUT_RATE values:\n",
        "\n",
        "- **DROPOUT_RATE = 0.0:** No dropout (same as Exercise 1A)\n",
        "- **DROPOUT_RATE = 0.2:** Light dropout (20% of neurons dropped)\n",
        "- **DROPOUT_RATE = 0.5:** Moderate dropout (50% of neurons dropped) - recommended\n",
        "- **DROPOUT_RATE = 0.7:** Strong dropout (70% of neurons dropped)\n",
        "- **DROPOUT_RATE = 0.9:** Very strong dropout (may underfit)\n",
        "\n",
        "**Too little dropout:** Model may still overfit\n",
        "\n",
        "**Too much dropout:** Model may underfit and not learn effectively\n",
        "\n",
        "---\n",
        "\n",
        "##### **Where to Place Dropout**\n",
        "\n",
        "In this exercise, dropout is placed after the ReLU activation of the hidden layer. Common placement strategies:\n",
        "\n",
        "1. **After activation functions** (our approach) - most common\n",
        "2. **Before activation functions** - less common\n",
        "3. **After input layer** - useful for noisy input data\n",
        "4. **Multiple dropout layers** - for deeper networks\n",
        "\n",
        "For this simple network with one hidden layer, placing dropout after the ReLU activation is the standard approach.\n",
        "\n",
        "---\n",
        "\n",
        "##### **Quick Checklist**\n",
        "\n",
        "Before running your code, verify you have made these changes:\n",
        "\n",
        "- Step 1: Added DROPOUT_RATE parameter in the Parameters section\n",
        "- Step 2: Added self.dropout = nn.Dropout(p=DROPOUT_RATE) in __init__\n",
        "- Step 3: Added x = self.dropout(x) in the forward method\n",
        "\n",
        "Once you've completed all steps, run your code and compare the results to Exercise 1A. You should observe how dropout affects training and validation accuracy!\n",
        "\n",
        "---\n",
        "\n",
        "##### **Expected Behavior**\n",
        "\n",
        "When you run this code with dropout:\n",
        "- Training accuracy will be noticeably lower than Exercise 1A (because neurons are being dropped)\n",
        "- Validation accuracy may be higher than Exercise 1A (better generalization)\n",
        "- The gap between training and validation accuracy should decrease\n",
        "- The model should be more robust to overfitting\n",
        "- You may see more variation in training metrics between epochs\n",
        "\n",
        "**Note:** The model automatically switches between training mode (dropout active) and evaluation mode (dropout inactive) using model.train() and model.eval().\n"
      ],
      "metadata": {
        "id": "Qv96Srt17k3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 3A: Dropout\n",
        "\n"
      ],
      "metadata": {
        "id": "35iiG_fKSXmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "```text\n",
        "Exercise 3A: Dropout\n",
        "✅ Seeds set to: 666\n",
        "Using device: cpu\n",
        "------Training Starting for 100 epochs --------------\n",
        "\n",
        "Training finished\n",
        "Best val accuracy: 0.6621621621621622\n",
        "Elapsed time: 0:00:04.54\n",
        "```"
      ],
      "metadata": {
        "id": "kDiVzAPmSXmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3B: Visualize Effects of Dropout**\n",
        "\n",
        "`Copy-and-paste` the code from **`Exercise 1B`** into the cell below. Change the titles of axis 1 and axis 2 to read \"Dropout\")."
      ],
      "metadata": {
        "id": "pWqR8MLORylJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 3B: Visualize Effects of Dropout\n",
        "\n"
      ],
      "metadata": {
        "id": "rRy0ujxskyO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to these plots.\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image06A.png)"
      ],
      "metadata": {
        "id": "6qX0pqDo8ZlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Comparative Analysis: Dropout (Exercise 3A) vs. Baseline (Exercise 1A)**\n",
        "\n",
        "This comparison highlights how **Dropout** functions on the Heart Disease dataset. While it doesn't create the perfect \"inverted\" graph seen in the Obesity example, it significantly dampens the catastrophic overfitting seen in the baseline.\n",
        "\n",
        "##### **1. Model Loss (Left Plot)**\n",
        "* **Exercise 1A (Baseline - Image 02E):**\n",
        "    * **Behavior:** Total divergence. The Training Loss drops to ~0.5, while Validation Loss skyrockets to **>3.0**.\n",
        "    * **Diagnosis:** The model is completely memorizing the training data.\n",
        "* **Exercise 3A (Dropout - Image 07E):**\n",
        "    * **Behavior:** The divergence is **constrained**. The Validation Loss starts low but rises to **~1.2**.\n",
        "    * **Comparison:** While the Validation Loss is still increasing (indicating the model is struggling to generalize perfectly), Dropout has prevented the massive explosion seen in 1A. The gap between Train and Validation is much smaller.\n",
        "\n",
        "##### **2. Model Accuracy (Right Plot)**\n",
        "* **Exercise 1A (Baseline - Image 02E):**\n",
        "    * **Behavior:** Training Accuracy climbs to **~77%**, but Validation Accuracy crashes to **~50%**.\n",
        "    * **Diagnosis:** The model's \"knowledge\" is useless on new data.\n",
        "* **Exercise 3A (Dropout - Image 07E):**\n",
        "    * **Behavior:** Training and Validation Accuracy are tightly coupled.\n",
        "    * **Training:** Reaches ~64%.\n",
        "    * **Validation:** Hovers around **64-65%**.\n",
        "    * **Observation:** The model is no longer hallucinating high accuracy. By dropping neurons, the training accuracy has been forced down (from 77% to 64%), but the validation accuracy has stabilized and is actually performing *better* than the baseline (64% vs 50%).\n",
        "\n",
        "##### **Summary Table**\n",
        "\n",
        "| Metric | Exercise 1A (None) | Exercise 3A (Dropout) | Conclusion |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Peak Val Loss** | **~3.5** (Explosion) | **~1.2** (Rising) | Dropout reduced error magnitude by ~65%. |\n",
        "| **Val Accuracy** | **~50%** (Random) | **~64%** (Stable) | Dropout improved real-world performance. |\n",
        "| **Diagnosis** | Total Failure | **Stabilized** | The model is now stable, though likely needs further tuning (e.g., higher dropout rate or simpler architecture) to reduce the rising loss further. |"
      ],
      "metadata": {
        "id": "AbdhGv4SRylK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Batch Normalization**\n",
        "\n",
        "**Batch Normalization (BatchNorm)** is a technique introduced in 2015 by Ioffe and Szegedy that addresses one of the most difficult problems in training deep neural networks: **Internal Covariate Shift**. While primarily designed to speed up training, it also acts as a powerful regularizer that helps reduce overfitting.\n",
        "\n",
        "##### **The Problem: Internal Covariate Shift**\n",
        "As a neural network trains, the weights in the early layers change. This changes the distribution of the data (the \"covariate\") fed into the subsequent layers.\n",
        "* **The Consequence:** The deeper layers are constantly trying to adapt to a \"moving target.\" They have to re-adjust not just to the errors, but to the changing input distribution from the previous layer.\n",
        "* **The Result:** Training is slow, unstable, and requires very small learning rates.\n",
        "\n",
        "##### **The Solution: Normalize Every Layer**\n",
        "Batch Normalization solves this by forcing the inputs of a layer to have a mean of 0 and a variance of 1. It does this by normalizing the data using the statistics (mean and standard deviation) of the current **mini-batch**.\n",
        "\n",
        "After normalizing, the layer applies two learnable parameters, $\\gamma$ (scale) and $\\beta$ (shift), so the network can \"undo\" the normalization if it decides that the raw data was actually better.\n",
        "\n",
        "##### **How Batch Normalization Reduces Overfitting**\n",
        "Although BatchNorm was designed for speed, it has a side effect that functions like **Regularization**:\n",
        "\n",
        "1.  **Noise Injection:** Because the mean and variance are calculated based on a small *random* batch of data (e.g., 32 samples) rather than the entire dataset, there is statistical noise in these estimates.\n",
        "2.  **Regularization Effect:** This noise prevents the specific weights of a hidden neuron from becoming hyper-sensitive to a specific input sample. The neuron must learn to work with the data regardless of how it is scaled by the rest of the batch.\n",
        "3.  **Result:** This slight randomization (similar to Dropout, but less aggressive) discourages the network from memorizing the training data.\n",
        "\n",
        "##### **PyTorch Implementation**\n",
        "In PyTorch, you use `nn.BatchNorm1d` (for standard feedforward networks) or `nn.BatchNorm2d` (for images/CNNs).\n",
        "\n",
        "**Key Rules:**\n",
        "1.  **Placement:** It is typically placed **after** the linear layer but **before** the activation function (Linear $\\rightarrow$ BatchNorm $\\rightarrow$ ReLU).\n",
        "2.  **Train/Eval Modes:** Like Dropout, BatchNorm behaves differently during training and validation.\n",
        "    * **`model.train()`:** It calculates the mean/std from the current batch.\n",
        "    * **`model.eval()`:** It uses a \"running average\" of mean/std that it learned during training. **You must switch to eval mode for validation, or your results will be wrong.**\n"
      ],
      "metadata": {
        "id": "4kOKiKD6z4ZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4A: Batch Normalization\n",
        "\n",
        "While the data loading and loop structures are nearly identical, **Example 4A** introduces structural changes to the neural network to implement Batch Normalization.\n",
        "\n",
        "##### **1. Model Architecture (Section 7)**\n",
        "To add Batch Normalization to the neural network, a specific normalization layer is inserted into the network's architecture.\n",
        "\n",
        "* **In `__init__`:** We define the Batch Norm layer using `nn.BatchNorm1d`.\n",
        "    * *Note:* We use `1d` because our data is a flat vector of numbers. If we were processing images, we would use `2d`.\n",
        "    * *Parameter:* The argument `32` matches the number of neurons in the preceding layer.\n",
        "* **In `forward`:** The standard order of operations is applied: **Linear Layer $\\rightarrow$ Batch Norm $\\rightarrow$ Activation**.\n",
        "\n",
        "**Difference in Code:**\n",
        "\n",
        "The following code is used to change the architecture of the neural network to incorporate batch normalization:\n",
        "\n",
        "```text\n",
        "class ObesityNetBN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNetBN, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        \n",
        "        # --- ADDED IN EXAMPLE 4A ---\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        # ---------------------------\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        \n",
        "        # --- ADDED IN EXAMPLE 4A ---\n",
        "        x = self.bn1(x)  # Normalize the linear output\n",
        "        # ---------------------------\n",
        "        \n",
        "        x = self.relu(x) # Apply activation to normalized data\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "```\n"
      ],
      "metadata": {
        "id": "Wtm0qoCDpt_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4A: Batch Normalization\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# -------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# -------------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "VERBOSE       = 0\n",
        "LEARNING_RATE = 0.05\n",
        "BATCH_SIZE    = 32\n",
        "\n",
        "# Print title\n",
        "print(\"Example 4A: Batch Normalization\")\n",
        "\n",
        "# Set Seed\n",
        "seed_value = 1984\n",
        "set_seeds(seed_value)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# -------------------------------------------------------------------\n",
        "eg_df = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "    na_values=['NA','?']\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# -------------------------------------------------------------------\n",
        "eg_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "eg_df[eg_target_col] = le.fit_transform(eg_df[eg_target_col])\n",
        "\n",
        "eg_X = eg_df.drop(columns=[eg_target_col])\n",
        "eg_y = eg_df[eg_target_col].astype('int32')\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# -------------------------------------------------------------------\n",
        "eg_X_temp, eg_X_test, eg_y_temp, eg_y_test = train_test_split(\n",
        "    eg_X, eg_y, test_size=0.2, random_state=42, stratify=eg_y\n",
        ")\n",
        "\n",
        "eg_X_train, eg_X_val, eg_y_train, eg_y_val = train_test_split(\n",
        "    eg_X_temp, eg_y_temp, test_size=0.1, random_state=42, stratify=eg_y_temp\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# -------------------------------------------------------------------\n",
        "categorical_cols = [c for c in eg_X.columns if eg_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in eg_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Fit & transform training data\n",
        "eg_X_train_proc = preprocessor.fit_transform(eg_X_train).astype(np.float32)\n",
        "eg_X_val_proc   = preprocessor.transform(eg_X_val).astype(np.float32)\n",
        "eg_X_test_proc  = preprocessor.transform(eg_X_test).astype(np.float32)\n",
        "\n",
        "eg_y_train = eg_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "eg_y_val   = eg_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "eg_y_test  = eg_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6️⃣  Convert to PyTorch Tensors & DataLoaders\n",
        "# -------------------------------------------------------------------\n",
        "eg_X_train_t = torch.tensor(eg_X_train_proc).to(device)\n",
        "eg_X_val_t   = torch.tensor(eg_X_val_proc).to(device)\n",
        "eg_X_test_t  = torch.tensor(eg_X_test_proc).to(device)\n",
        "\n",
        "eg_y_train_t = torch.tensor(eg_y_train, dtype=torch.long).to(device)\n",
        "eg_y_val_t   = torch.tensor(eg_y_val, dtype=torch.long).to(device)\n",
        "eg_y_test_t  = torch.tensor(eg_y_test, dtype=torch.long).to(device)\n",
        "\n",
        "eg_train_dataset = TensorDataset(eg_X_train_t, eg_y_train_t)\n",
        "eg_val_dataset   = TensorDataset(eg_X_val_t, eg_y_val_t)\n",
        "\n",
        "eg_train_loader = DataLoader(eg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "eg_val_loader   = DataLoader(eg_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 7️⃣  Build Model with Batch Normalization\n",
        "# -------------------------------------------------------------------\n",
        "n_classes = len(np.unique(eg_y_train))\n",
        "input_dim = eg_X_train_proc.shape[1]\n",
        "\n",
        "class ObesityNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNet, self).__init__()\n",
        "\n",
        "        # Hidden Layer\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "\n",
        "        # Batch Norm Layer (32 features coming from layer1)\n",
        "        # 1D is used for Dense/Linear layers\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Output Layer\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Order: Linear -> BatchNorm -> ReLU\n",
        "        x = self.layer1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "eg_model = ObesityNet(input_dim, n_classes).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(eg_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 8️⃣  Train model\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"------Training Starting for {EPOCHS} epochs --------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "eg_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Training Phase ---\n",
        "    # Important: eg_model.train() tells BN to use batch statistics\n",
        "    eg_model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in eg_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = eg_model(X_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(eg_train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    # Important: eg_model.eval() tells BN to use running stats (not batch stats)\n",
        "    eg_model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in eg_val_loader:\n",
        "            outputs = eg_model(X_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(eg_val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    # Store metrics\n",
        "    eg_history['accuracy'].append(train_acc)\n",
        "    eg_history['val_accuracy'].append(val_acc)\n",
        "    eg_history['loss'].append(avg_train_loss)\n",
        "    eg_history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "    if VERBOSE > 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "            f\"loss: {avg_train_loss:.4f} - \"\n",
        "            f\"acc: {train_acc:.4f} - \"\n",
        "            f\"val_loss: {avg_val_loss:.4f} - \"\n",
        "            f\"val_acc: {val_acc:.4f}\"\n",
        "        )\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished\")\n",
        "print(\"Best val accuracy:\", max(eg_history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
      ],
      "metadata": {
        "id": "2tuHT0xyp0hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "```text\n",
        "Example 4A: Batch Normalization\n",
        "✅ Seeds set to: 1984\n",
        "Using device: cpu\n",
        "------Training Starting for 100 epochs --------------\n",
        "\n",
        "Training finished\n",
        "Best val accuracy: 0.9349112426035503\n",
        "Elapsed time: 0:00:17.19\n",
        "```"
      ],
      "metadata": {
        "id": "ZQNU8NuT2DKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4B: Visualize Effects of Batch Normalization\n",
        "\n",
        "The code in the cell below is **exactly** the same as that used in `Example 1B` except the axis titles have been changed to specify the type of overfitting protection that was used."
      ],
      "metadata": {
        "id": "xQm3iTI2qgO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4B: Visualize Effects of Batch Normalization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "\n",
        "# Print title\n",
        "print(\"# Example 4B: Visualize Effects of Batch Normalization\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 1: Loss (Left Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax1.plot(eg_history['loss'], label='Train Loss')\n",
        "ax1.plot(eg_history['val_loss'], label='Val Loss')\n",
        "ax1.set_title('Model Loss (Batch Normization)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 2: Accuracy (Right Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax2.plot(eg_history['accuracy'], label='Train Acc')\n",
        "ax2.plot(eg_history['val_accuracy'], label='Val Acc')\n",
        "ax2.set_title('Model Accuracy (Batch Normization)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V9jKOf_uqiXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to these plots.\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image07A.png)\n"
      ],
      "metadata": {
        "id": "qP2egILqQBFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparative Analysis: Batch Normalization (Example 4A) vs. Baseline (Example 1A)**\n",
        "\n",
        "This comparison highlights a distinct characteristic of Batch Normalization: **Noise Injection**. While L2 Regularization smoothes the curves and Dropout inverts them, Batch Normalization often makes the training dynamics more volatile (jagged) while preventing pure memorization.\n",
        "\n",
        "##### **1. Model Loss (Left Plot)**\n",
        "* **Example 1A (Baseline - Image 01E):**\n",
        "    * **Behavior:** The Training Loss dropped smoothly to nearly **0**.\n",
        "    * **Stability:** After the initial drop, the curves were relatively smooth, indicating the model settled into a specific solution (memorization).\n",
        "* **Example 4A (Batch Norm - Image 09E):**\n",
        "    * **Behavior:** The Training Loss does **not** reach 0; it hovers around **0.15**.\n",
        "    * **Volatility:** The lines are extremely **jagged/spiky**.\n",
        "    * **Diagnosis:** This is the \"Noise Injection\" property of Batch Normalization in action. Because the model effectively sees slightly different data statistics in every batch, it cannot easily settle into a perfect memorization state. The loss remains higher because the model is constantly being forced to readjust.\n",
        "\n",
        "##### **2. Model Accuracy (Right Plot)**\n",
        "* **Example 1A (Baseline):**\n",
        "    * **Behavior:** The model achieved **100%** Training Accuracy and **~96%** Validation Accuracy.\n",
        "    * **Diagnosis:** Perfect memorization.\n",
        "* **Example 4A (Batch Norm - Image 09E):**\n",
        "    * **Behavior:** The Training Accuracy is capped at **~94%**, and Validation Accuracy hovers around **90-91%**.\n",
        "    * **Diagnosis:** Batch Normalization successfully prevented the model from reaching 100% training accuracy (memorization).\n",
        "\n",
        "##### **Key Takeaway**\n",
        "In this specific dataset, Batch Normalization acted as a very aggressive regularizer.\n",
        "* **The Trade-off:** While it successfully stopped the \"perfect memorization\" seen in Example 1A, the injected noise reduced the peak validation accuracy slightly (from ~96% to ~91%).\n",
        "* **When to use it:** This \"jagged\" behavior often stabilizes on larger datasets or with larger batch sizes. On smaller datasets (like this one), the statistical noise from small batches can sometimes be too aggressive, as seen in the volatility of the orange line."
      ],
      "metadata": {
        "id": "lMN7Gy283g0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4A: Batch Normalization**\n",
        "\n",
        "Follow these step-by-step instructions to convert your Exercise 1A code (without batch normalization) into Exercise 4A code (with batch normalization).\n",
        "\n",
        "---\n",
        "##### **Step 1: Set the seed value**\n",
        "\n",
        "set the `seed_value` = 2001\n",
        "\n",
        "##### **Step 2: Modify the Model Class to Include Batch Normalization**\n",
        "\n",
        "**Location:** Section 7️⃣ Build Model & Optimizer (around line 109)\n",
        "\n",
        "**Original code:**\n",
        "\n",
        "    class HeartDiseaseNet(nn.Module):\n",
        "        def __init__(self, input_dim, output_dim):\n",
        "            super(HeartDiseaseNet, self).__init__()\n",
        "            self.layer1 = nn.Linear(input_dim, 32)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.output = nn.Linear(32, output_dim)\n",
        "            # Note: No Softmax here. CrossEntropyLoss expects raw logits.\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.relu(self.layer1(x))\n",
        "            x = self.output(x)\n",
        "            return x\n",
        "\n",
        "**New code:**\n",
        "\n",
        "    class HeartDiseaseNet(nn.Module):\n",
        "        def __init__(self, input_dim, output_dim):\n",
        "            super(HeartDiseaseNet, self).__init__()\n",
        "            # Hidden Layer\n",
        "            self.layer1 = nn.Linear(input_dim, 32)\n",
        "            # Batch Norm Layer (32 features coming from layer1)\n",
        "            self.bn1 = nn.BatchNorm1d(32)\n",
        "            self.relu = nn.ReLU()\n",
        "            # Output Layer\n",
        "            self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Order: Linear -> BatchNorm -> ReLU\n",
        "            x = self.layer1(x)\n",
        "            x = self.bn1(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.output(x)\n",
        "            return x\n",
        "\n",
        "**Why:**\n",
        "- Added nn.BatchNorm1d(32) layer to normalize the outputs from layer1\n",
        "- The number 32 matches the output size of layer1\n",
        "- Modified forward method to apply batch normalization between the linear layer and activation\n",
        "- Standard order is: Linear -> BatchNorm -> Activation\n",
        "\n",
        "---\n",
        "\n",
        "##### **Summary of Changes**\n",
        "\n",
        "1. Set the `seed_value` = 2001\n",
        "2. Added self.bn1 = nn.BatchNorm1d(32) in the __init__ method\n",
        "3. Modified the forward method to apply batch normalization\n",
        "4. Changed the order of operations to: Linear -> BatchNorm -> ReLU -> Output\n",
        "\n",
        "---\n",
        "\n",
        "##### **What Batch Normalization Does**\n",
        "\n",
        "**Batch Normalization** is a technique that normalizes the inputs to each layer, providing several benefits:\n",
        "\n",
        "**Benefits:**\n",
        "- **Faster training:** Allows higher learning rates and faster convergence\n",
        "- **Reduces internal covariate shift:** Stabilizes the distribution of layer inputs during training\n",
        "- **Regularization effect:** Provides some regularization, reducing the need for dropout\n",
        "- **Less sensitive to initialization:** Makes the network less dependent on careful weight initialization\n",
        "- **Improves gradient flow:** Helps prevent vanishing/exploding gradients\n",
        "\n",
        "**How it works:**\n",
        "1. For each mini-batch, compute the mean and variance of the layer outputs\n",
        "2. Normalize the outputs to have zero mean and unit variance\n",
        "3. Apply learnable scale (gamma) and shift (beta) parameters\n",
        "4. During training: uses batch statistics\n",
        "5. During evaluation: uses running statistics accumulated during training\n",
        "\n",
        "---\n",
        "\n",
        "##### **Understanding `BatchNorm1d`**\n",
        "\n",
        "The BatchNorm1d layer requires one parameter: the number of features to normalize.\n",
        "\n",
        "    self.bn1 = nn.BatchNorm1d(32)\n",
        "\n",
        "- **32** is the number of features (output size of the previous linear layer)\n",
        "- BatchNorm1d is used for 1D data (like in fully connected networks)\n",
        "- For convolutional networks, you would use BatchNorm2d\n",
        "\n",
        "---\n",
        "\n",
        "##### **Layer Order: Why Linear -> BatchNorm -> Activation?**\n",
        "\n",
        "The standard and most common order is:\n",
        "\n",
        "    Linear -> BatchNorm -> Activation\n",
        "\n",
        "**Why this order?**\n",
        "1. **Normalizes pre-activations:** Batch norm normalizes the linear outputs before applying the activation function\n",
        "2. **Empirically better:** Research shows this order generally works best\n",
        "3. **Prevents saturation:** Helps activation functions (like ReLU) work in their optimal range\n",
        "\n",
        "**Alternative order (less common):**\n",
        "- Linear -> Activation -> BatchNorm (works but generally less effective)\n",
        "\n",
        "---\n",
        "\n",
        "##### **Training vs Evaluation Modes**\n",
        "\n",
        "Batch normalization behaves differently during training and evaluation:\n",
        "\n",
        "**Training mode (model.train()):**\n",
        "- Uses statistics (mean/variance) from the current mini-batch\n",
        "- Updates running statistics for use during evaluation\n",
        "- Adds some noise due to batch-level statistics (provides regularization)\n",
        "\n",
        "**Evaluation mode (model.eval()):**\n",
        "- Uses accumulated running statistics (computed during training)\n",
        "- Provides consistent, deterministic outputs\n",
        "- No noise added\n",
        "\n",
        "PyTorch automatically handles this switching through model.train() and model.eval().\n",
        "\n",
        "---\n",
        "\n",
        "#### **Important Notes**\n",
        "\n",
        "**No additional parameters needed:** Unlike dropout or L2 regularization, batch normalization doesn't require a hyperparameter to tune.\n",
        "\n",
        "**Automatic behavior:** The code already includes model.train() and model.eval() calls, so batch normalization will automatically:\n",
        "- Use batch statistics during training\n",
        "- Use running statistics during validation/testing\n",
        "\n",
        "**Learnable parameters:** Batch normalization adds two learnable parameters per feature:\n",
        "- **Gamma (scale):** Allows the network to scale the normalized values\n",
        "- **Beta (shift):** Allows the network to shift the normalized values\n",
        "- These are learned during training through backpropagation\n",
        "\n",
        "---\n",
        "\n",
        "##### **Where to Place Batch Normalization**\n",
        "\n",
        "In this exercise, batch normalization is placed after the linear layer and before the activation:\n",
        "\n",
        "    Linear(input_dim, 32) -> BatchNorm1d(32) -> ReLU() -> Linear(32, output_dim)\n",
        "\n",
        "**For deeper networks:**\n",
        "- Add batch normalization after each linear/convolutional layer\n",
        "- Place before the activation function (most common)\n",
        "- Don't add batch norm before the output layer (usually)\n",
        "\n",
        "**Example for a deeper network:**\n",
        "\n",
        "    Linear -> BatchNorm -> ReLU -> Linear -> BatchNorm -> ReLU -> Output\n",
        "\n",
        "---\n",
        "\n",
        "#### **Quick Checklist**\n",
        "\n",
        "Before running your code, verify you have made these changes:\n",
        "\n",
        "- Step 1: Added self.bn1 = nn.BatchNorm1d(32) in the __init__ method\n",
        "- Step 1: Modified forward method to apply batch normalization\n",
        "- Step 1: Ensured order is Linear -> BatchNorm -> ReLU\n",
        "\n",
        "Once you've completed the step, run your code and compare the results to Exercise 1A!\n",
        "\n",
        "---\n",
        "\n",
        "##### **Expected Behavior**\n",
        "\n",
        "When you run this code with batch normalization:\n",
        "- **Training may converge faster:** You might reach good accuracy in fewer epochs\n",
        "- **More stable training:** Loss curves should be smoother\n",
        "- **Potentially better accuracy:** Both training and validation accuracy may improve\n",
        "- **Slight regularization effect:** May see reduced overfitting\n",
        "- **Faster per-epoch training:** Modern GPUs are highly optimized for batch normalization\n",
        "\n",
        "**Comparison to Exercise 1A:**\n",
        "- Training should be more stable with less fluctuation in metrics\n",
        "- The model may reach higher validation accuracy\n",
        "- You might be able to use higher learning rates successfully\n",
        "\n",
        "---\n",
        "\n",
        "##### **Batch Normalization Statistics**\n",
        "\n",
        "Behind the scenes, BatchNorm1d maintains:\n",
        "- **running_mean:** Exponential moving average of batch means\n",
        "- **running_var:** Exponential moving average of batch variances\n",
        "- **num_batches_tracked:** Counter of batches seen during training\n",
        "\n",
        "These statistics are automatically updated during training and used during evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "##### **Additional Information**\n",
        "\n",
        "**Batch size considerations:**\n",
        "- Batch normalization works best with larger batch sizes (32 or more)\n",
        "- Very small batch sizes (< 8) may have unstable batch statistics\n",
        "- Current BATCH_SIZE = 32 is appropriate for batch normalization\n",
        "\n",
        "**When batch normalization may not help:**\n",
        "- Very small datasets (< 100 samples)\n",
        "- When batch size is very small (< 8)\n",
        "- For some recurrent architectures (use LayerNorm instead)\n",
        "\n",
        "For this heart disease dataset and configuration, batch normalization should work well!\n"
      ],
      "metadata": {
        "id": "eGEdL5DtBPmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 4A: Batch Normalization\n",
        "\n"
      ],
      "metadata": {
        "id": "1KmXgVa3rDRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "```text\n",
        "Exercise 4A: Batch Normalization\n",
        "✅ Seeds set to: 2001\n",
        "Using device: cpu\n",
        "------Training Starting for 100 epochs --------------\n",
        "\n",
        "Training finished\n",
        "Best val accuracy: 0.6351351351351351\n",
        "Elapsed time: 0:00:04.63\n",
        "```"
      ],
      "metadata": {
        "id": "Bzkab1ZCmDNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4B: Visualize the Effects of Batch Normalization**\n",
        "\n",
        "`Copy-and-paste` the code from **`Exercise 1B`** into the cell below. Change the titles of axis 1 and axis 2 to read \"Batch Normalization\")."
      ],
      "metadata": {
        "id": "73H_ZedPrD-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 4B: Visualize the Effects of Batch Normalization\n",
        "\n"
      ],
      "metadata": {
        "id": "qBa3zuS_rdfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to these plots.\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image08A.png)\n"
      ],
      "metadata": {
        "id": "or1Nr0ciRsDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparative Analysis: Batch Normalization (Exercise 4A) vs. Baseline (Exercise 1A)**\n",
        "\n",
        "This comparison illustrates how Batch Normalization affects model training on the `RestingECG` prediction task. Similar to the previous examples, it acts as a regularizer, though the effects on this specific dataset are nuanced.\n",
        "\n",
        "##### **1. Model Loss (Left Plot)**\n",
        "* **Exercise 1A (Baseline - Image 02E):**\n",
        "    * **Behavior:** Catastrophic failure. The Validation Loss (Orange) diverges almost immediately and skyrockets to **>3.0**.\n",
        "    * **Diagnosis:** Complete memorization of training data with no ability to generalize.\n",
        "* **Exercise 4A (Batch Norm - Image 10E):**\n",
        "    * **Behavior:** The Validation Loss rises to **~1.3** by Epoch 100.\n",
        "    * **Comparison:** While the loss is still rising (indicating some overfitting is still occurring), Batch Normalization successfully **dampened the explosion**. It reduced the peak error by more than 50% compared to the baseline.\n",
        "    * **Dynamics:** Notice the training loss (Blue) flattens out around 0.7 rather than dropping to 0.5 like the baseline. This confirms that Batch Norm is making it harder for the model to \"cheat\" and memorize the training set.\n",
        "\n",
        "##### **2. Model Accuracy (Right Plot)**\n",
        "* **Exercise 1A (Baseline - Image 02E):**\n",
        "    * **Behavior:** Validation accuracy collapses to **~50%**, which is effectively random guessing for this task.\n",
        "    * **Stability:** Highly unstable.\n",
        "* **Exercise 4A (Batch Norm - Image 10E):**\n",
        "    * **Behavior:** Validation accuracy fluctuates between **50% and 60%**.\n",
        "    * **Comparison:** While not a \"solved\" problem (accuracy is still low), the model with Batch Normalization performs consistently better than the baseline. It avoids the complete collapse seen in 1A.\n",
        "\n",
        "##### **Summary Table**\n",
        "\n",
        "| Metric | Exercise 1A (None) | Exercise 4A (Batch Norm) | Conclusion |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Peak Val Loss** | **~3.5** | **~1.3** | Batch Norm significantly reduced the magnitude of overfitting errors. |\n",
        "| **Val Accuracy** | **~50%** (Random) | **~55-60%** (Slight Gain) | The model is more stable but struggles to learn high-quality features from this small dataset. |\n",
        "| **Overall** | **Total Failure** | **Stabilized Failure** | Batch Norm prevented the worst of the overfitting but didn't miraculously solve the problem. This suggests the model might need **stronger regularization** (like higher Dropout) or more data. |"
      ],
      "metadata": {
        "id": "vC8ea9yZ7Vky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Early Stopping in PyTorch to Prevent Overfitting**\n",
        "\n",
        "Now that we have established baselines in `Example 1` and in **`Exercise 1`** that **overfitting** occurs, we will now examine techniques that can be used to control overfitting. In **`Example 5`**, we will look at **`Early Stopping`**.\n",
        "\n",
        "It can be difficult to determine exactly how many epochs to cycle through to optimally train a neural network. **_Overfitting_** will occur if you train the neural network for too many epochs; the model will begin to memorize the training set and will not perform well on **new data**, despite attaining a high accuracy during training.\n",
        "\n",
        "Overfitting occurs when a neural network is trained to the point that it begins to **memorize rather than generalize**, as demonstrated in this figure.\n",
        "\n",
        "![Training vs. Validation Error for Overfitting](https://biologicslab.co/BIO1173/images/class_02/class_3_training_val.png \"Training vs. Validation Error for Overfitting\")\n",
        "**Training vs. Validation Error for Overfitting**\n",
        "\n",
        "### **Implementation in PyTorch**\n",
        "Unlike Keras, which provides a built-in callback for this, **PyTorch** requires us to implement Early Stopping logic manually within the training loop. This generally involves:\n",
        "1.  **Monitoring** the Validation Loss after every epoch.\n",
        "2.  **Saving** the model weights whenever the Validation Loss improves (decreases).\n",
        "3.  **Stopping** the training loop if the Validation Loss has not improved for a set number of epochs (a parameter often called `patience`).\n",
        "\n",
        "It is important to segment the original dataset into several datasets to support this process:\n",
        "\n",
        "* **Training Set**\n",
        "* **Validation Set**\n",
        "* **Holdout Set**\n",
        "\n",
        "You can construct these sets in several different ways. The following code blocks demonstrate the standard approach.\n",
        "\n",
        "The most common method uses a **`training`** and **`validation set`**. We use the `training data` to train the neural network and the `validation set` to determine when to stop. This attempts to halt training at the \"sweet spot\" before the validation error begins to rise.\n",
        "\n",
        "This method will only give accurate \"out of sample\" predictions for the `validation set`; this is usually 20% of the data. The predictions for the training data will be overly optimistic, as these were the data that we used to train the neural network.\n",
        "\n",
        "This figure demonstrates how we divide the dataset.\n",
        "\n",
        "![Training with a Validation Set](https://biologicslab.co/BIO1173/images/class_02/class_1_train_val.png \"Training with a Validation Set\")\n",
        "\n",
        "**Training with a Validation Set**"
      ],
      "metadata": {
        "id": "lTXppu2b8LZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5A: Early Stopping\n",
        "\n",
        "Adding Early Stopping fundamentally changes the **Training Loop**. Instead of blindly running for 100 epochs, it now includes logic to monitor performance, adjust learning rates, and stop training automatically.\n",
        "\n",
        "##### **1. New Parameters (Section 1)**\n",
        "Example 5A adds a parameter to define how long the model should wait for improvement before giving up.\n",
        "* **Code Change:**\n",
        "    ```python\n",
        "    PATIENCE = 10  # Stop if no improvement after 10 epochs\n",
        "    ```\n",
        "\n",
        "##### **2. Learning Rate Scheduler (Section 7)**\n",
        "Example 5A introduces a **Scheduler**. This is often used alongside Early Stopping to squeeze out a bit more performance before stopping.\n",
        "* **Code Change:** `optim.lr_scheduler.ReduceLROnPlateau` is added.\n",
        "* **Function:** If the validation accuracy stalls, it reduces the learning rate (by half in this code) to help the model fine-tune its weights.\n",
        "\n",
        "##### **3. Early Stopping Logic (Section 8 - Inside Loop)**\n",
        "In Example 1A, the loop simply ran `for epoch in range(EPOCHS)`. In Example 5A, a sophisticated check is added at the end of every epoch:\n",
        "\n",
        "1.  **Check Improvement:** It compares the current `val_acc` to the `best_val_acc` seen so far.\n",
        "2.  **Save Best Model:** If the current model is the best, it saves the weights immediately using `torch.save()`.\n",
        "    ```python\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(eg_model.state_dict(), checkpoint_path) # <--- CRITICAL\n",
        "        patience_counter = 0\n",
        "    ```\n",
        "3.  **Trigger Stop:** If performance didn't improve, it increases the `patience_counter`. If the counter hits the limit (`10`), the loop is broken immediately.\n",
        "    ```python\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(f\"Early stopping triggered...\")\n",
        "        break\n",
        "    ```\n",
        "\n",
        "##### **4. Restoring the Best Weights (Section 9)**\n",
        "This is a critical step often missed by beginners.\n",
        "* **The Problem:** When Early Stopping triggers (e.g., at Epoch 50), the model in memory is the one from Epoch 50. However, because patience was 10, the *best* model was actually from Epoch 40.\n",
        "* **The Fix:** Example 5A reloads the saved checkpoint at the very end to ensure `eg_model` contains the optimal weights, not the \"overfitted\" weights that triggered the stop.\n",
        "    \n",
        "  ```python\n",
        "    eg_model.load_state_dict(torch.load(checkpoint_path))\n",
        "  ```"
      ],
      "metadata": {
        "id": "d2Kns2kcIFgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5A: Early Stopping\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# ------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# ------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "PATIENCE      = 10\n",
        "VERBOSE       = 2     # Set to 1 or 2 to see training progress\n",
        "LEARNING_RATE = 0.05\n",
        "BATCH_SIZE    = 32\n",
        "\n",
        "# Print title\n",
        "print(\"Example 5A: Early Stopping\")\n",
        "\n",
        "# Set Seed\n",
        "seed_value = 1999\n",
        "set_seeds(seed_value)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "eg_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# ------------------------------------------------------------\n",
        "eg_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "eg_df[eg_target_col] = le.fit_transform(eg_df[eg_target_col])\n",
        "\n",
        "eg_X = eg_df.drop(columns=[eg_target_col])\n",
        "eg_y = eg_df[eg_target_col].astype('int32')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# ------------------------------------------------------------\n",
        "# First split into train+val and test\n",
        "eg_X_temp, eg_X_test, eg_y_temp, eg_y_test = train_test_split(\n",
        "    eg_X, eg_y, test_size=0.2, random_state=42, stratify=eg_y)\n",
        "\n",
        "# Then split train+val into train and val\n",
        "eg_X_train, eg_X_val, eg_y_train, eg_y_val = train_test_split(\n",
        "    eg_X_temp, eg_y_temp, test_size=0.1, random_state=42, stratify=eg_y_temp)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# ------------------------------------------------------------\n",
        "categorical_cols = [c for c in eg_X.columns if eg_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in eg_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Convert data to correct numeric type (Numpy arrays)\n",
        "eg_X_train_proc = preprocessor.fit_transform(eg_X_train).astype(np.float32)\n",
        "eg_X_val_proc   = preprocessor.transform(eg_X_val).astype(np.float32)\n",
        "eg_X_test_proc  = preprocessor.transform(eg_X_test).astype(np.float32)\n",
        "\n",
        "eg_y_train = eg_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "eg_y_val   = eg_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "eg_y_test  = eg_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6️⃣  Convert to PyTorch Tensors & DataLoaders\n",
        "# ------------------------------------------------------------\n",
        "eg_X_train_t = torch.tensor(eg_X_train_proc).to(device)\n",
        "eg_X_val_t   = torch.tensor(eg_X_val_proc).to(device)\n",
        "eg_X_test_t  = torch.tensor(eg_X_test_proc).to(device)\n",
        "\n",
        "eg_y_train_t = torch.tensor(eg_y_train, dtype=torch.long).to(device)\n",
        "eg_y_val_t   = torch.tensor(eg_y_val, dtype=torch.long).to(device)\n",
        "eg_y_test_t  = torch.tensor(eg_y_test, dtype=torch.long).to(device)\n",
        "\n",
        "eg_train_dataset = TensorDataset(eg_X_train_t, eg_y_train_t)\n",
        "eg_val_dataset   = TensorDataset(eg_X_val_t, eg_y_val_t)\n",
        "\n",
        "eg_train_loader = DataLoader(eg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "eg_val_loader   = DataLoader(eg_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7️⃣  Build Model, Optimizer & Scheduler\n",
        "# ------------------------------------------------------------\n",
        "n_classes = len(np.unique(eg_y_train))\n",
        "input_dim = eg_X_train_proc.shape[1]\n",
        "\n",
        "class ObesityNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "eg_model = ObesityNet(input_dim, n_classes).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(eg_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# ReduceLROnPlateau Scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
        "                                                 factor=0.5,\n",
        "                                                 patience=int(PATIENCE/2))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8️⃣  Train model with Early Stopping\n",
        "# ------------------------------------------------------------\n",
        "print(f\"-----Training Starting for up to {EPOCHS} epochs --------------------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "eg_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "# Variables for Early Stopping\n",
        "best_val_acc = 0.0\n",
        "patience_counter = 0\n",
        "checkpoint_path = \"eg_best_classification_model.pth\"\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Training Phase ---\n",
        "    eg_model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in eg_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = eg_model(X_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(eg_train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    eg_model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in eg_val_loader:\n",
        "            outputs = eg_model(X_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(eg_val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    # Update History\n",
        "    eg_history['accuracy'].append(train_acc)\n",
        "    eg_history['val_accuracy'].append(val_acc)\n",
        "    eg_history['loss'].append(avg_train_loss)\n",
        "    eg_history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "    # Step Scheduler (monitor val_accuracy)\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    # Print Progress\n",
        "    if VERBOSE > 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "            f\"loss: {avg_train_loss:.4f} - \"\n",
        "            f\"acc: {train_acc:.4f} - \"\n",
        "            f\"val_loss: {avg_val_loss:.4f} - \"\n",
        "            f\"val_acc: {val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "    # --- Early Stopping & Checkpointing Logic ---\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0 # Reset counter\n",
        "        torch.save(eg_model.state_dict(), checkpoint_path) # Save Best Model\n",
        "        if VERBOSE > 0:\n",
        "            print(f\"  -> Val Acc improved to {val_acc:.4f}. Model saved.\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if VERBOSE > 0:\n",
        "            print(f\"  -> No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training & Restore Best Weights\n",
        "# ---------------------------------------------------------------------------\n",
        "# Load the best weights back into the model\n",
        "eg_model.load_state_dict(torch.load(checkpoint_path))\n",
        "print(\"\\nRestored best model weights.\")\n",
        "\n",
        "print(f\"Training finished\")\n",
        "print(\"Best val accuracy:\", max(eg_history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
      ],
      "metadata": {
        "id": "NOqpiwIgeATc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "```text\n",
        "Example 5A: Early Stopping\n",
        "✅ Seeds set to: 1999\n",
        "Using device: cpu\n",
        "-----Training Starting for up to 100 epochs --------------------------\n",
        "Epoch 1/100 - loss: 0.8942 - acc: 0.6359 - val_loss: 0.5034 - val_acc: 0.7988\n",
        "  -> Val Acc improved to 0.7988. Model saved.\n",
        "Epoch 2/100 - loss: 0.3573 - acc: 0.8565 - val_loss: 0.3905 - val_acc: 0.8639\n",
        "  -> Val Acc improved to 0.8639. Model saved.\n",
        "Epoch 3/100 - loss: 0.2208 - acc: 0.9236 - val_loss: 0.2494 - val_acc: 0.9172\n",
        "  -> Val Acc improved to 0.9172. Model saved.\n",
        "Epoch 4/100 - loss: 0.2173 - acc: 0.9131 - val_loss: 0.2398 - val_acc: 0.8935\n",
        "  -> No improvement. Patience: 1/10\n",
        "Epoch 5/100 - loss: 0.1758 - acc: 0.9335 - val_loss: 0.1906 - val_acc: 0.9172\n",
        "  -> No improvement. Patience: 2/10\n",
        "Epoch 6/100 - loss: 0.0994 - acc: 0.9605 - val_loss: 0.2461 - val_acc: 0.8994\n",
        "  -> No improvement. Patience: 3/10\n",
        "Epoch 7/100 - loss: 0.1186 - acc: 0.9605 - val_loss: 0.2412 - val_acc: 0.8994\n",
        "  -> No improvement. Patience: 4/10\n",
        "Epoch 8/100 - loss: 0.1307 - acc: 0.9546 - val_loss: 0.1993 - val_acc: 0.9527\n",
        "  -> Val Acc improved to 0.9527. Model saved.\n",
        "Epoch 9/100 - loss: 0.0942 - acc: 0.9605 - val_loss: 0.1666 - val_acc: 0.9467\n",
        "  -> No improvement. Patience: 1/10\n",
        "Epoch 10/100 - loss: 0.1357 - acc: 0.9533 - val_loss: 0.4260 - val_acc: 0.8817\n",
        "  -> No improvement. Patience: 2/10\n",
        "Epoch 11/100 - loss: 0.1572 - acc: 0.9519 - val_loss: 0.1854 - val_acc: 0.9408\n",
        "  -> No improvement. Patience: 3/10\n",
        "Epoch 12/100 - loss: 0.1093 - acc: 0.9605 - val_loss: 0.2438 - val_acc: 0.9112\n",
        "  -> No improvement. Patience: 4/10\n",
        "Epoch 13/100 - loss: 0.1492 - acc: 0.9526 - val_loss: 0.2189 - val_acc: 0.9349\n",
        "  -> No improvement. Patience: 5/10\n",
        "Epoch 14/100 - loss: 0.0838 - acc: 0.9691 - val_loss: 0.1766 - val_acc: 0.9231\n",
        "  -> No improvement. Patience: 6/10\n",
        "Epoch 15/100 - loss: 0.0298 - acc: 0.9888 - val_loss: 0.1116 - val_acc: 0.9467\n",
        "  -> No improvement. Patience: 7/10\n",
        "Epoch 16/100 - loss: 0.0198 - acc: 0.9934 - val_loss: 0.1202 - val_acc: 0.9645\n",
        "  -> Val Acc improved to 0.9645. Model saved.\n",
        "Epoch 17/100 - loss: 0.0197 - acc: 0.9941 - val_loss: 0.0972 - val_acc: 0.9586\n",
        "  -> No improvement. Patience: 1/10\n",
        "Epoch 18/100 - loss: 0.0145 - acc: 0.9974 - val_loss: 0.1235 - val_acc: 0.9527\n",
        "  -> No improvement. Patience: 2/10\n",
        "Epoch 19/100 - loss: 0.0101 - acc: 0.9987 - val_loss: 0.1120 - val_acc: 0.9645\n",
        "  -> No improvement. Patience: 3/10\n",
        "Epoch 20/100 - loss: 0.0119 - acc: 0.9980 - val_loss: 0.1364 - val_acc: 0.9586\n",
        "  -> No improvement. Patience: 4/10\n",
        "Epoch 21/100 - loss: 0.0092 - acc: 0.9974 - val_loss: 0.1158 - val_acc: 0.9645\n",
        "  -> No improvement. Patience: 5/10\n",
        "Epoch 22/100 - loss: 0.0058 - acc: 0.9993 - val_loss: 0.1163 - val_acc: 0.9586\n",
        "  -> No improvement. Patience: 6/10\n",
        "Epoch 23/100 - loss: 0.0044 - acc: 1.0000 - val_loss: 0.1189 - val_acc: 0.9586\n",
        "  -> No improvement. Patience: 7/10\n",
        "Epoch 24/100 - loss: 0.0048 - acc: 1.0000 - val_loss: 0.1163 - val_acc: 0.9586\n",
        "  -> No improvement. Patience: 8/10\n",
        "Epoch 25/100 - loss: 0.0045 - acc: 1.0000 - val_loss: 0.1329 - val_acc: 0.9645\n",
        "  -> No improvement. Patience: 9/10\n",
        "Epoch 26/100 - loss: 0.0040 - acc: 1.0000 - val_loss: 0.1235 - val_acc: 0.9645\n",
        "  -> No improvement. Patience: 10/10\n",
        "\n",
        "Early stopping triggered at epoch 26\n",
        "\n",
        "Restored best model weights.\n",
        "Training finished\n",
        "Best val accuracy: 0.9644970414201184\n",
        "Elapsed time: 0:00:02.00\n",
        "```\n",
        "\n",
        "While the number of Epochs was set to `100`, training stopped in this particular training run after the `24th` Epoch. This was due to the `Early Stopping` monitor terminating training after waiting `10` epochs for the `val_accuracy` to start improving."
      ],
      "metadata": {
        "id": "XbpzAmsBJXBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Variables that Control Early Stopping**\n",
        "\n",
        "Since PyTorch requires us to implement Early Stopping manually (as shown in **Example 5A**), these \"arguments\" become **variables** or **logic checks** that you define in your training loop.\n",
        "\n",
        "* **`min_delta` (Threshold)**: This represents the minimum change required to qualify as an improvement. In your code, you can enforce this by adding a small buffer to your comparison (e.g., `if val_acc > best_val_acc + min_delta`). This prevents the model from continuing just because of tiny, insignificant fluctuations.\n",
        "* **`patience`**: Defined as a constant (e.g., `PATIENCE = 10`). This controls how many epochs the loop continues without improvement before breaking. You implement this using a counter variable that increments when performance stalls and resets when a new record is set.\n",
        "* **`verbose`**: A variable (e.g., `VERBOSE = 1`) used to control how much output is generated. You wrap your `print()` statements inside an `if VERBOSE > 0:` block to keep your output clean.\n",
        "* **`mode`**: In PyTorch, this is determined by the logic in your `if` statement.\n",
        "    * **\"min\" mode:** Use `<` (e.g., `if val_loss < best_val_loss`). Use this for metrics you want to minimize, like Loss or RMSE.\n",
        "    * **\"max\" mode:** Use `>` (e.g., `if val_acc > best_val_acc`). Use this for metrics you want to maximize, like Accuracy.\n",
        "* **`restore_best_weights`**: This is the most critical manual step. You must explicitly **save** the model (`torch.save`) whenever a new best metric is found, and then **reload** it (`model.load_state_dict`) after the loop finishes. If you skip this, your model will retain the weights from the *last* epoch (which are likely overfitted) rather than the *best* epoch."
      ],
      "metadata": {
        "id": "hSBtBlXV-MSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5B: Visualize Effects of Early Stopping\n",
        "\n",
        "The code in the cell below is **exactly** the same as that used in `Example 1B` except the axis titles have been changed to specify the type of overfitting protection that was used."
      ],
      "metadata": {
        "id": "QdLBStCj_yR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5B: Visualize effects of Early Stopping\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print title\n",
        "print(\"Example 5B: Visualize effects of Early Stopping\")\n",
        "\n",
        "# Create a figure with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 1: Loss (Left Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax1.plot(eg_history['loss'], label='Train Loss')\n",
        "ax1.plot(eg_history['val_loss'], label='Val Loss')\n",
        "ax1.set_title('Model Loss (Early Stopping)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 2: Accuracy (Right Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax2.plot(eg_history['accuracy'], label='Train Acc')\n",
        "ax2.plot(eg_history['val_accuracy'], label='Val Acc')\n",
        "ax2.set_title('Model Accuracy (Early Stopping)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zLN6M1_0f8jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image09A.png)\n"
      ],
      "metadata": {
        "id": "Gq0p_DizTCT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparative Analysis: Early Stopping (Example 5A) vs. Baseline (Example 1A)**\n",
        "\n",
        "This comparison highlights the efficiency and safety provided by **Early Stopping**. Unlike previous techniques that modify *how* the model learns (Regularization, Dropout, BatchNorm), Early Stopping simply modifies *how long* it learns.\n",
        "\n",
        "##### **1. The X-Axis (Epochs)**\n",
        "* **Example 1A (Baseline):**\n",
        "    * **Behavior:** The training ran for the full **100 Epochs**, regardless of whether the model was improving.\n",
        "    * **Result:** Resources were wasted for roughly 50+ epochs where the model was just memorizing data without improving validation performance.\n",
        "* **Example 5A (Early Stopping - Image 12E):**\n",
        "    * **Behavior:** The training stopped automatically at **Epoch 23**.\n",
        "    * **Result:** The technique detected that the model peaked around Epoch 13 (since Patience was 10, it waited 10 more epochs before stopping). This saved ~75% of the computational time.\n",
        "\n",
        "##### **2. Model Loss (Left Plot)**\n",
        "* **Example 1A:** The Training Loss (Blue) dropped to 0, while Validation Loss (Orange) drifted apart, indicating overfitting.\n",
        "* **Example 5A:** The Training Loss still drops rapidly, but the process cuts off before the Validation Loss has a chance to diverge significantly. The model is effectively \"frozen\" at its point of minimum error (roughly Epoch 13, where Val Loss is ~0.1).\n",
        "\n",
        "##### **3. Model Accuracy (Right Plot)**\n",
        "* **Example 1A:** Reached **100%** Training Accuracy and **~96-97%** Validation Accuracy.\n",
        "* **Example 5A:** Reached **100%** Training Accuracy and a very high Validation Accuracy (**~97-98%**).\n",
        "* **Observation:** By restoring the \"Best Weights\" (from Epoch 13) rather than using the \"Final Weights\" (from Epoch 23), Early Stopping ensures we keep the version of the model that generalizes best, rather than the one that memorized the most.\n",
        "\n",
        "##### **Summary Table**\n",
        "\n",
        "| Metric | Example 1A (No Prevention) | Example 5A (Early Stopping) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Training Time** | 100 Epochs (100%) | 23 Epochs (~25%) |\n",
        "| **Validation Loss** | ~0.25 (Drifting) | **~0.10** (Optimal) |\n",
        "| **Outcome** | Overfitting & Wasted Time | **Optimal Generalization & Efficiency** |"
      ],
      "metadata": {
        "id": "R7rzD3dW_yR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5A: Early Stopping**\n",
        "\n",
        "In the cell below, write the code to add Early Stopping to your `ex_model`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "First, start by copying-and-pasting Exercise 1A into the cell below.\n",
        "\n",
        "##### **1. New Parameters (Section 1)**\n",
        "Add a parameter to define how long the model should wait for improvement before giving up.\n",
        "* **Code Change:**\n",
        "    ```python\n",
        "    PATIENCE = 10  # Stop if no improvement after 10 epochs\n",
        "    ```\n",
        "\n",
        "##### **2. Learning Rate Scheduler (Section 7)**\n",
        "You will need to introduce a **Scheduler**. This is often used alongside Early Stopping to squeeze out a bit more performance before stopping.\n",
        "* **Code Change:** `optim.lr_scheduler.ReduceLROnPlateau` is added.\n",
        "* **Function:** If the validation accuracy stalls, it reduces the learning rate (by half in this code) to help the model fine-tune its weights.\n",
        "\n",
        "##### **3. Early Stopping Logic (Section 8 - Inside Loop)**\n",
        "In **Exercise 1A**, the loop simply ran `for epoch in range(EPOCHS)`. For **Exercise 5A**, you need to add a check to the end of every epoch using the following code:\n",
        "\n",
        "```Python\n",
        "    # --- Early Stopping & Checkpointing Logic ---\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0 # Reset counter\n",
        "        torch.save(ex_model.state_dict(), checkpoint_path) # Save Best Model\n",
        "        if VERBOSE > 0:\n",
        "            print(f\"  -> Val Acc improved to {val_acc:.4f}. Model saved.\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if VERBOSE > 0:\n",
        "            print(f\"  -> No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "```\n",
        "This code chunk compares the current `val_acc` to the `best_val_acc` seen so far. If the current model is the best, it saves the weights immediately using\n",
        "    ```Python\n",
        "      torch.save(ex_model.state_dict(), checkpoint_path) # Save Best Model\n",
        "    ```\n",
        "If performance didn't improve, it increases the `patience_counter`. If the counter hits the limit (`10`), the loop is broken immediately.\n",
        "    ```python\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(f\"Early stopping triggered...\")\n",
        "        break\n",
        "    ```\n",
        "\n",
        "##### **4. Restoring the Best Weights (Section 9)**\n",
        "This is a critical step often missed by beginners.\n",
        "* **The Problem:** When Early Stopping triggers (e.g., at Epoch 50), the model in memory is the one from Epoch 50. However, because patience was 10, the *best* model was actually from Epoch 40.\n",
        "* **The Fix:** You need the saved checkpoint at the very end to ensure `ex_model` contains the optimal weights, not the \"overfitted\" weights that triggered the stop.\n",
        "    \n",
        "  ```python\n",
        "    ex_model.load_state_dict(torch.load(checkpoint_path))\n",
        "  ```"
      ],
      "metadata": {
        "id": "mwyLo-hvs1JS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5A: Early Stopping**\n",
        "\n",
        "Follow these step-by-step instructions to convert your Exercise 1A code (without early stopping) into Exercise 5A code (with early stopping).\n",
        "\n",
        "---\n",
        "##### **Step 1: Change set seed**\n",
        "Change the `seed_value` = 1001\n",
        "\n",
        "##### **Step 2: Change VERBOSE Parameter**\n",
        "\n",
        "**Location:** Section 1️⃣ Parameters (around line 18)\n",
        "\n",
        "**Original code:**\n",
        "\n",
        "    VERBOSE = 0     # 0 means no output during training\n",
        "\n",
        "**New code:**\n",
        "\n",
        "    VERBOSE = 2     # Set to 1 or 2 to see training progress\n",
        "\n",
        "**Why:** Setting VERBOSE to 2 allows you to see the training progress and early stopping messages during training.\n",
        "\n",
        "---\n",
        "\n",
        "##### **Step 3: Add Early Stopping Variables and Scheduler**\n",
        "\n",
        "**Location:** Section 7️⃣ Build Model & Optimizer, immediately after defining the optimizer (after line 125)\n",
        "\n",
        "**Add these lines:**\n",
        "\n",
        "    # Variables for Early Stopping\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    checkpoint_path = \"ex_best_classification_model.pth\"\n",
        "\n",
        "    # Learning Rate Scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
        "                                                     factor=0.5,\n",
        "                                                     patience=int(PATIENCE/2))\n",
        "\n",
        "**Why:** These variables track the best validation accuracy, count epochs without improvement, define where to save the model, and create a learning rate scheduler that reduces the learning rate when validation accuracy plateaus.\n",
        "\n",
        "---\n",
        "\n",
        "##### **Step 4: Add Scheduler Step**\n",
        "\n",
        "**Location:** Inside the training loop, after storing metrics and before the verbose print (around line 188)\n",
        "\n",
        "**Add this line:**\n",
        "\n",
        "    # Step the Scheduler\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "**Why:** This updates the learning rate based on validation accuracy performance.\n",
        "\n",
        "---\n",
        "\n",
        "##### **Step 5: Add Early Stopping Logic**\n",
        "\n",
        "**Location:** Inside the training loop, after the verbose print section (around line 197)\n",
        "\n",
        "**Add this entire block:**\n",
        "\n",
        "    # --- Early Stopping & Checkpointing Logic ---\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0  # Reset counter\n",
        "        torch.save(ex_model.state_dict(), checkpoint_path)  # Save Best Model\n",
        "        if VERBOSE > 0:\n",
        "            print(f\"  -> Val Acc improved to {val_acc:.4f}. Model saved.\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if VERBOSE > 0:\n",
        "            print(f\"  -> No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "**Why:** This code saves the model when validation accuracy improves, increments a counter when it doesn't improve, and stops training early if there's no improvement for PATIENCE epochs.\n",
        "\n",
        "---\n",
        "\n",
        "##### **Step 6: Restore Best Model Weights**\n",
        "\n",
        "**Location:** Section 9️⃣ Inspect training, before printing results (around line 205)\n",
        "\n",
        "**Add these lines before the final print statements:**\n",
        "\n",
        "    # Load the best weights back into the model\n",
        "    ex_model.load_state_dict(torch.load(checkpoint_path))\n",
        "    print(\"\\nRestored best model weights.\")\n",
        "\n",
        "**Why:** This ensures you're using the best model (with the highest validation accuracy) rather than the model from the last epoch.\n",
        "\n",
        "---\n",
        "\n",
        "##### **Summary of Changes**\n",
        "\n",
        "1. Set the `seed_value` = 1001\n",
        "2. Changed VERBOSE from 0 to 2\n",
        "3. Added early stopping variables: best_val_acc, patience_counter, checkpoint_path\n",
        "4. Added learning rate scheduler\n",
        "5. Added scheduler.step(val_acc) inside training loop\n",
        "6. Added early stopping logic to save best model and stop training\n",
        "7. Added code to restore best model weights after training\n",
        "\n",
        "---\n",
        "\n",
        "##### **What Early Stopping Does**\n",
        "\n",
        "**Early stopping** monitors the validation accuracy during training:\n",
        "- When validation accuracy improves, it saves the model weights and resets the patience counter\n",
        "- When validation accuracy doesn't improve, it increments the patience counter\n",
        "- When the patience counter reaches PATIENCE (10 epochs), training stops early\n",
        "- After training, the best model weights are restored\n",
        "\n",
        "This prevents overfitting and saves training time by stopping when the model is no longer improving.\n",
        "\n",
        "---\n",
        "\n",
        "##### **Quick Checklist**\n",
        "\n",
        "Before running your code, verify you have made all these changes:\n",
        "\n",
        "- Step 1: Changed `seed_value` to 1001\n",
        "- Step 2: Changed VERBOSE to 2\n",
        "- Step 3: Renamed class to HeartDiseaseNet (2 places)\n",
        "- Step 4: Added early stopping variables and scheduler\n",
        "- Step 5: Updated print message to \"up to {EPOCHS} epochs\"\n",
        "- Step 6: Added scheduler.step(val_acc)\n",
        "- Step 6: Added early stopping logic block\n",
        "- Step 8: Added code to restore best model weights\n",
        "\n",
        "Once you've completed all steps, run your code and observe the early stopping in action!\n"
      ],
      "metadata": {
        "id": "HurYdINDxpZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 5A: Early Stopping\n",
        "\n"
      ],
      "metadata": {
        "id": "j0cg1r85uKkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "```text\n",
        "Exercise 5A: Early Stopping\n",
        "✅ Seeds set to: 1001\n",
        "Using device: cpu\n",
        "------Training Starting for up to 100 epochs --------------\n",
        "Epoch 1/100 - loss: 0.9466 - acc: 0.5939 - val_loss: 0.9982 - val_acc: 0.6081\n",
        "  -> Val Acc improved to 0.6081. Model saved.\n",
        "Epoch 2/100 - loss: 0.9001 - acc: 0.6091 - val_loss: 0.9040 - val_acc: 0.5946\n",
        "  -> No improvement. Patience: 1/10\n",
        "Epoch 3/100 - loss: 0.8611 - acc: 0.6258 - val_loss: 0.9661 - val_acc: 0.6216\n",
        "  -> Val Acc improved to 0.6216. Model saved.\n",
        "Epoch 4/100 - loss: 0.8432 - acc: 0.6152 - val_loss: 0.8957 - val_acc: 0.5946\n",
        "  -> No improvement. Patience: 1/10\n",
        "Epoch 5/100 - loss: 0.8211 - acc: 0.6288 - val_loss: 0.9278 - val_acc: 0.6216\n",
        "  -> No improvement. Patience: 2/10\n",
        "Epoch 6/100 - loss: 0.8099 - acc: 0.6333 - val_loss: 0.9400 - val_acc: 0.5946\n",
        "  -> No improvement. Patience: 3/10\n",
        "Epoch 7/100 - loss: 0.8113 - acc: 0.6227 - val_loss: 1.0119 - val_acc: 0.6081\n",
        "  -> No improvement. Patience: 4/10\n",
        "Epoch 8/100 - loss: 0.7772 - acc: 0.6439 - val_loss: 0.9164 - val_acc: 0.6081\n",
        "  -> No improvement. Patience: 5/10\n",
        "Epoch 9/100 - loss: 0.7726 - acc: 0.6394 - val_loss: 0.9511 - val_acc: 0.6081\n",
        "  -> No improvement. Patience: 6/10\n",
        "Epoch 10/100 - loss: 0.7474 - acc: 0.6364 - val_loss: 0.9782 - val_acc: 0.5946\n",
        "  -> No improvement. Patience: 7/10\n",
        "Epoch 11/100 - loss: 0.7108 - acc: 0.6667 - val_loss: 1.0596 - val_acc: 0.5811\n",
        "  -> No improvement. Patience: 8/10\n",
        "Epoch 12/100 - loss: 0.6979 - acc: 0.6682 - val_loss: 1.0719 - val_acc: 0.5811\n",
        "  -> No improvement. Patience: 9/10\n",
        "Epoch 13/100 - loss: 0.6973 - acc: 0.6682 - val_loss: 1.0970 - val_acc: 0.5541\n",
        "  -> No improvement. Patience: 10/10\n",
        "\n",
        "Early stopping triggered at epoch 13\n",
        "\n",
        "Restored best model weights.\n",
        "Training finished\n",
        "Best val accuracy: 0.6216216216216216\n",
        "Elapsed time: 0:00:00.40\n",
        "```\n",
        "\n",
        "While the number of Epochs was set to `100` training terminated in this particular training run after the `11th` Epoch due to the `Early Stopping` monitor."
      ],
      "metadata": {
        "id": "R5CRi6QoU3e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5B: Visualize Effects of Early Stopping**\n",
        "\n",
        "`Copy-and-paste` your code from **Exercise 1B`** into the cell below and change the axis titles to specify the type of overfitting protection."
      ],
      "metadata": {
        "id": "06ILcia_CBcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 5B: Visualize Effects of Early Stopping\n",
        "\n"
      ],
      "metadata": {
        "id": "RbHt9Xv6h0bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following pair of graphs:\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image10A.png)\n"
      ],
      "metadata": {
        "id": "cQnBpGMmUvZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparative Analysis: Early Stopping (Exercise 5A) vs. Baseline (Exercise 1A)**\n",
        "\n",
        "This comparison demonstrates how **Early Stopping** functions as a \"safety brake\" for the model. On the difficult Heart Disease dataset (`RestingECG`), where the model is prone to rapid overfitting, this technique prevents the catastrophic failure seen in the baseline.\n",
        "\n",
        "##### **1. The X-Axis (Training Duration)**\n",
        "* **Exercise 1A (Baseline - Image 02E):**\n",
        "    * **Behavior:** The model ran for the full **100 Epochs**.\n",
        "    * **Result:** It spent ~95 epochs actively making the model worse (increasing validation error).\n",
        "* **Exercise 5A (Early Stopping - Image 14E):**\n",
        "    * **Behavior:** Training terminated automatically at **Epoch 10**.\n",
        "    * **Result:** The algorithm detected that the model was not improving and pulled the plug immediately, saving 90% of the compute time.\n",
        "\n",
        "##### **2. Model Loss (Left Plot)**\n",
        "* **Exercise 1A (Baseline):**\n",
        "    * **Behavior:** Validation Loss exploded from **~1.0** to **>3.5**.\n",
        "    * **Diagnosis:** Severe divergence.\n",
        "* **Exercise 5A (Early Stopping):**\n",
        "    * **Behavior:** The Validation Loss fluctuates between **0.90** and **0.98**.\n",
        "    * **Impact:** By stopping early, we prevented the loss from spiraling out of control. We effectively captured the model in its initial state before it could start memorizing noise.\n",
        "\n",
        "##### **3. Model Accuracy (Right Plot)**\n",
        "* **Exercise 1A (Baseline):**\n",
        "    * **Behavior:** Validation Accuracy collapsed to **~50%** (random guessing) and stayed there.\n",
        "    * **Diagnosis:** The model's weights became so distorted by overfitting that it lost all predictive power on new data.\n",
        "* **Exercise 5A (Early Stopping):**\n",
        "    * **Behavior:** The Validation Accuracy peaked early at **~63.5%** (Epoch 0 and Epoch 9).\n",
        "    * **Impact:** By stopping and restoring the best weights, the final model retains this **63% accuracy**.\n",
        "    * **Comparison:** **63%** (Early Stopping) vs **50%** (Baseline). This is a massive relative improvement, simply by not allowing the training to continue too long.\n",
        "\n",
        "##### **Summary Table**\n",
        "\n",
        "| Metric | Exercise 1A (No Prevention) | Exercise 5A (Early Stopping) | Conclusion |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Duration** | 100 Epochs | **10 Epochs** | Saved 90% compute time. |\n",
        "| **Peak Val Loss** | **~3.5** (Huge Error) | **~0.98** (Contained) | Prevented error explosion. |\n",
        "| **Final Accuracy** | **~50%** | **~63%** | captured the model at its peak performance. |"
      ],
      "metadata": {
        "id": "okpZ9uQHCBcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Summary: Overfitting Prevention Techniques in PyTorch**\n",
        "\n",
        "In this lesson, we explored four distinct techniques to combat overfitting. It is important to remember that there is no \"silver bullet\"—each technique has its own strengths, weaknesses, and ideal use cases.\n",
        "\n",
        "##### **1. L2 Regularization (Weight Decay)**\n",
        "* **Mechanism:** Adds a penalty to the loss function proportional to the size of the weights ($Loss + \\lambda \\sum w^2$). This forces the model to keep weights small and simple.\n",
        "* **Strengths:**\n",
        "    * Very easy to implement (just one argument in the optimizer: `weight_decay`).\n",
        "    * Mathematically smooths the model's decision boundaries.\n",
        "    * Works well on almost all types of problems.\n",
        "* **Weaknesses:**\n",
        "    * Requires tuning the strength parameter ($\\lambda$). Too weak = no effect; Too strong = underfitting (model can't learn).\n",
        "* **Best For:** A good \"first line of defense\" for almost any neural network.\n",
        "\n",
        "##### **2. Dropout**\n",
        "* **Mechanism:** Randomly deactivates (zeroes out) a percentage of neurons during training.\n",
        "* **Strengths:**\n",
        "    * Extremely effective at preventing \"memorization\" of noise.\n",
        "    * Forces the network to learn robust, redundant features (like an ensemble of many networks).\n",
        "* **Weaknesses:**\n",
        "    * Can slow down convergence (training takes longer because the model is \"handicapped\").\n",
        "    * Can lead to lower performance on the training set (which is expected, but can be confusing for beginners).\n",
        "* **Best For:** Large, deep networks where the model capacity far exceeds the amount of training data.\n",
        "\n",
        "##### **3. Batch Normalization**\n",
        "* **Mechanism:** Normalizes the inputs of each layer to have a mean of 0 and variance of 1, injecting a small amount of statistical noise from the batch.\n",
        "* **Strengths:**\n",
        "    * **Primary Goal:** Drastically speeds up training and stabilizes gradients.\n",
        "    * **Secondary Goal:** Acts as a mild regularizer, often removing the need for heavy Dropout.\n",
        "* **Weaknesses:**\n",
        "    * Adds complexity to the model architecture (extra layers).\n",
        "    * Can be unstable with very small batch sizes (e.g., Batch Size < 16).\n",
        "* **Best For:** Deep networks (CNNs, ResNets) to ensure they actually train successfully.\n",
        "\n",
        "##### **4. Early Stopping**\n",
        "* **Mechanism:** Monitors validation performance and stops training when it stops improving.\n",
        "* **Strengths:**\n",
        "    * Saves computational resources (time and electricity).\n",
        "    * Prevents the model from training long enough to memorize noise.\n",
        "    * **Restoring Best Weights:** Ensures you keep the \"smartest\" version of your model, not just the \"latest\" one.\n",
        "* **Weaknesses:**\n",
        "    * It doesn't fix the model architecture; if your model is fundamentally flawed, stopping early won't make it better, just faster at failing.\n",
        "* **Best For:** **Every single training run.** There is rarely a good reason *not* to use Early Stopping.\n",
        "\n",
        "---\n",
        "\n",
        "### **Combining Techniques (The \"Swiss Cheese\" Model)**\n",
        "\n",
        "In professional Deep Learning, you rarely use just one of these. You usually combine them to cover different \"holes\" in your defense against overfitting.\n",
        "\n",
        "* **Standard Recipe:**\n",
        "    * **Always** use **Early Stopping**.\n",
        "    * **Almost Always** use a small amount of **L2 Regularization** (`1e-4` or `1e-5`).\n",
        "    * **If the model is deep:** Use **Batch Normalization** between layers.\n",
        "    * **If the model is still overfitting:** Add **Dropout** layers, starting with a low rate (0.2) and increasing if necessary (up to 0.5).\n",
        "\n",
        "**Example of a robust architecture:**\n",
        "```python\n",
        "Linear Layer -> Batch Norm -> ReLU -> Dropout -> Linear Layer ...\n",
        "```"
      ],
      "metadata": {
        "id": "lL4OMPdVD6UV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Electronic Submission**\n",
        "\n",
        "When you run the code in the cell below, it will grade your Colab notebook and tell you your pending grade as it currently stands. You will be given the choice to either submit your notebook for final grading or the option to continue your work on one (or more) Exercises. You no longer have the option to upload a PDF of your Colab notebook to Canvas for grading.\n",
        "\n",
        "**NOTE:** You grade on this Colab notebook will be based solely on the code in your **Exercises**. Failure to run one (or more) Examples will not affect your grade."
      ],
      "metadata": {
        "id": "TUBenURFANIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Electronic Submission\n",
        "\n",
        "import urllib.request\n",
        "import ssl\n",
        "import time\n",
        "\n",
        "url = \"https://biologicslab.co/BIO1173/backend_code/validate.py?v=\" + str(time.time())\n",
        "\n",
        "ctx = ssl.create_default_context()\n",
        "ctx.check_hostname = False\n",
        "ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "req = urllib.request.Request(\n",
        "    url,\n",
        "    headers={\n",
        "        \"Cache-Control\": \"no-cache, no-store, must-revalidate\",\n",
        "        \"Pragma\": \"no-cache\",\n",
        "        \"Expires\": \"0\"\n",
        "    }\n",
        ")\n",
        "\n",
        "with urllib.request.urlopen(req, context=ctx) as r:\n",
        "    exec(r.read().decode(\"utf-8\"))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "-RxndftGxUvV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}