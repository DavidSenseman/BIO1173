{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPURTmlPQu7pHVT0KAEQE93",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_02_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ],
      "metadata": {
        "id": "QGXRJ8-TG2mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "uvvKzBf7G5eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Module 2: Neural Networks with PyTorch**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 2 Material\n",
        "\n",
        "* Part 2.1: Introduction to Neural Networks with PyTorch\n",
        "* Part 2.2: Encoding Feature Vectors\n",
        "* **Part 2.3: Controlling Overfitting**\n",
        "* Part 2.4: Saving and Loading a PyTorch Neural Network"
      ],
      "metadata": {
        "id": "xLU60GttHArg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ],
      "metadata": {
        "id": "4Xti6nC3HLiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXdTjc1EGXos"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_01/class_01_6_image01A.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "4U4gC8qEHRLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Custom Function\n",
        "\n",
        "Run the cell below to create a function that will be needed later in this lesson."
      ],
      "metadata": {
        "id": "n3fJxCKKHYbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "mHV76pNoHf4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any output."
      ],
      "metadata": {
        "id": "EwbytKCPHvQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Sets for this Lesson**\n",
        "\n",
        "For this lesson, the **Obesity Data Set** will be used for the Examples and the **Heart Disease Data Set** will be used for the **Exercises**. Information about the Heart Disease Data set was presented in the previous lesson. Here is the information about the Obesity Data set.\n",
        "\n",
        "### **Obesity Data Set**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/obesity.jpg)\n",
        "\n",
        "[Obesity Data Set](https://archive.ics.uci.edu/ml/datasets/)\n",
        "\n",
        "**Description:**\n",
        "\n",
        "The **Obesity Data Set** includes data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. The data contains 17 attributes and 2111 records, the records are labeled with the class variable NObesity (Obesity Level), that allows classification of the data using the values of `Insufficient Weight`, `Normal Weight`, `Overweight Level I`, `Overweight Level II`, `Obesity Type I`, `Obesity Type II` and `Obesity Type III`.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "* **Gender-** Female/Male\n",
        "* **Age-** Numeric value\n",
        "* **Height-** Numeric value in meters\n",
        "* **Weight-** Numeric value in kilograms\n",
        "* **family_history_with_overweight-** Has a family member suffered or suffers from overweight - Yes/No\n",
        "* **FAVC-** Do you eat high caloric food frequently - Yes/No\n",
        "* **FCVC-** Do you usually eat vegetables in your meals - Never/Sometimes/Always\n",
        "* **NCP-** How many main meals do you have daily - Between 1 y 2/Three/More than three\n",
        "* **CAEC-** Do you eat any food between meals? - No/Sometimes/Frequently/Always\n",
        "* **SMOKE-** Do you smoke? - Yes/No\n",
        "* **CH2O-** How much water do you drink daily? - Less than a liter/Between 1 and 2 L/More than 2 L\n",
        "* **SCC-** Do you monitor the calories you eat daily - Yes/No\n",
        "* **FAF-** How often do you have physical activity? - I do not have/1 or 2 days/2 or 4 days/4 or 5 days\n",
        "* **TUE-** How much time do you use technological devices such as cell phone, videogames, television, computer and others - 0–2 hours/3–5 hours/More than 5 hours\n",
        "* **CALC-** How often do you drink alcohol? - I do not drink/Sometimes/Frequently/Always\n",
        "* **MTRANS-** Which transportation do you usually use? Automobile/Motorbike/Bike/Public Transportation/Walking\n",
        "* **NObeyesdad-** Obesity levels: 'Insufficient_Weight', 'Obesity_Type_III', 'Normal_Weight', 'Obesity_Type_II', 'Overweight_Level_I', 'Obesity_Type_I', 'Overweight_Level_II'\n",
        "\n",
        "For our classification neural network (`ob_model`), the response variable (`Y`) will be the column `NObeyesdad`. This column contains 7 categorical variables shown here sorted from lowest to highest obesity:\n",
        "\n",
        "* Insufficient_Weight\n",
        "* Normal_Weight\n",
        "* Overweight_Level_I\n",
        "* Overweight_Level_II\n",
        "* Obesity_Type_I  \n",
        "* Obesity_Type_II\n",
        "* Obesity_Type_III\n"
      ],
      "metadata": {
        "id": "YFOohVHIFS3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------\n",
        "\n",
        "# **The Problem of `Overfitting`**\n",
        "\n",
        "When a neural network learns a task, it adjusts its parameters so that the error on the training data decreases.\n",
        "\n",
        "However, it can be quite easy for the model to becomes **too good** at reproducing the training sample by **memorizing idiosyncratic noise** instead of capturing the true underlying relationship. This situation is called **overfitting**.\n",
        "\n",
        "#### **How `Overfitting` Happens**\n",
        "\n",
        "* **Too many trainable parameters:**\n",
        "A very deep or wide network can represent a vast space of functions.\n",
        "With few training examples, the model can fit every detail, including random fluctuations.\n",
        "\n",
        "* **Insufficient or unrepresentative data:**\n",
        "If the training set does not cover the variability of the problem domain, the model learns patterns that only exist in the training data.\n",
        "\n",
        "* **Training for too many epochs:**\n",
        "Continuing the optimisation past the point where validation error stops decreasing lets the network fine‑tune to noise.\n",
        "\n",
        "* **Lack of regularisation:**\n",
        "No constraints on weights, activations or hidden‑layer outputs allow the model to swing wildly to minimise the training loss.\n",
        "\n",
        "#### **Why `Overfitting` is Bad**\n",
        "\n",
        "* **Poor generalisation:**\n",
        "The model's predictions on new data (test set, real-world inputs) are much worse than its performance on the training set.\n",
        "\n",
        "* **Misleading confidence:**\n",
        "An overfitted network often reports low loss or high accuracy, giving a false sense of reliability.\n",
        "\n",
        "* **Wasted resources:**\n",
        "Training longer or with more complex architectures is unnecessary when the model will not perform better on unseen data.\n",
        "\n",
        "* **Deployment risks:**\n",
        "In safety-critical applications (self-driving cars, medical diagnosis), an overfitted model can produce dangerous errors."
      ],
      "metadata": {
        "id": "ZS0GhueQiew0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Controlling `Overfitting`**\n",
        "\n",
        "This lesson focuses on how you can control overfitting during the training of neural networks. Here are the 4 common strategies that are typically used in PyTorch to deal with the issues related to overfitting:\n",
        "\n",
        "\n",
        "* **`L2 Regularization`**\n",
        "* **`Dropout` Layers**\n",
        "* **`Batch Processing`**\n",
        "* **`Early Stopping`**\n",
        "\n",
        "To illustrate the strengths and weaknesses of each strategy, this lesson will use four Examples and four companion **Exercises**.\n",
        "\n",
        "1. `Example 1`/**`Exercise 1`**: A classification neural network will be trained _without_ any measures to prevent overfitting. This will serve as a **baseline** against which to judge the remaining Examples and **Exercises**.\n",
        "2. `Example 2`/**`Exercise 2`**: Exactly the same neural network will be trained using **`L2 Regularization`** to control overfitting.\n",
        "3. `Example 3`/**`Exercise 3`**: Instead of L2 Regularization, the technique of **`Dropout Layers`** will be used to limit overfitting.\n",
        "4. `Example 4`/**`Exercise 4`**: The technique of **`Batch Processing`** will be used to limit overfitting.\n",
        "4. `Example 5`/**`Exercise 5`**: Finally, the technique of adding **`Early Stopping`** will be used to limit overfitting.\n",
        "\n",
        "It should be noted that there is no technical reason to limit yourself to a single technique to limit overfitting -- two (or more) strategies are often used in combination depending upon the particular situation."
      ],
      "metadata": {
        "id": "YvVbIsf8fKXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1A: No Overfitting Prevention\n",
        "\n",
        "In earlier lessons the training of neural networks was divided into a series of smaller steps to help students master the underlying programming concepts. The code in the cell below, however, provides a all the code in a single block that is needed to:\n",
        "1. Read and preparing the tabular data\n",
        "2. Build and compile a classification neural network using PyTorch\n",
        "3. Train the neural network.\n",
        "4. Visualize the training.\n",
        "\n",
        "While a detailed explanation of the different code chucks will not be provided, here are a few key points.\n",
        "\n",
        "1. The following code chunk reads the `Obesity Data Set` from the course web server and stores it in a DataFrame called `ob_df`.\n",
        "```text\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "ob_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "```\n",
        "You will of course have to modify this line of code to read the Heart Disease data set for **`Exercise 1`**.\n",
        "\n",
        "You should note that the variable **`VERSBOSE`** has been set to `0`. This means there will be no output during training. This has been done to help keep the length of you Colab PDF a more reasonable value. However, in the last example/exercise pair, the verbose variable is set to `2` to print out the values generated during training."
      ],
      "metadata": {
        "id": "oQ4ZLvruwovE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1A: No Overfitting Prevention\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# ------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# ------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "PATIENCE      = 10\n",
        "VERBOSE       = 0     # 0 means no output during training\n",
        "LEARNING_RATE = 0.05\n",
        "BATCH_SIZE    = 32\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "ob_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# ------------------------------------------------------------\n",
        "ob_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "ob_df[ob_target_col] = le.fit_transform(ob_df[ob_target_col])\n",
        "\n",
        "ob_X = ob_df.drop(columns=[ob_target_col])\n",
        "ob_y = ob_df[ob_target_col].astype('int32')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# ------------------------------------------------------------\n",
        "# First split into train+val and test\n",
        "ob_X_temp, ob_X_test, ob_y_temp, ob_y_test = train_test_split(\n",
        "    ob_X, ob_y, test_size=0.2, random_state=42, stratify=ob_y)\n",
        "\n",
        "# Then split train+val into train and val\n",
        "ob_X_train, ob_X_val, ob_y_train, ob_y_val = train_test_split(\n",
        "    ob_X_temp, ob_y_temp, test_size=0.1, random_state=42, stratify=ob_y_temp)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# ------------------------------------------------------------\n",
        "categorical_cols = [c for c in ob_X.columns if ob_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in ob_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Convert data to correct numeric type (Numpy arrays)\n",
        "ob_X_train_proc = preprocessor.fit_transform(ob_X_train).astype(np.float32)\n",
        "ob_X_val_proc   = preprocessor.transform(ob_X_val).astype(np.float32)\n",
        "ob_X_test_proc  = preprocessor.transform(ob_X_test).astype(np.float32)\n",
        "\n",
        "ob_y_train = ob_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_val   = ob_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_test  = ob_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6️⃣  Convert to PyTorch Tensors & DataLoaders\n",
        "# ------------------------------------------------------------\n",
        "# Convert Features to Float Tensors\n",
        "ob_X_train_t = torch.tensor(ob_X_train_proc).to(device)\n",
        "ob_X_val_t   = torch.tensor(ob_X_val_proc).to(device)\n",
        "ob_X_test_t  = torch.tensor(ob_X_test_proc).to(device)\n",
        "\n",
        "# Convert Targets to Long Tensors (Required for CrossEntropyLoss)\n",
        "ob_y_train_t = torch.tensor(ob_y_train, dtype=torch.long).to(device)\n",
        "ob_y_val_t   = torch.tensor(ob_y_val, dtype=torch.long).to(device)\n",
        "ob_y_test_t  = torch.tensor(ob_y_test, dtype=torch.long).to(device)\n",
        "\n",
        "# Create DataLoaders for batching\n",
        "ob_train_dataset = TensorDataset(ob_X_train_t, ob_y_train_t)\n",
        "ob_val_dataset   = TensorDataset(ob_X_val_t, ob_y_val_t)\n",
        "\n",
        "ob_train_loader = DataLoader(ob_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "ob_val_loader   = DataLoader(ob_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7️⃣  Build Model & Optimizer\n",
        "# ------------------------------------------------------------\n",
        "n_classes = len(np.unique(ob_y_train))\n",
        "input_dim = ob_X_train_proc.shape[1]\n",
        "\n",
        "# Define PyTorch Model\n",
        "class ObesityNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "        # Note: No Softmax here. CrossEntropyLoss expects raw logits.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "ob_model = ObesityNet(input_dim, n_classes).to(device)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(ob_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8️⃣  Train model (Manual Loop)\n",
        "# ------------------------------------------------------------\n",
        "print(f\"------Training Starting for {EPOCHS} epochs --------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Dictionary to store history\n",
        "ob_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Training Phase ---\n",
        "    ob_model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in ob_train_loader:\n",
        "        optimizer.zero_grad()               # Reset gradients\n",
        "        outputs = ob_model(X_batch)         # Forward pass\n",
        "        loss = loss_fn(outputs, y_batch)    # Calculate loss\n",
        "        loss.backward()                     # Backward pass\n",
        "        optimizer.step()                    # Update weights\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(ob_train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    ob_model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad(): # No gradient needed for validation\n",
        "        for X_batch, y_batch in ob_val_loader:\n",
        "            outputs = ob_model(X_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(ob_val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    # Store metrics\n",
        "    ob_history['accuracy'].append(train_acc)\n",
        "    ob_history['val_accuracy'].append(val_acc)\n",
        "    ob_history['loss'].append(avg_train_loss)\n",
        "    ob_history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "    # Verbose print\n",
        "    if VERBOSE > 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "            f\"loss: {avg_train_loss:.4f} - \"\n",
        "            f\"acc: {train_acc:.4f} - \"\n",
        "            f\"val_loss: {avg_val_loss:.4f} - \"\n",
        "            f\"val_acc: {val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished\")\n",
        "print(\"Best val accuracy:\", max(ob_history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
      ],
      "metadata": {
        "id": "zQWzxBXeZkvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image22C.png)\n",
        "\n",
        "Our `ob_model` neural network appears to done a great job since the best validation accuracy (`val accuracy`) is above 95%. However, let's examine the model's training accuracy more carefully in `Example 1B` below.\n"
      ],
      "metadata": {
        "id": "hkO0__mdwpWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1B: Visualize Effects of No Overfitting Prevention\n",
        "\n",
        "After training we can assess its efficacy by visualizing two training curves —specifically **train loss vs. validation loss** and **train accuracy vs. validation accuracy**.\n",
        "\n",
        "The code in the cell below uses the `matplotlib.pyplot` graphics library to generate the two views of the training."
      ],
      "metadata": {
        "id": "SVC1D2YoyowA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1B: No Overfitting Prevention\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 1: Loss (Left Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax1.plot(ob_history['loss'], label='Train Loss')\n",
        "ax1.plot(ob_history['val_loss'], label='Val Loss')\n",
        "ax1.set_title('Model Loss (No Overfitting Prevention)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 2: Accuracy (Right Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax2.plot(ob_history['accuracy'], label='Train Acc')\n",
        "ax2.plot(ob_history['val_accuracy'], label='Val Acc')\n",
        "ax2.set_title('Model Accuracy (No Overfitting Prevention)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)sudo\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FSti1s6igOJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image01E.png)\n",
        "\n",
        "There can be considerable variability in these plots from one run to the next. Here is an analysis of the two plots shown above.\n",
        "\n",
        "#### **Analysis of Model Performance**\n",
        "\n",
        "Based on the training plots, the model is exhibiting a textbook case of **overfitting**.\n",
        "\n",
        "##### **Model Loss (Left Plot)**\n",
        "* **Train Loss (Blue):** This line drops consistently and rapidly approaches 0. This indicates the model is successfully learning the training data and, by Epoch 55, has essentially \"memorized\" it.\n",
        "* **Validation Loss (Orange):** This represents how the model performs on data it hasn't seen before. While it decreases initially, it is highly volatile (spiky) and eventually plateaus around 0.25.\n",
        "* **The Gap:** The large gap between the Training Loss (near 0) and Validation Loss (~0.25) signifies that the model is fitting noise in the training data rather than generalizing the underlying patterns.\n",
        "\n",
        "##### **Model Accuracy (Right Plot)**\n",
        "* **Train Accuracy (Blue):** The model achieves perfect accuracy (1.0 or 100%) on the training data around Epoch 50 and stays there.\n",
        "* **Validation Accuracy (Orange):** The validation accuracy improves quickly but hits a ceiling around 96-97%. It never reaches the perfect score of the training set.\n",
        "* **Observation:** While 96% accuracy is objectively good, the fact that training is at 100% confirms the model has sufficient capacity to learn the task but lacks the constraints to ignore irrelevant details (noise).\n",
        "\n",
        "##### **Diagnosis**\n",
        "Here is what happened mechanistically:\n",
        "\n",
        "1.  **Memorization:** The neural network is likely too complex (too many parameters/layers) relative to the amount of data available. It has enough \"memory\" to simply recall specific training examples rather than learning rules.\n",
        "2.  **Volatility:** The spikes in the orange validation loss line (specifically between epochs 10 and 50) suggest the model is unstable. Small updates to the weights are causing large swings in error when applied to unseen data."
      ],
      "metadata": {
        "id": "KGCgzxUnzj2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1A: No Overfitting Prevention**\n",
        "\n",
        "In the cell below write the code to to build, compile and train a classification neural network called `hd_model`. As usual, start by copying all of `Example 1` into the cell below.\n",
        "\n",
        "Since you will be using the Heart Disease dataset for all of the **`Exercises`**, use this code chunk to read your dataset from the course file server and create a DataFrame called `hd_df`.\n",
        "```text\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "hd_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/heart_disease.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "```\n",
        "\n",
        "Your objective is to predict the kind of `Resting ECG` a patient will likely display given his/her other clinical measurements. Therefore the data in the column `RestingECG` will be your `Y-values` and the data in the other columns will be your `X-values`. Since your target column has 3 classes: `Normal`, `ST` and `LVH`, you will be training your neural network (`hd_model`) to predict which of type of Resting ECG a particular patient will likely have given his/her other clinical measurements (`X-values`).\n",
        "\n",
        "You will therefore need to specify your target column as follows:\n",
        "```text\n",
        "hd_target_col = \"RestingECG\"\n",
        "```\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Change the prefix `ob_` to `hd_` everywhere in the code copied from Example 1."
      ],
      "metadata": {
        "id": "mPS3lEus1KOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1A here\n",
        "\n"
      ],
      "metadata": {
        "id": "dAd0jgO9cg9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image25C.png)\n"
      ],
      "metadata": {
        "id": "zrLIG-k-1KOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1B: Visualize Effects of No Overfitting Prevention**\n",
        "\n",
        "In the cell below write the code to visualize your training by creating a  **train loss vs. validation loss** plot and **train accuracy vs. validation accuracy** plot.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Your `history` object should be called `hd_history` instead of `ob_history`."
      ],
      "metadata": {
        "id": "yNwREBwM8ZPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1B here\n",
        "\n"
      ],
      "metadata": {
        "id": "qBtO4qEo8ZPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image02E.png)\n",
        "\n",
        "### **Analysis of Model Performance (Severe Overfitting)**\n",
        "\n",
        "Based on these training plots, the model is exhibiting a case of **severe overfitting**, arguably worse than the previous example because the validation performance is actively degrading rather than just stalling.\n",
        "\n",
        "##### **Model Loss (Left Plot)**\n",
        "* **Train Loss (Blue):** The training loss decreases steadily from ~1.0 down to ~0.5. This confirms the model is learning to minimize error on the training set.\n",
        "* **Validation Loss (Orange):** This is the critical warning sign. Instead of decreasing or plateauing, the validation loss **increases** dramatically, rising from ~1.0 to over 3.0.\n",
        "* **The Gap:** The lines diverge almost immediately (around Epoch 5). The fact that validation loss is exploding upwards indicates the model is becoming increasingly confident in its wrong predictions on unseen data.\n",
        "\n",
        "##### **Model Accuracy (Right Plot)**\n",
        "* **Train Accuracy (Blue):** The model steadily improves its accuracy on the training data, climbing from 60% to nearly 77%.\n",
        "* **Validation Accuracy (Orange):** The accuracy on validation data degrades over time. It starts near 60% but drops to fluctuate between 45% and 55%—essentially performing no better (or worse) than random guessing as training continues.\n",
        "* **Observation:** The model is \"learning\" patterns that are specific only to the training data and detrimental to generalization. As it optimizes for the training set, it actively gets worse at the general task.\n",
        "\n",
        "##### **Diagnosis**\n",
        "Here is what is happening mechanistically:\n",
        "\n",
        "1.  **Immediate Divergence:** Unlike the previous graph where the model generalized for a while before overfitting, this model begins overfitting almost instantly.\n",
        "2.  **Memorization of Noise:** The rising orange loss line suggests the model is fitting the \"noise\" or random fluctuations in the training data so aggressively that it is losing the underlying signal.\n",
        "3.  **Potential Causes:** This specific shape (diverging lines) often hints that the model capacity is far too high for the dataset size, or the learning rate might be too high, causing the model to overshoot optimal weights for the validation set."
      ],
      "metadata": {
        "id": "NbsfMz779PFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **L2 Regularization (Weight Decay)**\n",
        "\n",
        "**L2 Regularization** is a standard technique used to reduce overfitting by preventing a neural network's weights from becoming too large. In the PyTorch ecosystem, this is commonly referred to as **Weight Decay**.\n",
        "\n",
        "##### **What is L2 Regularization?**\n",
        "* **The Concept:** It adds a penalty to the model's loss function proportional to the square of the magnitude of the weights.\n",
        "* **The Goal:** By penalizing large weights, the model is forced to learn simpler patterns rather than complex, high-frequency noise (memorization).\n",
        "* **The Result:** The model becomes less sensitive to small changes in input data, leading to a smoother decision boundary and better generalization.\n",
        "\n",
        "##### **How to Implement it in PyTorch**\n",
        "Unlike some frameworks where you add a penalty term manually to your loss function, PyTorch builds L2 regularization directly into the **optimizer**.\n",
        "\n",
        "You simply set the `weight_decay` parameter when initializing your optimizer.\n",
        "\n",
        "* **Parameter:** `weight_decay` (float)\n",
        "* **Typical Values:** Between `1e-5` (0.00001) and `1e-3` (0.001). A value of `0` means no regularization.\n",
        "\n",
        "##### **Code Example**\n",
        "Here is how you would add L2 regularization to an Adam optimizer:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Define your model (standard step)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 50),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(50, 1)\n",
        ")\n",
        "\n",
        "# 2. Define the Optimizer with L2 Regularization\n",
        "# 'weight_decay=1e-4' applies the L2 penalty\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# 3. The training loop remains exactly the same\n",
        "# PyTorch automatically adds the regularization term during the optimizer.step()"
      ],
      "metadata": {
        "id": "4-pad6a9cnPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2A: L2 Regularization\n",
        "\n",
        "The following chages were made to add L2 Regularization to the code shown in Example 1.\n",
        "\n",
        "##### **New Hyperparameter (Section 1)**\n",
        "* **Example 1A:** Only defines standard training parameters (`EPOCHS`, `LEARNING_RATE`, etc.).\n",
        "* **Example 2A:** Adds a specific variable for the regularization strength:\n",
        "```text\n",
        "  L2_REG = 0.001  # <--- New Parameter: Weight Decay Strength\n",
        "```\n",
        "\n",
        "##### **Optimizer Configuration (Section 7)**\n",
        "This is the most critical functional difference. PyTorch implements L2 Regularization directly inside the optimizer via the weight_decay argument.\n",
        "\n",
        "Example 1A (No Regularization): The optimizer is initialized with only the model parameters and learning rate.\n",
        "\n",
        "```text\n",
        "optimizer = optim.Adam(ob_model.parameters(), lr=LEARNING_RATE)\n",
        "```\n",
        "\n",
        "Example 2A (With L2 Regularization): The optimizer includes the weight_decay argument, passing in the L2_REG value defined earlier.\n",
        "\n",
        "```text\n",
        "optimizer = optim.Adam(ob_model.parameters(),\n",
        "                       lr=LEARNING_RATE,\n",
        "                       weight_decay=L2_REG) # <--- Applies L2 Penalty\n",
        "```\n"
      ],
      "metadata": {
        "id": "DMG8y4CE38W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2A: L2 Regularization (Weight Decay)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# -------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# -------------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "VERBOSE       = 0\n",
        "LEARNING_RATE = 0.05\n",
        "BATCH_SIZE    = 32\n",
        "L2_REG        = 0.001  # <--- New Parameter: Weight Decay Strength\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# -------------------------------------------------------------------\n",
        "ob_df = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "    na_values=['NA','?']\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# -------------------------------------------------------------------\n",
        "ob_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "ob_df[ob_target_col] = le.fit_transform(ob_df[ob_target_col])\n",
        "\n",
        "ob_X = ob_df.drop(columns=[ob_target_col])\n",
        "ob_y = ob_df[ob_target_col].astype('int32')\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# -------------------------------------------------------------------\n",
        "ob_X_temp, ob_X_test, ob_y_temp, ob_y_test = train_test_split(\n",
        "    ob_X, ob_y, test_size=0.2, random_state=42, stratify=ob_y\n",
        ")\n",
        "\n",
        "ob_X_train, ob_X_val, ob_y_train, ob_y_val = train_test_split(\n",
        "    ob_X_temp, ob_y_temp, test_size=0.1, random_state=42, stratify=ob_y_temp\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# -------------------------------------------------------------------\n",
        "categorical_cols = [c for c in ob_X.columns if ob_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in ob_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Fit & transform training data\n",
        "ob_X_train_proc = preprocessor.fit_transform(ob_X_train).astype(np.float32)\n",
        "ob_X_val_proc   = preprocessor.transform(ob_X_val).astype(np.float32)\n",
        "ob_X_test_proc  = preprocessor.transform(ob_X_test).astype(np.float32)\n",
        "\n",
        "ob_y_train = ob_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_val   = ob_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_test  = ob_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6️⃣  Convert to PyTorch Tensors & DataLoaders\n",
        "# -------------------------------------------------------------------\n",
        "ob_X_train_t = torch.tensor(ob_X_train_proc).to(device)\n",
        "ob_X_val_t   = torch.tensor(ob_X_val_proc).to(device)\n",
        "ob_X_test_t  = torch.tensor(ob_X_test_proc).to(device)\n",
        "\n",
        "ob_y_train_t = torch.tensor(ob_y_train, dtype=torch.long).to(device)\n",
        "ob_y_val_t   = torch.tensor(ob_y_val, dtype=torch.long).to(device)\n",
        "ob_y_test_t  = torch.tensor(ob_y_test, dtype=torch.long).to(device)\n",
        "\n",
        "ob_train_dataset = TensorDataset(ob_X_train_t, ob_y_train_t)\n",
        "ob_val_dataset   = TensorDataset(ob_X_val_t, ob_y_val_t)\n",
        "\n",
        "ob_train_loader = DataLoader(ob_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "ob_val_loader   = DataLoader(ob_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 7️⃣  Build Model (Standard)\n",
        "# -------------------------------------------------------------------\n",
        "n_classes = len(np.unique(ob_y_train))\n",
        "input_dim = ob_X_train_proc.shape[1]\n",
        "\n",
        "class ObesityNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        # No Dropout layer here, we are testing L2 specifically\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "ob_model = ObesityNet(input_dim, n_classes).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 'weight_decay' applies L2 penalty to the parameters\n",
        "optimizer = optim.Adam(ob_model.parameters(),\n",
        "                       lr=LEARNING_RATE,\n",
        "                       weight_decay=L2_REG)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 8️⃣  Train model\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"------Training Starting for {EPOCHS} epochs --------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "ob_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Training Phase ---\n",
        "    ob_model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in ob_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = ob_model(X_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(ob_train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    ob_model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in ob_val_loader:\n",
        "            outputs = ob_model(X_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(ob_val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    # Store metrics\n",
        "    ob_history['accuracy'].append(train_acc)\n",
        "    ob_history['val_accuracy'].append(val_acc)\n",
        "    ob_history['loss'].append(avg_train_loss)\n",
        "    ob_history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "    # Print if verbose\n",
        "    if VERBOSE > 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "            f\"loss: {avg_train_loss:.4f} - \"\n",
        "            f\"acc: {train_acc:.4f} - \"\n",
        "            f\"val_loss: {avg_val_loss:.4f} - \"\n",
        "            f\"val_acc: {val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished\")\n",
        "print(\"Best val accuracy:\", max(ob_history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
      ],
      "metadata": {
        "id": "u7Z2UPQamW1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image37C.png)"
      ],
      "metadata": {
        "id": "CH7l-JmUbVaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2B: Visualize Effects of `L2 Regularization`.\n",
        "\n",
        "The code in the cell below is **exactly** the same as that used in `Example 1B` except the axis titles have been changed to specify the type of overfitting protection that was used."
      ],
      "metadata": {
        "id": "7rtE2hc_arZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2B: Visualize effects of L2 Regularization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 1: Loss (Left Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax1.plot(ob_history['loss'], label='Train Loss')\n",
        "ax1.plot(ob_history['val_loss'], label='Val Loss')\n",
        "ax1.set_title('Model Loss (L2 Regularization)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 2: Accuracy (Right Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax2.plot(ob_history['accuracy'], label='Train Acc')\n",
        "ax2.plot(ob_history['val_accuracy'], label='Val Acc')\n",
        "ax2.set_title('Model Accuracy (L2 Regularization)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XRnko8uZm68r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to these graphs.\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image03E.png)\n",
        "\n",
        "### **Comparative Analysis: L2 Regularization vs. No Prevention**\n",
        "\n",
        "These two sets of graphs provide a stark \"Before and After\" demonstration of how **L2 Regularization** (Weight Decay) stabilizes model training.\n",
        "\n",
        "##### **1. Top Graphs: Model with L2 Regularization**\n",
        "* **Model Loss (Left):** The most critical observation here is the **tight coupling** between the Training Loss (Blue) and Validation Loss (Orange). Unlike the chaotic divergence seen previously, the Validation line roughly follows the Training line downwards. They settle near each other (~0.1 for Train, ~0.2 for Val).\n",
        "* **Model Accuracy (Right):** The Validation Accuracy tracks the Training Accuracy very closely, reaching high performance (~95%).\n",
        "* **Diagnosis:** **Successful Generalization.** The L2 penalty effectively constrained the model weights. Instead of memorizing noise, the model was forced to learn robust patterns that applied equally well to unseen data.\n",
        "\n",
        "##### **2. Bottom Graphs: Model without Overfitting Prevention**\n",
        "* **Model Loss (Left):** This represents a catastrophic failure. While the Training Loss drops, the Validation Loss **explodes upwards** (from 1.0 to 3.5). The lines diverge immediately.\n",
        "* **Model Accuracy (Right):** The Training Accuracy climbs to ~77%, but the Validation Accuracy collapses to ~50% (essentially random guessing).\n",
        "* **Diagnosis:** **Severe Overfitting / Instability.** Without the constraint of L2 regularization, the model likely learned large, unstable weights that worked for the training data but produced massive errors on the validation data.\n",
        "\n",
        "##### **Key Takeaway**\n",
        "The addition of `weight_decay` in the optimizer turned a failing model (Bottom) into a highly accurate one (Top).\n",
        "* **Without L2:** The model became \"over-confident\" on training data, leading to massive errors on validation data.\n",
        "* **With L2:** The model remained \"disciplined,\" sacrificing a tiny bit of training perfection for massive gains in validation stability and accuracy."
      ],
      "metadata": {
        "id": "Z8tPwcphcsl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2A: L2 Regularization**\n",
        "\n",
        "In the cell below, write the code to add L2 Regularization to your `hd_model`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Copy-and-paste your code for **Excercise 1A** into the cell below\n",
        "\n",
        "1. Add the hyperparameter `L2_REG = 0.001` to Section 1.\n",
        "2. Change the optimizer code from:\n",
        "```text\n",
        "optimizer = optim.Adam(ob_model.parameters(), lr=LEARNING_RATE)\n",
        "```\n",
        "to read as\n",
        "\n",
        "```text\n",
        "optimizer = optim.Adam(ob_model.parameters(),\n",
        "                       lr=LEARNING_RATE,\n",
        "                       weight_decay=L2_REG) # <--- Applies L2 Penalty\n",
        "```\n"
      ],
      "metadata": {
        "id": "4rON1qKFejd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2A here\n",
        "\n"
      ],
      "metadata": {
        "id": "LHiecUkXejd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image39C.png)"
      ],
      "metadata": {
        "id": "5JZbjZlZejd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2B: Visualize Effects of L2 Regularization**\n",
        "\n",
        "To visualize the effects of L2 Regularization, `copy-and-paste` the code from **`Exercise 1B`** into the cell below.\n",
        "\n",
        "**Code Hints:**\n",
        "1. Change the axis title for the left graph to read:\n",
        "```text\n",
        "ax1.set_title('Model Loss (L2 Regularization)')\n",
        "```\n",
        "2. Change the axis title for the right graph to read:\n",
        "```text\n",
        "ax2.set_title('Model Accuracy (L2 Regularization)')\n",
        "```"
      ],
      "metadata": {
        "id": "G-ggkrEwejd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2B here\n",
        "\n"
      ],
      "metadata": {
        "id": "Cz-CJxPDpU5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image04E.png)\n",
        "\n",
        "### **Comparative Analysis: Exercise 2A vs. Exercise 1A (Heart Disease Dataset)**\n",
        "\n",
        "This analysis compares the performance of the neural network on the **Heart Disease** dataset (Target: `RestingECG`) with and without L2 Regularization.\n",
        "\n",
        "##### **1. The Baseline: Exercise 1A (Image 02E - No Prevention)**\n",
        "* **Behavior:** This graph represents the \"worst-case scenario\" for the `RestingECG` classification task.\n",
        "* **Loss:** The Training Loss (Blue) drops smoothly, but the Validation Loss (Orange) explodes violently, rising from **~1.0 to over 3.0**.\n",
        "* **Accuracy:** While Training Accuracy reaches ~77%, the Validation Accuracy degrades to ~50% (random guessing).\n",
        "* **Diagnosis:** The model completely memorized the training data and failed to learn any generalizable features about the `RestingECG` patterns.\n",
        "\n",
        "##### **2. The Attempted Fix: Exercise 2A (Image 04E - L2 Regularization)**\n",
        "* **Behavior:** This model introduced **L2 Regularization** (Weight Decay) to penalize large weights.\n",
        "* **Loss Comparison:**\n",
        "    * **The Improvement:** The regularization *did* have an effect. The peak Validation Loss dropped from **>3.0** (in Exercise 1A) to **~1.4**. The \"explosion\" was dampened.\n",
        "    * **The Failure:** Despite the lower peak, the lines still diverge significantly. The Validation Loss is still increasing, not decreasing.\n",
        "* **Accuracy Comparison:**\n",
        "    * The Validation Accuracy remains highly volatile and low (fluctuating between 45% and 60%). It did not significantly improve over the baseline.\n",
        "* **Diagnosis:** **Insufficient Regularization.** The L2 penalty used in this exercise was likely too weak (e.g., a low `weight_decay` value). It curbed the extreme numeric instability seen in Exercise 1A but was not strong enough to force the model to learn a robust solution for the `RestingECG` target.\n",
        "\n",
        "##### **Summary**\n",
        "| Exercise | Technique | Val Loss Peak | Outcome |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Exercise 1A** | None | **~3.5** | **Catastrophic Overfitting** |\n",
        "| **Exercise 2A** | L2 Regularization | **~1.4** | **Dampened Overfitting** (Partial effect, but still failing to generalize) |"
      ],
      "metadata": {
        "id": "OrbI4dBzejd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dropout to Decrease Overfitting**\n",
        "\n",
        "Hinton, Srivastava, Krizhevsky, Sutskever, & Salakhutdinov (2012) introduced the **_dropout regularization_** algorithm.\n",
        "\n",
        "Although `dropout` works differently than `L2 Regularization`, it accomplishes the same goal—the prevention of **overfitting**. However, the algorithm does the task by actually **_removing_** neurons and connections—at least temporarily. Unlike `L2`, no weight penalty is added. `Dropout` does not directly seek to train small weights.\n",
        "\n",
        "`Dropout` works by causing hidden neurons of the neural network to be unavailable during part of the training. Dropping part of the neural network causes the remaining portion to be trained to still achieve a good score even without the dropped neurons. This technique decreases co-adaptation between neurons, which results in less overfitting.\n",
        "\n",
        "**Implementation in PyTorch**\n",
        "\n",
        "In PyTorch, `dropout` is implemented as a specific module (`nn.Dropout`) that is inserted into the model architecture, typically immediately after a dense (linear) layer or an activation function. The `nn.Dropout` module does not contain learnable weights itself; instead, it applies a \"mask\" to the data flowing through it.\n",
        "\n",
        "A critical aspect of using Dropout in PyTorch is managing the model's **state**:\n",
        "\n",
        "1.  **Training Mode (`model.train()`):** When the model is in training mode, the dropout layer is **active**. It randomly zeroes out (drops) a percentage of the elements in the input tensor based on the probability `p` you define.\n",
        "2.  **Evaluation Mode (`model.eval()`):** When the model is in evaluation mode (used for validation or testing), the dropout layer is **inactive**. It allows all data to pass through unchanged, ensuring that the full capacity of the network is used for making predictions.\n",
        "\n",
        "**How it Works Mechanically**\n",
        "\n",
        "PyTorch implements the `dropout layer` by temporarily masking neurons rather than permanently removing them. In other words, a dropout layer does **_not_** lose any of its neurons during the training process, and the model will still have the same number of neurons after training.\n",
        "\n",
        "This figure shows how a dropout layer might be situated with other layers.\n",
        "\n",
        "![Dropout Regularization](https://biologicslab.co/BIO1173/images/class_02/class_9_dropout.png \"Dropout Regularization\")\n",
        "\n",
        "**Dropout Regularization**\n",
        "\n",
        "The discarded neurons and their connections are shown as dashed lines. The input layer has two input neurons as well as a bias neuron. The second layer is a dense layer with three neurons and a bias neuron. The third layer shows the effect of dropout, where regular neurons are \"masked\" (ignored) even though they still exist in the architecture.\n",
        "\n",
        "While PyTorch drops these neurons during a training step, it neither calculates gradients for them nor updates their incoming weights. However, the final neural network will use _all_ of these neurons for the output when you switch to `model.eval()`.\n",
        "\n",
        "The specific neurons that are dropped change constantly. Although we might choose a probability of 50% for dropout, the computer will not necessarily drop exactly half the neurons every time. It is as if we flipped a coin for each of the dropout candidate neurons to choose if that neuron was dropped out. In PyTorch, a new random mask is generated for every **batch** of data during the forward pass.\n",
        "\n",
        "*Note: The bias neuron is typically never dropped; only the regular neurons connected to a dropout layer are candidates for removal.*"
      ],
      "metadata": {
        "id": "lryoaO4ErTKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Why does Dropout work?**\n",
        "\n",
        "A common question is why simply removing neurons would decrease overfitting. The answer lies in two main mechanisms: preventing **co-adaptation** (codependency) and simulating **ensemble learning**.\n",
        "\n",
        "### **1. Reducing Co-adaptation**\n",
        "Dropout reduces the chance of **codependency** developing between neurons. In a standard network, neurons can learn to fix up the mistakes of other neurons, creating complex co-adaptations that do not generalize to unseen data.\n",
        "\n",
        "When dropout is applied, a neuron can no longer rely on the presence of specific other neurons because they might be deactivated at any moment. This forces each neuron to be more robust and independent, learning features that are useful in a wide variety of contexts rather than just in specific combinations. As the attached video explains, this forces the network to learn a **redundant representation** of the data—ensuring that if one pathway is \"squashed,\" the information still flows through another.\n",
        "\n",
        "### **2. The Ensemble Effect (Bootstrapping)**\n",
        "Dropout also functions as a highly efficient form of **Ensemble Learning**.\n",
        "\n",
        "**Ensembling** is a standard machine learning technique where multiple models are combined to produce a better result than any individual model could achieve alone. (The term originates from musical ensembles, where the final sound is a harmonious combination of many instruments).\n",
        "\n",
        "One common ensemble method involves **Bootstrapping** (or Bagging), where a practitioner trains several independent neural networks on the same task. Because of random initialization and data sampling, each network makes different errors. When you average their outputs, the errors cancel out, resulting in a stronger, more generalized prediction.\n",
        "\n",
        "**How Dropout Mimics This:**\n",
        "Dropout mathematically approximates training a massive ensemble of different neural networks with shared weights.\n",
        "* **During Training:** Every time a batch of data is processed, a random set of neurons is dropped. This effectively creates a slightly different, \"thinned\" neural network for that specific step. Over many iterations, you are training exponentially many different variations of the network.\n",
        "* **During Testing:** When we stop dropping neurons (evaluation mode), we are essentially using a single \"averaged\" network that represents the consensus of all those temporary, thinned networks.\n",
        "\n",
        "Unlike traditional ensembling, which requires training and storing many separate models (computationally expensive), dropout allows you to achieve the same robustness within a **single model**.\n",
        "\n",
        "### **Video Explanation**\n",
        "This short `YouTube` video by Udacity provides a clear visualization of how dropout forces the network to learn redundant representations:\n",
        "\n",
        "[Dropout tutorial](https://youtu.be/NhZVe50QwPM?si=Zr-6qrPdE9YXTj3Q)"
      ],
      "metadata": {
        "id": "HJoxQ1Ehr-Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3A: Dropout\n",
        "\n",
        "The code in the cell below is an **exact** copy of the code in `Example 1A` with the following changes to the Model Architecture and Parameters to implement Dropout.\n",
        "\n",
        "##### **1. New Hyperparameter (Section 1)**\n",
        "Example 3A introduces a probability parameter that defines how aggressive the dropout should be.\n",
        "\n",
        "* **Code Change:**\n",
        "```text\n",
        "    DROPOUT_RATE = 0.5   # <--- New Parameter\n",
        "```\n",
        "* **Purpose:** This tells the model to randomly zero out 50% of the neurons in the hidden layer during every training step.\n",
        "\n",
        "##### **2. Model Architecture (Section 7)**\n",
        "This is where the actual structure of the neural network changes.\n",
        "\n",
        "* **In `__init__`:** A specific Dropout layer is defined using the standard PyTorch module `nn.Dropout`.\n",
        "* **In `forward`:** The dropout layer is applied to the data stream, typically immediately after the activation function (ReLU).\n",
        "\n",
        "**Difference in Code:**\n",
        "```text\n",
        "class ObesityNetDropout(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNetDropout, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        # --- ADDED IN EXAMPLE 3A ---\n",
        "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
        "        # ---------------------------\n",
        "        \n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        \n",
        "        # --- ADDED IN EXAMPLE 3A ---\n",
        "        x = self.dropout(x)  # Apply mask here\n",
        "        # ---------------------------\n",
        "        \n",
        "        x = self.output(x)\n",
        "        return x\n",
        "```\n"
      ],
      "metadata": {
        "id": "S1YlQAbQqaYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3A: Dropout\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# -------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# -------------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "PATIENCE      = 10\n",
        "VERBOSE       = 0\n",
        "LEARNING_RATE = 0.05\n",
        "BATCH_SIZE    = 32\n",
        "DROPOUT_RATE  = 0.5   # <--- New Parameter: Probability of zeroing a neuron\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# -------------------------------------------------------------------\n",
        "ob_df = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "    na_values=['NA','?']\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# -------------------------------------------------------------------\n",
        "ob_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "ob_df[ob_target_col] = le.fit_transform(ob_df[ob_target_col])\n",
        "\n",
        "ob_X = ob_df.drop(columns=[ob_target_col])\n",
        "ob_y = ob_df[ob_target_col].astype('int32')\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# -------------------------------------------------------------------\n",
        "ob_X_temp, ob_X_test, ob_y_temp, ob_y_test = train_test_split(\n",
        "    ob_X, ob_y, test_size=0.2, random_state=42, stratify=ob_y\n",
        ")\n",
        "\n",
        "ob_X_train, ob_X_val, ob_y_train, ob_y_val = train_test_split(\n",
        "    ob_X_temp, ob_y_temp, test_size=0.1, random_state=42, stratify=ob_y_temp\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# -------------------------------------------------------------------\n",
        "categorical_cols = [c for c in ob_X.columns if ob_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in ob_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Fit & transform training data\n",
        "ob_X_train_proc = preprocessor.fit_transform(ob_X_train).astype(np.float32)\n",
        "ob_X_val_proc   = preprocessor.transform(ob_X_val).astype(np.float32)\n",
        "ob_X_test_proc  = preprocessor.transform(ob_X_test).astype(np.float32)\n",
        "\n",
        "ob_y_train = ob_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_val   = ob_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_test  = ob_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6️⃣  Convert to PyTorch Tensors & DataLoaders\n",
        "# -------------------------------------------------------------------\n",
        "ob_X_train_t = torch.tensor(ob_X_train_proc).to(device)\n",
        "ob_X_val_t   = torch.tensor(ob_X_val_proc).to(device)\n",
        "ob_X_test_t  = torch.tensor(ob_X_test_proc).to(device)\n",
        "\n",
        "ob_y_train_t = torch.tensor(ob_y_train, dtype=torch.long).to(device)\n",
        "ob_y_val_t   = torch.tensor(ob_y_val, dtype=torch.long).to(device)\n",
        "ob_y_test_t  = torch.tensor(ob_y_test, dtype=torch.long).to(device)\n",
        "\n",
        "ob_train_dataset = TensorDataset(ob_X_train_t, ob_y_train_t)\n",
        "ob_val_dataset   = TensorDataset(ob_X_val_t, ob_y_val_t)\n",
        "\n",
        "ob_train_loader = DataLoader(ob_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "ob_val_loader   = DataLoader(ob_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 7️⃣  Build Model with Dropout (Standard PyTorch Practice)\n",
        "# -------------------------------------------------------------------\n",
        "n_classes = len(np.unique(ob_y_train))\n",
        "input_dim = ob_X_train_proc.shape[1]\n",
        "\n",
        "class ObesityNetDropout(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNetDropout, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        # Dropout Layer: Randomly zeroes some elements of the input tensor\n",
        "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)  # Apply Dropout after activation\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "ob_model = ObesityNetDropout(input_dim, n_classes).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(ob_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 8️⃣  Train model (Standard Loop - No Manual L1 Math needed)\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"------Training Starting for {EPOCHS} epochs --------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "ob_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Training Phase ---\n",
        "    ob_model.train() # Important: Enables Dropout\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in ob_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = ob_model(X_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(ob_train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    ob_model.eval() # Important: Disables Dropout (uses full network)\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in ob_val_loader:\n",
        "            outputs = ob_model(X_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(ob_val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    # Store metrics\n",
        "    ob_history['accuracy'].append(train_acc)\n",
        "    ob_history['val_accuracy'].append(val_acc)\n",
        "    ob_history['loss'].append(avg_train_loss)\n",
        "    ob_history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "    # Print if verbose\n",
        "    if VERBOSE > 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "            f\"loss: {avg_train_loss:.4f} - \"\n",
        "            f\"acc: {train_acc:.4f} - \"\n",
        "            f\"val_loss: {avg_val_loss:.4f} - \"\n",
        "            f\"val_acc: {val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished\")\n",
        "print(\"Best val accuracy:\", max(ob_history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
      ],
      "metadata": {
        "id": "is5sgK40jkN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the last part of the output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image05E.png)"
      ],
      "metadata": {
        "id": "Sn99dbOer2U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3B: Visualize Effects of Dropout\n",
        "\n",
        "The code in the cell below is **exactly** the same as that used in `Example 1B` except the axis titles have been changed to specify the type of overfitting protection that was used."
      ],
      "metadata": {
        "id": "XKfbOra3J4QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3B: Visualize effects of Dropout\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 1: Loss (Left Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax1.plot(ob_history['loss'], label='Train Loss')\n",
        "ax1.plot(ob_history['val_loss'], label='Val Loss')\n",
        "ax1.set_title('Model Loss (Dropout)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 2: Accuracy (Right Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax2.plot(ob_history['accuracy'], label='Train Acc')\n",
        "ax2.plot(ob_history['val_accuracy'], label='Val Acc')\n",
        "ax2.set_title('Model Accuracy (Dropout)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pR2KRCVHj64e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image06E.png)\n",
        "\n",
        "### **Comparative Analysis: Dropout (Example 3A) vs. No Prevention (Example 1A)**\n",
        "\n",
        "These graphs illustrate the unique behavioral signature of **Dropout**. Unlike L2 Regularization, which simply \"squeezes\" the training and validation lines together, Dropout often inverts them.\n",
        "\n",
        "##### **1. Model Loss (Left Plot)**\n",
        "* **Example 1A (Baseline):**\n",
        "    * **Behavior:** The Training Loss dropped to nearly **0**, while the Validation Loss plateaued or rose.\n",
        "    * **The Gap:** A clear sign of overfitting where the model performed significantly better on training data than validation data.\n",
        "* **Example 3A (Dropout - Image 06E):**\n",
        "    * **Behavior:** The Training Loss (Blue) is actually **higher** than the Validation Loss (Orange).\n",
        "    * **The Inversion:** This \"inverted gap\" is a hallmark of strong Dropout (rate = 0.5).\n",
        "    * **Why this happens:**\n",
        "        * **During Training:** The model is \"handicapped\"—50% of its brain is turned off at random. It struggles to fit the data, resulting in higher loss (~0.55).\n",
        "        * **During Validation:** The \"handicap\" is removed (all neurons are active). The full power of the network is unleashed, resulting in lower loss (~0.30) and better performance.\n",
        "\n",
        "##### **2. Model Accuracy (Right Plot)**\n",
        "* **Example 1A (Baseline):**\n",
        "    * **Behavior:** The model reached **100% (1.0)** accuracy on the Training set.\n",
        "    * **Diagnosis:** Pure memorization.\n",
        "* **Example 3A (Dropout - Image 06E):**\n",
        "    * **Behavior:** The Training Accuracy struggles to reach **82%**, while the Validation Accuracy soars to **~92%**.\n",
        "    * **Diagnosis:** The model is prevented from memorizing the training data. It is forced to learn robust features that work well even when parts of the network are missing.\n",
        "\n",
        "##### **Key Takeaway**\n",
        "Dropout successfully solved the overfitting problem but introduced a trade-off:\n",
        "* **Stability:** The validation performance is extremely stable and higher than the training performance.\n",
        "* **Peak Performance:** You might notice the peak validation accuracy (~92%) is slightly lower than the unconstrained baseline (~96-97%). This is the **cost of regularization**—we sacrifice a small amount of theoretical peak accuracy to ensure the model isn't just hallucinating patterns (overfitting)."
      ],
      "metadata": {
        "id": "8vDGQnP-L5yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3A: Dropout**\n",
        "\n",
        "In the cell below write the code to add `Dropout` to your `hd_model`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "In the cell below `copy-and-paste` your **`Exercise 1A`** into the cell below.\n",
        "\n",
        "##### **1. New Hyperparameter (Section 1)**\n",
        "\n",
        "Add a probability parameter `DROPUT_RATE` that defines how aggressive the dropout should be.\n",
        "\n",
        "* **Code Change:**\n",
        "```text\n",
        "    DROPOUT_RATE = 0.5   # <--- New Parameter\n",
        "```\n",
        "* **Purpose:** This tells the model to randomly zero out 50% of the neurons in the hidden layer during every training step.\n",
        "\n",
        "##### **2. Model Architecture (Section 7)**\n",
        "This is where the actual structure of the neural network changes.\n",
        "\n",
        "* **In `__init__`:** A specific Dropout layer is defined using the standard PyTorch module `nn.Dropout`.\n",
        "* **In `forward`:** The dropout layer is applied to the data stream, typically immediately after the activation function (ReLU).\n",
        "\n",
        "**Difference in Code:**\n",
        "\n",
        "Use the code below to change the architecture of your `hd_model` to add dropout reguation.\n",
        "\n",
        "```text\n",
        "# -------------------------------------------------------------------\n",
        "# 7️⃣  Build Model with Dropout\n",
        "# -------------------------------------------------------------------\n",
        "n_classes = len(np.unique(hd_y_train))\n",
        "input_dim = hd_X_train_proc.shape[1]\n",
        "\n",
        "class HeartDiseaseNetDropout(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(HeartDiseaseNetDropout, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        # Dropout Layer\n",
        "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)  # Apply Dropout after activation\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "sVY8KiI-SXl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3A here\n",
        "\n"
      ],
      "metadata": {
        "id": "35iiG_fKSXmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to this output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image35C.png)"
      ],
      "metadata": {
        "id": "kDiVzAPmSXmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3B: Visualize Effects of Dropout**\n",
        "\n",
        "`Copy-and-paste` the code from **`Exercise 1B`** into the cell below. Change the titles of axis 1 and axis 2 to read \"Dropout\")."
      ],
      "metadata": {
        "id": "pWqR8MLORylJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3B here\n",
        "\n"
      ],
      "metadata": {
        "id": "rRy0ujxskyO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to these plots.\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image07E.png)\n",
        "\n",
        "### **Comparative Analysis: Dropout (Exercise 3A) vs. Baseline (Exercise 1A)**\n",
        "\n",
        "This comparison highlights how **Dropout** functions on the Heart Disease dataset. While it doesn't create the perfect \"inverted\" graph seen in the Obesity example, it significantly dampens the catastrophic overfitting seen in the baseline.\n",
        "\n",
        "##### **1. Model Loss (Left Plot)**\n",
        "* **Exercise 1A (Baseline - Image 02E):**\n",
        "    * **Behavior:** Total divergence. The Training Loss drops to ~0.5, while Validation Loss skyrockets to **>3.0**.\n",
        "    * **Diagnosis:** The model is completely memorizing the training data.\n",
        "* **Exercise 3A (Dropout - Image 07E):**\n",
        "    * **Behavior:** The divergence is **constrained**. The Validation Loss starts low but rises to **~1.2**.\n",
        "    * **Comparison:** While the Validation Loss is still increasing (indicating the model is struggling to generalize perfectly), Dropout has prevented the massive explosion seen in 1A. The gap between Train and Validation is much smaller.\n",
        "\n",
        "##### **2. Model Accuracy (Right Plot)**\n",
        "* **Exercise 1A (Baseline - Image 02E):**\n",
        "    * **Behavior:** Training Accuracy climbs to **~77%**, but Validation Accuracy crashes to **~50%**.\n",
        "    * **Diagnosis:** The model's \"knowledge\" is useless on new data.\n",
        "* **Exercise 3A (Dropout - Image 07E):**\n",
        "    * **Behavior:** Training and Validation Accuracy are tightly coupled.\n",
        "    * **Training:** Reaches ~64%.\n",
        "    * **Validation:** Hovers around **64-65%**.\n",
        "    * **Observation:** The model is no longer hallucinating high accuracy. By dropping neurons, the training accuracy has been forced down (from 77% to 64%), but the validation accuracy has stabilized and is actually performing *better* than the baseline (64% vs 50%).\n",
        "\n",
        "##### **Summary Table**\n",
        "\n",
        "| Metric | Exercise 1A (None) | Exercise 3A (Dropout) | Conclusion |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Peak Val Loss** | **~3.5** (Explosion) | **~1.2** (Rising) | Dropout reduced error magnitude by ~65%. |\n",
        "| **Val Accuracy** | **~50%** (Random) | **~64%** (Stable) | Dropout improved real-world performance. |\n",
        "| **Diagnosis** | Total Failure | **Stabilized** | The model is now stable, though likely needs further tuning (e.g., higher dropout rate or simpler architecture) to reduce the rising loss further. |"
      ],
      "metadata": {
        "id": "AbdhGv4SRylK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Batch Normalization**\n",
        "\n",
        "**Batch Normalization (BatchNorm)** is a technique introduced in 2015 by Ioffe and Szegedy that addresses one of the most difficult problems in training deep neural networks: **Internal Covariate Shift**. While primarily designed to speed up training, it also acts as a powerful regularizer that helps reduce overfitting.\n",
        "\n",
        "##### **The Problem: Internal Covariate Shift**\n",
        "As a neural network trains, the weights in the early layers change. This changes the distribution of the data (the \"covariate\") fed into the subsequent layers.\n",
        "* **The Consequence:** The deeper layers are constantly trying to adapt to a \"moving target.\" They have to re-adjust not just to the errors, but to the changing input distribution from the previous layer.\n",
        "* **The Result:** Training is slow, unstable, and requires very small learning rates.\n",
        "\n",
        "##### **The Solution: Normalize Every Layer**\n",
        "Batch Normalization solves this by forcing the inputs of a layer to have a mean of 0 and a variance of 1. It does this by normalizing the data using the statistics (mean and standard deviation) of the current **mini-batch**.\n",
        "\n",
        "After normalizing, the layer applies two learnable parameters, $\\gamma$ (scale) and $\\beta$ (shift), so the network can \"undo\" the normalization if it decides that the raw data was actually better.\n",
        "\n",
        "##### **How Batch Normalization Reduces Overfitting**\n",
        "Although BatchNorm was designed for speed, it has a side effect that functions like **Regularization**:\n",
        "\n",
        "1.  **Noise Injection:** Because the mean and variance are calculated based on a small *random* batch of data (e.g., 32 samples) rather than the entire dataset, there is statistical noise in these estimates.\n",
        "2.  **Regularization Effect:** This noise prevents the specific weights of a hidden neuron from becoming hyper-sensitive to a specific input sample. The neuron must learn to work with the data regardless of how it is scaled by the rest of the batch.\n",
        "3.  **Result:** This slight randomization (similar to Dropout, but less aggressive) discourages the network from memorizing the training data.\n",
        "\n",
        "##### **PyTorch Implementation**\n",
        "In PyTorch, you use `nn.BatchNorm1d` (for standard feedforward networks) or `nn.BatchNorm2d` (for images/CNNs).\n",
        "\n",
        "**Key Rules:**\n",
        "1.  **Placement:** It is typically placed **after** the linear layer but **before** the activation function (Linear $\\rightarrow$ BatchNorm $\\rightarrow$ ReLU).\n",
        "2.  **Train/Eval Modes:** Like Dropout, BatchNorm behaves differently during training and validation.\n",
        "    * **`model.train()`:** It calculates the mean/std from the current batch.\n",
        "    * **`model.eval()`:** It uses a \"running average\" of mean/std that it learned during training. **You must switch to eval mode for validation, or your results will be wrong.**\n"
      ],
      "metadata": {
        "id": "4kOKiKD6z4ZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4A: Batch Normalization\n",
        "\n",
        "While the data loading and loop structures are nearly identical, **Example 4A** introduces structural changes to the neural network to implement Batch Normalization.\n",
        "\n",
        "##### **1. Model Architecture (Section 7)**\n",
        "To add Batch Normalization to the neural network, a specific normalization layer is inserted into the network's architecture.\n",
        "\n",
        "* **In `__init__`:** We define the Batch Norm layer using `nn.BatchNorm1d`.\n",
        "    * *Note:* We use `1d` because our data is a flat vector of numbers. If we were processing images, we would use `2d`.\n",
        "    * *Parameter:* The argument `32` matches the number of neurons in the preceding layer.\n",
        "* **In `forward`:** The standard order of operations is applied: **Linear Layer $\\rightarrow$ Batch Norm $\\rightarrow$ Activation**.\n",
        "\n",
        "**Difference in Code:**\n",
        "\n",
        "The following code is used to change the architecture of the neural network to incorporate batch normalization:\n",
        "\n",
        "```text\n",
        "class ObesityNetBN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNetBN, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        \n",
        "        # --- ADDED IN EXAMPLE 4A ---\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        # ---------------------------\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        \n",
        "        # --- ADDED IN EXAMPLE 4A ---\n",
        "        x = self.bn1(x)  # Normalize the linear output\n",
        "        # ---------------------------\n",
        "        \n",
        "        x = self.relu(x) # Apply activation to normalized data\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "```\n"
      ],
      "metadata": {
        "id": "Wtm0qoCDpt_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4A: Batch Normalization\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# -------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# -------------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "VERBOSE       = 0\n",
        "LEARNING_RATE = 0.05\n",
        "BATCH_SIZE    = 32\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# -------------------------------------------------------------------\n",
        "ob_df = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "    na_values=['NA','?']\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# -------------------------------------------------------------------\n",
        "ob_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "ob_df[ob_target_col] = le.fit_transform(ob_df[ob_target_col])\n",
        "\n",
        "ob_X = ob_df.drop(columns=[ob_target_col])\n",
        "ob_y = ob_df[ob_target_col].astype('int32')\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# -------------------------------------------------------------------\n",
        "ob_X_temp, ob_X_test, ob_y_temp, ob_y_test = train_test_split(\n",
        "    ob_X, ob_y, test_size=0.2, random_state=42, stratify=ob_y\n",
        ")\n",
        "\n",
        "ob_X_train, ob_X_val, ob_y_train, ob_y_val = train_test_split(\n",
        "    ob_X_temp, ob_y_temp, test_size=0.1, random_state=42, stratify=ob_y_temp\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# -------------------------------------------------------------------\n",
        "categorical_cols = [c for c in ob_X.columns if ob_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in ob_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Fit & transform training data\n",
        "ob_X_train_proc = preprocessor.fit_transform(ob_X_train).astype(np.float32)\n",
        "ob_X_val_proc   = preprocessor.transform(ob_X_val).astype(np.float32)\n",
        "ob_X_test_proc  = preprocessor.transform(ob_X_test).astype(np.float32)\n",
        "\n",
        "ob_y_train = ob_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_val   = ob_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_test  = ob_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6️⃣  Convert to PyTorch Tensors & DataLoaders\n",
        "# -------------------------------------------------------------------\n",
        "ob_X_train_t = torch.tensor(ob_X_train_proc).to(device)\n",
        "ob_X_val_t   = torch.tensor(ob_X_val_proc).to(device)\n",
        "ob_X_test_t  = torch.tensor(ob_X_test_proc).to(device)\n",
        "\n",
        "ob_y_train_t = torch.tensor(ob_y_train, dtype=torch.long).to(device)\n",
        "ob_y_val_t   = torch.tensor(ob_y_val, dtype=torch.long).to(device)\n",
        "ob_y_test_t  = torch.tensor(ob_y_test, dtype=torch.long).to(device)\n",
        "\n",
        "ob_train_dataset = TensorDataset(ob_X_train_t, ob_y_train_t)\n",
        "ob_val_dataset   = TensorDataset(ob_X_val_t, ob_y_val_t)\n",
        "\n",
        "ob_train_loader = DataLoader(ob_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "ob_val_loader   = DataLoader(ob_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 7️⃣  Build Model with Batch Normalization\n",
        "# -------------------------------------------------------------------\n",
        "n_classes = len(np.unique(ob_y_train))\n",
        "input_dim = ob_X_train_proc.shape[1]\n",
        "\n",
        "class ObesityNetBN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNetBN, self).__init__()\n",
        "\n",
        "        # Hidden Layer\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "\n",
        "        # Batch Norm Layer (32 features coming from layer1)\n",
        "        # 1D is used for Dense/Linear layers\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Output Layer\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Order: Linear -> BatchNorm -> ReLU\n",
        "        x = self.layer1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "ob_model = ObesityNetBN(input_dim, n_classes).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(ob_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 8️⃣  Train model\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"------Training Starting for {EPOCHS} epochs --------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "ob_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Training Phase ---\n",
        "    # Important: ob_model.train() tells BN to use batch statistics\n",
        "    ob_model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in ob_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = ob_model(X_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(ob_train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    # Important: ob_model.eval() tells BN to use running stats (not batch stats)\n",
        "    ob_model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in ob_val_loader:\n",
        "            outputs = ob_model(X_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(ob_val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    # Store metrics\n",
        "    ob_history['accuracy'].append(train_acc)\n",
        "    ob_history['val_accuracy'].append(val_acc)\n",
        "    ob_history['loss'].append(avg_train_loss)\n",
        "    ob_history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "    if VERBOSE > 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "            f\"loss: {avg_train_loss:.4f} - \"\n",
        "            f\"acc: {train_acc:.4f} - \"\n",
        "            f\"val_loss: {avg_val_loss:.4f} - \"\n",
        "            f\"val_acc: {val_acc:.4f}\"\n",
        "        )\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished\")\n",
        "print(\"Best val accuracy:\", max(ob_history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
      ],
      "metadata": {
        "id": "2tuHT0xyp0hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to this output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image08E.png)"
      ],
      "metadata": {
        "id": "ZQNU8NuT2DKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4B: Visualize Effects of Batch Normalization\n",
        "\n",
        "The code in the cell below is **exactly** the same as that used in `Example 1B` except the axis titles have been changed to specify the type of overfitting protection that was used."
      ],
      "metadata": {
        "id": "xQm3iTI2qgO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4B: Visualize effects of Batch Normalization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 1: Loss (Left Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax1.plot(ob_history['loss'], label='Train Loss')\n",
        "ax1.plot(ob_history['val_loss'], label='Val Loss')\n",
        "ax1.set_title('Model Loss (Batch Normization)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 2: Accuracy (Right Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax2.plot(ob_history['accuracy'], label='Train Acc')\n",
        "ax2.plot(ob_history['val_accuracy'], label='Val Acc')\n",
        "ax2.set_title('Model Accuracy (Batch Normization)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V9jKOf_uqiXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to these plots.\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image09E.png)\n",
        "\n",
        "### **Comparative Analysis: Batch Normalization (Example 4A) vs. Baseline (Example 1A)**\n",
        "\n",
        "This comparison highlights a distinct characteristic of Batch Normalization: **Noise Injection**. While L2 Regularization smoothes the curves and Dropout inverts them, Batch Normalization often makes the training dynamics more volatile (jagged) while preventing pure memorization.\n",
        "\n",
        "##### **1. Model Loss (Left Plot)**\n",
        "* **Example 1A (Baseline - Image 01E):**\n",
        "    * **Behavior:** The Training Loss dropped smoothly to nearly **0**.\n",
        "    * **Stability:** After the initial drop, the curves were relatively smooth, indicating the model settled into a specific solution (memorization).\n",
        "* **Example 4A (Batch Norm - Image 09E):**\n",
        "    * **Behavior:** The Training Loss does **not** reach 0; it hovers around **0.15**.\n",
        "    * **Volatility:** The lines are extremely **jagged/spiky**.\n",
        "    * **Diagnosis:** This is the \"Noise Injection\" property of Batch Normalization in action. Because the model effectively sees slightly different data statistics in every batch, it cannot easily settle into a perfect memorization state. The loss remains higher because the model is constantly being forced to readjust.\n",
        "\n",
        "##### **2. Model Accuracy (Right Plot)**\n",
        "* **Example 1A (Baseline):**\n",
        "    * **Behavior:** The model achieved **100%** Training Accuracy and **~96%** Validation Accuracy.\n",
        "    * **Diagnosis:** Perfect memorization.\n",
        "* **Example 4A (Batch Norm - Image 09E):**\n",
        "    * **Behavior:** The Training Accuracy is capped at **~94%**, and Validation Accuracy hovers around **90-91%**.\n",
        "    * **Diagnosis:** Batch Normalization successfully prevented the model from reaching 100% training accuracy (memorization).\n",
        "\n",
        "##### **Key Takeaway**\n",
        "In this specific dataset, Batch Normalization acted as a very aggressive regularizer.\n",
        "* **The Trade-off:** While it successfully stopped the \"perfect memorization\" seen in Example 1A, the injected noise reduced the peak validation accuracy slightly (from ~96% to ~91%).\n",
        "* **When to use it:** This \"jagged\" behavior often stabilizes on larger datasets or with larger batch sizes. On smaller datasets (like this one), the statistical noise from small batches can sometimes be too aggressive, as seen in the volatility of the orange line."
      ],
      "metadata": {
        "id": "lMN7Gy283g0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4A: Batch Normalization**\n",
        "\n",
        "In the cell below write the code to add `Batch Normalization` to your `hd_model`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "First, `copy-and-paste` your **`Exercise 1A`** into the cell below.\n",
        "\n",
        "##### **1. Model Architecture (Section 7)**\n",
        "To add Batch Normalization to the neural network, a specific normalization layer is inserted into the network's architecture.\n",
        "\n",
        "* **In `__init__`:** We define the Batch Norm layer using `nn.BatchNorm1d`.\n",
        "    * *Note:* We use `1d` because our data is a flat vector of numbers. If we were processing images, we would use `2d`.\n",
        "    * *Parameter:* The argument `32` matches the number of neurons in the preceding layer.\n",
        "* **In `forward`:** The standard order of operations is applied: **Linear Layer $\\rightarrow$ Batch Norm $\\rightarrow$ Activation**.\n",
        "\n",
        "**Difference in Code:**\n",
        "\n",
        "Add batch normalization to the architecture of your `hd_model` by using the following code snippet:\n",
        "\n",
        "```text\n",
        "# -------------------------------------------------------------------\n",
        "# 7️⃣  Build Model with Batch Normalization\n",
        "# -------------------------------------------------------------------\n",
        "n_classes = len(np.unique(hd_y_train))\n",
        "input_dim = hd_X_train_proc.shape[1]\n",
        "\n",
        "class HeartDiseaseNetBN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(HeartDiseaseNetBN, self).__init__()\n",
        "\n",
        "        # Hidden Layer\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "\n",
        "        # Batch Norm Layer (32 features coming from layer1)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Output Layer\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Order: Linear -> BatchNorm -> ReLU\n",
        "        x = self.layer1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "jGt5f0uUrB0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4A here\n",
        "\n"
      ],
      "metadata": {
        "id": "1KmXgVa3rDRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4B: Visualize the Effects of Batch Normalization**\n",
        "\n",
        "`Copy-and-paste` the code from **`Exercise 1B`** into the cell below. Change the titles of axis 1 and axis 2 to read \"Batch Normalization\")."
      ],
      "metadata": {
        "id": "73H_ZedPrD-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4B here\n",
        "\n"
      ],
      "metadata": {
        "id": "qBa3zuS_rdfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to these plots.\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image10E.png)\n",
        "\n",
        "\n",
        "### **Comparative Analysis: Batch Normalization (Exercise 4A) vs. Baseline (Exercise 1A)**\n",
        "\n",
        "This comparison illustrates how Batch Normalization affects model training on the `RestingECG` prediction task. Similar to the previous examples, it acts as a regularizer, though the effects on this specific dataset are nuanced.\n",
        "\n",
        "##### **1. Model Loss (Left Plot)**\n",
        "* **Exercise 1A (Baseline - Image 02E):**\n",
        "    * **Behavior:** Catastrophic failure. The Validation Loss (Orange) diverges almost immediately and skyrockets to **>3.0**.\n",
        "    * **Diagnosis:** Complete memorization of training data with no ability to generalize.\n",
        "* **Exercise 4A (Batch Norm - Image 10E):**\n",
        "    * **Behavior:** The Validation Loss rises to **~1.3** by Epoch 100.\n",
        "    * **Comparison:** While the loss is still rising (indicating some overfitting is still occurring), Batch Normalization successfully **dampened the explosion**. It reduced the peak error by more than 50% compared to the baseline.\n",
        "    * **Dynamics:** Notice the training loss (Blue) flattens out around 0.7 rather than dropping to 0.5 like the baseline. This confirms that Batch Norm is making it harder for the model to \"cheat\" and memorize the training set.\n",
        "\n",
        "##### **2. Model Accuracy (Right Plot)**\n",
        "* **Exercise 1A (Baseline - Image 02E):**\n",
        "    * **Behavior:** Validation accuracy collapses to **~50%**, which is effectively random guessing for this task.\n",
        "    * **Stability:** Highly unstable.\n",
        "* **Exercise 4A (Batch Norm - Image 10E):**\n",
        "    * **Behavior:** Validation accuracy fluctuates between **50% and 60%**.\n",
        "    * **Comparison:** While not a \"solved\" problem (accuracy is still low), the model with Batch Normalization performs consistently better than the baseline. It avoids the complete collapse seen in 1A.\n",
        "\n",
        "##### **Summary Table**\n",
        "\n",
        "| Metric | Exercise 1A (None) | Exercise 4A (Batch Norm) | Conclusion |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Peak Val Loss** | **~3.5** | **~1.3** | Batch Norm significantly reduced the magnitude of overfitting errors. |\n",
        "| **Val Accuracy** | **~50%** (Random) | **~55-60%** (Slight Gain) | The model is more stable but struggles to learn high-quality features from this small dataset. |\n",
        "| **Overall** | **Total Failure** | **Stabilized Failure** | Batch Norm prevented the worst of the overfitting but didn't miraculously solve the problem. This suggests the model might need **stronger regularization** (like higher Dropout) or more data. |"
      ],
      "metadata": {
        "id": "vC8ea9yZ7Vky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Early Stopping in PyTorch to Prevent Overfitting**\n",
        "\n",
        "Now that we have established baselines in `Example 1` and in **`Exercise 1`** that **overfitting** occurs, we will now examine techniques that can be used to control overfitting. In **`Example 5`**, we will look at **`Early Stopping`**.\n",
        "\n",
        "It can be difficult to determine exactly how many epochs to cycle through to optimally train a neural network. **_Overfitting_** will occur if you train the neural network for too many epochs; the model will begin to memorize the training set and will not perform well on **new data**, despite attaining a high accuracy during training.\n",
        "\n",
        "Overfitting occurs when a neural network is trained to the point that it begins to **memorize rather than generalize**, as demonstrated in this figure.\n",
        "\n",
        "![Training vs. Validation Error for Overfitting](https://biologicslab.co/BIO1173/images/class_02/class_3_training_val.png \"Training vs. Validation Error for Overfitting\")\n",
        "**Training vs. Validation Error for Overfitting**\n",
        "\n",
        "### **Implementation in PyTorch**\n",
        "Unlike Keras, which provides a built-in callback for this, **PyTorch** requires us to implement Early Stopping logic manually within the training loop. This generally involves:\n",
        "1.  **Monitoring** the Validation Loss after every epoch.\n",
        "2.  **Saving** the model weights whenever the Validation Loss improves (decreases).\n",
        "3.  **Stopping** the training loop if the Validation Loss has not improved for a set number of epochs (a parameter often called `patience`).\n",
        "\n",
        "It is important to segment the original dataset into several datasets to support this process:\n",
        "\n",
        "* **Training Set**\n",
        "* **Validation Set**\n",
        "* **Holdout Set**\n",
        "\n",
        "You can construct these sets in several different ways. The following code blocks demonstrate the standard approach.\n",
        "\n",
        "The most common method uses a **`training`** and **`validation set`**. We use the `training data` to train the neural network and the `validation set` to determine when to stop. This attempts to halt training at the \"sweet spot\" before the validation error begins to rise.\n",
        "\n",
        "This method will only give accurate \"out of sample\" predictions for the `validation set`; this is usually 20% of the data. The predictions for the training data will be overly optimistic, as these were the data that we used to train the neural network.\n",
        "\n",
        "This figure demonstrates how we divide the dataset.\n",
        "\n",
        "![Training with a Validation Set](https://biologicslab.co/BIO1173/images/class_02/class_1_train_val.png \"Training with a Validation Set\")\n",
        "\n",
        "**Training with a Validation Set**"
      ],
      "metadata": {
        "id": "lTXppu2b8LZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5A: Early Stopping\n",
        "\n",
        "Adding Early Stopping fundamentally changes the **Training Loop**. Instead of blindly running for 100 epochs, it now includes logic to monitor performance, adjust learning rates, and stop training automatically.\n",
        "\n",
        "##### **1. New Parameters (Section 1)**\n",
        "Example 5A adds a parameter to define how long the model should wait for improvement before giving up.\n",
        "* **Code Change:**\n",
        "    ```python\n",
        "    PATIENCE = 10  # Stop if no improvement after 10 epochs\n",
        "    ```\n",
        "\n",
        "##### **2. Learning Rate Scheduler (Section 7)**\n",
        "Example 5A introduces a **Scheduler**. This is often used alongside Early Stopping to squeeze out a bit more performance before stopping.\n",
        "* **Code Change:** `optim.lr_scheduler.ReduceLROnPlateau` is added.\n",
        "* **Function:** If the validation accuracy stalls, it reduces the learning rate (by half in this code) to help the model fine-tune its weights.\n",
        "\n",
        "##### **3. Early Stopping Logic (Section 8 - Inside Loop)**\n",
        "In Example 1A, the loop simply ran `for epoch in range(EPOCHS)`. In Example 5A, a sophisticated check is added at the end of every epoch:\n",
        "\n",
        "1.  **Check Improvement:** It compares the current `val_acc` to the `best_val_acc` seen so far.\n",
        "2.  **Save Best Model:** If the current model is the best, it saves the weights immediately using `torch.save()`.\n",
        "    ```python\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(ob_model.state_dict(), checkpoint_path) # <--- CRITICAL\n",
        "        patience_counter = 0\n",
        "    ```\n",
        "3.  **Trigger Stop:** If performance didn't improve, it increases the `patience_counter`. If the counter hits the limit (`10`), the loop is broken immediately.\n",
        "    ```python\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(f\"Early stopping triggered...\")\n",
        "        break\n",
        "    ```\n",
        "\n",
        "##### **4. Restoring the Best Weights (Section 9)**\n",
        "This is a critical step often missed by beginners.\n",
        "* **The Problem:** When Early Stopping triggers (e.g., at Epoch 50), the model in memory is the one from Epoch 50. However, because patience was 10, the *best* model was actually from Epoch 40.\n",
        "* **The Fix:** Example 5A reloads the saved checkpoint at the very end to ensure `ob_model` contains the optimal weights, not the \"overfitted\" weights that triggered the stop.\n",
        "    \n",
        "  ```python\n",
        "    ob_model.load_state_dict(torch.load(checkpoint_path))\n",
        "  ```"
      ],
      "metadata": {
        "id": "d2Kns2kcIFgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5A: Early Stopping\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# ------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# ------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "PATIENCE      = 10\n",
        "VERBOSE       = 2     # Set to 1 or 2 to see training progress\n",
        "LEARNING_RATE = 0.05\n",
        "BATCH_SIZE    = 32\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "ob_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# ------------------------------------------------------------\n",
        "ob_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "ob_df[ob_target_col] = le.fit_transform(ob_df[ob_target_col])\n",
        "\n",
        "ob_X = ob_df.drop(columns=[ob_target_col])\n",
        "ob_y = ob_df[ob_target_col].astype('int32')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# ------------------------------------------------------------\n",
        "# First split into train+val and test\n",
        "ob_X_temp, ob_X_test, ob_y_temp, ob_y_test = train_test_split(\n",
        "    ob_X, ob_y, test_size=0.2, random_state=42, stratify=ob_y)\n",
        "\n",
        "# Then split train+val into train and val\n",
        "ob_X_train, ob_X_val, ob_y_train, ob_y_val = train_test_split(\n",
        "    ob_X_temp, ob_y_temp, test_size=0.1, random_state=42, stratify=ob_y_temp)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# ------------------------------------------------------------\n",
        "categorical_cols = [c for c in ob_X.columns if ob_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in ob_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Convert data to correct numeric type (Numpy arrays)\n",
        "ob_X_train_proc = preprocessor.fit_transform(ob_X_train).astype(np.float32)\n",
        "ob_X_val_proc   = preprocessor.transform(ob_X_val).astype(np.float32)\n",
        "ob_X_test_proc  = preprocessor.transform(ob_X_test).astype(np.float32)\n",
        "\n",
        "ob_y_train = ob_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_val   = ob_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_test  = ob_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6️⃣  Convert to PyTorch Tensors & DataLoaders\n",
        "# ------------------------------------------------------------\n",
        "ob_X_train_t = torch.tensor(ob_X_train_proc).to(device)\n",
        "ob_X_val_t   = torch.tensor(ob_X_val_proc).to(device)\n",
        "ob_X_test_t  = torch.tensor(ob_X_test_proc).to(device)\n",
        "\n",
        "ob_y_train_t = torch.tensor(ob_y_train, dtype=torch.long).to(device)\n",
        "ob_y_val_t   = torch.tensor(ob_y_val, dtype=torch.long).to(device)\n",
        "ob_y_test_t  = torch.tensor(ob_y_test, dtype=torch.long).to(device)\n",
        "\n",
        "ob_train_dataset = TensorDataset(ob_X_train_t, ob_y_train_t)\n",
        "ob_val_dataset   = TensorDataset(ob_X_val_t, ob_y_val_t)\n",
        "\n",
        "ob_train_loader = DataLoader(ob_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "ob_val_loader   = DataLoader(ob_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7️⃣  Build Model, Optimizer & Scheduler\n",
        "# ------------------------------------------------------------\n",
        "n_classes = len(np.unique(ob_y_train))\n",
        "input_dim = ob_X_train_proc.shape[1]\n",
        "\n",
        "class ObesityNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ObesityNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output = nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "ob_model = ObesityNet(input_dim, n_classes).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(ob_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# ReduceLROnPlateau Scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
        "                                                 factor=0.5,\n",
        "                                                 patience=int(PATIENCE/2))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8️⃣  Train model with Early Stopping\n",
        "# ------------------------------------------------------------\n",
        "print(f\"-----Training Starting for up to {EPOCHS} epochs --------------------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "ob_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "\n",
        "# Variables for Early Stopping\n",
        "best_val_acc = 0.0\n",
        "patience_counter = 0\n",
        "checkpoint_path = \"ob_best_classification_model.pth\"\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Training Phase ---\n",
        "    ob_model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in ob_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = ob_model(X_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(ob_train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    ob_model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in ob_val_loader:\n",
        "            outputs = ob_model(X_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(ob_val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    # Update History\n",
        "    ob_history['accuracy'].append(train_acc)\n",
        "    ob_history['val_accuracy'].append(val_acc)\n",
        "    ob_history['loss'].append(avg_train_loss)\n",
        "    ob_history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "    # Step Scheduler (monitor val_accuracy)\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    # Print Progress\n",
        "    if VERBOSE > 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
        "            f\"loss: {avg_train_loss:.4f} - \"\n",
        "            f\"acc: {train_acc:.4f} - \"\n",
        "            f\"val_loss: {avg_val_loss:.4f} - \"\n",
        "            f\"val_acc: {val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "    # --- Early Stopping & Checkpointing Logic ---\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0 # Reset counter\n",
        "        torch.save(ob_model.state_dict(), checkpoint_path) # Save Best Model\n",
        "        if VERBOSE > 0:\n",
        "            print(f\"  -> Val Acc improved to {val_acc:.4f}. Model saved.\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if VERBOSE > 0:\n",
        "            print(f\"  -> No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training & Restore Best Weights\n",
        "# ---------------------------------------------------------------------------\n",
        "# Load the best weights back into the model\n",
        "ob_model.load_state_dict(torch.load(checkpoint_path))\n",
        "print(\"\\nRestored best model weights.\")\n",
        "\n",
        "print(f\"Training finished\")\n",
        "print(\"Best val accuracy:\", max(ob_history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
      ],
      "metadata": {
        "id": "NOqpiwIgeATc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to last part of the output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image11E.png)\n",
        "\n",
        "While the number of Epochs was set to `100`, training stopped in this particular training run after the `24th` Epoch. This was due to the `Early Stopping` monitor terminating training after waiting `10` epochs for the `val_accuracy` to start improving."
      ],
      "metadata": {
        "id": "XbpzAmsBJXBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Variables that Control Early Stopping**\n",
        "\n",
        "Since PyTorch requires us to implement Early Stopping manually (as shown in **Example 5A**), these \"arguments\" become **variables** or **logic checks** that you define in your training loop.\n",
        "\n",
        "* **`min_delta` (Threshold)**: This represents the minimum change required to qualify as an improvement. In your code, you can enforce this by adding a small buffer to your comparison (e.g., `if val_acc > best_val_acc + min_delta`). This prevents the model from continuing just because of tiny, insignificant fluctuations.\n",
        "* **`patience`**: Defined as a constant (e.g., `PATIENCE = 10`). This controls how many epochs the loop continues without improvement before breaking. You implement this using a counter variable that increments when performance stalls and resets when a new record is set.\n",
        "* **`verbose`**: A variable (e.g., `VERBOSE = 1`) used to control how much output is generated. You wrap your `print()` statements inside an `if VERBOSE > 0:` block to keep your output clean.\n",
        "* **`mode`**: In PyTorch, this is determined by the logic in your `if` statement.\n",
        "    * **\"min\" mode:** Use `<` (e.g., `if val_loss < best_val_loss`). Use this for metrics you want to minimize, like Loss or RMSE.\n",
        "    * **\"max\" mode:** Use `>` (e.g., `if val_acc > best_val_acc`). Use this for metrics you want to maximize, like Accuracy.\n",
        "* **`restore_best_weights`**: This is the most critical manual step. You must explicitly **save** the model (`torch.save`) whenever a new best metric is found, and then **reload** it (`model.load_state_dict`) after the loop finishes. If you skip this, your model will retain the weights from the *last* epoch (which are likely overfitted) rather than the *best* epoch."
      ],
      "metadata": {
        "id": "hSBtBlXV-MSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5B: Visualize Effects of Early Stopping\n",
        "\n",
        "The code in the cell below is **exactly** the same as that used in `Example 1B` except the axis titles have been changed to specify the type of overfitting protection that was used."
      ],
      "metadata": {
        "id": "QdLBStCj_yR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5B: Visualize effects of Early Stopping\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 1: Loss (Left Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax1.plot(ob_history['loss'], label='Train Loss')\n",
        "ax1.plot(ob_history['val_loss'], label='Val Loss')\n",
        "ax1.set_title('Model Loss (Early Stopping)')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot 2: Accuracy (Right Graph)\n",
        "# ------------------------------------------------------------------\n",
        "ax2.plot(ob_history['accuracy'], label='Train Acc')\n",
        "ax2.plot(ob_history['val_accuracy'], label='Val Acc')\n",
        "ax2.set_title('Model Accuracy (Early Stopping)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zLN6M1_0f8jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image12E.png)\n",
        "\n",
        "### **Comparative Analysis: Early Stopping (Example 5A) vs. Baseline (Example 1A)**\n",
        "\n",
        "This comparison highlights the efficiency and safety provided by **Early Stopping**. Unlike previous techniques that modify *how* the model learns (Regularization, Dropout, BatchNorm), Early Stopping simply modifies *how long* it learns.\n",
        "\n",
        "##### **1. The X-Axis (Epochs)**\n",
        "* **Example 1A (Baseline):**\n",
        "    * **Behavior:** The training ran for the full **100 Epochs**, regardless of whether the model was improving.\n",
        "    * **Result:** Resources were wasted for roughly 50+ epochs where the model was just memorizing data without improving validation performance.\n",
        "* **Example 5A (Early Stopping - Image 12E):**\n",
        "    * **Behavior:** The training stopped automatically at **Epoch 23**.\n",
        "    * **Result:** The technique detected that the model peaked around Epoch 13 (since Patience was 10, it waited 10 more epochs before stopping). This saved ~75% of the computational time.\n",
        "\n",
        "##### **2. Model Loss (Left Plot)**\n",
        "* **Example 1A:** The Training Loss (Blue) dropped to 0, while Validation Loss (Orange) drifted apart, indicating overfitting.\n",
        "* **Example 5A:** The Training Loss still drops rapidly, but the process cuts off before the Validation Loss has a chance to diverge significantly. The model is effectively \"frozen\" at its point of minimum error (roughly Epoch 13, where Val Loss is ~0.1).\n",
        "\n",
        "##### **3. Model Accuracy (Right Plot)**\n",
        "* **Example 1A:** Reached **100%** Training Accuracy and **~96-97%** Validation Accuracy.\n",
        "* **Example 5A:** Reached **100%** Training Accuracy and a very high Validation Accuracy (**~97-98%**).\n",
        "* **Observation:** By restoring the \"Best Weights\" (from Epoch 13) rather than using the \"Final Weights\" (from Epoch 23), Early Stopping ensures we keep the version of the model that generalizes best, rather than the one that memorized the most.\n",
        "\n",
        "##### **Summary Table**\n",
        "\n",
        "| Metric | Example 1A (No Prevention) | Example 5A (Early Stopping) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Training Time** | 100 Epochs (100%) | 23 Epochs (~25%) |\n",
        "| **Validation Loss** | ~0.25 (Drifting) | **~0.10** (Optimal) |\n",
        "| **Outcome** | Overfitting & Wasted Time | **Optimal Generalization & Efficiency** |"
      ],
      "metadata": {
        "id": "R7rzD3dW_yR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5A: Early Stopping**\n",
        "\n",
        "In the cell below, write the code to add Early Stopping to your `hd_model`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "First, start by copying-and-pasting Exercise 1A into the cell below.\n",
        "\n",
        "##### **1. New Parameters (Section 1)**\n",
        "Add a parameter to define how long the model should wait for improvement before giving up.\n",
        "* **Code Change:**\n",
        "    ```python\n",
        "    PATIENCE = 10  # Stop if no improvement after 10 epochs\n",
        "    ```\n",
        "\n",
        "##### **2. Learning Rate Scheduler (Section 7)**\n",
        "You will need to introduce a **Scheduler**. This is often used alongside Early Stopping to squeeze out a bit more performance before stopping.\n",
        "* **Code Change:** `optim.lr_scheduler.ReduceLROnPlateau` is added.\n",
        "* **Function:** If the validation accuracy stalls, it reduces the learning rate (by half in this code) to help the model fine-tune its weights.\n",
        "\n",
        "##### **3. Early Stopping Logic (Section 8 - Inside Loop)**\n",
        "In **Exercise 1A**, the loop simply ran `for epoch in range(EPOCHS)`. For **Exercise 5A**, you need to add a check to the end of every epoch using the following code:\n",
        "\n",
        "```Python\n",
        "    # --- Early Stopping & Checkpointing Logic ---\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0 # Reset counter\n",
        "        torch.save(hd_model.state_dict(), checkpoint_path) # Save Best Model\n",
        "        if VERBOSE > 0:\n",
        "            print(f\"  -> Val Acc improved to {val_acc:.4f}. Model saved.\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if VERBOSE > 0:\n",
        "            print(f\"  -> No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "```\n",
        "This code chunk compares the current `val_acc` to the `best_val_acc` seen so far. If the current model is the best, it saves the weights immediately using\n",
        "    ```Python\n",
        "      torch.save(hd_model.state_dict(), checkpoint_path) # Save Best Model\n",
        "    ```\n",
        "If performance didn't improve, it increases the `patience_counter`. If the counter hits the limit (`10`), the loop is broken immediately.\n",
        "    ```python\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(f\"Early stopping triggered...\")\n",
        "        break\n",
        "    ```\n",
        "\n",
        "##### **4. Restoring the Best Weights (Section 9)**\n",
        "This is a critical step often missed by beginners.\n",
        "* **The Problem:** When Early Stopping triggers (e.g., at Epoch 50), the model in memory is the one from Epoch 50. However, because patience was 10, the *best* model was actually from Epoch 40.\n",
        "* **The Fix:** You need the saved checkpoint at the very end to ensure `hd_model` contains the optimal weights, not the \"overfitted\" weights that triggered the stop.\n",
        "    \n",
        "  ```python\n",
        "    hd_model.load_state_dict(torch.load(checkpoint_path))\n",
        "  ```"
      ],
      "metadata": {
        "id": "mwyLo-hvs1JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5A here\n",
        "\n"
      ],
      "metadata": {
        "id": "TejZ0RE5bVl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to this output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image13E.png)\n",
        "\n",
        "While the number of Epochs was set to `100` training terminated in this particular training run after the `11th` Epoch due to the `Early Stopping` monitor."
      ],
      "metadata": {
        "id": "R5CRi6QoU3e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5B: Visualize Effects of Early Stopping**\n",
        "\n",
        "`Copy-and-paste` your code from **Exercise 1B`** into the cell below and change the axis titles to specify the type of overfitting protection."
      ],
      "metadata": {
        "id": "06ILcia_CBcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5B here\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "RbHt9Xv6h0bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following pair of graphs:\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image14E.png)\n",
        "\n",
        "### **Comparative Analysis: Early Stopping (Exercise 5A) vs. Baseline (Exercise 1A)**\n",
        "\n",
        "This comparison demonstrates how **Early Stopping** functions as a \"safety brake\" for the model. On the difficult Heart Disease dataset (`RestingECG`), where the model is prone to rapid overfitting, this technique prevents the catastrophic failure seen in the baseline.\n",
        "\n",
        "##### **1. The X-Axis (Training Duration)**\n",
        "* **Exercise 1A (Baseline - Image 02E):**\n",
        "    * **Behavior:** The model ran for the full **100 Epochs**.\n",
        "    * **Result:** It spent ~95 epochs actively making the model worse (increasing validation error).\n",
        "* **Exercise 5A (Early Stopping - Image 14E):**\n",
        "    * **Behavior:** Training terminated automatically at **Epoch 10**.\n",
        "    * **Result:** The algorithm detected that the model was not improving and pulled the plug immediately, saving 90% of the compute time.\n",
        "\n",
        "##### **2. Model Loss (Left Plot)**\n",
        "* **Exercise 1A (Baseline):**\n",
        "    * **Behavior:** Validation Loss exploded from **~1.0** to **>3.5**.\n",
        "    * **Diagnosis:** Severe divergence.\n",
        "* **Exercise 5A (Early Stopping):**\n",
        "    * **Behavior:** The Validation Loss fluctuates between **0.90** and **0.98**.\n",
        "    * **Impact:** By stopping early, we prevented the loss from spiraling out of control. We effectively captured the model in its initial state before it could start memorizing noise.\n",
        "\n",
        "##### **3. Model Accuracy (Right Plot)**\n",
        "* **Exercise 1A (Baseline):**\n",
        "    * **Behavior:** Validation Accuracy collapsed to **~50%** (random guessing) and stayed there.\n",
        "    * **Diagnosis:** The model's weights became so distorted by overfitting that it lost all predictive power on new data.\n",
        "* **Exercise 5A (Early Stopping):**\n",
        "    * **Behavior:** The Validation Accuracy peaked early at **~63.5%** (Epoch 0 and Epoch 9).\n",
        "    * **Impact:** By stopping and restoring the best weights, the final model retains this **63% accuracy**.\n",
        "    * **Comparison:** **63%** (Early Stopping) vs **50%** (Baseline). This is a massive relative improvement, simply by not allowing the training to continue too long.\n",
        "\n",
        "##### **Summary Table**\n",
        "\n",
        "| Metric | Exercise 1A (No Prevention) | Exercise 5A (Early Stopping) | Conclusion |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Duration** | 100 Epochs | **10 Epochs** | Saved 90% compute time. |\n",
        "| **Peak Val Loss** | **~3.5** (Huge Error) | **~0.98** (Contained) | Prevented error explosion. |\n",
        "| **Final Accuracy** | **~50%** | **~63%** | captured the model at its peak performance. |"
      ],
      "metadata": {
        "id": "okpZ9uQHCBcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Summary: Overfitting Prevention Techniques in PyTorch**\n",
        "\n",
        "In this lesson, we explored four distinct techniques to combat overfitting. It is important to remember that there is no \"silver bullet\"—each technique has its own strengths, weaknesses, and ideal use cases.\n",
        "\n",
        "##### **1. L2 Regularization (Weight Decay)**\n",
        "* **Mechanism:** Adds a penalty to the loss function proportional to the size of the weights ($Loss + \\lambda \\sum w^2$). This forces the model to keep weights small and simple.\n",
        "* **Strengths:**\n",
        "    * Very easy to implement (just one argument in the optimizer: `weight_decay`).\n",
        "    * Mathematically smooths the model's decision boundaries.\n",
        "    * Works well on almost all types of problems.\n",
        "* **Weaknesses:**\n",
        "    * Requires tuning the strength parameter ($\\lambda$). Too weak = no effect; Too strong = underfitting (model can't learn).\n",
        "* **Best For:** A good \"first line of defense\" for almost any neural network.\n",
        "\n",
        "##### **2. Dropout**\n",
        "* **Mechanism:** Randomly deactivates (zeroes out) a percentage of neurons during training.\n",
        "* **Strengths:**\n",
        "    * Extremely effective at preventing \"memorization\" of noise.\n",
        "    * Forces the network to learn robust, redundant features (like an ensemble of many networks).\n",
        "* **Weaknesses:**\n",
        "    * Can slow down convergence (training takes longer because the model is \"handicapped\").\n",
        "    * Can lead to lower performance on the training set (which is expected, but can be confusing for beginners).\n",
        "* **Best For:** Large, deep networks where the model capacity far exceeds the amount of training data.\n",
        "\n",
        "##### **3. Batch Normalization**\n",
        "* **Mechanism:** Normalizes the inputs of each layer to have a mean of 0 and variance of 1, injecting a small amount of statistical noise from the batch.\n",
        "* **Strengths:**\n",
        "    * **Primary Goal:** Drastically speeds up training and stabilizes gradients.\n",
        "    * **Secondary Goal:** Acts as a mild regularizer, often removing the need for heavy Dropout.\n",
        "* **Weaknesses:**\n",
        "    * Adds complexity to the model architecture (extra layers).\n",
        "    * Can be unstable with very small batch sizes (e.g., Batch Size < 16).\n",
        "* **Best For:** Deep networks (CNNs, ResNets) to ensure they actually train successfully.\n",
        "\n",
        "##### **4. Early Stopping**\n",
        "* **Mechanism:** Monitors validation performance and stops training when it stops improving.\n",
        "* **Strengths:**\n",
        "    * Saves computational resources (time and electricity).\n",
        "    * Prevents the model from training long enough to memorize noise.\n",
        "    * **Restoring Best Weights:** Ensures you keep the \"smartest\" version of your model, not just the \"latest\" one.\n",
        "* **Weaknesses:**\n",
        "    * It doesn't fix the model architecture; if your model is fundamentally flawed, stopping early won't make it better, just faster at failing.\n",
        "* **Best For:** **Every single training run.** There is rarely a good reason *not* to use Early Stopping.\n",
        "\n",
        "---\n",
        "\n",
        "### **Combining Techniques (The \"Swiss Cheese\" Model)**\n",
        "\n",
        "In professional Deep Learning, you rarely use just one of these. You usually combine them to cover different \"holes\" in your defense against overfitting.\n",
        "\n",
        "* **Standard Recipe:**\n",
        "    * **Always** use **Early Stopping**.\n",
        "    * **Almost Always** use a small amount of **L2 Regularization** (`1e-4` or `1e-5`).\n",
        "    * **If the model is deep:** Use **Batch Normalization** between layers.\n",
        "    * **If the model is still overfitting:** Add **Dropout** layers, starting with a low rate (0.2) and increasing if necessary (up to 0.5).\n",
        "\n",
        "**Example of a robust architecture:**\n",
        "```python\n",
        "Linear Layer -> Batch Norm -> ReLU -> Dropout -> Linear Layer ...\n",
        "```"
      ],
      "metadata": {
        "id": "lL4OMPdVD6UV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_02_3.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas. Make sure you submitted a COPY of this lesson that had been saved to your GDrive instead of the orignal Colab notebook if you want this lesson graded."
      ],
      "metadata": {
        "id": "-yzuCJFCiaxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lizard Tail**\n",
        "\n",
        "\n",
        "![___](https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Apple_II_typical_configuration_1977.png/2560px-Apple_II_typical_configuration_1977.png)\n",
        "\n",
        "## **Apple II (original)**\n",
        "\n",
        "The **Apple II** (stylized as apple ][) is a personal computer released by Apple Inc. in June 1977. It was one of the first successful mass-produced microcomputer products and is widely regarded as one of the most important personal computers of all time due to its role in popularizing home computing and influencing later software development.\n",
        "\n",
        "The Apple II was designed primarily by Steve Wozniak. The system is based around the 8-bit MOS Technology 6502 microprocessor. Jerry Manock designed the foam-molded plastic case, Rod Holt developed the switching power supply, while Steve Jobs was not involved in the design of the computer. It was introduced by Jobs and Wozniak at the 1977 West Coast Computer Faire, and marks Apple's first launch of a computer aimed at a consumer market—branded toward American households rather than businessmen or computer hobbyists.\n",
        "\n",
        "\n",
        "The three computers that Byte magazine referred to as the \"1977 Trinity\" of home computing: Commodore PET 2001, Apple II, and TRS-80 Model I\n",
        "Byte magazine referred to the Apple II, Commodore PET 2001, and TRS-80 as the \"1977 Trinity\". As the Apple II had the defining feature of being able to display color graphics, the Apple logo was redesigned to have a spectrum of colors.\n",
        "\n",
        "The Apple II was the first in a series of computers collectively referred to by the Apple II name. It was followed by the Apple II+, Apple IIe, Apple IIc, Apple IIc Plus, and the 16-bit Apple IIGS—all of which remained compatible. Production of the last available model, the Apple IIe, ceased in November 1993.\n",
        "\n",
        "**History**\n",
        "\n",
        "By 1976, Steve Jobs had convinced product designer Jerry Manock (who had formerly worked at Hewlett Packard designing calculators) to create the \"shell\" for the Apple II—a smooth case inspired by kitchen appliances that concealed the internal mechanics. The earliest Apple II computers were assembled in Silicon Valley and later in Texas; printed circuit boards were manufactured in Ireland and Singapore. The first computers went on sale on June 10, 1977 with an MOS Technology 6502 microprocessor running at 1.023 MHz (2⁄7 of the NTSC color subcarrier), two game paddles (bundled until 1980, when they were found to violate FCC regulations), 4 KiB of RAM, an audio cassette interface for loading programs and storing data, and the Integer BASIC programming language built into ROMs. The video controller displayed 24 lines by 40 columns of monochrome, uppercase-only text on the screen (the original character set matches ASCII characters 20h to 5Fh), with NTSC composite video output suitable for display on a video monitor or on a regular TV set (by way of a separate RF modulator).\n",
        "\n",
        "The original retail price of the computer with 4 KiB of RAM was US \\$1,298 (equivalent to \\$6,530 in 2023) and with the maximum 48 KiB of RAM, it was US \\$2,638 (equivalent to \\$13,260 in 2023) To reflect the computer's color graphics capability, the Apple logo on the casing has rainbow stripes, which remained a part of Apple's corporate logo until early 1998. Perhaps most significantly, the Apple II was a catalyst for personal computers across many industries; it opened the doors to software marketed at consumers.\n",
        "\n",
        "Certain aspects of the system's design were influenced by Atari, Inc.'s arcade video game Breakout (1976), which was designed by Wozniak, who said: \"A lot of features of the Apple II went in because I had designed Breakout for Atari. I had designed it in hardware. I wanted to write it in software now\". This included his design of color graphics circuitry, the addition of game paddle support and sound, and graphics commands in Integer BASIC, with which he wrote Brick Out, a software clone of his own hardware game. Wozniak said in 1984: \"Basically, all the game features were put in just so I could show off the game I was familiar with—Breakout—at the Homebrew Computer Club. It was the most satisfying day of my life I demonstrated Breakout—totally written in BASIC. It seemed like a huge step to me. After designing hardware arcade games, I knew that being able to program them in BASIC was going to change the world.\"\n",
        "\n",
        "# Summary of the Apple II Computer\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The **Apple II**, introduced in **1977**, was one of the first highly successful mass-produced microcomputers. It was designed primarily by **Steve Wozniak**, with marketing and business strategy led by **Steve Jobs**. It played a crucial role in launching the personal computer revolution.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Processor**: MOS Technology 6502 @ 1 MHz\n",
        "- **Memory**: 4 KB RAM (expandable to 48 KB)\n",
        "- **Storage**: Cassette tape initially; later supported 5.25\" floppy disks via the Disk II drive\n",
        "- **Display**: Color graphics with text and low/high-resolution modes\n",
        "- **Operating System**: Initially used Integer BASIC in ROM; later supported Apple DOS\n",
        "\n",
        "## Innovations\n",
        "\n",
        "- First personal computer with **color graphics**.\n",
        "- Included **expansion slots**, allowing users to add peripherals and customize the system.\n",
        "- Came fully assembled and ready to use, unlike many kit computers of the era.\n",
        "\n",
        "## Impact\n",
        "\n",
        "- Widely adopted in **education**, **business**, and **home computing**.\n",
        "- Helped establish Apple as a major player in the tech industry.\n",
        "- Spawned a long line of successors, including the Apple IIe and Apple IIGS.\n",
        "\n",
        "## Legacy\n",
        "\n",
        "The Apple II is remembered as a pioneering product that made computing accessible to the general public. Its success laid the foundation for Apple's future innovations and the broader personal computing industry.\n",
        "\n",
        "# 🖥️ The Original Apple II (1977)\n",
        "\n",
        "The **Apple II**, released in **June 1977**, was one of the first highly successful mass-produced microcomputers. Designed primarily by **Steve Wozniak**, it played a pivotal role in launching the personal computer revolution.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Key Specifications\n",
        "\n",
        "FeatureDetailsCPUMOS Technology 6502 @ 1 MHz| **RAM**              | 4 KB (expandable up to 48 KB)                |\n",
        "| **Storage**          | Cassette tape (initially), later floppy disk |\n",
        "| **Display**          | Color graphics (up to 280×192 resolution)    |\n",
        "| **Sound**            | Single-bit speaker                           |\n",
        "| **Keyboard**         | Built-in QWERTY keyboard                     |\n",
        "| **Expansion Slots**  | 8 internal slots for peripherals             |\n",
        "| **Operating System** | Integer BASIC (in ROM), later Apple DOS      |\n",
        "\n",
        "---\n",
        "\n",
        "## 🎨 Graphics Capabilities\n",
        "\n",
        "- Supported **color graphics**, a major innovation at the time.\n",
        "- Text mode: 40×24 characters.\n",
        "- Graphics modes: Low-res (40×48) and High-res (280×192).\n",
        "- Could display up to **6 colors** in high-res mode.\n",
        "\n",
        "---\n",
        "\n",
        "## 💾 Storage Evolution\n",
        "\n",
        "- Initially used **cassette tapes** for data storage.\n",
        "- In 1978, Apple introduced the **Disk II** floppy drive, which dramatically improved speed and reliability.\n",
        "- Apple DOS 3.1 was the first disk operating system for the Apple II.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Software Ecosystem\n",
        "\n",
        "- Early programs included **VisiCalc**, the first spreadsheet software, which made the Apple II popular in business.\n",
        "- Supported a wide range of educational, gaming, and productivity software.\n",
        "- Programming languages: Integer BASIC, Applesoft BASIC, Pascal, and assembly.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏛️ Historical Impact\n",
        "\n",
        "- One of the first computers marketed to **individuals and schools**, not just hobbyists.\n",
        "- Helped establish Apple as a major player in the tech industry.\n",
        "- Remained in production (with upgrades) until **1993**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📸 Fun Fact\n",
        "\n",
        "The Apple II was famously used in the movie *WarGames* (1983), showcasing its cultural relevance.\n"
      ],
      "metadata": {
        "id": "yk1AyVYCiipk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "# Embed the video using its ID\n",
        "YouTubeVideo('-1F7vaNP9w0', width=800, height=450)\n"
      ],
      "metadata": {
        "id": "5OO7LH5XkXT2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}