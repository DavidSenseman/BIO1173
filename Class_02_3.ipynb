{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn5tbUUk3tN8C/EvIJE491",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_02_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ],
      "metadata": {
        "id": "QGXRJ8-TG2mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "uvvKzBf7G5eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Module 2: Neural Networks with Tensorflow and Keras**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 2 Material\n",
        "\n",
        "* Part 2.1: Introduction to Neural Networks with Tensorflow and Keras\n",
        "* Part 2.2: Encoding Feature Vectors\n",
        "* **Part 2.3: Controlling Overfitting**\n",
        "* Part 2.4: Saving and Loading a Keras Neural Network"
      ],
      "metadata": {
        "id": "xLU60GttHArg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ],
      "metadata": {
        "id": "4Xti6nC3HLiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXdTjc1EGXos"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_01/class_01_6_image01A.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "4U4gC8qEHRLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Custom Function\n",
        "\n",
        "Run the cell below to create a function that will be needed later in this lesson."
      ],
      "metadata": {
        "id": "n3fJxCKKHYbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "mHV76pNoHf4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any output."
      ],
      "metadata": {
        "id": "EwbytKCPHvQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Sets for this Lesson**\n",
        "\n",
        "For this lesson, the **Obesity Data Set** will be used for the Examples and the **Heart Disease Data Set** will be used for the **Exercises**. Information about the Heart Disease Data set was presented in the previous lesson. Here is the information about the Obesity Data set.\n",
        "\n",
        "### **Obesity Data Set**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/obesity.jpg)\n",
        "\n",
        "[Obesity Data Set](https://archive.ics.uci.edu/ml/datasets/)\n",
        "\n",
        "**Description:**\n",
        "\n",
        "The **Obesity Data Set** includes data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. The data contains 17 attributes and 2111 records, the records are labeled with the class variable NObesity (Obesity Level), that allows classification of the data using the values of `Insufficient Weight`, `Normal Weight`, `Overweight Level I`, `Overweight Level II`, `Obesity Type I`, `Obesity Type II` and `Obesity Type III`.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "* **Gender-** Female/Male\n",
        "* **Age-** Numeric value\n",
        "* **Height-** Numeric value in meters\n",
        "* **Weight-** Numeric value in kilograms\n",
        "* **family_history_with_overweight-** Has a family member suffered or suffers from overweight - Yes/No\n",
        "* **FAVC-** Do you eat high caloric food frequently - Yes/No\n",
        "* **FCVC-** Do you usually eat vegetables in your meals - Never/Sometimes/Always\n",
        "* **NCP-** How many main meals do you have daily - Between 1 y 2/Three/More than three\n",
        "* **CAEC-** Do you eat any food between meals? - No/Sometimes/Frequently/Always\n",
        "* **SMOKE-** Do you smoke? - Yes/No\n",
        "* **CH2O-** How much water do you drink daily? - Less than a liter/Between 1 and 2 L/More than 2 L\n",
        "* **SCC-** Do you monitor the calories you eat daily - Yes/No\n",
        "* **FAF-** How often do you have physical activity? - I do not have/1 or 2 days/2 or 4 days/4 or 5 days\n",
        "* **TUE-** How much time do you use technological devices such as cell phone, videogames, television, computer and others - 0–2 hours/3–5 hours/More than 5 hours\n",
        "* **CALC-** How often do you drink alcohol? - I do not drink/Sometimes/Frequently/Always\n",
        "* **MTRANS-** Which transportation do you usually use? Automobile/Motorbike/Bike/Public Transportation/Walking\n",
        "* **NObeyesdad-** Obesity levels: 'Insufficient_Weight', 'Obesity_Type_III', 'Normal_Weight', 'Obesity_Type_II', 'Overweight_Level_I', 'Obesity_Type_I', 'Overweight_Level_II'\n",
        "\n",
        "For our classification neural network (`ob_model`), the response variable (`Y`) will be the column `NObeyesdad`. This column contains 7 categorical variables shown here sorted from lowest to highest obesity:\n",
        "\n",
        "* Insufficient_Weight\n",
        "* Normal_Weight\n",
        "* Overweight_Level_I\n",
        "* Overweight_Level_II\n",
        "* Obesity_Type_I  \n",
        "* Obesity_Type_II\n",
        "* Obesity_Type_III\n"
      ],
      "metadata": {
        "id": "YFOohVHIFS3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------\n",
        "\n",
        "# **The Problem of `Overfitting`**\n",
        "\n",
        "When a neural network learns a task, it adjusts its parameters so that the error on the training data decreases.\n",
        "\n",
        "However, it can be quite easy for the model to becomes **too good** at reproducing the training sample by **memorizing idiosyncratic noise** instead of capturing the true underlying relationship. This situation is called **overfitting**.\n",
        "\n",
        "#### **How `Overfitting` Happens**\n",
        "\n",
        "* **Too many trainable parameters:**\n",
        "A very deep or wide network can represent a vast space of functions.\n",
        "With few training examples, the model can fit every detail, including random fluctuations.\n",
        "\n",
        "* **Insufficient or unrepresentative data:**\n",
        "If the training set does not cover the variability of the problem domain, the model learns patterns that only exist in the training data.\n",
        "\n",
        "* **Training for too many epochs:**\n",
        "Continuing the optimisation past the point where validation error stops decreasing lets the network fine‑tune to noise.\n",
        "\n",
        "* **Lack of regularisation:**\n",
        "No constraints on weights, activations or hidden‑layer outputs allow the model to swing wildly to minimise the training loss.\n",
        "\n",
        "#### **Why `Overfitting` is Bad**\n",
        "\n",
        "* **Poor generalisation:**\n",
        "The model's predictions on new data (test set, real-world inputs) are much worse than its performance on the training set.\n",
        "\n",
        "* **Misleading confidence:**\n",
        "An overfitted network often reports low loss or high accuracy, giving a false sense of reliability.\n",
        "\n",
        "* **Wasted resources:**\n",
        "Training longer or with more complex architectures is unnecessary when the model will not perform better on unseen data.\n",
        "\n",
        "* **Deployment risks:**\n",
        "In safety-critical applications (self-driving cars, medical diagnosis), an overfitted model can produce dangerous errors."
      ],
      "metadata": {
        "id": "ZS0GhueQiew0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Controlling `Overfitting`**\n",
        "\n",
        "This lesson focuses on how you can control overfitting during the training of neural networks. Here are the 3 common strategies that are typically used to deal with the issues related to overfitting:\n",
        "\n",
        "* **`Early Stopping`**\n",
        "* **`L1/L2 Regularization`**\n",
        "* **`Dropout` Layers**\n",
        "\n",
        "To illustrate the strengths and weaknesses of each strategy, this lesson will use four Examples and four companion **Exercises**.\n",
        "\n",
        "1. `Example 1`/**`Exercise 1`**: A classification neural network will be trained _without_ any measures to prevent overfitting. This will serve as a **baseline** against which to judge the remaining Examples and **Exercises**.\n",
        "2. `Example 2`/**`Exercise 2`**: Exactly the same neural network will be trained using **`EarlyStopping`** to control overfitting.\n",
        "3. `Example 3`/**`Exercise 3`**: Instead of EarlyStopping, the technique of **`L1 Regularization`** will be used to limit overfitting.\n",
        "4. `Example 4`/**`Exercise 4`**: Finally, the technique of adding **`Dropout`** layers will be used to limit overfitting.\n",
        "\n",
        "It should be noted that there is no technical reason to use only a single technique to limit overfitting -- two (or more) strategies are often used in combination depending upon the particular situation."
      ],
      "metadata": {
        "id": "YvVbIsf8fKXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1A: Neural Network Training: No Overfitting Prevention\n",
        "\n",
        "In earlier lessons the training of neural networks was divided into a series of smaller steps to help students master the underlying programming concepts. The code in the cell below, however, provides a **_all_**  the code in a single block needed to:\n",
        "1. read and preparing the tabular data\n",
        "2. build and compile a classification neural network\n",
        "3. train the neural network.\n",
        "\n",
        "With a few minor but notable exceptions, **exactly** this same code block will be used for all 4 Examples as well as for their corresponding **Exercises**.\n",
        "\n",
        "While a detailed explanation of the different code chucks will not be provided, here are a few key points.\n",
        "\n",
        "1. The following code chunk reads the `Obesity Data Set` from the course web server and stores it in a DataFrame called `ob_df`.\n",
        "```text\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "ob_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "```\n",
        "You will of course have to modify this line of code to read the Heart Disease data set for **`Exercise 1`**.\n",
        "\n",
        "2. The **target column** (Y-values) for the classification is the column called\n",
        "\n",
        "```text\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# ------------------------------------------------------------\n",
        "ob_target_col = \"NObeyesdad\"\n",
        "```\n",
        "This column contains 7-levels of obesity encoded as strings:\n",
        "* Insufficient_Weight\n",
        "* Normal_Weight\n",
        "* Overweight_Level_I\n",
        "* Overweight_Level_II\n",
        "* Obesity_Type_I\n",
        "* Obesity_Type_II\n",
        "* Obesity_Type_III\n",
        "\n",
        "These string values will be automatically `one-hot encoded` as part of the data preparation code.\n",
        "```text\n",
        "le = LabelEncoder()\n",
        "ob_df[ob_target_col] = le.fit_transform(ob_df[ob_target_col])\n",
        "```\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "It should be noted that within the code block\n",
        "\n",
        " `7️⃣  Callbacks (monitor accuracy)`\n",
        "\n",
        "the following code is included to implement **`EarlyStopping`**\n",
        "```text\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"val_accuracy\",\n",
        "                  patience=PATIENCE,\n",
        "                  restore_best_weights=True),\n",
        "```\n",
        "However, **`EarlyStopping`** is not implemented in `Example 1` due to the fact that during training, the line that would normally invoke the `EarlyStopping callback` has been \"commented-out: by placing a `#` in front of the line:\n",
        "```text\n",
        "# Train (fit)\n",
        "ob_history = ob_model.fit(\n",
        "    ob_X_train_proc, ob_y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=32,\n",
        "    validation_data=(ob_X_val_proc, ob_y_val),\n",
        "    #callbacks=callbacks,  # Prevent EarlyStopping\n",
        "    verbose=VERBOSE\n",
        ")\n",
        "```\n",
        "So even though the code for `EarlyStopping` is included within the complete code block, it will not be used since this line has been commented out.\n",
        "\n",
        "You should also note that the variable **`VERSBOSE`** has been set to `0` which means there will be not output during training. This has been done to help keep the length of you Colab PDF a more reasonable value."
      ],
      "metadata": {
        "id": "oQ4ZLvruwovE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1A: Neural Network Training: No Overfitting Prevention\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# ------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# ------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "PATIENCE      = 10\n",
        "VERBOSE       = 0     # 0 means no output during training\n",
        "LEARNING_RATE = 0.05\n",
        "optimizer     = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "ob_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# ------------------------------------------------------------\n",
        "ob_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "ob_df[ob_target_col] = le.fit_transform(ob_df[ob_target_col])\n",
        "\n",
        "ob_X = ob_df.drop(columns=[ob_target_col])\n",
        "ob_y = ob_df[ob_target_col].astype('int32')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# ------------------------------------------------------------\n",
        "# First split into train+val and test\n",
        "ob_X_temp, ob_X_test, ob_y_temp, ob_y_test = train_test_split(\n",
        "    ob_X, ob_y, test_size=0.2, random_state=42, stratify=ob_y)\n",
        "\n",
        "# Then split train+val into train and val\n",
        "ob_X_train, ob_X_val, ob_y_train, ob_y_val = train_test_split(\n",
        "    ob_X_temp, ob_y_temp, test_size=0.1, random_state=42, stratify=ob_y_temp)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# ------------------------------------------------------------\n",
        "categorical_cols = [c for c in ob_X.columns if ob_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in ob_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Convert data to correct numeric type\n",
        "\n",
        "# Convert X data\n",
        "ob_X_train_proc = preprocessor.fit_transform(ob_X_train).astype(np.float32)\n",
        "ob_X_val_proc   = preprocessor.transform(ob_X_val).astype(np.float32)\n",
        "ob_X_test_proc  = preprocessor.transform(ob_X_test).astype(np.float32)\n",
        "\n",
        "# Convert Y data\n",
        "ob_y_train = ob_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_val   = ob_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_test  = ob_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6️⃣  Build & compile model\n",
        "# ------------------------------------------------------------\n",
        "n_classes = len(np.unique(ob_y_train))\n",
        "\n",
        "# Build model\n",
        "ob_model = models.Sequential([\n",
        "    layers.Input(shape=(ob_X_train_proc.shape[1],)),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(n_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "# Compile model\n",
        "ob_model.compile(optimizer=optimizer,\n",
        "                 loss=loss_fn,\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7️⃣  Callbacks (monitor accuracy)\n",
        "# ------------------------------------------------------------\n",
        "checkpoint_path = \"ob_best_classification_model.keras\"\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"val_accuracy\",\n",
        "                  patience=PATIENCE,\n",
        "                  restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=checkpoint_path,\n",
        "                    monitor=\"val_accuracy\",\n",
        "                    save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
        "                      factor=0.5,\n",
        "                      patience=int(PATIENCE/2),\n",
        "                      verbose=0)\n",
        "]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8️⃣  Train model\n",
        "# ------------------------------------------------------------\n",
        "print(f\"------Training Starting for {EPOCHS} epochs --------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Train (fit)\n",
        "ob_history = ob_model.fit(\n",
        "    ob_X_train_proc, ob_y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=32,\n",
        "    validation_data=(ob_X_val_proc, ob_y_val),\n",
        "    #callbacks=callbacks,  # Prevent EarlyStopping\n",
        "    verbose=VERBOSE\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished\")\n",
        "print(\"Best val accuracy:\", max(ob_history.history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))\n"
      ],
      "metadata": {
        "id": "-qh13X90wpWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image22C.png)\n",
        "\n",
        "Our `ob_model` neural network appears to done a great job since the best validation accuracy (`val accuracy`) is above 95%. However, let's examine the model's training accuracy more carefully in `Example 1B` below.\n"
      ],
      "metadata": {
        "id": "hkO0__mdwpWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1B: Visualize Effects of No Overfitting Prevention\n",
        "\n",
        "After training we can assess its efficacy by visualizing two training curves —specifically **train loss vs. validation loss** and **train accuracy vs. validation accuracy**.\n",
        "\n",
        "Inspection of these curves can tell us if overfitting has occurred during training. Specifically, if training loss keeps decreasing while validation loss starts increasing, the model is likely **overfitting**—memorizing training data rather than learning general patterns. Similarly, if **training accuracy increases** but **validation accuracy plateaus or drops**, it's another sign of overfitting.\n",
        "\n",
        "This code in the cell below was introduced in a previous lesson. It uses the `matplotlib.pyplot` graphics library to generate the two plots."
      ],
      "metadata": {
        "id": "SVC1D2YoyowA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1B: Visualize effects of no overfitting prevention\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot train loss vs val loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ob_history.history['loss'], label='train loss')\n",
        "plt.plot(ob_history.history['val_loss'], label='val loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot train acc vs val acc\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ob_history.history['accuracy'], label='train acc')\n",
        "plt.plot(ob_history.history['val_accuracy'], label='val acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bX2gVm0zyowB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image23C.png)\n",
        "\n",
        "There can be considerable variability in these plots from one run to the next.\n",
        "\n",
        "### **Analysis**\n",
        "\n",
        "While your plots will probably look rather different, the two plots shown in the example above clearly illustrate that **overfitting** occurred during training. Here's a breakdown of the interpretation:\n",
        "\n",
        "#### **Loss Plot (Top)**\n",
        "* **Training Loss (blue):** Decreases steadily and approaches zero, indicating the model is learning the training data very well.\n",
        "* **Validation Loss (orange):** Starts off decreasing but then fluctuates and remains significantly higher than the training loss, suggesting the model is not generalizing well to unseen data.\n",
        "\n",
        "#### **Accuracy Plot (Bottom)**\n",
        "* **Training Accuracy (blue):** Increases to nearly 100%, showing the model is fitting the training data extremely well.\n",
        "* **Validation Accuracy (orange):** Shows fluctuations and plateaus at a lower level than training accuracy, reinforcing the idea that the model is not performing well on validation data.\n",
        "\n",
        "#### **Conclusion**\n",
        "This behavior is typical of a model that is memorizing the training data rather than learning generalizable patterns."
      ],
      "metadata": {
        "id": "KGCgzxUnzj2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1A: Classification Neural Network: No Overfitting Prevention**\n",
        "\n",
        "In the cell below write the code to to build, compile and train a classification neural network called `hd_model`. As usual, start by copying all of `Example 1` into the cell below.\n",
        "\n",
        "Since you will be using the Heart Disease dataset for all of the **`Exercises`**, use this code chunk to read your dataset from the course file server and create a DataFrame called `hd_df`.\n",
        "```text\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "hd_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/heart_disease.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "```\n",
        "\n",
        "Your objective is to predict the kind of `Resting ECG` a patient will likely display given his/her other clinical measurements. Therefore the data in the column `RestingECG` will be your `Y-values` and the data in the other columns will be your `X-values`. Since your target column has 3 classes: `Normal`, `ST` and `LVH`, you will be training your neural network (`hd_model`) to predict which of type of Resting ECG a particular patient will likely have given his/her other clinical measurements (`X-values`).\n",
        "\n",
        "You will therefore need to specify your target column as follows:\n",
        "```text\n",
        "hd_target_col = \"RestingECG\"\n",
        "```\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Change the prefix `ob_` to `hd_` everywhere in the code copied from Example 1."
      ],
      "metadata": {
        "id": "mPS3lEus1KOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n"
      ],
      "metadata": {
        "id": "RsZF2kK01KOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image25C.png)\n"
      ],
      "metadata": {
        "id": "zrLIG-k-1KOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1B: Visualize Effects of No Overfitting Prevention**\n",
        "\n",
        "In the cell below write the code to visualize your training by creating a  **train loss vs. validation loss** plot and **train accuracy vs. validation accuracy** plot.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Your `history` object should be called `hd_history` instead of `ob_history`."
      ],
      "metadata": {
        "id": "yNwREBwM8ZPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1B here\n",
        "\n"
      ],
      "metadata": {
        "id": "qBtO4qEo8ZPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image26C.png)\n",
        "\n",
        "### **Analysis**\n",
        "\n",
        "An analysis of the two plots shown in the example above, again show clear signs of **overfitting** in your classification neural network. Here's a detailed interpretation:\n",
        "\n",
        "#### **Loss Plot (Top)**\n",
        "* **Training Loss (blue):** Gradually decreases over time, indicating the model is learning the training data well.\n",
        "* **Validation Loss (orange):** Initially decreases but then starts increasing significantly after around **epoch 20**, which is a classic sign of **overfitting**. The model begins to **memorize the training data** and loses generalization ability.\n",
        "\n",
        "#### **Accuracy Plot (Bottom)**\n",
        "* **Training Accuracy (blue):** Rapidly reaches near-perfect accuracy and stays high, showing the model is fitting the training data extremely well.\n",
        "* **Validation Accuracy (orange):** Increases initially but plateaus and remains lower than training accuracy, further confirming that the model is not generalizing well.\n",
        "\n",
        "#### **Overall Interpretation**\n",
        "* The model is overfitting after about **20 epochs**.\n",
        "* This is likely due to the lack of regularization techniques such as:\n",
        "* * EarlyStopping (to halt training when validation loss starts increasing),\n",
        "* * Dropout layers (to reduce reliance on specific neurons),\n",
        "* * L1/L2 Regularization (to penalize overly complex models)."
      ],
      "metadata": {
        "id": "NbsfMz779PFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Early Stopping in Keras to Prevent Overfitting**\n",
        "\n",
        "Now that we have established baselines in `Exmaple 1` and in **`Exercise 1`** that **overfitting** occurs, we will now examine techniques that can be used to control overfitting. In `Example 2` will look at **`EarlyStopping`**.\n",
        "\n",
        "It can be difficult to determine how many epochs to cycle through to optimally train a neural network. **_Overfitting_** will occur if you train the neural network for too many epochs, and the neural network will not perform well on **new data**, despite attaining a good accuracy on the training set.\n",
        "\n",
        "Overfitting occurs when a neural network is trained to the point that it begins to **memorize rather than generalize**, as demonstrated in this figure.\n",
        "\n",
        "\n",
        "![Training vs. Validation Error for Overfitting](https://biologicslab.co/BIO1173/images/class_02/class_3_training_val.png \"Training vs. Validation Error for Overfitting\")\n",
        "**Training vs. Validation Error for Overfitting**\n",
        "\n",
        "It is important to segment the original dataset into several datasets:\n",
        "\n",
        "* **Training Set**\n",
        "* **Validation Set**\n",
        "* **Holdout Set**\n",
        "\n",
        "You can construct these sets in several different ways. The following programs demonstrate some of these.\n",
        "\n",
        "The first method is a **`training`** and **`validation set`**. We use the `training data` to train the neural network until the `validation set` no longer improves. This attempts to stop at a near-optimal training point.\n",
        "\n",
        "This method will only give accurate \"out of sample\" predictions for the `validation set`; this is usually 20% of the data. The predictions for the training data will be overly optimistic, as these were the data that we used to train the neural network.\n",
        "\n",
        "This figure demonstrates how we divide the dataset.\n",
        "\n",
        "![Training with a Validation Set](https://biologicslab.co/BIO1173/images/class_02/class_1_train_val.png \"Training with a Validation Set\")\n",
        "\n",
        "**Training with a Validation Set**"
      ],
      "metadata": {
        "id": "mzaZAPuxH1Hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2A: Neural Network Training: Overfitting Prevention with `EarlyStopping`\n",
        "\n",
        "As mentioned above, `Example 1` contained all of the code to implement **`EarlyStopping`** in section 7:\n",
        "```text\n",
        "# ------------------------------------------------------------\n",
        "# 7️⃣  Callbacks (monitor accuracy)\n",
        "# ------------------------------------------------------------\n",
        "checkpoint_path = \"ob_best_classification_model.keras\"\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"val_accuracy\",\n",
        "                  patience=PATIENCE,\n",
        "                  restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=checkpoint_path,\n",
        "                    monitor=\"val_accuracy\",\n",
        "                    save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
        "                      factor=0.5,\n",
        "                      patience=int(PATIENCE/2),\n",
        "                      verbose=0)\n",
        "]\n",
        "```\n",
        "As you can see, the `EarlyStopping` is \"monitoring\" the values of **validation accuracy (val_accuracy)**. Since the variable **PATIENCE** is set to `10` the EarlyStopping monitor will wait for `10` Epochs for the `val_accuracy` to improve after reaching a maximum before terminating the training run.\n",
        "\n",
        "Here is the code chunk that controls the training\n",
        "```text\n",
        "ob_history = ob_model.fit(\n",
        "    ob_X_train_proc, ob_y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=32,\n",
        "    validation_data=(ob_X_val_proc, ob_y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=VERBOSE\n",
        ")\n",
        "```\n",
        "Notice that that the next to the last line:\n",
        "```text\n",
        "callbacks=callbacks,\n",
        "```\n",
        "allows the `EarlyStopping monitor` to exert control over the training cycle. In `Example 2` the `#` has been removed to allow `EarlyStopping` to be implemented. The only other change in the code from `Example 1` has been to set the `VERBOSE` variable to `2` so we can watch the training output.\n",
        "```text\n",
        "VERBOSE       = 2\n",
        "```"
      ],
      "metadata": {
        "id": "d2Kns2kcIFgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2A: Neural Network Training: Overfitting Prevention with Early Stopping\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# ------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# ------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "PATIENCE      = 10\n",
        "VERBOSE       = 2\n",
        "LEARNING_RATE = 0.05\n",
        "optimizer     = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "ob_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# ------------------------------------------------------------\n",
        "ob_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "ob_df[ob_target_col] = le.fit_transform(ob_df[ob_target_col])\n",
        "\n",
        "ob_X = ob_df.drop(columns=[ob_target_col])\n",
        "ob_y = ob_df[ob_target_col].astype('int32')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# ------------------------------------------------------------\n",
        "# First split into train+val and test\n",
        "ob_X_temp, ob_X_test, ob_y_temp, ob_y_test = train_test_split(\n",
        "    ob_X, ob_y, test_size=0.2, random_state=42, stratify=ob_y)\n",
        "\n",
        "# Then split train+val into train and val\n",
        "ob_X_train, ob_X_val, ob_y_train, ob_y_val = train_test_split(\n",
        "    ob_X_temp, ob_y_temp, test_size=0.1, random_state=42, stratify=ob_y_temp)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# ------------------------------------------------------------\n",
        "categorical_cols = [c for c in ob_X.columns if ob_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in ob_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Convert data to correct numeric type\n",
        "\n",
        "# Convert X data\n",
        "ob_X_train_proc = preprocessor.fit_transform(ob_X_train).astype(np.float32)\n",
        "ob_X_val_proc   = preprocessor.transform(ob_X_val).astype(np.float32)\n",
        "ob_X_test_proc  = preprocessor.transform(ob_X_test).astype(np.float32)\n",
        "\n",
        "# Convert Y data\n",
        "ob_y_train = ob_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_val   = ob_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_test  = ob_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6️⃣  Build & compile model\n",
        "# ------------------------------------------------------------\n",
        "n_classes = len(np.unique(ob_y_train))\n",
        "\n",
        "# Build model\n",
        "ob_model = models.Sequential([\n",
        "    layers.Input(shape=(ob_X_train_proc.shape[1],)),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(n_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "# Compile model\n",
        "ob_model.compile(optimizer=optimizer,\n",
        "                 loss=loss_fn,\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7️⃣  Callbacks (monitor accuracy)\n",
        "# ------------------------------------------------------------\n",
        "checkpoint_path = \"ob_best_classification_model.keras\"\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"val_accuracy\",\n",
        "                  patience=PATIENCE,\n",
        "                  restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=checkpoint_path,\n",
        "                    monitor=\"val_accuracy\",\n",
        "                    save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
        "                      factor=0.5,\n",
        "                      patience=int(PATIENCE/2),\n",
        "                      verbose=0)\n",
        "]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8️⃣  Train model\n",
        "# ------------------------------------------------------------\n",
        "print(f\"-----Training Starting for up to {EPOCHS} epochs --------------------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Train (fit)\n",
        "ob_history = ob_model.fit(\n",
        "    ob_X_train_proc, ob_y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=32,\n",
        "    validation_data=(ob_X_val_proc, ob_y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=VERBOSE\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished\")\n",
        "print(\"Best val accuracy:\", max(ob_history.history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))\n"
      ],
      "metadata": {
        "id": "hq1ahgmGaVwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to last part of the output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image27C.png)\n",
        "\n",
        "While the number of Epochs was set to `100`, training stopped in this particular training run after the `25th` Epoch. This was due to the `EarlyStopping` monitor terminating training after waiting `10` epochs for the `val_accuracy` to start improving."
      ],
      "metadata": {
        "id": "XbpzAmsBJXBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------\n",
        "\n",
        "### **Arguments that Control the `EarlyStopping` Monitor**\n",
        "\n",
        "There are a number of parameters (arguments) that are specified to the **EarlyStopping** monitor.\n",
        "\n",
        "* **`min_delta`** This value should be kept small. It simply means the minimum change in error to be registered as an improvement.  Setting it even smaller will not likely have a great deal of impact.\n",
        "* **`patience`** How long should the training wait for the validation error to improve?  \n",
        "* **`verbose`** How much progress information do you want?\n",
        "* **`mode`** In general, always set this to \"auto\".  This allows you to specify if the error should be minimized or maximized.  Consider accuracy, where higher numbers are desired vs log-loss/RMSE where lower numbers are desired.\n",
        "* **`restore_best_weights`** This should always be set to `True`.  This restores the weights to the values they were at when the validation set is the highest.  Unless you are manually tracking the weights yourself (we do not use this technique in this course), you should have Keras perform this step for you.\n",
        "\n",
        "-----------------------------"
      ],
      "metadata": {
        "id": "KCPk7caRKsOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2B: Visualize Effects of EarlyStopping\n",
        "\n",
        "The code in the cell below is **exactly** the same as `Example 1B`."
      ],
      "metadata": {
        "id": "QdLBStCj_yR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2B: Visualize effects of EarlyStopping\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot train loss vs val loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ob_history.history['loss'], label='train loss')\n",
        "plt.plot(ob_history.history['val_loss'], label='val loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot train acc vs val acc\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ob_history.history['accuracy'], label='train acc')\n",
        "plt.plot(ob_history.history['val_accuracy'], label='val acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pVyKSXdX_yR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image28C.png)\n",
        "\n",
        "### **Analysis**\n",
        "\n",
        "The two plots shown in the example above, reveal a well-managed training process with **EarlyStopping** effectively preventing **overfitting**. Here's a detailed interpretation:\n",
        "\n",
        "#### **Loss Plot (Top)**\n",
        "* **Training Loss (blue):** Decreases steadily and stabilizes, indicating consistent learning.\n",
        "* **Validation Loss (orange):** Fluctuates slightly but also trends downward, suggesting the model is generalizing reasonably well.\n",
        "\n",
        "#### **Accuracy Plot (Bottom)**\n",
        "* **Training Accuracy (blue):** Rises quickly and stabilizes near 1.0, showing strong performance on training data.\n",
        "* **Validation Accuracy (orange):** Also rises quickly and stabilizes close to training accuracy, indicating good generalization.\n",
        "\n",
        "#### **EarlyStopping Behavior**\n",
        "* **Patience** = 10, monitoring val_accuracy.\n",
        "* Training stopped at **epoch 25**, meaning validation accuracy did not improve for 10 consecutive epochs.\n",
        "* This early stop likely prevented the model from entering an overfitting regime, which is supported by the close alignment between training and validation metrics.\n",
        "\n",
        "#### **Conclusion**\n",
        "Your model appears to have reached its optimal performance around epoch 15-20, and **EarlyStopping** correctly halted training before overfitting could occur. This is a textbook example of how EarlyStopping can be used to maintain generalization while avoiding unnecessary training."
      ],
      "metadata": {
        "id": "R7rzD3dW_yR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2A: Neural Network Training: Overfitting Prevention with `EarlyStopping`**\n",
        "\n",
        "In the cell below write the code to to implement EarlyStopping in a classification model called `hd_model`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Copy-and-paste your code from **`Exercise 1`** in the cell below.\n",
        "2. Remove the **`#`** before the line\n",
        "```text\n",
        "callbacks=callbacks,\n",
        "```\n",
        "in Section 8️⃣  `Train model`\n",
        "\n",
        "3. Change the variable `VERBOSE` to equal `2` instead of `0`."
      ],
      "metadata": {
        "id": "mwyLo-hvs1JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2A here\n",
        "\n"
      ],
      "metadata": {
        "id": "TejZ0RE5bVl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the last part of the output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image31C.png)\n",
        "\n",
        "While the number of Epochs was set to `100` training terminated in this particular training run after the `17th` Epoch due to the `EarlyStopping` monitor."
      ],
      "metadata": {
        "id": "R5CRi6QoU3e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2B: Visualize Effects of `EarlyStopping`**\n",
        "\n",
        "`Copy-and-paste` your code from **Exercise 1B`** into the cell below and run it."
      ],
      "metadata": {
        "id": "06ILcia_CBcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2B here\n",
        "\n"
      ],
      "metadata": {
        "id": "zNhxB6MtCBcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image30C.png)\n",
        "\n",
        "### **Analysis**\n",
        "\n",
        "The two plots shown in the example above, reveal a well-managed training process with **EarlyStopping** effectively preventing **overfitting**. Here's a detailed interpretation:\n",
        "\n",
        "#### **Loss Plot (Top)**\n",
        "* **Training Loss (blue):** Decreases gradually, indicating the model is learning from the training data. Starts off decreasing but then fluctuates and even slightly increases toward the end, suggesting the model may be starting to overfit.\n",
        "* **Validation Loss (orange):** Also decreases initially but begins to fluctuate and slightly rise toward the end, suggesting the onset of overfitting.\n",
        "\n",
        "#### **Accuracy Plot (Bottom)**\n",
        "* **Training Accuracy (blue):**  Rises quickly and stabilizes near 1.0, showing strong performance on the training set.\n",
        "* **Validation Accuracy (orange):** Increases initially and then plateaus, with no significant improvement after around epoch 7-8\n",
        "\n",
        "#### **`EarlyStopping` Behavior**\n",
        "* Since **val_accuracy** was monitored and **patience = 10**, the model stopped at **epoch 17** because validation accuracy did not improve for 10 consecutive epochs.\n",
        "* This suggests the model reached its peak generalization performance early, and further training would not have improved validation results.\n",
        "\n",
        "#### **Conclusion**\n",
        "* The model was starting to overfit, and EarlyStopping correctly halted training before validation errors would have started to increase.\n"
      ],
      "metadata": {
        "id": "okpZ9uQHCBcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **L1 and L2 Regularization to Decrease Overfitting**\n",
        "\n",
        "`L1` and `L2 regularization` are two common regularization techniques that can reduce the effects of overfitting [[Cite:ng2004feature]](http://cseweb.ucsd.edu/~elkan/254spring05/Hammon.pdf). These algorithms can either work with an objective function or as a part of the backpropagation algorithm. In both cases, the regularization algorithm is attached to the training algorithm by adding an objective.\n",
        "\n",
        "## **L1 Regularization**\n",
        "In the context of a neural network with `L1 regularization`, the objective function typically consists of two main components: the `original loss function` and the `L1 regularization term`. The objective function serves as a measure that the optimization algorithm aims to minimize during the training process.\n",
        "\n",
        "The objective function with `L1 regularization` can be represented as:\n",
        "```text\n",
        "Objective function = Loss function + λ * L1 regularization term\n",
        "```\n",
        "where:\n",
        "\n",
        "* **Loss function:** The original loss function used to evaluate the performance of the neural network on the training data, such as the cross-entropy loss or mean squared error.\n",
        "* **λ (lambda):** The regularization parameter that controls the strength of the L1 regularization penalty.\n",
        "* **L1 regularization term:** The sum of absolute values of the weights in the neural network.\n",
        "\n",
        "The addition of the `L1 regularization` term to the objective function encourages sparsity in the weights of the neural network by penalizing large weights. This helps prevent **overfitting** and can lead to a simpler and more interpretable model. The trade-off between minimizing the loss function and reducing the magnitude of the weights is controlled by the regularization parameter λ.\n",
        "\n",
        "During the training process, the neural network adjusts its weights by minimizing the composite objective function, striking a balance between fitting the training data well (minimizing the loss function) and reducing model complexity (`L1 regularization`).\n",
        "\n",
        "These algorithms work by adding a **weight penalty** to the neural network training. This **penalty** encourages the neural network to keep the weights to small values. Both `L1` and `L2` calculate this penalty differently. You can add this penalty calculation to the calculated gradients for gradient-descent-based algorithms, such as backpropagation. The penalty is negatively combined with the objective score for objective-function-based training, such as simulated annealing.\n",
        "\n",
        "\n",
        "## **`L1` vs`L2` Regularization**\n",
        "Both `L1` and `L2` work similarily in that they penalize the size of the weight, but in significantly different ways. `L2` will force the weights into a pattern similar to a `Gaussian distribution` while the `L1` will force the weights into a pattern similar to a `Laplace distribution`, as demonstrated in the following figure.\n",
        "\n",
        "![L1 vs L2](https://biologicslab.co/BIO1173/images/class_9_l1_l2.png \"L1 vs L2\")\n",
        "\n",
        "As you can see, the `L1 algorithm` is more tolerant of weights further from `0`, whereas the `L2 algorithm` is less tolerant."
      ],
      "metadata": {
        "id": "FTuajIg4cZbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3A: Neural Network Training: Overfitting Prevention with `L1 Regularization`\n",
        "\n",
        "The code in the cell below is an **exact** copy of the code in `Example 1` except for the following modifications:\n",
        "\n",
        "1. The code for building the neural network:\n",
        "```text\n",
        "# Build model\n",
        "ob_model = models.Sequential([\n",
        "    layers.Input(shape=(ob_X_train_proc.shape[1],)),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(n_classes, activation=\"softmax\")\n",
        "])\n",
        "```\n",
        "has been modifed to include `L1 regularization` as follows:\n",
        "```text\n",
        "ob_model = models.Sequential([\n",
        "    layers.Input(shape=(ob_X_train_proc.shape[1],)),\n",
        "    layers.Dense(32, activation=\"relu\",\n",
        "                 kernel_regularizer=regularizers.l1(l1_reg)),\n",
        "    layers.Dense(n_classes, activation=\"softmax\")  # no regularizer on output\n",
        "])\n",
        "\n",
        "```\n",
        "Where the strength of the `L1` was set by this line of code added to the end of Section 1️⃣  `Parameters`\n",
        "```text\n",
        "# L1 regularization strength\n",
        "l1_reg = 1e-4          # <-- you can change this\n",
        "```\n"
      ],
      "metadata": {
        "id": "S1YlQAbQqaYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3A: L1 Regularization of a Classification Neural Network\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# -------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# -------------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "PATIENCE      = 10\n",
        "VERBOSE       = 0\n",
        "LEARNING_RATE = 0.05\n",
        "optimizer     = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "# L1 regularization strength\n",
        "l1_reg = 1e-4          # <-- you can change this\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# -------------------------------------------------------------------\n",
        "ob_df = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "    na_values=['NA','?']\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# -------------------------------------------------------------------\n",
        "ob_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "ob_df[ob_target_col] = le.fit_transform(ob_df[ob_target_col])\n",
        "\n",
        "ob_X = ob_df.drop(columns=[ob_target_col])\n",
        "ob_y = ob_df[ob_target_col].astype('int32')\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# -------------------------------------------------------------------\n",
        "ob_X_temp, ob_X_test, ob_y_temp, ob_y_test = train_test_split(\n",
        "    ob_X, ob_y, test_size=0.2, random_state=42, stratify=ob_y\n",
        ")\n",
        "\n",
        "ob_X_train, ob_X_val, ob_y_train, ob_y_val = train_test_split(\n",
        "    ob_X_temp, ob_y_temp, test_size=0.1, random_state=42, stratify=ob_y_temp\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# -------------------------------------------------------------------\n",
        "categorical_cols = [c for c in ob_X.columns if ob_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in ob_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Fit & transform training data, then transform val & test\n",
        "ob_X_train_proc = preprocessor.fit_transform(ob_X_train).astype(np.float32)\n",
        "ob_X_val_proc   = preprocessor.transform(ob_X_val).astype(np.float32)\n",
        "ob_X_test_proc  = preprocessor.transform(ob_X_test).astype(np.float32)\n",
        "\n",
        "# Convert Y to numpy arrays (already int32)\n",
        "ob_y_train = ob_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_val   = ob_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_test  = ob_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6️⃣  Build & compile model (with L1 regularization)\n",
        "# -------------------------------------------------------------------\n",
        "n_classes = len(np.unique(ob_y_train))\n",
        "\n",
        "ob_model = models.Sequential([\n",
        "    layers.Input(shape=(ob_X_train_proc.shape[1],)),\n",
        "    layers.Dense(32, activation=\"relu\",\n",
        "                 kernel_regularizer=regularizers.l1(l1_reg)),\n",
        "    layers.Dense(n_classes, activation=\"softmax\")  # no regularizer on output\n",
        "])\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "ob_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_fn,\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 7️⃣  Callbacks (monitor accuracy)\n",
        "# -------------------------------------------------------------------\n",
        "checkpoint_path = \"ob_best_classification_model.keras\"\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"val_accuracy\",\n",
        "                  patience=PATIENCE,\n",
        "                  restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=checkpoint_path,\n",
        "                    monitor=\"val_accuracy\",\n",
        "                    save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
        "                      factor=0.5,\n",
        "                      patience=int(PATIENCE/2),\n",
        "                      verbose=0)\n",
        "]\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 8️⃣  Train model\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"------Training Starting for {EPOCHS} epochs --------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "ob_history = ob_model.fit(\n",
        "    ob_X_train_proc, ob_y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=32,\n",
        "    validation_data=(ob_X_val_proc, ob_y_val),\n",
        "    # callbacks=callbacks,  # Turn off EarlyStopping\n",
        "    verbose=VERBOSE\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished\")\n",
        "print(\"Best val accuracy:\", max(hd_history.history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))\n"
      ],
      "metadata": {
        "id": "9VY5NsSzqpAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the last part of the output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image33C.png)"
      ],
      "metadata": {
        "id": "Sn99dbOer2U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3B: Visualize Effects of L1 Regularization\n",
        "\n",
        "The code in the cell below is **exactly** the same as that used in `Example 1B`."
      ],
      "metadata": {
        "id": "XKfbOra3J4QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3B: Visualize effects of L1 regularization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot train loss vs val loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ob_history.history['loss'], label='train loss')\n",
        "plt.plot(ob_history.history['val_loss'], label='val loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot train acc vs val acc\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ob_history.history['accuracy'], label='train acc')\n",
        "plt.plot(ob_history.history['val_accuracy'], label='val acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rlMkqWqJr23v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image34C.png)\n",
        "\n",
        "### **Analysis**\n",
        "\n",
        "The two example plots shown above reflect the training dynamics of a neural network that used **L1 regularization** as its sole method for overfitting prevention. Here's a detailed interpretation:\n",
        "\n",
        "#### **Loss Plot (Top)**\n",
        "* **Training Loss (blue):** Decreases steadily, showing that the model is learning from the training data.\n",
        "* **Validation Loss (orange):** Initially decreases but then begins to fluctuate and slightly rise, indicating the model may be starting to overfit or that validation performance is unstable.\n",
        "\n",
        "#### **Accuracy Plot (Bottom)**\n",
        "* **Training Accuracy (blue):** Rises quickly and stabilizes near 1.0, suggesting strong performance on the training set.\n",
        "* **Validation Accuracy (orange):** Increases initially but plateaus and shows minimal improvement after around epoch 7-8.\n",
        "\n",
        "#### **Impact of L1 Regularization**\n",
        "* **L1 regularization** encourages sparsity in the model weights, which can help reduce overfitting by simplifying the model.\n",
        "* However, the relatively small regularization strength (l1_reg = 1e-4) may not have been strong enough to fully prevent overfitting, as indicated by the divergence between training and validation metrics toward the end.\n",
        "\n",
        "#### **Conclusion**\n",
        "* The model learned effectively but began to show signs of overfitting or stagnation in validation performance.\n"
      ],
      "metadata": {
        "id": "8vDGQnP-L5yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3A: Neural Network Training: Overfitting Prevention with `L1 Regularization`**\n",
        "\n",
        "In the cell below write the code to add `L1 Regularization` to your `hd_model`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. In the cell below `copy-and-paste` your **`Exercise 1A`** into the cell below. (Do **not** use **Exercise 2A`**).\n",
        "\n",
        "2. Add the following line after the end of Section  1️⃣  `Parameters`\n",
        "```text\n",
        "# L1 regularization strength\n",
        "l1_reg = 1e-4          # <-- you can change this\n",
        "```\n",
        "3. Add `L1 regularization` by changing the building code from\n",
        "```text\n",
        "# Build model\n",
        "hd_model = models.Sequential([\n",
        "    layers.Input(shape=(hd_X_train_proc.shape[1],)),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(n_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "```\n",
        ">to the following:\n",
        "```text\n",
        "# Buid model\n",
        "hd_model = models.Sequential([\n",
        "    layers.Input(shape=(hd_X_train_proc.shape[1],)),\n",
        "    layers.Dense(32, activation=\"relu\",\n",
        "                 kernel_regularizer=regularizers.l1(l1_reg)),\n",
        "    layers.Dense(n_classes, activation=\"softmax\")\n",
        "])\n",
        "```"
      ],
      "metadata": {
        "id": "sVY8KiI-SXl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3A here\n",
        "\n"
      ],
      "metadata": {
        "id": "35iiG_fKSXmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the last part of the output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image35C.png)"
      ],
      "metadata": {
        "id": "kDiVzAPmSXmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3B: Visualize Effects of L1 Regularization**\n",
        "\n",
        "`Copy-and-paste` the code from **`Exercise 1B`** into the cell below."
      ],
      "metadata": {
        "id": "pWqR8MLORylJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3B here\n",
        "\n"
      ],
      "metadata": {
        "id": "iJZi9YefRylJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to this plot.\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image36C.png)\n",
        "\n",
        "### **Analysis**\n",
        "\n",
        "The two example plots shown above, that came from a training run with **L1 regularization**, show clear signs of **overfitting** and **limited generalization**. Here's a detailed analysis:\n",
        "\n",
        "#### **Loss Plot (Top)**\n",
        "* **Training Loss (blue):** Decreases gradually, indicating the model is learning the training data.\n",
        "* **Validation Loss (orange):** Increases significantly over time, which is a strong indicator of overfitting. The model is fitting the training data well but failing to generalize to unseen data.\n",
        "\n",
        "#### **Accuracy Plot (Bottom)**\n",
        "* **Training Accuracy (blue):** Rises steadily and reaches around 0.80, showing decent performance on the training set.\n",
        "* **Validation Accuracy (orange):** Fluctuates between 0.55 and 0.65, with no clear upward trend. This suggests the model is not improving on validation data and may be memorizing training patterns.\n",
        "\n",
        "\n",
        "#### **Interpretation**\n",
        "**L1 regularization alone** (especially with a modest strength like 1e-4) is **not sufficient** to prevent overfitting in this case.\n",
        "* The absence of **EarlyStopping** allowed training to continue even as validation performance worsened.\n",
        "* The lack of **Dropout layers** means the model may be relying too heavily on specific neurons, further contributing to overfitting."
      ],
      "metadata": {
        "id": "AbdhGv4SRylK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Drop Out for Keras to Decrease Overfitting**\n",
        "\n",
        "Hinton, Srivastava, Krizhevsky, Sutskever, & Salakhutdinov (2012) introduced the **_dropout regularization_** algorithm. [[Cite:srivastava2014dropout]](http://www.jmlr.org/papers/volume15/nandan14a/nandan14a.pdf)\n",
        "\n",
        "Although `dropout` works differently than `L1` and `L2`, it accomplishes the same goal—the prevention of **overfitting**. However, the algorithm does the task by actually **_removing_**  neurons and connections—at least temporarily. Unlike `L1` and `L2`, no weight penalty is added. `Dropout` does not directly seek to train small weights.\n",
        "\n",
        "`Dropout` works by causing hidden neurons of the neural network to be unavailable during part of the training. Dropping part of the neural network causes the remaining portion to be trained to still achieve a good score even without the dropped neurons. This technique decreases co-adaptation between neurons, which results in less overfitting.\n",
        "\n",
        "Most neural network frameworks implement `dropout` as a separate layer. `Dropout layers` function like a regular, densely connected neural network layer. The only difference is that the `dropout layers` will periodically drop some of their neurons during training. You can use dropout layers on regular feedforward neural networks.\n",
        "\n",
        "A _program_ can implement a `dropout layer` as a dense layer that can eliminate some of its neurons. Contrary to popular belief about the `dropout layer`, such a program does not permanently remove these discarded neurons. In other words, a dropout layer does **_not_** lose any of its neurons during the training process, and it will still have the same number of neurons after training. In this way, the program only _temporarily masks_ the neurons rather than dropping them.\n",
        "\n",
        "This figure shows how a dropout layer might be situated with other layers.\n",
        "\n",
        "\n",
        "![Dropout Regularization](https://biologicslab.co/BIO1173/images/class_02/class_9_dropout.png \"Dropout Regularization\")\n",
        "\n",
        "**Figure 5.DROPOUT: Dropout Regularization**\n",
        "\n",
        "The discarded neurons and their connections are shown as dashed lines. The input layer has two input neurons as well as a bias neuron. The second layer is a dense layer with three neurons and a bias neuron. The third layer is a dropout layer with six regular neurons even though the program has dropped 50% of them.\n",
        "\n",
        "While the program drops these neurons, it neither calculates nor trains them. However, the final neural network will use _all_ of these neurons for the output. As previously mentioned, the program only temporarily discards the neurons.\n",
        "\n",
        "The program chooses different sets of neurons from the dropout layer during subsequent training iterations. Although we chose a probability of 50% for dropout, the computer will not necessarily drop three neurons. It is as if we flipped a coin for each of the dropout candidate neurons to choose if that neuron was dropped out. You must know that the program should never drop the bias neuron. Only the regular neurons on a dropout layer are candidates.\n",
        "\n",
        "The implementation of the training algorithm influences the process of discarding neurons. The dropout set frequently changes once per training iteration or batch. The program can also provide intervals where all neurons are present. Some neural network frameworks give additional hyper-parameters to allow you to specify exactly the rate of this interval."
      ],
      "metadata": {
        "id": "IZYlUaXA3xEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------\n",
        "\n",
        "## **Why does Dropout work?**\n",
        "\n",
        "Why dropout is capable of decreasing overfitting is a common question. The answer is that dropout can reduce the chance of **codependency** developing between two neurons. Two neurons that develop codependency will not be able to operate effectively when one is dropped out. As a result, the neural network can no longer rely on the presence of every neuron, and it trains accordingly. This characteristic decreases its ability to memorize the information presented, thereby forcing generalization.\n",
        "\n",
        "Dropout also decreases overfitting by **forcing a bootstrapping process** upon the neural network. Bootstrapping is a prevalent ensemble technique. Ensembling is a technique of machine learning that combines multiple models to produce a better result than those achieved by individual models. The ensemble is a term that originates from the musical ensembles in which the final music product that the audience hears is the combination of many instruments.  \n",
        "\n",
        "**Bootstrapping** is one of the most simple ensemble techniques. The bootstrapping programmer simply trains several neural networks to perform precisely the same task. However, each neural network will perform differently because of some training techniques and the random numbers used in the neural network weight initialization. The difference in weights causes the performance variance. The output from this ensemble of neural networks becomes the average output of the members taken together. This process decreases overfitting through the consensus of differently trained neural networks.  \n",
        "\n",
        "Dropout works somewhat like bootstrapping. You might think of each neural network that results from a different set of neurons being dropped out as an individual member in an ensemble. As training progresses, the program creates more neural networks in this way. However, dropout does not require the same amount of processing as bootstrapping. The new neural networks created are temporary; they exist only for a training iteration. The final result is also a single neural network rather than an ensemble of neural networks to be averaged together.\n",
        "\n",
        "This short `YouTube` video shows how dropout works: [Dropout tutorial](https://youtu.be/NhZVe50QwPM?si=Zr-6qrPdE9YXTj3Q)\n",
        "\n",
        "------------------------"
      ],
      "metadata": {
        "id": "9zVa6tCv33AG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4A: Neural Network Training: Overfitting Prevention with `Dropout`\n",
        "\n",
        "The code in the cell below is an **exact** copy of the code in `Example 1` except for the following modification:\n",
        "\n",
        "1. The code for building the neural network:\n",
        "```text\n",
        "# Build model\n",
        "ob_model = models.Sequential([\n",
        "    layers.Input(shape=(ob_X_train_proc.shape[1],)),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(n_classes, activation=\"softmax\")\n",
        "])\n",
        "```\n",
        "has been modifed to include `Dropout` layers as follows:\n",
        "```text\n",
        "# Build model\n",
        "ob_model = models.Sequential([\n",
        "    layers.Input(shape=(ob_X_train_proc.shape[1],)),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),  # Dropout after first dense layer\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),  # Dropout after second dense layer\n",
        "    layers.Dense(n_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "DMG8y4CE38W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4A: Neural Network Training: Overfitting Prevention with `Dropout`\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# ------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# ------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "PATIENCE      = 10\n",
        "VERBOSE       = 0\n",
        "LEARNING_RATE = 0.05\n",
        "optimizer     = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "ob_df = pd.read_csv(\"https://biologicslab.co/BIO1173/data/ObesityDataSet.csv\",\n",
        "                    na_values=['NA','?'])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣  Target (Y-values)\n",
        "# ------------------------------------------------------------\n",
        "ob_target_col = \"NObeyesdad\"\n",
        "le = LabelEncoder()\n",
        "ob_df[ob_target_col] = le.fit_transform(ob_df[ob_target_col])\n",
        "\n",
        "ob_X = ob_df.drop(columns=[ob_target_col])\n",
        "ob_y = ob_df[ob_target_col].astype('int32')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Train–Validation–Test Split\n",
        "# ------------------------------------------------------------\n",
        "# First split into train+val and test\n",
        "ob_X_temp, ob_X_test, ob_y_temp, ob_y_test = train_test_split(\n",
        "    ob_X, ob_y, test_size=0.2, random_state=42, stratify=ob_y)\n",
        "\n",
        "# Then split train+val into train and val\n",
        "ob_X_train, ob_X_val, ob_y_train, ob_y_val = train_test_split(\n",
        "    ob_X_temp, ob_y_temp, test_size=0.1, random_state=42, stratify=ob_y_temp)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣  Preprocessing\n",
        "# ------------------------------------------------------------\n",
        "categorical_cols = [c for c in ob_X.columns if ob_X[c].dtype == \"object\"]\n",
        "numeric_cols     = [c for c in ob_X.columns if c not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\",  StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    (\"cat\", Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(drop=\"if_binary\", sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Convert data to correct numeric type\n",
        "\n",
        "# Convert X data\n",
        "ob_X_train_proc = preprocessor.fit_transform(ob_X_train).astype(np.float32)\n",
        "ob_X_val_proc   = preprocessor.transform(ob_X_val).astype(np.float32)\n",
        "ob_X_test_proc  = preprocessor.transform(ob_X_test).astype(np.float32)\n",
        "\n",
        "# Convert Y data\n",
        "ob_y_train = ob_y_train.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_val   = ob_y_val.to_numpy().astype(np.int32).reshape(-1)\n",
        "ob_y_test  = ob_y_test.to_numpy().astype(np.int32).reshape(-1)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6️⃣  Build & compile model (with Dropout)\n",
        "# ------------------------------------------------------------\n",
        "n_classes = len(np.unique(ob_y_train))\n",
        "\n",
        "# Build model\n",
        "ob_model = models.Sequential([\n",
        "    layers.Input(shape=(ob_X_train_proc.shape[1],)),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),  # Dropout after first dense layer\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),  # Dropout after second dense layer\n",
        "    layers.Dense(n_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "# Compile model\n",
        "ob_model.compile(optimizer=optimizer,\n",
        "                 loss=loss_fn,\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7️⃣  Callbacks (monitor accuracy)\n",
        "# ------------------------------------------------------------\n",
        "checkpoint_path = \"ob_best_classification_model.keras\"\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"val_accuracy\",\n",
        "                  patience=PATIENCE,\n",
        "                  restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=checkpoint_path,\n",
        "                    monitor=\"val_accuracy\",\n",
        "                    save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
        "                      factor=0.5,\n",
        "                      patience=int(PATIENCE/2),\n",
        "                      verbose=0)\n",
        "]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8️⃣  Train model\n",
        "# ------------------------------------------------------------\n",
        "print(f\"------Training Starting for {EPOCHS} epochs --------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Train (fit)\n",
        "ob_history = ob_model.fit(\n",
        "    ob_X_train_proc, ob_y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=32,\n",
        "    validation_data=(ob_X_val_proc, ob_y_val),\n",
        "    # callbacks=callbacks,    # Turn off EarlyStopping\n",
        "    verbose=VERBOSE\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished\")\n",
        "print(\"Best val accuracy:\", max(ob_history.history[\"val_accuracy\"]))\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
      ],
      "metadata": {
        "id": "t2GR7dSFZuP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image37C.png)"
      ],
      "metadata": {
        "id": "CH7l-JmUbVaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4B: Visualize Effects of `Dropout`\n",
        "\n",
        "The code in the cell below is **exactly** the same as that used in `Example 1B`."
      ],
      "metadata": {
        "id": "7rtE2hc_arZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4B: Visualize effects of dropout\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot train loss vs val loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ob_history.history['loss'], label='train loss')\n",
        "plt.plot(ob_history.history['val_loss'], label='val loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot train acc vs val acc\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ob_history.history['accuracy'], label='train acc')\n",
        "plt.plot(ob_history.history['val_accuracy'], label='val acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qRGYGpZMarZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the top plot `With Dropout`.\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image38C.png)\n",
        "\n",
        "### **Analysis**\n",
        "\n",
        "These plots reflect the training dynamics of a neural network that used only **Dropout layers**. Here's a detailed analysis:\n",
        "\n",
        "#### **Loss Plot (Top)**\n",
        "* **Training Loss (blue):** Shows a general downward trend but with noticeable fluctuations, which is expected due to the **stochastic nature of Dropout**.\n",
        "* **Validation Loss (orange)**: Also fluctuates significantly and does not show a consistent downward trend. It even appears to increase in later epochs, suggesting potential overfitting or instability.\n",
        "\n",
        "* **Accuracy Plot (Bottom)**\n",
        "* **Training Accuracy (blue):** Increases over time but with some variability, again consistent with Dropout's effect.\n",
        "* **Validation Accuracy (orange):** Fluctuates throughout training and does not consistently improve, hovering between **0.5 and 0.75**, which is relatively low compared to training accuracy.\n",
        "\n",
        "#### **Interpretation**\n",
        "* **Dropout(( introduces randomness during training, which helps prevent overfitting by forcing the network to learn more robust features. However, without **EarlyStopping**, the model continues training even when validation performance stagnates or worsens.\n",
        "* The absence of **L1 regularization** means there's no additional penalty for model complexity, which could allow the network to overfit despite Dropout.\n",
        "* The **fluctuations** in validation metrics suggest that the model might benefit from:\n",
        "* * **EarlyStopping** to halt training when validation accuracy stops improving.\n",
        "* * **Tuning** the dropout rate (e.g., trying 0.2 or 0.4 instead of 0.3).\n",
        "* * **Combining Dropout with L1 or L2 regularization** for more robust control over overfitting."
      ],
      "metadata": {
        "id": "Z8tPwcphcsl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4A: Neural Network Training: Overfitting Prevention with `Dropout`**\n",
        "\n",
        "In the cell below write the code to add `Dropout` layers to your `hd_model`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. In the cell below `copy-and-paste` your **`Exercise 1A`** into the cell below. (Do **not** use **Exercise 2A`**).\n",
        "\n",
        "2. Add `Dropout` layers by changing the building code from\n",
        "```text\n",
        "# Build model\n",
        "hd_model = models.Sequential([\n",
        "    layers.Input(shape=(hd_X_train_proc.shape[1],)),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(n_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "```\n",
        "> to the following:\n",
        "```text\n",
        "# Build model with dropout layers\n",
        "hd_model = models.Sequential([\n",
        "    layers.Input(shape=(hd_X_train_proc.shape[1],)),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),  # Dropout after first dense layer\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),  # Dropout after second dense layer\n",
        "    layers.Dense(n_classes, activation=\"softmax\")\n",
        "])\n",
        "```"
      ],
      "metadata": {
        "id": "4rON1qKFejd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4A here\n",
        "\n"
      ],
      "metadata": {
        "id": "LHiecUkXejd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image39C.png)"
      ],
      "metadata": {
        "id": "5JZbjZlZejd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4B: Visualize Effects of `Dropout`**\n",
        "\n",
        "`Copy-and-paste` the code from **`Exercise 1B`** into the cell below."
      ],
      "metadata": {
        "id": "G-ggkrEwejd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4B here\n",
        "\n"
      ],
      "metadata": {
        "id": "9NFANhD-ejd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_3_image40C.png)\n",
        "\n",
        "### **Analysis**\n",
        "\n",
        "The training run, shown in the examples above, used **`Dropout`** layers for regularization. In this instance we have a model that **struggled to learn effectively**. Here's a detailed analysis:\n",
        "\n",
        "#### **Loss Plot (Top)**\n",
        "* **Training Loss (blue):** Fluctuates but trends slightly downward, indicating some learning.\n",
        "* **Validation Loss (orange):** Shows significant spikes and instability, suggesting inconsistent generalization and possibly poor convergence.\n",
        "\n",
        "#### **Accuracy Plot (Bottom)**\n",
        "* **Training Accuracy (blue):** Starts low and increases gradually, stabilizing around **0.59-0.60**, which is relatively modest.\n",
        "* **Validation Accuracy (orange):** Drops early and then remains flat around **0.60**, showing little improvement over time.\n",
        "\n",
        "#### **Interpretation**\n",
        "* The model is **underfitting**: both training and validation accuracy are low and close to each other, which suggests the model hasn't captured the underlying patterns in the data well.\n",
        "* **Dropout alone**, especially at a rate of 0.3 in multiple layers, may be too aggressive for this architecture or dataset, preventing the model from learning effectively.\n",
        "* The absence of **EarlyStopping** means training continued despite stagnation in validation performance.\n",
        "* The lack of **L1 regularization** means there's no penalty for complexity, but in this case, the model isn't even reaching high complexity—it’s just not learning well.\n",
        "\n",
        "### **Recommendations**\n",
        "To improve performance:\n",
        "\n",
        "* * **Reduce the dropout rate** (e.g., from 0.3 to 0.2 or 0.1).\n",
        "* * **Add EarlyStopping** to avoid wasting epochs once validation accuracy plateaus.\n",
        "* * Consider adding **L1 or L2 regularization** to complement Dropout.\n",
        "* * **Increase model capacity** slightly (e.g., more neurons or layers) if underfitting persists."
      ],
      "metadata": {
        "id": "OrbI4dBzejd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_02_3.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas. Make sure you submitted a COPY of this lesson that had been saved to your GDrive instead of the orignal Colab notebook if you want this lesson graded."
      ],
      "metadata": {
        "id": "-yzuCJFCiaxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lizard Tail**\n",
        "\n",
        "\n",
        "![___](https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Apple_II_typical_configuration_1977.png/2560px-Apple_II_typical_configuration_1977.png)\n",
        "\n",
        "## **Apple II (original)**\n",
        "\n",
        "The **Apple II** (stylized as apple ][) is a personal computer released by Apple Inc. in June 1977. It was one of the first successful mass-produced microcomputer products and is widely regarded as one of the most important personal computers of all time due to its role in popularizing home computing and influencing later software development.\n",
        "\n",
        "The Apple II was designed primarily by Steve Wozniak. The system is based around the 8-bit MOS Technology 6502 microprocessor. Jerry Manock designed the foam-molded plastic case, Rod Holt developed the switching power supply, while Steve Jobs was not involved in the design of the computer. It was introduced by Jobs and Wozniak at the 1977 West Coast Computer Faire, and marks Apple's first launch of a computer aimed at a consumer market—branded toward American households rather than businessmen or computer hobbyists.\n",
        "\n",
        "\n",
        "The three computers that Byte magazine referred to as the \"1977 Trinity\" of home computing: Commodore PET 2001, Apple II, and TRS-80 Model I\n",
        "Byte magazine referred to the Apple II, Commodore PET 2001, and TRS-80 as the \"1977 Trinity\". As the Apple II had the defining feature of being able to display color graphics, the Apple logo was redesigned to have a spectrum of colors.\n",
        "\n",
        "The Apple II was the first in a series of computers collectively referred to by the Apple II name. It was followed by the Apple II+, Apple IIe, Apple IIc, Apple IIc Plus, and the 16-bit Apple IIGS—all of which remained compatible. Production of the last available model, the Apple IIe, ceased in November 1993.\n",
        "\n",
        "**History**\n",
        "\n",
        "By 1976, Steve Jobs had convinced product designer Jerry Manock (who had formerly worked at Hewlett Packard designing calculators) to create the \"shell\" for the Apple II—a smooth case inspired by kitchen appliances that concealed the internal mechanics. The earliest Apple II computers were assembled in Silicon Valley and later in Texas; printed circuit boards were manufactured in Ireland and Singapore. The first computers went on sale on June 10, 1977 with an MOS Technology 6502 microprocessor running at 1.023 MHz (2⁄7 of the NTSC color subcarrier), two game paddles (bundled until 1980, when they were found to violate FCC regulations), 4 KiB of RAM, an audio cassette interface for loading programs and storing data, and the Integer BASIC programming language built into ROMs. The video controller displayed 24 lines by 40 columns of monochrome, uppercase-only text on the screen (the original character set matches ASCII characters 20h to 5Fh), with NTSC composite video output suitable for display on a video monitor or on a regular TV set (by way of a separate RF modulator).\n",
        "\n",
        "The original retail price of the computer with 4 KiB of RAM was US \\$1,298 (equivalent to \\$6,530 in 2023) and with the maximum 48 KiB of RAM, it was US \\$2,638 (equivalent to \\$13,260 in 2023) To reflect the computer's color graphics capability, the Apple logo on the casing has rainbow stripes, which remained a part of Apple's corporate logo until early 1998. Perhaps most significantly, the Apple II was a catalyst for personal computers across many industries; it opened the doors to software marketed at consumers.\n",
        "\n",
        "Certain aspects of the system's design were influenced by Atari, Inc.'s arcade video game Breakout (1976), which was designed by Wozniak, who said: \"A lot of features of the Apple II went in because I had designed Breakout for Atari. I had designed it in hardware. I wanted to write it in software now\". This included his design of color graphics circuitry, the addition of game paddle support and sound, and graphics commands in Integer BASIC, with which he wrote Brick Out, a software clone of his own hardware game. Wozniak said in 1984: \"Basically, all the game features were put in just so I could show off the game I was familiar with—Breakout—at the Homebrew Computer Club. It was the most satisfying day of my life I demonstrated Breakout—totally written in BASIC. It seemed like a huge step to me. After designing hardware arcade games, I knew that being able to program them in BASIC was going to change the world.\"\n",
        "\n",
        "# Summary of the Apple II Computer\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The **Apple II**, introduced in **1977**, was one of the first highly successful mass-produced microcomputers. It was designed primarily by **Steve Wozniak**, with marketing and business strategy led by **Steve Jobs**. It played a crucial role in launching the personal computer revolution.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Processor**: MOS Technology 6502 @ 1 MHz\n",
        "- **Memory**: 4 KB RAM (expandable to 48 KB)\n",
        "- **Storage**: Cassette tape initially; later supported 5.25\" floppy disks via the Disk II drive\n",
        "- **Display**: Color graphics with text and low/high-resolution modes\n",
        "- **Operating System**: Initially used Integer BASIC in ROM; later supported Apple DOS\n",
        "\n",
        "## Innovations\n",
        "\n",
        "- First personal computer with **color graphics**.\n",
        "- Included **expansion slots**, allowing users to add peripherals and customize the system.\n",
        "- Came fully assembled and ready to use, unlike many kit computers of the era.\n",
        "\n",
        "## Impact\n",
        "\n",
        "- Widely adopted in **education**, **business**, and **home computing**.\n",
        "- Helped establish Apple as a major player in the tech industry.\n",
        "- Spawned a long line of successors, including the Apple IIe and Apple IIGS.\n",
        "\n",
        "## Legacy\n",
        "\n",
        "The Apple II is remembered as a pioneering product that made computing accessible to the general public. Its success laid the foundation for Apple's future innovations and the broader personal computing industry.\n",
        "\n",
        "# 🖥️ The Original Apple II (1977)\n",
        "\n",
        "The **Apple II**, released in **June 1977**, was one of the first highly successful mass-produced microcomputers. Designed primarily by **Steve Wozniak**, it played a pivotal role in launching the personal computer revolution.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Key Specifications\n",
        "\n",
        "FeatureDetailsCPUMOS Technology 6502 @ 1 MHz| **RAM**              | 4 KB (expandable up to 48 KB)                |\n",
        "| **Storage**          | Cassette tape (initially), later floppy disk |\n",
        "| **Display**          | Color graphics (up to 280×192 resolution)    |\n",
        "| **Sound**            | Single-bit speaker                           |\n",
        "| **Keyboard**         | Built-in QWERTY keyboard                     |\n",
        "| **Expansion Slots**  | 8 internal slots for peripherals             |\n",
        "| **Operating System** | Integer BASIC (in ROM), later Apple DOS      |\n",
        "\n",
        "---\n",
        "\n",
        "## 🎨 Graphics Capabilities\n",
        "\n",
        "- Supported **color graphics**, a major innovation at the time.\n",
        "- Text mode: 40×24 characters.\n",
        "- Graphics modes: Low-res (40×48) and High-res (280×192).\n",
        "- Could display up to **6 colors** in high-res mode.\n",
        "\n",
        "---\n",
        "\n",
        "## 💾 Storage Evolution\n",
        "\n",
        "- Initially used **cassette tapes** for data storage.\n",
        "- In 1978, Apple introduced the **Disk II** floppy drive, which dramatically improved speed and reliability.\n",
        "- Apple DOS 3.1 was the first disk operating system for the Apple II.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Software Ecosystem\n",
        "\n",
        "- Early programs included **VisiCalc**, the first spreadsheet software, which made the Apple II popular in business.\n",
        "- Supported a wide range of educational, gaming, and productivity software.\n",
        "- Programming languages: Integer BASIC, Applesoft BASIC, Pascal, and assembly.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏛️ Historical Impact\n",
        "\n",
        "- One of the first computers marketed to **individuals and schools**, not just hobbyists.\n",
        "- Helped establish Apple as a major player in the tech industry.\n",
        "- Remained in production (with upgrades) until **1993**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📸 Fun Fact\n",
        "\n",
        "The Apple II was famously used in the movie *WarGames* (1983), showcasing its cultural relevance.\n"
      ],
      "metadata": {
        "id": "yk1AyVYCiipk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "# Embed the video using its ID\n",
        "YouTubeVideo('-1F7vaNP9w0', width=800, height=450)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "5OO7LH5XkXT2",
        "outputId": "a7674835-081e-45e2-84b1-b9024dbe3927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7d74b7889ca0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"800\"\n",
              "            height=\"450\"\n",
              "            src=\"https://www.youtube.com/embed/-1F7vaNP9w0\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    }
  ]
}