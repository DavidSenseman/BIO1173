{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_03_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYZVwSpdbE3Y"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExN-OzpYbE3Y"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4imk1kbE3Y"
      },
      "source": [
        "##### **Module 3: Convolutional Neural Networks (CNN's)**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 3 Material\n",
        "\n",
        "* Part 3.1: Using Convolutional Neural Networks\n",
        "* **Part 3.2: Using Pre-Trained Neural Networks with PyTorch**\n",
        "* Part 3.3: Facial Recognition and Analysis\n",
        "* Part 3.4: Introduction to GAN's for Image and Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ult76BB_wSzg"
      },
      "source": [
        "#### **Change your Runtime Now!**\n",
        "\n",
        "For this lesson you must have a GPU hardware accelerator (e.g. `A100` if available)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_-lPkxLbE3Z"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG3_sXTDfyjA"
      },
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_01/class_01_6_image01A.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYHuoONPZGtV"
      },
      "source": [
        "## **TIME ALERT!**\n",
        "\n",
        "This lesson will probably **require close to 3 hours to complete**. Besides the normal issues there are two instances in which you are required to train a neural network. Training time for both neural networks is about 1 hour.\n",
        "\n",
        "Don't start working on this lesson if you don't have sufficient free time to finish it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKhQzBV1wu2v"
      },
      "source": [
        "## Accelerated Run-time Check\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be running a Graphics Processing Unit (GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kty-X7j9tDn"
      },
      "outputs": [],
      "source": [
        "# You must run this cell second\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check for GPU\n",
        "def check_colab_gpu():\n",
        "    print(\"=== Colab GPU Check ===\")\n",
        "\n",
        "    # Check PyTorch\n",
        "    pt_gpu = torch.cuda.is_available()\n",
        "    print(f\"PyTorch GPU available: {pt_gpu}\")\n",
        "\n",
        "    if pt_gpu:\n",
        "        print(f\"PyTorch device count: {torch.cuda.device_count()}\")\n",
        "        print(f\"PyTorch current device: {torch.cuda.current_device()}\")\n",
        "        print(f\"PyTorch device name: {torch.cuda.get_device_name()}\")\n",
        "        print(\"You are good to go!\")\n",
        "\n",
        "    else:\n",
        "        print(\"No compatible device found\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now and start over!\")\n",
        "\n",
        "check_colab_gpu()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIAs3kcq-WSt"
      },
      "source": [
        "If you current `Runtime` is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image17E.png)\n",
        "\n",
        "\n",
        "If your output is different, don't continue until change your `Runtime`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu5xJAWl_9vZ"
      },
      "source": [
        "### Create Custom Function\n",
        "\n",
        "The cell below creates a custom function called `hms_string()`. This function is needed to record the time required to train your neural network model.\n",
        "\n",
        "If you fail to run this cell now, you will receive one (or more) error message(s) later in this lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STtFj1QKTVcL"
      },
      "outputs": [],
      "source": [
        "# Create custom function\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 0️⃣  Create hms_string()\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7_mOnlRZ7m1"
      },
      "source": [
        "### **YouTube Introduction to ResNet**\n",
        "\n",
        "Run the next cell to see short YouTube introduction to ResNet. This is a suggested, but optional, part of the lesson."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"nc7FzLiB_AY\"\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\"> </iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "C--GmMB2aVfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5gvsd-lxR3_"
      },
      "source": [
        "### **DOWNLOAD AND INSTALL PRE-TRAINED NEURAL NETWORKS**\n",
        "\n",
        "We will be using two pre-trained neural networks in this lesson, `ResNet50` and `ResNet101`. Run the next couple of code cells to download these neural networks to your Colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YQBCLR3ZGil"
      },
      "source": [
        "### Download `ResNet50`\n",
        "\n",
        "Run the code in the cell below to download the ResNet101 model.\n",
        "\n",
        "**Using \"Colab Magic\":**\n",
        "\n",
        "Adding `%%capture` at the start of the cell hides all outputs, including standard out and progress bars."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ResNet50\n",
        "%%capture\n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Download ResNet50 with pre-trained weights\n",
        "weights = models.ResNet50_Weights.DEFAULT\n",
        "ResNet50_model_244 = models.resnet50(weights=weights)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "ResNet50_model_244.eval()"
      ],
      "metadata": {
        "id": "3TTk78Ptswp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download ResNet101\n",
        "\n",
        "Run the code in the cell below to download the ResNet101 model. As above the `%%capture` is used to suppress text being written to the screen."
      ],
      "metadata": {
        "id": "gp9byLWdaxQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ResNet101\n",
        "%%capture\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Download ResNet50 with pre-trained weights\n",
        "weights = models.ResNet50_Weights.DEFAULT\n",
        "ResNet101_model_512 = models.resnet50(weights=weights)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "ResNet101_model_512.eval()"
      ],
      "metadata": {
        "id": "APWP2m9nayC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMqH7jsZuuz"
      },
      "source": [
        "### Install Helper Functions for Examples and **Exercises**\n",
        "\n",
        "The code in the cell below creates a two functions that we will need to use classify images in Examples 1 and 2.\n",
        "\n",
        "* **make_square()** Since MobileNet is designed to classify images with the same number of horizontal and vertical pixels (i.e. a 'square' image), this function uses a combination of padding and cropping to convert any image into a 'square` image.\n",
        "\n",
        "* **classify_image()** This function does most of the work. It first retrives the image from the HTTPS server and resizes it before processing it by the `ResNet50 model` that we previously downloaded. The actual prediction is made by this specific line of code:\n",
        "```python\n",
        "    prediction = ResNet_model_244(batch).squeeze(0)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Helper Functions\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from PIL import Image, ImageFile, UnidentifiedImageError\n",
        "import requests\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from IPython.display import display\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Global settings\n",
        "# ----------------------------------------------------------------------\n",
        "IMAGE_SIZE = 224\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
        "\n",
        "# Load weights and model\n",
        "weights = models.ResNet50_Weights.DEFAULT\n",
        "ResNet_model_244 = models.resnet50(weights=weights)\n",
        "ResNet_model_244.eval() # Set to inference mode\n",
        "\n",
        "# Access the category labels and the specific preprocessing for these weights\n",
        "categories = weights.meta[\"categories\"]\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# Base URL of the images\n",
        "ROOT = \"https://biologicslab.co/BIO1173/images/class_03/\"\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Utility functions\n",
        "# ----------------------------------------------------------------------\n",
        "def make_square(img):\n",
        "    \"\"\"Crop the image to a square (center‑aligned).\"\"\"\n",
        "    width, height = img.size\n",
        "    side = min(width, height)\n",
        "    left   = (width  - side) // 2\n",
        "    top    = (height - side) // 2\n",
        "    right  = left + side\n",
        "    bottom = top  + side\n",
        "    return img.crop((left, top, right, bottom))\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Core function\n",
        "# ----------------------------------------------------------------------\n",
        "def classify_image(url):\n",
        "    \"\"\"\n",
        "    Download an image, preprocess with Torchvision transforms,\n",
        "    run through ResNet‑50, and display results.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return\n",
        "\n",
        "    # 1. Standardize the image (optional: use make_square first)\n",
        "    img_square = make_square(img)\n",
        "\n",
        "    # 2. Apply the weight-specific preprocessing (Resizing, Normalizing, and Tensor conversion)\n",
        "    # PyTorch expects a batch dimension: [Batch, Channels, Height, Width]\n",
        "    batch = preprocess(img_square).unsqueeze(0)\n",
        "\n",
        "    # 3. Predict\n",
        "    with torch.no_grad(): # Disable gradient calculation for efficiency\n",
        "        prediction = ResNet_model_244(batch).squeeze(0)\n",
        "        # Apply Softmax to get probabilities\n",
        "        probs = torch.nn.functional.softmax(prediction, dim=0)\n",
        "\n",
        "    # 4. Get Top-5\n",
        "    top5_prob, top5_catid = torch.topk(probs, 5)\n",
        "\n",
        "    # Show the image\n",
        "    display(img_square.resize((IMAGE_SIZE, IMAGE_SIZE)))\n",
        "\n",
        "    # Print the top‑5 predictions\n",
        "    print(\"\\nTop‑5 predictions:\")\n",
        "    for i in range(top5_prob.size(0)):\n",
        "        label = categories[top5_catid[i]]\n",
        "        score = top5_prob[i].item()\n",
        "        print(f\"  {label:<25} : {score*100:5.2f}%\")"
      ],
      "metadata": {
        "id": "9ybEC9ENtUz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbe54CjtWtSS"
      },
      "source": [
        "If the code is correct you should **not** see any output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiYyHO41Zurj"
      },
      "source": [
        "### Example 1: Classify Images with ResNet50\n",
        "\n",
        "The code in the cell below downloads an image of a dog from the course fileserver, https://biologicslab.co and then uses the pre-trained `ResNet50` neural network to to classify it.\n",
        "\n",
        "The image name is \"pembroke_corgi.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Classify Image with ResNet50\n",
        "\n",
        "# Enter image name\n",
        "image_name = \"pembroke_corgi.jpg\"\n",
        "\n",
        "# Generate image path using the ROOT variable defined earlier\n",
        "image_path = ROOT + image_name\n",
        "\n",
        "# Print path\n",
        "print(f\"Analyzing {image_path}\")\n",
        "\n",
        "# Use the PyTorch-based classify_image function\n",
        "classify_image(image_path)"
      ],
      "metadata": {
        "id": "NUDs-xdgtqJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWiRgSgP0lPG"
      },
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image16E.png)\n",
        "\n",
        "`ResNet50` has been trained to recognize a wide range of common objectes. Here is a basic list:\n",
        "\n",
        "| Domain                           | Example classes (just a few from each)                                   |\n",
        "|----------------------------------|--------------------------------------------------------------------------|\n",
        "| Animals – mammals                | chimpanzee, tiger, lion, zebra, elephant                                |\n",
        "| Animals – birds                  | eagle, sparrow, penguin, flamingo, crane                                 |\n",
        "| Animals – reptiles & amphibians  | alligator, snake, frog, turtle                                          |\n",
        "| Animals – fish & crustaceans     | goldfish, salmon, shrimp, lobster                                       |\n",
        "| Animals – insects                | bee, butterfly, ant, dragonfly                                          |\n",
        "| Plants & flowers                 | daisy, sunflower, orchid, cactus                                        |\n",
        "| Fruits & vegetables              | apple, orange, broccoli, carrot                                         |\n",
        "| Vegetables & nuts                | potato, tomato, almond, cashew                                          |\n",
        "| Instruments & musical equipment  | guitar, piano, violin, saxophone                                        |\n",
        "| Sports & equipment               | tennis racket, golf ball, basketball, soccer ball                       |\n",
        "| Vehicles – land                  | car, truck, motorcycle, bicycle                                         |\n",
        "| Vehicles – air                   | airplane, helicopter, jet, glider                                       |\n",
        "| Vehicles – water                 | boat, ship, submarine, ferry                                            |\n",
        "| Buildings & architecture         | bridge, house, church, skyscraper                                       |\n",
        "| Furniture                        | chair, table, sofa, bed                                                  |\n",
        "| Home appliances                  | microwave, refrigerator, toaster, blender                               |\n",
        "| Tools & hardware                 | hammer, screwdriver, wrench, drill                                      |\n",
        "| Clothing & accessories           | t‑shirt, hat, shoes, glasses                                            |\n",
        "| Food & drinks                    | coffee, tea, pizza, cake                                                |\n",
        "| Miscellaneous                    | toilet paper, keyboard, watch, trophy                                   |\n",
        "\n",
        "\n",
        "Clearly `ResNet50` was trained to classify dogs. What is somewhat interesting, is that `ResNet50` appears to be quite good as to correctly identify a dog's breed. `ResNet50` was absolutedly correct that the image showed a `Welsh Pembroke Corgi`.\n",
        "\n",
        "The **Pembroke Welsh Corgi** is a spirited, compact herding dog originally from Wales. Known for its short legs, fox-like ears, and expressive eyes, it's affectionate, intelligent, and highly trainable. Pembrokes thrive on activity, enjoy family life, and are renowned for their loyal, playful nature and distinctive “corgi grin.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKO_1OUi8rCz"
      },
      "source": [
        "### **Exercise 1: Classify Images with ResNet50**\n",
        "\n",
        "In the cell below write the code to download an another dog image from the course fileserver:\n",
        "\n",
        "```type\n",
        "image_name=\"bouvier_des_flandres.jpg\"\n",
        "```\n",
        "and then uses `ResNet50` to classify it. This species is much less common than Corgis.\n",
        "\n",
        "The **Bouvier des Flandres**, or **Bouvier de Flanders**, is a muscular, medium‑to‑large herding dog originally bred in the Flemish region of Belgium to manage cattle, sheep and pack loads. With a dense, double‑coated coat that comes in black, brown, red or tricolor, they are built for endurance and can thrive in both wet and dry climates. Their temperament is confident and affectionate, yet they possess a strong work ethic and a naturally protective instinct, making them excellent companion animals as well as valuable in search‑and‑rescue, therapy, and police work. Bouviers are intelligent and trainable, but they require consistent socialization and mental stimulation to prevent stubbornness or frustration. Health concerns are relatively few—chiefly hip dysplasia, gastric dilatation‑volvulus (bloat) and certain eye conditions—so with proper care, they can live 10–12 years."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n"
      ],
      "metadata": {
        "id": "pTweNY_8t_kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWXHzH8o8rC0"
      },
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image15E.png)\n",
        "\n",
        "Again, `ResNet50` got the specific dog breed correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unspA6MzAzOA"
      },
      "source": [
        "### Example 2: Classify Retinal Image with ResNet50\n",
        "\n",
        "What about medical image data? Can `ResNet50` analyze a **color fundus photograph** of the interior surface of a human retina?\n",
        "\n",
        "Run the code in the Example 2 to see how `ResNet50` does with the following retinal image:\n",
        "\n",
        "```text\n",
        "\"Retina_Score_0.png\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Classify Retinal Image with ResNet50\n",
        "\n",
        "# Enter image name\n",
        "image_name = \"Retina_Score_0.png\"\n",
        "\n",
        "# Generate image path\n",
        "image_path = ROOT + image_name\n",
        "\n",
        "# Print path\n",
        "print(f\"Analyzing {image_path}\")\n",
        "\n",
        "# Use the PyTorch-based classify_image function\n",
        "classify_image(image_path)"
      ],
      "metadata": {
        "id": "zYQ7Fy-wucHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBP5nv_i_E8o"
      },
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image14E.png)\n",
        "\n",
        "\n",
        "As expected, `ResNet50` was not trained to recognize retinal images. For this particular retinal image, `ResNet50` best guess (33.39% probability) that this image was a chambered nautilus. Here is one image of a chambered nautilus that is vaguely similar:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/chambered_nautilus.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvc2IU6q6gmc"
      },
      "source": [
        "### **Exercise 2: Classify Retinal Image with `ResNet50`**\n",
        "\n",
        "In the cell below write the code to analyze the another retinal color fundus image:\n",
        "```text\n",
        "\"Retina_Score_1.png\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n"
      ],
      "metadata": {
        "id": "81rubn1Ruzs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L694Na2AzOB"
      },
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image13E.png)\n",
        "\n",
        "Once again, it would be charitable to say that `ResNet50` was a little weak when it comes to analyzing clinical images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCjV86HnxkND"
      },
      "source": [
        "# **Transfer Learning for Computer Vision**\n",
        "\n",
        "## **`ResNet`**\n",
        "\n",
        "Many advanced prebuilt neural networks are available for computer vision, and Keras provides direct access to many networks. **Transfer Learning** is the technique where you use these prebuilt neural networks.\n",
        "\n",
        "There are several different levels of transfer learning.\n",
        "\n",
        "* Use a prebuilt neural network in its entirety\n",
        "* Use a prebuilt neural network's structure\n",
        "* Use a prebuilt neural network's weights\n",
        "\n",
        "In this lesson we will use a popular prebuilt CNN called **`ResNet (Residual Network)`** built by Microsoft Research in 2015. The name comes from the fact that this network was designed to address the **vanishing gradient problem** that occurs when training very deep neural networks.\n",
        "\n",
        "Instead of learning the direct mapping from input to output, `ResNet` learns the residual (i.e., the difference between the input and the output).\n",
        "This is achieved using **skip connections** (also called shortcut connections), which allow the input to bypass one or more layers and be added directly to the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9O3j51Kh6yS"
      },
      "source": [
        "## **Transfer Learning**\n",
        "\n",
        "**Transfer learning** is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second task. In the context of image recognition, this approach is particularly effective when using deep convolutional neural networks (CNNs) like **`ResNet`**.\n",
        "\n",
        "### **Why Use Transfer Learning?**\n",
        "\n",
        "Training deep neural networks from scratch requires large datasets and significant computational resources. Transfer learning mitigates this by leveraging pre-trained models—typically trained on large benchmark datasets like ImageNet—to extract general features from images. These features can then be fine-tuned for a specific task with a smaller, domain-specific dataset.\n",
        "\n",
        "### **How `ResNet` Supports Transfer Learning**\n",
        "\n",
        "`ResNet` is a widely used CNN architecture known for its use of **residual connections**, which help in training very deep networks by addressing the vanishing gradient problem. Pre-trained versions of `ResNet` (e.g., `ResNet-50`, `ResNet-101`) are commonly used as feature extractors in transfer learning workflows.\n",
        "\n",
        "### **Benefits**\n",
        "\n",
        "- **Reduced Training Time**: Leverages existing learned features.\n",
        "- **Improved Performance**: Often achieves better accuracy with less data.\n",
        "- **Flexibility**: Can be adapted to a wide range of image classification tasks.\n",
        "\n",
        "Transfer learning with `ResNet` is a powerful and efficient approach for developing high-performing image recognition models, especially when data or computational resources are limited.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2qSKv8AyFMn"
      },
      "source": [
        "### Example 3: Improving `ResNet` Classification Accuracy of Diabetic Retinopathy\n",
        "\n",
        "We know from from above that the native (base) `ResNet50` neural network is unable to effectively analyze clinical retinal images.\n",
        "\n",
        "Example 3 will demonstrate how **transfer learning** can leverage `ResNet50` as a starting point to create a new neural network that can effectively classify retinal images as to their degree of diabetic retinopathy.  \n",
        "\n",
        "#### **`Diabetic Retinopathy Image Dataset` Classes**\n",
        "\n",
        "The dataset we will be using is called the **`Diabetic Retinopathy Image Dataset`**. This image dataset consists of color fundus photographs—high‑resolution RGB images of the interior surface of the eye (the retina).\n",
        "\n",
        "These are standard clinical retinal images obtained with a fundus camera (usually a 45° or 50° field-of-view, non‑mydriatic or mydriatic camera) and capture the posterior pole (macula, optic disc, retinal vessels, and surrounding retinal tissue). The images are typically stored as JPEG/PNG files with dimensions on the order of several thousand pixels (e.g., 3500x2333 px) and are used for grading the severity of diabetic retinopathy.\n",
        "\n",
        "The severity of diabetic retinopathy was divided into **five classes** as follows:\n",
        "1. **Class 0** - No Diabetic Retinopathy (No_DR)\n",
        "2. **Class 1** - Mild Non-Proliferative Diabetic Retinopathy (NPDR)\n",
        "3. **Class 2** - Moderate NPDR\n",
        "4. **Class 3** - Severe NPDR\n",
        "5. **Class 4** - Proliferative Diabetic Retinopathy (PDR)\n",
        "\n",
        "An example of each class is shown below:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/DiabeticRetinopathyData.png)\n",
        "\n",
        "It should be noted that differences in apparent retinal size has nothing to do with the degree of retinopathy but instead reflects the fact that the retinal images were taken by a large number of clinicians, with different imaging equipment and procedures. In other words, there is considerable \"noise\" in the image set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use `torchvision` for Data Loading\n",
        "\n",
        "For Example 3 and **Exercise 3**, we will use **Torchvision** to handle image loading and preprocessing. In PyTorch, image generation is handled by a combination of `torchvision.transforms` (for image manipulation) and `torchvision.datasets` (for loading images from directories).\n",
        "\n",
        "Since `torchvision` is part of the standard PyTorch installation, no additional package installs are usually required."
      ],
      "metadata": {
        "id": "Nlb1P8VUvhrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch and Torchvision\n",
        "!pip install -q torch torchvision"
      ],
      "metadata": {
        "id": "nWmDFZ4ovP3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QtItokWvZM5"
      },
      "source": [
        "If the code is correct you should _not_ see any output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LFjCGJIpjat"
      },
      "source": [
        "### Example 3 - Step 1: Set ENVIRONMENTAL VARIABLES\n",
        "\n",
        "The code in the cell below defines a number of `ENVIRONMENTAL VARIABLES` that are needed for later code cells.\n",
        "\n",
        "The use of **Enivornment Variables** can allow code to be **configurable without modification**. For example, you might use different database URLs for development, testing, and production environments. As you will see later, you will re-use this code cell for **Exercise 3 - Step 1** below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D5kdxBeJeCT"
      },
      "outputs": [],
      "source": [
        "# Example 3 - Step 1: Set ENVIRONMENTAL VARIABLES\n",
        "\n",
        "import os\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 1️⃣  Create variables for downloading loading Zip file\n",
        "# ------------------------------------------------------------------------\n",
        "URL = \"https://biologicslab.co/BIO1173/data/\"\n",
        "DOWNLOAD_SOURCE = URL+\"diabetic_retinopathy_train_244.zip\"\n",
        "DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfind('/')+1:]\n",
        "print(\"DOWNLOAD_SOURCE=\",DOWNLOAD_SOURCE)\n",
        "print(\"DOWNLOAD_NAME=\",DOWNLOAD_NAME)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 2️⃣  Create variables for extracting the Zip file\n",
        "# ------------------------------------------------------------------------\n",
        "PATH = \"./\"\n",
        "EXTRACT_TARGET = os.path.join(PATH,\"retinopathy_244\")\n",
        "SOURCE = os.path.join(EXTRACT_TARGET, \"train_244\")\n",
        "print(\"EXTRACT_TARGET=\",EXTRACT_TARGET)\n",
        "print(\"SOURCE=\",SOURCE)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 3️⃣  Print variables for debugging\n",
        "# ------------------------------------------------------------------------\n",
        "print(\"ENVIRONMENTAL VARIABLES were successfully created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97lHNy8gLTu-"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image08C.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LfOSfJZpWB-"
      },
      "source": [
        "### Example 3 - Step 2: Download and Extract Image Data\n",
        "\n",
        "The code in the cell below downloads a zip file and extracts it. The names of the file server, zip file and folders were set above as **`ENVIRONMENTAL VARIABLES`** in the previous step.\n",
        "\n",
        "**TIME ALERT:** Even when compressed (i.e. \"zipped\"), image data file are typically quite large so download and extraction times can often take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrUA_JcKb7qO"
      },
      "outputs": [],
      "source": [
        "# Example 3 - Step 2: Download and Extract Image Data\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 1️⃣  Create directories\n",
        "# --------------------------------------------------------------\n",
        "print(\"Creating necessary directories...\", end='')\n",
        "# Create necessary directories\n",
        "os.makedirs(SOURCE, exist_ok=True)\n",
        "os.makedirs(EXTRACT_TARGET, exist_ok=True)\n",
        "print(\"done.\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 2️⃣  Download Zip file\n",
        "# --------------------------------------------------------------\n",
        "print(f\"Downloading {DOWNLOAD_NAME}...\", end='')\n",
        "# Define paths and URLs\n",
        "download_path = os.path.join(PATH, DOWNLOAD_NAME)\n",
        "extract_path = os.path.join(EXTRACT_TARGET, DOWNLOAD_NAME)\n",
        "# Download the file\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [\"wget\", \"-O\", DOWNLOAD_NAME, DOWNLOAD_SOURCE],\n",
        "        check=True,\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    print(\"done.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Download failed with error: {e}\")\n",
        "    print(f\"Error output: {e.stderr}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 3️⃣  Extract Zip file\n",
        "# --------------------------------------------------------------\n",
        "print(f\"Extracting {DOWNLOAD_NAME} to {EXTRACT_TARGET}...\", end='')\n",
        "\n",
        "# Check if zip file exists and has content\n",
        "if not os.path.exists(DOWNLOAD_NAME):\n",
        "    print(f\"Error: Zip file {DOWNLOAD_NAME} does not exist\")\n",
        "    sys.exit(1)\n",
        "\n",
        "if os.path.getsize(DOWNLOAD_NAME) == 0:\n",
        "    print(f\"Error: Zip file {DOWNLOAD_NAME} is empty\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Extract the file with error handling\n",
        "try:\n",
        "    # Use -o flag to overwrite files without prompting\n",
        "    # Use -q for quiet mode\n",
        "    result = subprocess.run(\n",
        "        [\"unzip\", \"-o\", \"-q\", DOWNLOAD_NAME, \"-d\", EXTRACT_TARGET],\n",
        "        check=True,\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    print(\"done.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error: Unzipping failed with return code {e.returncode}\")\n",
        "    print(f\"Error output: {e.stderr}\")\n",
        "    print(f\"Command that failed: {e.cmd}\")\n",
        "    sys.exit(1)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: unzip command not found. Please install unzip.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4️⃣  Verify Extraction was successful\n",
        "# --------------------------------------------------------------\n",
        "print(\"Verifying Extraction...\")\n",
        "try:\n",
        "    # Check if extraction directory exists\n",
        "    if not os.path.exists(EXTRACT_TARGET):\n",
        "        print(f\"Error: Extraction directory {EXTRACT_TARGET} does not exist\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # List contents to verify\n",
        "    contents = os.listdir(EXTRACT_TARGET)\n",
        "    if len(contents) == 0:\n",
        "        print(\"Warning: Extraction directory is empty\")\n",
        "    else:\n",
        "        print(f\"Successfully extracted {len(contents)} items:\")\n",
        "        for item in sorted(contents)[:10]:  # Show first 10 items\n",
        "            item_path = os.path.join(EXTRACT_TARGET, item)\n",
        "            if os.path.isfile(item_path):\n",
        "                size = os.path.getsize(item_path)\n",
        "                print(f\"  - {item} ({size} bytes)\")\n",
        "            else:\n",
        "                print(f\"  - {item} (directory)\")\n",
        "\n",
        "        if len(contents) > 10:\n",
        "            print(f\"  ... and {len(contents) - 10} more items\")\n",
        "\n",
        "    # Try to get more detailed information about what was extracted\n",
        "    result = subprocess.run(\n",
        "        [\"ls\", \"-la\", EXTRACT_TARGET],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        check=False  # Don't raise exception for this command\n",
        "    )\n",
        "    if result.returncode == 0:\n",
        "        print(\"Directory contents:\")\n",
        "        print(result.stdout)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during verification: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"Extraction completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZWjAy503F8"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image09D.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksYDMhoSzVQQ"
      },
      "source": [
        "### Example 3 - Step 3: Load Labels for the Training Set\n",
        "\n",
        "CNNs are trained by _supervised_ learning, which means that during each training step the network's prediction must be compared to a known correct answer. The label supplies this target: it allows a loss function (e.g., cross‑entropy, IoU, etc.) to be computed, which in turn provides gradients that tell the optimizer how to adjust the weights. Without a label, the loss cannot be evaluated, no gradients can be derived, and the network has no signal to improve its predictions. Thus, labels are essential for defining the training objective and enabling the network to learn from data.\n",
        "\n",
        "The file `trainLabels.csv` contains the label information for our retinal images. This file has just two columns, **image** and **level**. The `image` specifies the image's filename and from which eye the image was obtained; for example, `10_left.png`. The `level` column contains the a numerical value between 0 and 4 which indicates the serverity of diabetic retinopathy. So in this example the **_level_** is the **image label**.\n",
        "\n",
        "The code in the cell below reads the file `trainLabels.csv` and creates a Pandas dataframe called `eg_raw_df` to store the label information. A short amount of `eg_raw_df` is printed out for inspection to make sure the code worked as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y3dlgXu15Wx"
      },
      "outputs": [],
      "source": [
        "# Example 3 - Step 3: Load the labels for the training set\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read labels and create dataframe\n",
        "eg_raw_df = pd.read_csv(\n",
        "        os.path.join(EXTRACT_TARGET,\"trainLabels.csv\"),\n",
        "        na_values=['NA', '?'])\n",
        "\n",
        "# Add file extention\n",
        "image_col = 'image'\n",
        "eg_raw_df[image_col] = eg_raw_df[image_col].astype(str) + '.png'\n",
        "\n",
        "# Print sample for verification\n",
        "eg_raw_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy1q3kKwNTtV"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image04C.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9rzoplyVBO7"
      },
      "source": [
        "### Example 3 - Step 4: Validate Images\n",
        "\n",
        "The code in the cell below checks to see if there is an actual retinal image for each image label in contained the DataFrame `eg_raw`, that was generated in the previous step.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdv0eSrGVB6y"
      },
      "outputs": [],
      "source": [
        "# Example 3 - Step 4: Validate Images\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Validate that all image files actually exist\n",
        "def validate_image_files(df, source_path):\n",
        "    source = Path(source_path)\n",
        "    existing_files = set()\n",
        "\n",
        "    # Get all actual files in the directory\n",
        "    for file_path in source.rglob('*'):\n",
        "        if file_path.is_file() and file_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
        "            existing_files.add(file_path.name)\n",
        "\n",
        "    # Check which files in DataFrame actually exist\n",
        "    df['file_exists'] = df['image'].apply(lambda x: x in existing_files)\n",
        "\n",
        "    print(f\"Total images in DataFrame: {len(df)}\")\n",
        "    print(f\"Images that exist: {df['file_exists'].sum()}\")\n",
        "    print(f\"Missing images: {(~df['file_exists']).sum()}\")\n",
        "\n",
        "    # Filter to only include existing files\n",
        "    valid_df = df[df['file_exists']].copy()\n",
        "    print(f\"Valid DataFrame size: {len(valid_df)}\")\n",
        "\n",
        "    return valid_df\n",
        "\n",
        "# Use it:\n",
        "eg_raw_df = validate_image_files(eg_raw_df, SOURCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL2bcF55WsuN"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image24D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyRcw-DlXoNw"
      },
      "source": [
        "### Example 3 - Step 5: Split Images into Training and Validation Sets\n",
        "\n",
        "If you judge the model only on the same data you used to update its weights, you’ll get an *optimistic* estimate of its performance – the model will look perfect on the training set but will usually fail on new images.  \n",
        "Splitting the data into **training + validation** lets you:\n",
        "\n",
        "1. **Measure true generalization** - see how the network behaves on unseen samples.  \n",
        "2. **Tune hyper‑parameters** - learning rate, number of layers, data‑augmentation policies, etc.  \n",
        "3. **Detect & prevent over‑fitting** - monitor validation loss/accuracy during training and stop or adjust when the model starts to degrade.  \n",
        "4. **Select the best model** - keep the checkpoint that had the lowest validation error.  \n",
        "\n",
        "Without a validation set, you'll be blind to over-fitting, you'll have no reliable metric for early stopping, and you'll have no principled way to pick the best architecture or hyper-parameters.\n",
        "\n",
        "The code in the cell below, splits the retinal images into a training set and a validation set. How much of the train set is used for the validation set depends on the variable `FRAC`. In the cell below, `FRAC` is set to 0.8 which means 80% of the images will be put into the training set (`eg_train_df`) and the remaining 20% will be put into the validation set (`eg_val_df`). Which images are used in each set is randomize. It should be noted that only the **image names** are being \"split\" into a training and validation pool. Later an image generator will use these two DataFrames to actually generate two image sets.\n",
        "\n",
        "After the split, the number images assigned to both sets is printed out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsYUYOm5UyLA"
      },
      "outputs": [],
      "source": [
        "# Example 3 - Step 5: Split Images into Training and Validation Sets\n",
        "\n",
        "# Set split fraction\n",
        "FRAC=0.8  # 80% training / 20% validation\n",
        "\n",
        "# Convert the class column to string – required for `flow_from_dataframe`\n",
        "eg_raw_df['level'] = eg_raw_df['level'].astype(str)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "#  1️⃣ Randomly split data into training and validation sets\n",
        "# ------------------------------------------------------------------\n",
        "eg_train_df = eg_raw_df.sample(frac=FRAC, random_state=42)\n",
        "eg_val_df   = eg_raw_df.drop(eg_train_df.index)\n",
        "\n",
        "# Calculate the split fraction as sanity check\n",
        "split_fraction = len(eg_train_df) / (len(eg_val_df) + len(eg_train_df))\n",
        "\n",
        "# Print out numbers\n",
        "print(f\"Training set size   : {len(eg_train_df)}\")\n",
        "print(f\"Validation set size : {len(eg_val_df)}\")\n",
        "print(f\"Calculated split fraction =\", split_fraction)\n",
        "\n",
        "# Quick sanity check\n",
        "print(\"\\nSample training rows:\")\n",
        "print(eg_train_df[['image', 'level']].head())\n",
        "\n",
        "print(\"\\nSample validation rows:\")\n",
        "print(eg_val_df[['image', 'level']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF4iuKw5KpBX"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image25D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL-2WIiRW81b"
      },
      "source": [
        "### Example 3 - Step 6: Final Check for Valid Images\n",
        "\n",
        "The code in the cell below does one more final check for valid image files before image transforms occur and the dataloader is created in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1BAvweOW9db"
      },
      "outputs": [],
      "source": [
        "# Example 3 - Step 6: Final Check for Valid Images\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Check which files are actually valid\n",
        "def check_valid_images(df, source_path):\n",
        "    valid_files = []\n",
        "    invalid_files = []\n",
        "\n",
        "    for filename in df['image']:\n",
        "        file_path = Path(source_path) / filename\n",
        "        if file_path.exists() and file_path.is_file():\n",
        "            valid_files.append(filename)\n",
        "        else:\n",
        "            invalid_files.append(filename)\n",
        "\n",
        "    print(f\"Total files: {len(df)}\")\n",
        "    print(f\"Valid files: {len(valid_files)}\")\n",
        "    print(f\"Invalid files: {len(invalid_files)}\")\n",
        "\n",
        "    # Remove invalid files from your dataframe\n",
        "    df_clean = df[df['image'].isin(valid_files)]\n",
        "    print(f\"After cleaning: {len(df_clean)} samples\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# Apply cleaning to both train and validation sets\n",
        "eg_train_df = check_valid_images(eg_train_df, SOURCE)\n",
        "eg_val_df = check_valid_images(eg_val_df, SOURCE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNezz4kbXjaJ"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image22D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkj3iP2ggmiv"
      },
      "source": [
        "### Example 3 - Step 7: Create Image Transforms and DataLoaders\n",
        "\n",
        "The code in the cell below uses **torchvision.transforms** to define the sequence of augmentations and DataLoader to manage memory and batching. This separation provides more flexibility and control over the preprocessing pipeline.\n",
        "\n",
        "**IMPORTANT NOTE:**\n",
        "\n",
        "You need to be very careful to correctly set the **IMAGE SIZE**. This is especially true when performing transfer learning with pre-trained neural networks like `ResNet50`. `ResNet50` is designed to work with square images of exactly `244` pixels wide and `244` pixels high (i.e. `244 X 244`) that have 3 color channels (i.e. `RGB`) and no `alpha` channel.\n",
        "\n",
        "In the cell below, here is the code that specifies the image size:\n",
        "\n",
        "```type\n",
        "# Specify image size\n",
        "EG_IMG_W = 244\n",
        "EG_IMG_H = 244\n",
        "```\n",
        "You should also note that we set the `batch` sizes for training at this step in process:\n",
        "```text\n",
        "# Specify batch size\n",
        "EG_BATCH_TRAIN  = 64\n",
        "EG_BATCH_VAL    = 64\n",
        "```\n",
        "If we need to use a different batch size we will need to go back and re-run this cell again.\n",
        "\n",
        "You should also note that we only augment the **training images** --  the **validation images** are **not** augmented."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Step 7: Create Image Transforms and DataLoaders\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 0️⃣ IMPORTANT: Specify image size and batch size\n",
        "# ------------------------------------------------------------------\n",
        "EG_IMG_W = 244\n",
        "EG_IMG_H = 244\n",
        "EG_BATCH_TRAIN = 64\n",
        "EG_BATCH_VAL = 64\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1️⃣ Define the Custom Dataset\n",
        "# ------------------------------------------------------------------\n",
        "class RetinopathyDataset(Dataset):\n",
        "    def __init__(self, dataframe, root_dir, transform=None):\n",
        "        self.df = dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        # Create a mapping for categories if they are strings\n",
        "        self.classes = sorted(self.df['level'].unique())\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.df.iloc[idx]['image'])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        # In PyTorch, labels are usually long integers (CrossEntropyLoss)\n",
        "        # rather than one-hot encoded vectors.\n",
        "        label = self.class_to_idx[self.df.iloc[idx]['level']]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2️⃣ Define Transforms (Augmentation for Train, Clean for Val)\n",
        "# ------------------------------------------------------------------\n",
        "# PyTorch ResNet normalization values\n",
        "norm_mean = [0.485, 0.456, 0.406]\n",
        "norm_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = T.Compose([\n",
        "    T.Resize((EG_IMG_H, EG_IMG_W)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(20),\n",
        "    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(norm_mean, norm_std)\n",
        "])\n",
        "\n",
        "val_transforms = T.Compose([\n",
        "    T.Resize((EG_IMG_H, EG_IMG_W)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(norm_mean, norm_std)\n",
        "])\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3️⃣ Create DataLoaders\n",
        "# ------------------------------------------------------------------\n",
        "train_dataset = RetinopathyDataset(eg_train_df, str(SOURCE), transform=train_transforms)\n",
        "val_dataset = RetinopathyDataset(eg_val_df, str(SOURCE), transform=val_transforms)\n",
        "\n",
        "eg_train_loader = DataLoader(train_dataset, batch_size=EG_BATCH_TRAIN, shuffle=True, num_workers=0)\n",
        "eg_val_loader = DataLoader(val_dataset, batch_size=EG_BATCH_VAL, shuffle=False, num_workers=0)\n",
        "\n",
        "# Sanity Check\n",
        "eg_x_train, eg_y_train = next(iter(eg_train_loader))\n",
        "eg_x_val, eg_y_val = next(iter(eg_val_loader))\n",
        "\n",
        "print(f\"TRAIN batch images: {eg_x_train.shape}\") # [Batch, Channels, H, W]\n",
        "print(f\"TRAIN batch labels: {eg_y_train.shape}\") # [Batch]\n",
        "print(f\"VAL batch images:   {eg_x_val.shape}\")\n",
        "print(f\"VAL batch labels:   {eg_y_val.shape}\")"
      ],
      "metadata": {
        "id": "u6vIt8pZ0sdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRAatHmKZb6q"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image12E.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gh7tqrUXn8y"
      },
      "source": [
        "### Example 3 - Step 8: Check Class Distribution\n",
        "\n",
        "The code in the cell below generates a histogram showing the distribution of the 5 classes in the validation data set.\n",
        "\n",
        "#### **Why Check Class Distribution in CNN Classification?**\n",
        "\n",
        "When training a Convolutional Neural Network (CNN) for image classification, examining the distribution of classes in your dataset helps ensure that your model learns effectively and generalizes well. Here's why it's important:\n",
        "\n",
        "##### **1. Detecting Class Imbalance**\n",
        "If one class has significantly more samples than others, the model may become biased toward predicting the majority class. This can lead to:\n",
        "- High accuracy but poor performance on minority classes.\n",
        "- Misleading evaluation metrics.\n",
        "\n",
        "##### **2. Choosing the Right Metrics**\n",
        "In imbalanced datasets, accuracy alone is not a reliable metric. You may need to use:\n",
        "- Precision, recall, F1-score\n",
        "- Confusion matrix\n",
        "- ROC-AUC (for binary classification)\n",
        "\n",
        "##### **3. Designing Better Validation Strategies**\n",
        "Knowing the class distribution helps in:\n",
        "- Stratified sampling for train/test splits\n",
        "- Ensuring each class is represented in validation and test sets\n",
        "\n",
        "##### **4. Applying Corrective Techniques**\n",
        "If imbalance is detected, you can apply:\n",
        "- **Data augmentation** for minority classes\n",
        "- **Class weighting** in the loss function\n",
        "- **Oversampling** or **undersampling**\n",
        "- **Synthetic data generation** (e.g., SMOTE)\n",
        "\n",
        "##### **5. Improving Interpretability**\n",
        "Understanding class distribution helps interpret model behavior and debug issues like:\n",
        "- Why the model is misclassifying certain classes\n",
        "- Why training loss is low but validation performance is poor\n",
        "\n",
        "\n",
        "### **Best Practice**\n",
        "Always visualize class distribution before training using a bar chart or value counts.\n",
        "\n",
        "Following **Best Practice**, the code in the cell below generates a bar chart showing the class distribution before we start our training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 8: Check Class Distribution (PyTorch)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# In PyTorch, we can pull the labels directly from the dataset's dataframe\n",
        "# or use the class_to_idx mapping we created in the RetinopathyDataset\n",
        "eg_labels_val = val_dataset.df['level'].values\n",
        "\n",
        "# If labels are strings, map them to integers for bincount\n",
        "label_map = val_dataset.class_to_idx\n",
        "eg_labels_val_indices = [label_map[label] for label in eg_labels_val]\n",
        "\n",
        "# Count class distribution\n",
        "eg_class_counts = np.bincount(eg_labels_val_indices)\n",
        "\n",
        "# Plot distribution\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(range(len(eg_class_counts)), eg_class_counts, color='skyblue', edgecolor='black')\n",
        "\n",
        "# Add class names to the X-axis for better readability\n",
        "plt.xticks(range(len(eg_class_counts)), list(label_map.keys()))\n",
        "\n",
        "plt.xlabel(\"Class Name (Condition Severity)\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.title(\"Validation Set Class Distribution\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Print counts for verification\n",
        "for cls, idx in label_map.items():\n",
        "    print(f\"Class {idx} ({cls}): {eg_class_counts[idx]} samples\")"
      ],
      "metadata": {
        "id": "CBLtqck71Nux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXzm6mpsLfaN"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image11E.png)\n",
        "\n",
        "The classes are not balanced. Healthy retinal images (Class `0`) outnumbers the other classes. Unfortunately this situation is quite common in medical imaging of pathological states. The problem is that our neural network model can quickly learn that if it picks `0` as the image label, it will be correct _most_ of the time. We need to keep this imbalance in mind when we interpret the results of training our neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3 - Step 9: Setup Neural Network and Define Training Components\n",
        "\n",
        "The code in the cell below demonstrates how to use **transfer learning** with the **ResNet50** architecture in PyTorch for a custom image classification task involving 5 classes (degrees of diabetic retinopathy).\n",
        "\n",
        "#### **What It Does**\n",
        "\n",
        "- **Loads a pretrained `ResNet50` model**: We load the model with weights trained on ImageNet.\n",
        "- **Freezes the base model's weights**: We disable gradient calculations for the pre-trained layers to retain learned features and speed up training.\n",
        "- **Modifies the \"Head\" (Final Layer)**:\n",
        "  - In PyTorch, ResNet50 already has a Global Average Pooling layer.\n",
        "  - We replace the final Fully Connected (`fc`) layer with a custom **Sequential** block:\n",
        "    - **Linear Layer**: To transition from the base features (2048) to a hidden layer.\n",
        "    - **ReLU Activation**: For non-linearity.\n",
        "    - **Dropout**: For regularization to prevent overfitting.\n",
        "    - **Final Linear Layer**: To output scores for our **5 classes**.\n",
        "- **Defines the Loss Function and Optimizer**:\n",
        "  - **CrossEntropyLoss**: The PyTorch standard for multi-class classification (combines LogSoftmax and NLLLoss).\n",
        "  - **Adam Optimizer**: Specifically configured to only update the parameters of our new \"head.\"\n",
        "\n",
        "#### **Use Case**\n",
        "\n",
        "This approach is ideal when:\n",
        "- You have limited training data.\n",
        "- You want to leverage powerful pretrained models.\n",
        "- You need to adapt a general-purpose model to a specific classification task.\n",
        "\n",
        "In the cell below, we create `ResNet50_model_244`. We first freeze all parameters in the base model. Then, we overwrite `model.fc` with our custom architecture. Finally, we move the model to the **GPU** (if available) to ensure high-performance training."
      ],
      "metadata": {
        "id": "jLJerF6K134A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 9: Setup Neural Network and Define Training Components\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "\n",
        "# 1️⃣ Load the base ResNet50 model with pre-trained weights\n",
        "weights = models.ResNet50_Weights.DEFAULT\n",
        "ResNet50_model_244 = models.resnet50(weights=weights)\n",
        "\n",
        "# 2️⃣ Freeze all layers in the base model\n",
        "for param in ResNet50_model_244.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 3️⃣ Modify the \"Head\"\n",
        "# Replace the original 'fc' layer with a new Sequential block.\n",
        "num_features = ResNet50_model_244.fc.in_features # Get the input dimension (2048)\n",
        "\n",
        "ResNet50_model_244.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(256, 5) # Output 5 classes for retinopathy levels\n",
        ")\n",
        "\n",
        "# 4️⃣ Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ResNet50_model_244 = ResNet50_model_244.to(device)\n",
        "\n",
        "# 5️⃣ Define Optimizer and Loss Function\n",
        "optimizer = optim.Adam(ResNet50_model_244.fc.parameters(), lr=1e-4)\n",
        "\n",
        "# CrossEntropyLoss is the PyTorch equivalent of categorical_crossentropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Model successfully moved to: {device}\")\n",
        "\n",
        "# 5️⃣ Define Optimizer and Loss Function\n",
        "optimizer = optim.Adam(ResNet50_model_244.fc.parameters(), lr=1e-4)\n",
        "\n",
        "# nn.CrossEntropyLoss in PyTorch expects raw scores (logits),\n",
        "# so we don't need a Softmax layer at the end of our model.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Model moved to: {device}\")"
      ],
      "metadata": {
        "id": "fVhcJyMO17YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy1wH-0ML_qT"
      },
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image10E.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqiz6uT7Psx7"
      },
      "source": [
        "### Example 3 - Step 10: Train Neural Network\n",
        "\n",
        "The code in the cell below trains the neural network `ResNet50_model_244` for the number of epochs specified by the variable `EPOCHS`. A second variable, `PATIENCE`, controls the Early Stopping monitor. To keep training time reasonable the variable EPOCH has been set to 10 and the variable PATIENCE has been set to 3.\n",
        "\n",
        "**TIME ALERT!**\n",
        "\n",
        "We are analyzing a **LARGE** image dataset so training will take time. Even using Google fastest GPU hardware acceleration, it will probably take more than **1 hour to complete**. So don't start training if you can't afford to wait.\n",
        "\n",
        "**DON'T WASTE YOUR MONEY!**\n",
        "\n",
        "To get feedback on how long the training takes, the code in the cell below contains a \"timer function\". Specifically, the last line of code reads:\n",
        "\n",
        "```text\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")\n",
        "```\n",
        "This code snippet prints out how long the training required.\n",
        "\n",
        "If you weren't careful and skipped over running the cell that defined the custom function `hms_string()` at the start of this lesson, your model will train for an hour but **FAIL** at the end with an error message.\n",
        "\n",
        "Since every time you use a Colab GPU **costs you money**, don't waste your time and _money_ by running this code cell if you didn't run _all_ of the code cells above before you run the next one.\n",
        "\n",
        "**You have been warned.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 10: Train Neural Network\n",
        "\n",
        "import time\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1️⃣ Initialize History Dictionary\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "EPOCHS = 10\n",
        "PATIENCE = 3\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Scheduler for learning rate\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2)\n",
        "\n",
        "print(f\"-- Training started for {EPOCHS} epochs ----------------------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- TRAINING PHASE ---\n",
        "    ResNet50_model_244.train()\n",
        "    train_loss, train_correct = 0.0, 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(eg_train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = ResNet50_model_244(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        train_correct += torch.sum(preds == labels.data).item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    # --- VALIDATION PHASE ---\n",
        "    ResNet50_model_244.eval()\n",
        "    val_loss, val_correct = 0.0, 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in eg_val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = ResNet50_model_244(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_correct += torch.sum(preds == labels.data).item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    # 2️⃣ Calculate and Append Metrics to History\n",
        "    epoch_train_loss = train_loss / total_train\n",
        "    epoch_train_acc = train_correct / total_train\n",
        "    epoch_val_loss = val_loss / total_val\n",
        "    epoch_val_acc = val_correct / total_val\n",
        "\n",
        "    history['train_loss'].append(epoch_train_loss)\n",
        "    history['train_acc'].append(epoch_train_acc)\n",
        "    history['val_loss'].append(epoch_val_loss)\n",
        "    history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | \"\n",
        "          f\"Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # 3️⃣ Early Stopping & Best Model Saving\n",
        "    scheduler.step(epoch_val_loss)\n",
        "\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        # Save the best weights locally\n",
        "        torch.save(ResNet50_model_244.state_dict(), 'best_model_local.pth')\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished.\")\n",
        "\n",
        "# Compute elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")\n",
        "\n",
        "# Print final training statistics\n",
        "print(f\"Final training completed in {epoch+1} epochs\")"
      ],
      "metadata": {
        "id": "7luiYT6Okb3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FRIhXrVyrJf"
      },
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image09E.png)\n",
        "\n",
        "Using an older, less powerful GPU accelerator (`T4 High-RAM`) training for 10 epochs required about 20 min to complete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORsbZqckdC4"
      },
      "source": [
        "### Example 3 - Step 11: Plot Training/Validation Accuracy\n",
        "\n",
        "A **Training-versus-Validation Accuracy Plot** provides a quick visual gauge of how well a model is learning and generalizing. As training progresses, a rising training accuracy shows that the model is fitting the data, while the validation accuracy tracks performance on unseen samples.\n",
        "\n",
        "When both curves rise together and plateau, the model is likely well-balanced. If training accuracy climbs while validation accuracy lags or dips, the model is over-fitting; conversely, if both stay low, the model is under-fitting or too simple.\n",
        "\n",
        "The shape of the curves also reveals training issues—sharp jumps or oscillations can signal an inappropriate learning rate, and noisy validation performance may indicate label noise or class imbalance. Thus, the plot informs decisions about early stopping, regularisation, architecture changes, or hyper-parameter tuning.\n",
        "\n",
        "\n",
        "#### **Interpreting Common Patterns**\n",
        "\n",
        "| Curve Shape | Interpretation | Suggested Action |\n",
        "|-------------|----------------|------------------|\n",
        "| **Both curves rise together and plateau at high accuracy** | Good fit & generalisation. | Continue training if you want to squeeze a bit more. |\n",
        "| **Training rises, validation rises then drops** | Over‑fitting. | Add regularisation, dropout, data augmentation, or stop early. |\n",
        "| **Both curves rise slowly and stay low** | Under‑fitting. | Increase model capacity, train longer, or reduce regularisation. |\n",
        "| **Validation lags behind training by a fixed margin, but both improve** | Model learns but generalises less well. | Consider a larger training set, better augmentation, or a different architecture. |\n",
        "| **Validation fluctuates wildly** | High variance due to small batch or high learning rate. | Reduce learning rate, increase batch size, or use a learning‑rate scheduler. |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 11: Plot Training/Validation Accuracy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pull metrics from history dictionary\n",
        "eg_val_acc = [acc.cpu().item() if torch.is_tensor(acc) else acc for acc in history['val_acc']]\n",
        "eg_train_acc = [acc.cpu().item() if torch.is_tensor(acc) else acc for acc in history['train_acc']]\n",
        "\n",
        "# --- Find the epoch with the highest validation accuracy -------------\n",
        "best_epoch_idx = np.argmax(eg_val_acc)\n",
        "best_epoch_num = best_epoch_idx + 1\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(eg_val_acc, label='Validation Accuracy', marker='o')\n",
        "plt.plot(eg_train_acc, label='Training Accuracy', marker='x')\n",
        "\n",
        "# Vertical line at the best epoch\n",
        "plt.axvline(best_epoch_idx, color='r', linestyle='--',\n",
        "            label=f'Best epoch (epoch {best_epoch_num})')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training / Validation Accuracy (ResNet50)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Annotate the exact accuracy value at the best epoch\n",
        "best_eg_val_acc = eg_val_acc[best_epoch_idx]\n",
        "plt.text(best_epoch_idx, best_eg_val_acc,\n",
        "         f' {best_eg_val_acc:.4f}',\n",
        "         va='bottom', ha='left', color='r', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lutp6Qii6oYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jADKuEQrDei"
      },
      "source": [
        "If the code is correct you should see something _similar_ to the following graph.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image08E.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M2meHM83wzP"
      },
      "source": [
        "### Example 3 - Step 12: Save Model to GDrive\n",
        "\n",
        "Run the next cell to save your retrained `ResNet50_model_244` model to your GDrive.\n",
        "\n",
        "**IMPORTANT NOTE** You will be using this saved PyTorch model in a later class lesson so make sure **not** to delete it from your GDrive!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 10: Save Model Weights to Google Drive\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 1️⃣ Mount Google Drive (if not already done)\n",
        "# --------------------------------------------------------------\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 2️⃣ Define the names / paths\n",
        "# --------------------------------------------------------------\n",
        "model_name  = \"ResNet50_model_244\"\n",
        "gdrive_dir  = f\"/content/drive/My Drive/{model_name}\"  # Folder on Drive\n",
        "# PyTorch convention uses .pth or .pt\n",
        "gdrive_file = os.path.join(gdrive_dir, f\"{model_name}.pth\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 3️⃣ Make sure the Drive folder exists\n",
        "# --------------------------------------------------------------\n",
        "os.makedirs(gdrive_dir, exist_ok=True)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4️⃣ Save the weights *on* Drive\n",
        "# --------------------------------------------------------------\n",
        "# We save the state_dict, which is the dictionary of weights\n",
        "torch.save(ResNet50_model_244.state_dict(), gdrive_file)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 5️⃣ Verify and List\n",
        "# --------------------------------------------------------------\n",
        "if os.path.exists(gdrive_file):\n",
        "    print(f\"Success! Model weights saved to Drive: {gdrive_file}\")\n",
        "else:\n",
        "    print(\"Error: File not found. Check Drive connection.\")\n",
        "\n",
        "# List the directory contents\n",
        "!ls -lh \"/content/drive/My Drive/ResNet50_model_244\""
      ],
      "metadata": {
        "id": "Pg2f7Agq7JyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibbgwTSACd1d"
      },
      "source": [
        "If the code is correct you should see something similar to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image07E.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SofmEo6JmCZz"
      },
      "source": [
        "------------------------------------------\n",
        "\n",
        "\n",
        "# **EXERCISE 3**\n",
        "\n",
        "In **Exercise 3** you are going to re-analyze the Diabetic Retinopathy data set using `ResNet101` in conjunction with larger retinal images that are 512 X 512 pixels.\n",
        "\n",
        "**`ResNet-101`** was built to give a **deeper**, **larger-receptive-field** backbone that can be trained stably thanks to residual connections. That extra depth—and the corresponding increase in receptive field and representational capacity—makes the network particularly suited to the high-resolution, high-complexity images that dominate modern clinical imaging datasets (e.g., whole-slide pathology, high‑res CT/MRI, retinal photographs).  \n",
        "\n",
        "#### **1. Depth matters for large, complex images**\n",
        "\n",
        "| Aspect                       | Why it matters in clinical imaging                                                                                     |\n",
        "|------------------------------|------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Receptive field**         | • Deeper network ⇒ larger receptive field.<br>• Captures micro‑details (nuclei, capillaries) *and* macro‑context (lobule shape). |\n",
        "| **Hierarchical feature extraction** | • Early layers learn edges & textures.<br>• Deeper layers combine them into complex motifs.<br>• Needed for multi‑level abstraction (malignant vs. normal). |\n",
        "| **Parameter efficiency**    | • Residual blocks add depth without exploding the number of weights.<br>• 101‑layer stack packs more power while staying manageable. |\n",
        "\n",
        "\n",
        "#### **2. Residual connections: the training engine that makes depth feasible**\n",
        "\n",
        "* **Vanishing / exploding gradients** are especially problematic when you try to back‑propagate through dozens of nonlinear layers. Residual connections add a *shortcut* that lets the gradient flow almost unchanged from the output back to the input, so deeper models can be trained from scratch (or fine‑tuned on a new domain) without catastrophic loss of signal.\n",
        "* In medical-image research, practitioners often *fine-tune* ImageNet‑pretrained models on limited clinical data. Because residual connections preserve gradient flow, a 101‑layer network can be adapted with fewer epochs and fewer data points than a comparable plain CNN.\n",
        "\n",
        "#### **3. ResNet‑101 and typical clinical datasets**\n",
        "\n",
        "| Dataset type              | Typical image size                                    | Typical complexity                                   | ResNet‑101 benefits                                                                                                                                                 |\n",
        "|---------------------------|-------------------------------------------------------|-------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| Whole‑slide pathology     | 10k‑40k px × 10k‑40k px (usually tiled to 512×512)   | Ultra‑fine texture + large‑scale architecture        | • Deep residual layers capture both micro‑ and macro‑features.<br>• Receptive field ≈ 400 px after 5 stages                                                      |\n",
        "| Chest CT                  | 512×512 or larger                                    | Subtle texture changes, large organs                 | • Deeper encoder gives more context for slice‑to‑slice variation                                                                                                      |\n",
        "| Retinal fundus            | 5000×5000 px (often cropped to 1024×1024)            | Fine vascular patterns + large vessels               | • Extra depth boosts sensitivity to minute capillary changes                                                                         \n",
        "> **Bottom line**: ResNet‑101 was *not* invented *solely* for medical imaging, but its architectural traits (depth, residual links, large receptive field) make it a natural fit for the high‑resolution, highly variable images found in clinical datasets.\n",
        "\n",
        "#### **4. Practical use-cases that illustrate the point**\n",
        "\n",
        "1. **Whole‑slide analysis**  \n",
        "   *U‑Net + ResNet‑101 encoder* is the standard backbone for segmentation of tumor vs. normal tissue. The encoder’s depth allows the network to produce rich multi‑scale feature maps that are essential for accurate boundary delineation.\n",
        "\n",
        "2. **Histopathology classification**  \n",
        "   A 101‑layer residual network fine‑tuned on 2048×2048 patches yields a 2‑fold improvement in AUC over a 50‑layer variant, especially on datasets where subtle nuclear pleomorphism is the key discriminant.\n",
        "\n",
        "3. **Radiology reporting**  \n",
        "   ResNet‑101 pre‑trained on ImageNet is used as the feature extractor for a downstream transformer that produces radiology reports. The larger depth improves the semantic quality of the extracted features, translating into more coherent, clinically relevant reports.\n",
        "\n",
        "#### **5. What the “larger image size” means**\n",
        "\n",
        "When people say ResNet‑101 was created “in part to analyze larger image sizes,” they're really referring to two intertwined design goals:\n",
        "\n",
        "1. **Scale-aware receptive fields** - As the input resolution grows, the network’s receptive field must grow proportionally so that each unit still sees a meaningful portion of the image. Deeper networks naturally achieve this.\n",
        "\n",
        "2. **Robust training** - High-resolution images contain many more pixels, which can amplify the effects of noise, small mis-alignments, or class imbalance. Residual learning keeps gradients stable, allowing the model to learn meaningful patterns from a massive pixel set without **over-fitting** or getting stuck.\n",
        "\n",
        "---\n",
        "\n",
        "#### **TL;DR**\n",
        "\n",
        "ResNet‑101’s 101 layers give it a large receptive field and the capacity to learn complex, hierarchical features, which is vital for interpreting the fine‑grained details and global context in large medical images. Residual connections make training such a deep network feasible, allowing it to be fine‑tuned on the often limited clinical datasets that are typical in healthcare. That combination is why the architecture was chosen, and later adopted, for many high‑resolution clinical image‑analysis tasks.\n",
        "\n",
        "#### **Image Size Comparison**\n",
        "\n",
        "You should realize that `512×512×3` image is more than **4X larger** than a `244×244×3` image. Here's the math:\n",
        "\n",
        "\\begin{aligned}\n",
        "512 \\times 512 \\times 3 &= 786{,}432 \\\\\n",
        "244 \\times 244 \\times 3 &= 178{,}608 \\\\\n",
        "\\frac{786{,}432}{178{,}608} &\\approx 4.4\n",
        "\\end{aligned}\n",
        "\n",
        "\n",
        "So, the **512x512x3** image is approximately **4.4 times larger** than the **244×244×3** image in terms of pixel data. That additional detail might be critical for \"seeing\" small differences between the images in different diabetic retinal classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyqunLqtmQLj"
      },
      "source": [
        "### **Exercise 3 - Step 1: Set ENVIRONMENTAL VARIABLES**\n",
        "\n",
        "In the cell below write the code to define a your `ENVIRONMENTAL VARIABLES`. It is important to make the all of the following changes exactly as suggested.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change this line of code\n",
        "```text\n",
        "DOWNLOAD_SOURCE = URL+\"diabetic_retinopathy_train_244.zip\"\n",
        "```\n",
        "to read\n",
        "```text\n",
        "DOWNLOAD_SOURCE = URL+\"diabetic_retinopathy_train_512.zip\"\n",
        "```\n",
        "2. Change this line of code\n",
        "```text\n",
        "EXTRACT_TARGET = os.path.join(PATH,\"retinopathy_244\")\n",
        "```\n",
        "to read\n",
        "```text\n",
        "EXTRACT_TARGET = os.path.join(PATH,\"retinopathy_512\")\n",
        "```\n",
        "3. Change this line of code\n",
        "```text\n",
        "SOURCE = os.path.join(EXTRACT_TARGET, \"train_244\")\n",
        "```\n",
        "to read\n",
        "```text\n",
        "SOURCE = os.path.join(EXTRACT_TARGET, \"train_512\")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTJS3d5tmQLj"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 - Step 1 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ehx51EmQLj"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image37C.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owF95gW0mQLk"
      },
      "source": [
        "### **Exercise 3 - Step 2: Download and Extract Image Data**\n",
        "\n",
        "In the cell below write the code to (1) create the necessary directories, (2) download the Zip file containing the image data, (3) extract this data into the appropiate folder and (4) verify the sucess\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "You can re-use the code in Example 3 - Step 2 _without_ making any code changes.\n",
        "\n",
        "**TIME WARNING:** This Zip file is _substantially_ larger than the one you downloaded in Example 3. You can expect it to take approximately **7 minutes** to download and unzip the image files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr8eqTjtdCW1"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 - Step 2 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX3hmd-omQLk"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image10D.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69raKQFgmQLl"
      },
      "source": [
        "### **Exercise 3 - Step 3: Load Labels for the Training Set**\n",
        "\n",
        "In the cell below write the code to extract the label information from the file `trainLabel.csv` located in your `./retinopathy_512` folder and create a DataFrame called `ex_raw_df`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Copy the code from `Example 3 - Step 3` and then change the prefix `eg_` to `ex_` **everywhere** in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpmGEfHEmQLl"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 - Step 3 here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkYpMY5MmQLl"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image40C.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxmUxVqYcpHv"
      },
      "source": [
        "### **Exercise 3 - Step 4: Validate Images**\n",
        "\n",
        "In the cell below write the code to check whether an actual retinal image file exists for each image label in the DataFrame `ex_raw`.  \n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Make sure to change the prefix `eg_` to `ex_` in the print statements are the bottom of the code copied from Example 3 - Step 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI9Y8eGEcpHv"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Execise 3 - Step 4 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxHv6dHQcpHw"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image29D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9FsUIsDmQLl"
      },
      "source": [
        "### **Exercise 3 - Step 5: Split Images into Training and Validation Sets**\n",
        "\n",
        "In the cell below write the code to split your retinal images into a training set and a validation set with 80% of the images going into the training set. After splitting, print out the number images in both sets and a short sample of both the training set and the validation set.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Copy the code from `Example 3 - Step 5` and change the prefix `eg_` to `ex_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63vrz9p2L3X2"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 - Step 5 here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoNzAXAA4Pez"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image30D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urX5VTGfgG4w"
      },
      "source": [
        "### **Exercise 3 - Step 6: Final Check for Valid Images**\n",
        "\n",
        "In the cell below write the to make a final check that all the image files are valid.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Make sure to change the prefix `eg_` to `ex_` every where in the code that you copied from Example 3 - Step 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Eh0xPragHco"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exericse 3 - Step 6 here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb_9xI_rOt2p"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image31D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSSVk-dYaLYe"
      },
      "source": [
        "### **Exercise 3: Step 7: Create Image Transforms and DataLoaders**\n",
        "\n",
        "In the cell below write the code to create the image transforms and the dataloaders.\n",
        "\n",
        "**Code Notes:**\n",
        "\n",
        "1. Is is _very_ important that you change your image size.\n",
        "\n",
        "Specifically you must change this code snippet:\n",
        "```python\n",
        "    #------------------------------------------------------------------\n",
        "    # 0️⃣ IMPORTANT: Specify image size\n",
        "    #-----------------------------------------------------------------\n",
        "    EG_IMG_W = 244\n",
        "    EG_IMG_H = 244\n",
        "```\n",
        "to read as\n",
        "```python\n",
        "    #------------------------------------------------------------------\n",
        "    # 0️⃣ IMPORTANT: Specify image size\n",
        "    #-----------------------------------------------------------------\n",
        "    EX_IMG_W = 512\n",
        "    EX_IMG_H = 512\n",
        "```\n",
        "The main objective of **Exercise 3** is to analyze larger retinal imagee (512 X 512 pixels) to see if that will improve classification accuracy.\n",
        "\n",
        "2. Change batch name\n",
        "\n",
        "Specifically you must change this code snippet:\n",
        "```python\n",
        "    # Specify batch size\n",
        "    EG_BATCH_TRAIN  = 64\n",
        "    EG_BATCH_VAL    = 64\n",
        "```\n",
        "to read as\n",
        "```python\n",
        "    # Specify batch size\n",
        "    EX_BATCH_TRAIN  = 64\n",
        "    EX_BATCH_VAL    = 64\n",
        "```\n",
        "You will also need to change these values in the code block.\n",
        "\n",
        "\n",
        "3. Make sure to change the prefix `eg_` to `ex_` as well as `EG_` to `EX_` everywhere they appear in the code block."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Example 3: Step 7 here\n",
        "\n"
      ],
      "metadata": {
        "id": "-T283CC0tgYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaCyU5oz4p_C"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image06E.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry1JMAnfmQLm"
      },
      "source": [
        "### **Exercise 3 - Step 8: Check Class Distribution**\n",
        "\n",
        "In the cell below write the code to generate a bar graph showing the number of images in each of the 5 output classes.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Copy the code from `Example 3 - Step 6` and change the prefix `eg_` to `ex_` everywhere that it occurs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Example 3 - Step 8 here\n",
        "\n"
      ],
      "metadata": {
        "id": "qWPY0-adwcmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9hzpBna4-MB"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image05E.png)\n",
        "\n",
        "As before, the classes are not balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOOp2my2acdu"
      },
      "source": [
        "### **Exercise 3 - Step 9: Setup `ResNet101` Base Network**\n",
        "\n",
        "In the cell below write the code to setup your base network using `ResNet101` instead of `ResNet50`. Called your new model `ResNet101_model_512`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change this line of code\n",
        "```python\n",
        "    # 1️⃣ Load the base ResNet101 model with pre-trained weights\n",
        "    weights = models.ResNet50_Weights.DEFAULT\n",
        "```\n",
        "to read\n",
        "```python\n",
        "    # 1️⃣ Load the base ResNet101 model with pre-trained weights\n",
        "    weights = models.ResNet101_Weights.DEFAULT\n",
        "```\n",
        "\n",
        "2. Change this line of code\n",
        "```python\n",
        "    # 2️⃣ Freeze all layers in the base model\n",
        "    for param in ResNet50_model_244.parameters():\n",
        "```\n",
        "to read\n",
        "```python\n",
        "    # 2️⃣ Freeze all layers in the base model\n",
        "    for param in ResNet101_model_512.parameters():\n",
        "```\n",
        "\n",
        "Change `ResNet50_model_244` to `ResNet101_model_512` everywhere it occurs in the code block.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Example 3 - Step 9 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cZDZj7Y5xbML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMSFZmOW5k-g"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image04E.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi1Fbo8NmQLm"
      },
      "source": [
        "### **Exercise 3 - Step 10: Train Neural Network**\n",
        "\n",
        "In the cell below write the code to train your neural network `ResNet101_model_512`.\n",
        "\n",
        "#### **Please Read this Carefully**\n",
        "\n",
        "The `ResNet101_model_512` is perhaps the largest neural network that you will be asked to train in this course. Due to the large size of the images (512x512 pixels), training this model for even `10` epochs requires close to **2 hours!**\n",
        "\n",
        "To make this lesson a little more \"student friendly\", you are free to reduce your number of epochs to a **minumum value** of `5` and still get full credit.\n",
        "\n",
        "Just be aware that even 5 epochs will probably take about an hour to run. Be aware that if you don't run at least 5 epochs you will lose points.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change `ResNet50_model_244` to read `ResNet101_model_512` everywhere it occurs in the code block.\n",
        "\n",
        "2. Change the prefix `eg_` to `ex_` everywhere it occurs in the code block."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Example 3 - Step 10 here\n"
      ],
      "metadata": {
        "id": "sO9mDM5ayeRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lALh-nnSmQLm"
      },
      "source": [
        "If the code is correct you should see something similar to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image01E.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pSv_2shmQLn"
      },
      "source": [
        "### **Exercise 3 - Step 11: Plot Training History**\n",
        "\n",
        "In the cell below write the code to plot the training history of your `ResNet101_model_512`.\n",
        "\n",
        "Copy the code in Example 3 - Step 9 into the cell below.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change the prefix `eg_` to `ex_` everywhere in the copied code.\n",
        "2. Change the plot title from `ResNet50` to `ResNet101`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Example 3 - Step 11 here\n",
        "\n"
      ],
      "metadata": {
        "id": "3uDUB-eSDG2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjnUgGZSRYZr"
      },
      "source": [
        "If the code is correct you should see something similar to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image02E.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9yqR3rQmQLn"
      },
      "source": [
        "### **Exercise 3 - Step 12: Save Model to GDrive**\n",
        "\n",
        "In the next cell write the code to save your `ResNet101_model_512` to your GDrive.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Change this line of code\n",
        "```python\n",
        "    model_name = \"ResNet50_model_244\" # model object name\n",
        "```\n",
        "to read\n",
        "```python\n",
        "    model_name = \"ResNet101_model_512\"  # model object name\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Example 3 - Step 10 here\n",
        "\n"
      ],
      "metadata": {
        "id": "sTUHsz5ID5Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN2bqt7V_C1S"
      },
      "source": [
        "If the code is correct you should see something similar to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image03E.png)\n",
        "\n",
        "Your `ResNet101_model_512` is now safely stored on your GDrive.\n",
        "\n",
        "If you wanted to, you could easily copy your model back from your GDrive into you current Google Colab directory and use it analyze new retinal fundus images, looking for signs of diabetic retinopathy. Since your model is already trained, you could analyze the retinal images from your new patient as he/she sits in your office.\n",
        "\n",
        "Always keep in mind a common saying among members of the ML/AI community:\n",
        "\n",
        "> **_“Train-once, deploy-many”_**\n",
        "\n",
        "**IMPORTANT NOTE:** Make sure **not** to erase your saved file from your GDrive. You will need to use it later in a different class lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MhC_-6ebE3l"
      },
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Microsoft Print to PDF** to generate a PDF of your Colab notebook if you are using Windows; use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook if you are using a Mac. Call your PDF `Class_03_2.lastname.pdf` where _lastname_ is your last name, and upload your PDF to Canvas for grading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFG4NZGwRfSk"
      },
      "source": [
        "## **The Diabetic Retinopathy Dataset**\n",
        "\n",
        "The **Diabetic Retinopathy Dataset** used in this lesson was part of a 2015 Kaggle Competition.\n",
        "\n",
        "Here is a summary of the winners and what they did to win this competition.\n",
        "\n",
        "### Winner of the 2015 Kaggle Diabetic Retinopathy Detection competition\n",
        "**Team**: *o_O* (Mathis Antony & Stephan Brüggemann)  \n",
        "**Score**: 0.845 weighted quadratic‑weighted Kappa (private leaderboard)  \n",
        "**Public leaderboard**: 7th place (5.8 k K‑appa)\n",
        "\n",
        "| Item | Details | Source |\n",
        "|------|---------|--------|\n",
        "| **Winning team** | “o_O” (Mathis Antony & Stephan Brüggemann) | 5.8 k K‑appa on the private leaderboard, 7 th place on the public leaderboard【9†L18-L23】 |\n",
        "| **Overall performance** | 0.845 weighted quadratic weighted Kappa (private leaderboard) | 5.8 k K‑appa, 0.845 score【9†L19-L22】 |\n",
        "| **Core architecture** | Two custom 2‑D convolutional nets (Net A & Net B) with a **per‑patient blending network** (Table 2) | 13‑25 | 12‑15 |\n",
        "| **Training framework** | Lasagne + nolearn (Theano) | 10‑11 |\n",
        "| **Image size** | 128 × 128, 256 × 256 and 512 × 512 (large color images, cropped to remove background) | 21‑24 |\n",
        "| **Pre‑training strategy** | *First* train a small network on 128‑pixel images.  Weights are then used to initialise an intermediate‑size network (trained on 256 px) and finally a 512‑pixel network.  Orthogonal initialization for all weights. | 105‑108 |\n",
        "| **Data augmentation** | Translation, stretching, rotation, flipping, colour jitter; per‑channel zero‑mean/unit‑variance scaling; 112/224/448 output sizes for 128/256/512 input images. | 115‑121 |\n",
        "| **Class imbalance handling** | Dynamic resampling: oversample rare classes initially, then gradually reduce; resampling weights \\((1.36, 14.4, 6.64, 40.2, 49.6)\\) → \\((1,2,2,2,2)\\). | 82‑96 |\n",
        "| **Training schedule** | Nesterov momentum with a fixed learning‑rate schedule over 250 epochs; learning rates 0.003 (epoch 0) → 0.00003 (epoch 150); L2 weight decay 0.0005; dropout after convolution and dense layers. | 31‑36, 70‑73 |\n",
        "| **Loss & objective** | Mean‑squared‑error regression (output thresholded at (0.5,1.5,2.5,3.5) to obtain integer grades). | 75‑77 |\n",
        "| **Blending network** | Input: mean & std of the RMSPool layer over 50 augmentations for each eye (µ,σ) + eye‑side indicator; 8193‑input → Dense 32 → Maxout 16 → Dense 32 → Maxout 16; Adam optimiser with a schedule (5 e‑4 → 5 e‑7). | 148‑156, 158‑166 |\n",
        "| **Final ensemble** | Average of the two conv‑net predictions, blended with the patient‑level network; score 0.845 (private) vs 0.824 (no per‑patient blend). | 167‑169 |\n",
        "| **Key design choices that yielded the win** | 1. **Large input resolution** – 512 × 512 (and even 768 × 768 for 0.81 Kappa) to capture micro‑aneurysms. 2. **Stage‑wise pre‑training** – starting from 128 px to 512 px to stabilise training. 3. **Extensive data augmentation & per‑channel normalisation**. 4. **Dynamic resampling** to address class imbalance without a weighted loss. 5. **Per‑patient blending** that aggregates information from both eyes and multiple augmentations. 6. **Ensembling of two independently trained nets**. | 45‑69, 82‑96, 100‑108, 115‑121, 131‑139, 167‑169 |\n",
        "\n",
        "### **Why their approach won**\n",
        "\n",
        "The combination of **large‑resolution images**, staged pre‑training, aggressive augmentation, careful imbalance handling, and per‑patient feature blending allowed the o_O model to achieve the highest weighted quadratic‑weighted κ score in the 2015 competition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Xdnb5nXH1z"
      },
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "\n",
        "## **BASIC**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/7/7b/AtariBASIC.png)\n",
        "\n",
        "# Introduction to BASIC Programming Language\n",
        "\n",
        "## What is BASIC?\n",
        "\n",
        "**BASIC** (Beginner's All-purpose Symbolic Instruction Code) is a high-level programming language designed to be easy to learn and use. It was created with the goal of providing access to computing power for students and non-professional programmers, emphasizing simplicity and readability.\n",
        "\n",
        "BASIC uses straightforward syntax and commands that resemble English, making it an ideal first language for beginners. Over time, it has evolved into many dialects and influenced the development of numerous other programming languages.\n",
        "\n",
        "---\n",
        "\n",
        "## **Historical Summary**\n",
        "\n",
        "#### **Origins**\n",
        "\n",
        "- **Year Created**: 1964  \n",
        "- **Creators**: John G. Kemeny and Thomas E. Kurtz  \n",
        "- **Institution**: Dartmouth College\n",
        "\n",
        "Kemeny and Kurtz developed BASIC to enable students in fields other than science and mathematics to use computers. At the time, most programming languages were complex and required deep technical knowledge. BASIC was designed to democratize computing.\n",
        "\n",
        "#### **Key Milestones**\n",
        "\n",
        "- **1964**: First implementation of BASIC on a GE-225 mainframe at Dartmouth College.\n",
        "- **1970s**: BASIC became widely adopted on microcomputers, especially with the rise of personal computing.\n",
        "- **1975**: Microsoft was founded to develop a version of BASIC (Altair BASIC) for the Altair 8800, marking the beginning of Microsoft's software empire.\n",
        "- **1980s**: Variants like GW-BASIC, QuickBASIC, and Turbo BASIC became popular on MS-DOS systems.\n",
        "- **1991**: Microsoft introduced Visual Basic, combining BASIC with a graphical user interface (GUI) development environment.\n",
        "- **2000s–Present**: Modern dialects like VB.NET continue to be used, especially in enterprise and educational settings.\n",
        "\n",
        "#### **Legacy**\n",
        "\n",
        "BASIC played a crucial role in the early days of personal computing. It introduced millions of people to programming and laid the foundation for many modern languages. While its use has declined in favor of more powerful and flexible languages, its influence remains significant in the history of computer science.\n",
        "\n",
        "---\n",
        "\n",
        "## Example BASIC Code\n",
        "\n",
        "```basic\n",
        "10 PRINT \"HELLO, WORLD!\"\n",
        "20 END\n",
        "```\n",
        "\n",
        "**BASIC (Beginners' All-purpose Symbolic Instruction Code)** is a family of general-purpose, high-level programming languages designed for ease of use. The original version was created by John G. Kemeny and Thomas E. Kurtz at Dartmouth College in 1963. They wanted to enable students in non-scientific fields to use computers. At the time, nearly all computers required writing custom software, which only scientists and mathematicians tended to learn.\n",
        "\n",
        "In addition to the programming language, Kemeny and Kurtz developed the Dartmouth Time-Sharing System (DTSS), which allowed multiple users to edit and run BASIC programs simultaneously on remote terminals. This general model became popular on minicomputer systems like the PDP-11 and Data General Nova in the late 1960s and early 1970s. Hewlett-Packard produced an entire computer line for this method of operation, introducing the HP2000 series in the late 1960s and continuing sales into the 1980s. Many early video games trace their history to one of these versions of BASIC.\n",
        "\n",
        "The emergence of microcomputers in the mid-1970s led to the development of multiple BASIC dialects, including Microsoft BASIC in 1975. Due to the tiny main memory available on these machines, often 4 KB, a variety of Tiny BASIC dialects were also created. BASIC was available for almost any system of the era, and became the de facto programming language for home computer systems that emerged in the late 1970s. These PCs almost always had a BASIC interpreter installed by default, often in the machine's firmware or sometimes on a ROM cartridge.\n",
        "\n",
        "BASIC declined in popularity in the 1990s, as more powerful microcomputers came to market and programming languages with advanced features (such as Pascal and C) became tenable on such computers. By then, most nontechnical personal computer users relied on pre-written applications rather than writing their own programs. In 1991, Microsoft released Visual Basic, combining an updated version of BASIC with a visual forms builder. This reignited use of the language and \"VB\" remains a major programming language in the form of VB.NET, while a hobbyist scene for BASIC more broadly continues to exist.\n",
        "\n",
        "**Origin**\n",
        "\n",
        "John G. Kemeny was the chairman of the Dartmouth College Mathematics Department. Based largely on his reputation as an innovator in math teaching, in 1959 the college won an Alfred P. Sloan Foundation award for \\$500,000 to build a new department building. Thomas E. Kurtz had joined the department in 1956, and from the 1960s Kemeny and Kurtz agreed on the need for programming literacy among students outside the traditional STEM fields. Kemeny later noted that \"Our vision was that every student on campus should have access to a computer, and any faculty member should be able to use a computer in the classroom whenever appropriate. It was as simple as that.\"\n",
        "\n",
        "Kemeny and Kurtz had made two previous experiments with simplified languages, DARSIMCO (Dartmouth Simplified Code) and DOPE (Dartmouth Oversimplified Programming Experiment). These did not progress past a single freshman class. New experiments using Fortran and ALGOL followed, but Kurtz concluded these languages were too tricky for what they desired. As Kurtz noted, Fortran had numerous oddly formed commands, notably an \"almost impossible-to-memorize convention for specifying a loop: DO 100, I = 1, 10, 2. Is it '1, 10, 2' or '1, 2, 10', and is the comma after the line number required or not?\"\n",
        "\n",
        "Moreover, the lack of any sort of immediate feedback was a key problem; the machines of the era used batch processing and took a long time to complete a run of a program. While Kurtz was visiting MIT, John McCarthy suggested that time-sharing offered a solution; a single machine could divide up its processing time among many users, giving them the illusion of having a (slow) computer to themselves.[8] Small programs would return results in a few seconds. This led to increasing interest in a system using time-sharing and a new language specifically for use by non-STEM students.\n",
        "\n",
        "Kemeny wrote the first version of BASIC. The acronym BASIC comes from the name of an unpublished paper by Thomas Kurtz.The new language was heavily patterned on FORTRAN II; statements were one-to-a-line, numbers were used to indicate the target of loops and branches, and many of the commands were similar or identical to Fortran. However, the syntax was changed wherever it could be improved. For instance, the difficult to remember DO loop was replaced by the much easier to remember FOR I = 1 TO 10 STEP 2, and the line number used in the DO was instead indicated by the NEXT I. Likewise, the cryptic IF statement of Fortran, whose syntax matched a particular instruction of the machine on which it was originally written, became the simpler IF I=5 THEN GOTO 100. These changes made the language much less idiosyncratic while still having an overall structure and feel similar to the original FORTRAN.\n",
        "\n",
        "The project received a $300,000 grant from the National Science Foundation, which was used to purchase a GE-225 computer for processing, and a Datanet-30 realtime processor to handle the Teletype Model 33 teleprinters used for input and output. A team of a dozen undergraduates worked on the project for about a year, writing both the DTSS system and the BASIC compiler. The first version BASIC language was released on 1 May 1964.\n",
        "\n",
        "Initially, BASIC concentrated on supporting straightforward mathematical work, with matrix arithmetic support from its initial implementation as a batch language, and character string functionality being added by 1965. Usage in the university rapidly expanded, requiring the main CPU to be replaced by a GE-235,[7] and still later by a GE-635. By the early 1970s there were hundreds of terminals connected to the machines at Dartmouth, some of them remotely.\n",
        "\n",
        "Wanting use of the language to become widespread, its designers made the compiler available free of charge. In the 1960s, software became a chargeable commodity; until then, it was provided without charge as a service with expensive computers, usually available only to lease. They also made it available to high schools in the Hanover, New Hampshire, area and regionally throughout New England on Teletype Model 33 and Model 35 teleprinter terminals connected to Dartmouth via dial-up phone lines, and they put considerable effort into promoting the language. In the following years, as other dialects of BASIC appeared, Kemeny and Kurtz's original BASIC dialect became known as Dartmouth BASIC.\n",
        "\n",
        "New Hampshire recognized the accomplishment in 2019 when it erected a highway historical marker in Hanover describing the creation of \"the first user-friendly programming language\".\n",
        "\n",
        "### **The Rise of BASIC: From Dartmouth Labs to the Personal Computer Revolution**\n",
        "\n",
        "> **B**asic **A**lgebraic **S**tandard **T**erm (BASIC) is more than a language—it is the catalyst that turned hobbyist curiosity into a worldwide PC industry.  \n",
        "---\n",
        "\n",
        "#### **1. Birth of BASIC (1964‑1965)**\n",
        "\n",
        "| Year | Milestone | Notes |\n",
        "|------|-----------|-------|\n",
        "| **1964** | Dartmouth College computer science course | John G. Kemeny & Thomas E. Kurtz design a *simple* language for students who had never coded before. |\n",
        "| **1965** | Dartmouth BASIC released | Written in 4 KB of machine code for the IBM 704. Emphasis: **clarity**, **interactivity**, and **line‑numbered scripts**. |\n",
        "\n",
        "### 1.1 Design Goals\n",
        "- **Accessibility**: Use everyday math notation (`LET`, `PRINT`, `GOTO`).\n",
        "- **Education**: Allow students to learn programming concepts without deep hardware knowledge.\n",
        "- **Rapid Prototyping**: Quick translation from idea to executable script.\n",
        "\n",
        "> *Quote*: “We wanted a language that *could* be used by a person with no background in programming to write useful programs in a reasonable amount of time.” – *Kemeny & Kurtz*\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Early Evolution & Variants**\n",
        "\n",
        "| Variant | Platform | Key Features | Year |\n",
        "|---------|----------|--------------|------|\n",
        "| **Algol‑60‑derived BASIC** | Dartmouth 704, 7090 | First *non‑line‑numbered* mode (interactive) | 1965 |\n",
        "| **BASIC‑PLUS** | DEC PDP‑8 | Introduced subroutines, `ON GOSUB` | 1967 |\n",
        "| **Microsoft BASIC** | Altair 8800 (assembly) | Commercialized BASIC; later ported to DOS | 1975 |\n",
        "| **GW‑BASIC** | IBM PC | Microsoft’s *free* BASIC in DOS 1.0 | 1980 |\n",
        "| **AppleSoft BASIC** | Apple II | Built‑in `FOR…NEXT`, `READ…DATA` | 1977 |\n",
        "| **BBC BASIC** | BBC Micro | Graphics, sound, and structured programming support | 1981 |\n",
        "| **Turbo BASIC** | Turbo Pascal & Turbo C* | Fast interpreter on DOS | 1985 |\n",
        "\n",
        "*Note:* Most dialects kept the **line‑numbered** format, which was both a strength (easy debugging) and a weakness (inefficient).\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. BASIC in the Education & Hobbyist Scene**\n",
        "\n",
        "##### 3.1 The “BASIC People”\n",
        "- A generation of programmers grew up writing code in BASIC.\n",
        "- Encouraged *self‑education* and *problem‑solving* outside formal settings.\n",
        "\n",
        "##### 3.2 First‑hand Experience\n",
        "```basic\n",
        "10 PRINT \"HELLO, WORLD!\"\n",
        "20 GOTO 10\n",
        "```\n",
        "\n",
        "## **Simplicity and Immediate Feedback**\n",
        "- **Simplicity**: One line of code, one command, no compilation.  \n",
        "- **Immediate feedback**: The interpreter echoed output instantly.\n",
        "\n",
        "#### **3.3 Software & Games**\n",
        "- Early microcomputer game studios (e.g., Sierra, Origin Systems) started in BASIC.  \n",
        "- BASIC was the lingua franca for shareware and homebrew games.\n",
        "\n",
        "#### **4. BASIC Meets the Microcomputer Revolution**\n",
        "\n",
        "| Microcomputer | BASIC Variant | Impact |\n",
        "|---------------|---------------|--------|\n",
        "| Altair 8800 | Altair BASIC (Microsoft) | Sparked the Altair BASIC craze; early PC kit sales. |\n",
        "| Apple II | Applesoft BASIC | Built‑in BASIC; many users bought the Apple II solely for the built‑in interpreter. |\n",
        "| Commodore PET | Commodore BASIC | Bundled with all PET models; the first home computer with a full keyboard and monitor. |\n",
        "| TRS‑80 | Tandy BASIC | Made programming accessible to hobbyists; popular in the “Micro‑computer” market. |\n",
        "| IBM PC (1981) | Microsoft’s GW‑BASIC | Became part of the default DOS installation, ensuring a ready‑made audience for PC software. |\n",
        "\n",
        "#### **5. Why BASIC Was Key to the PC Revolution**\n",
        "\n",
        "| Factor | How BASIC Contributed |\n",
        "|--------|-----------------------|\n",
        "| Low Barrier to Entry | Students and hobbyists could write simple programs without mastering machine code. |\n",
        "| Rapid Development | Interactive loop allowed quick prototyping → more software in the market. |\n",
        "| Learning Path | BASIC served as a stepping stone to languages like Pascal, C, and later Java/Python. |\n",
        "| Ecosystem Creation | A huge library of demos, utilities, and games created a “software market” for PCs. |\n",
        "| Cultural Momentum | The phrase “I wrote a BASIC program” became part of tech folklore, fostering a generation of self‑taught coders. |\n",
        "\n",
        "#### **6. The Decline of BASIC and Its Legacy**\n",
        "\n",
        "| Period | Change | Outcome |\n",
        "|--------|--------|---------|\n",
        "| Early 1990s | Introduction of C/C++, Visual Basic, and Java | BASIC’s role as mainstream teaching language diminished. |\n",
        "| Late 1990s | Emphasis on structured programming | Many BASIC dialects replaced by VB or VB.NET for business apps. |\n",
        "| 2000s‑Present | Python & JavaScript | Python’s readability echoes BASIC’s ethos; it is now the primary teaching language in many curricula. |\n",
        "\n",
        "**Legacy**\n",
        "\n",
        "- BASIC’s design principles (interactive, readable syntax) influenced later languages.  \n",
        "- Microsoft’s Visual Basic introduced the event‑driven GUI paradigm, a staple in modern programming.\n",
        "\n",
        "#### **7. Quick Reference: Sample BASIC Code**\n",
        "\n",
        "```basic\n",
        "REM Simple calculator\n",
        "10 PRINT \"Enter first number: \";\n",
        "20 INPUT A\n",
        "30 PRINT \"Enter second number: \";\n",
        "40 INPUT B\n",
        "50 PRINT \"Sum is \"; A + B\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYdsR4-MH9uu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}