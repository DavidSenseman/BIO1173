{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_03_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- BIO1173_CLASS_03_2 -->"
      ],
      "metadata": {
        "id": "buPhvUm363S0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYZVwSpdbE3Y"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExN-OzpYbE3Y"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4imk1kbE3Y"
      },
      "source": [
        "##### **Module 3: Convolutional Neural Networks (CNN's)**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 3 Material\n",
        "\n",
        "* Part 3.1: Using Convolutional Neural Networks\n",
        "* **Part 3.2: Using Pre-Trained Neural Networks with PyTorch**\n",
        "* Part 3.3: Facial Recognition and Analysis\n",
        "* Part 3.4: Introduction to GAN's for Image and Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ult76BB_wSzg"
      },
      "source": [
        "#### **Change your Runtime Now!**\n",
        "\n",
        "For this lesson you must have a GPU hardware accelerator (e.g. `A100` if available)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_-lPkxLbE3Z"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# @title You MUST Run This Cell First\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    !curl ipinfo.io\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG3_sXTDfyjA"
      },
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "```text\n",
        "Mounted at /content/drive\n",
        "Note: Using Google CoLab\n",
        "{\n",
        "  \"ip\": \"34.19.16.177\",\n",
        "  \"hostname\": \"177.16.19.34.bc.googleusercontent.com\",\n",
        "  \"city\": \"The Dalles\",\n",
        "  \"region\": \"Oregon\",\n",
        "  \"country\": \"US\",\n",
        "  \"loc\": \"45.5946,-121.1787\",\n",
        "  \"org\": \"AS396982 Google LLC\",\n",
        "  \"postal\": \"97058\",\n",
        "  \"timezone\": \"America/Los_Angeles\",\n",
        "  \"readme\": \"https://ipinfo.io/missingauth\"\n",
        "}studentbio1173@gmail.com\n",
        "``\n",
        "If your GMAIL address does not appear. **Electronic Submission** will not process your Colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYHuoONPZGtV"
      },
      "source": [
        "# **----->TIME ALERT!<-----**\n",
        "\n",
        "This lesson will probably **require close to 3 hours to complete**. Besides the normal issues there are two instances in which you are required to train a neural network. Training time for both neural networks is about 1 hour.\n",
        "\n",
        "Don't start working on this lesson if you don't have sufficient free time to finish it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKhQzBV1wu2v"
      },
      "source": [
        "# Accelerated Run-time Check\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. The code in this cell checks what hardware acceleration you are using. To run this lesson, you must be running a Graphics Processing Unit (GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kty-X7j9tDn"
      },
      "outputs": [],
      "source": [
        "# @title Accelerated Run-time Check\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check for GPU\n",
        "def check_colab_gpu():\n",
        "    print(\"=== Colab GPU Check ===\")\n",
        "\n",
        "    # Check PyTorch\n",
        "    pt_gpu = torch.cuda.is_available()\n",
        "    print(f\"PyTorch GPU available: {pt_gpu}\")\n",
        "\n",
        "    if pt_gpu:\n",
        "        print(f\"PyTorch device count: {torch.cuda.device_count()}\")\n",
        "        print(f\"PyTorch current device: {torch.cuda.current_device()}\")\n",
        "        print(f\"PyTorch device name: {torch.cuda.get_device_name()}\")\n",
        "        print(\"You are good to go!\")\n",
        "\n",
        "    else:\n",
        "        print(\"No compatible device found\")\n",
        "        print(\"WARNING: You must run this assigment using either a GPU to earn credit\")\n",
        "        print(\"Change your RUNTIME now and start over!\")\n",
        "\n",
        "check_colab_gpu()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIAs3kcq-WSt"
      },
      "source": [
        "If you current `Runtime` is correct you should see the following output\n",
        "\n",
        "```text\n",
        "=== Colab GPU Check ===\n",
        "PyTorch GPU available: True\n",
        "PyTorch device count: 1\n",
        "PyTorch current device: 0\n",
        "PyTorch device name: Tesla T4\n",
        "You are good to go!\n",
        "```\n",
        "\n",
        "If your output is different, don't continue until change your `Runtime`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu5xJAWl_9vZ"
      },
      "source": [
        "# Create Custom Functions\n",
        "\n",
        "The cell below creates custom function that are required to run this Colab notebook correctly.\n",
        "\n",
        "If you fail to run this cell now, you will receive one (or more) error message(s) later in this lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STtFj1QKTVcL"
      },
      "outputs": [],
      "source": [
        "# @title Create Custom Functions\n",
        "\n",
        "\n",
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "\n",
        "# Set random seed\n",
        "def set_seed():\n",
        "    \"\"\"\n",
        "    Sets the seed for reproducibility across Python, NumPy, and PyTorch.\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    # Set seed value\n",
        "    seed_value=1173\n",
        "\n",
        "    # 1. Base Python\n",
        "    random.seed(seed_value)\n",
        "\n",
        "    # 2. NumPy\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    # 3. PyTorch (CPU and CUDA)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # For multi-GPU setups\n",
        "\n",
        "    # 4. CuDNN Determinism (Crucial for GPU consistency)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # 5. Ensure all operations are deterministic\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "print(f\"✅ All custom functions have been created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "```\n",
        "✅ All custom functions have been created.\n",
        "```"
      ],
      "metadata": {
        "id": "ekncUhZY9JFX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7_mOnlRZ7m1"
      },
      "source": [
        "### **YouTube Introduction to ResNet**\n",
        "\n",
        "Run the next cell to see short YouTube introduction to ResNet. This is a suggested, but optional, part of the lesson."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"nc7FzLiB_AY\"\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\"> </iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "C--GmMB2aVfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5gvsd-lxR3_"
      },
      "source": [
        "# **DOWNLOAD AND INSTALL PRE-TRAINED NEURAL NETWORKS**\n",
        "\n",
        "We will be using two pre-trained neural networks in this lesson, `ResNet50` and `ResNet101`. Run the next couple of code cells to download these neural networks to your Colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YQBCLR3ZGil"
      },
      "source": [
        "# Download `ResNet50`\n",
        "\n",
        "Run the code in the cell below to download the ResNet101 model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download ResNet50\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create a progress bar\n",
        "with tqdm(total=1, desc=\"Downloading ResNet50\") as pbar:\n",
        "    # Download ResNet50 with pre-trained weights\n",
        "    weights = models.ResNet50_Weights.DEFAULT\n",
        "    ResNet50_model_244 = models.resnet50(weights=weights)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    ResNet50_model_244.eval()\n",
        "\n",
        "    # Update progress bar\n",
        "    pbar.update(1)\n",
        "\n",
        "# Print out results\n",
        "print(\"\\n✅ ResNet50 has been downloaded.\")\n"
      ],
      "metadata": {
        "id": "tYGPS4vm-2cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "```text\n",
        "Downloading ResNet50:   0%|          | 0/1 [00:00<?, ?it/s]Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
        "\n",
        "  0%|          | 0.00/97.8M [00:00<?, ?B/s]\n",
        " 18%|█▊        | 17.6M/97.8M [00:00<00:00, 183MB/s]\n",
        " 44%|████▎     | 42.6M/97.8M [00:00<00:00, 230MB/s]\n",
        " 69%|██████▉   | 67.2M/97.8M [00:00<00:00, 242MB/s]\n",
        "100%|██████████| 97.8M/97.8M [00:00<00:00, 240MB/s]\n",
        "Downloading ResNet50: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n",
        "✅ ResNet50 has been downloaded.\n",
        "```"
      ],
      "metadata": {
        "id": "9-TvbxpRAluW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download ResNet101\n",
        "\n",
        "Run the code in the cell below to download the ResNet101 model."
      ],
      "metadata": {
        "id": "gp9byLWdaxQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download ResNet101\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create a progress bar\n",
        "with tqdm(total=1, desc=\"Downloading ResNet101\") as pbar:\n",
        "    # Download ResNet50 with pre-trained weights\n",
        "    weights = models.ResNet101_Weights.DEFAULT\n",
        "    ResNet101_model_512 = models.resnet101(weights=weights)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    ResNet101_model_512.eval()\n",
        "\n",
        "    # Update progress bar\n",
        "    pbar.update(1)\n",
        "\n",
        "# Print out results\n",
        "print(\"\\n✅ ResNet101 has been downloaded.\")"
      ],
      "metadata": {
        "id": "fs9ZGUWt_KSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "```text\n",
        "Downloading ResNet101:   0%|          | 0/1 [00:00<?, ?it/s]Downloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth\n",
        "\n",
        "  0%|          | 0.00/171M [00:00<?, ?B/s]\n",
        " 14%|█▎        | 23.2M/171M [00:00<00:00, 243MB/s]\n",
        " 28%|██▊       | 48.2M/171M [00:00<00:00, 254MB/s]\n",
        " 43%|████▎     | 73.8M/171M [00:00<00:00, 260MB/s]\n",
        " 58%|█████▊    | 98.6M/171M [00:00<00:00, 257MB/s]\n",
        " 72%|███████▏  | 123M/171M [00:00<00:00, 255MB/s]\n",
        "100%|██████████| 171M/171M [00:00<00:00, 257MB/s]\n",
        "Downloading ResNet101: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
        "✅ ResNet101 has been downloaded.\n",
        "````"
      ],
      "metadata": {
        "id": "uomVeaIHA7bW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMqH7jsZuuz"
      },
      "source": [
        "# Install Helper Functions for Examples and **Exercises**\n",
        "\n",
        "The code in the cell below creates a two functions that we will need to use classify images in Examples 1 and 2.\n",
        "\n",
        "* **make_square()** Since MobileNet is designed to classify images with the same number of horizontal and vertical pixels (i.e. a 'square' image), this function uses a combination of padding and cropping to convert any image into a 'square` image.\n",
        "\n",
        "* **classify_image()** This function does most of the work. It first retrives the image from the HTTPS server and resizes it before processing it by the `ResNet50 model` that we previously downloaded. The actual prediction is made by this specific line of code:\n",
        "```python\n",
        "    prediction = ResNet_model_244(batch).squeeze(0)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Install Helper Functions for Examples and Exercises\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from PIL import Image, ImageFile, UnidentifiedImageError\n",
        "import requests\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from IPython.display import display\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Global settings\n",
        "# ----------------------------------------------------------------------\n",
        "IMAGE_SIZE = 224\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
        "\n",
        "# Load weights and model\n",
        "weights = models.ResNet50_Weights.DEFAULT\n",
        "ResNet_model_244 = models.resnet50(weights=weights)\n",
        "ResNet_model_244.eval() # Set to inference mode\n",
        "\n",
        "# Access the category labels and the specific preprocessing for these weights\n",
        "categories = weights.meta[\"categories\"]\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# Base URL of the images\n",
        "ROOT = \"https://biologicslab.co/BIO1173/images/class_03/\"\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Utility functions\n",
        "# ----------------------------------------------------------------------\n",
        "def make_square(img):\n",
        "    \"\"\"Crop the image to a square (center‑aligned).\"\"\"\n",
        "    width, height = img.size\n",
        "    side = min(width, height)\n",
        "    left   = (width  - side) // 2\n",
        "    top    = (height - side) // 2\n",
        "    right  = left + side\n",
        "    bottom = top  + side\n",
        "    return img.crop((left, top, right, bottom))\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Core function\n",
        "# ----------------------------------------------------------------------\n",
        "def classify_image(url):\n",
        "    \"\"\"\n",
        "    Download an image, preprocess with Torchvision transforms,\n",
        "    run through ResNet‑50, and display results.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return\n",
        "\n",
        "    # 1. Standardize the image (optional: use make_square first)\n",
        "    img_square = make_square(img)\n",
        "\n",
        "    # 2. Apply the weight-specific preprocessing (Resizing, Normalizing, and Tensor conversion)\n",
        "    # PyTorch expects a batch dimension: [Batch, Channels, Height, Width]\n",
        "    batch = preprocess(img_square).unsqueeze(0)\n",
        "\n",
        "    # 3. Predict\n",
        "    with torch.no_grad(): # Disable gradient calculation for efficiency\n",
        "        prediction = ResNet_model_244(batch).squeeze(0)\n",
        "        # Apply Softmax to get probabilities\n",
        "        probs = torch.nn.functional.softmax(prediction, dim=0)\n",
        "\n",
        "    # 4. Get Top-5\n",
        "    top5_prob, top5_catid = torch.topk(probs, 5)\n",
        "\n",
        "    # Show the image\n",
        "    display(img_square.resize((IMAGE_SIZE, IMAGE_SIZE)))\n",
        "\n",
        "    # Print the top‑5 predictions\n",
        "    print(\"\\nTop‑5 predictions:\")\n",
        "    for i in range(top5_prob.size(0)):\n",
        "        label = categories[top5_catid[i]]\n",
        "        score = top5_prob[i].item()\n",
        "        print(f\"  {label:<25} : {score*100:5.2f}%\")\n",
        "\n",
        "# Print out results\n",
        "print(f\"✅ Helper functions have been created.\")"
      ],
      "metadata": {
        "id": "9ybEC9ENtUz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbe54CjtWtSS"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "```text\n",
        "✅ Helper functions have been created.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiYyHO41Zurj"
      },
      "source": [
        "# Example 1: Classify Images with ResNet50\n",
        "\n",
        "The code in the cell below downloads an image of a dog from the course fileserver, https://biologicslab.co and then uses the pre-trained `ResNet50` neural network to to classify it.\n",
        "\n",
        "The image name is \"pembroke_corgi.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 1: Classify Image with ResNet50\n",
        "\n",
        "# Enter image name\n",
        "image_name = \"pembroke_corgi.jpg\"\n",
        "\n",
        "# Generate image path using the ROOT variable defined earlier\n",
        "image_path = ROOT + image_name\n",
        "\n",
        "# Print path\n",
        "print(f\"Analyzing {image_path}\\n\")\n",
        "\n",
        "# Use the PyTorch-based classify_image function\n",
        "classify_image(image_path)"
      ],
      "metadata": {
        "id": "NUDs-xdgtqJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWiRgSgP0lPG"
      },
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/pembroke_corgi.jpg)\n",
        "```text\n",
        "Top‑5 predictions:\n",
        "  Pembroke                  : 35.55%\n",
        "  Cardigan                  :  5.37%\n",
        "  Bernese mountain dog      :  0.32%\n",
        "  collie                    :  0.29%\n",
        "  Shetland sheepdog         :  0.24%\n",
        "```\n",
        "\n",
        "`ResNet50` has been trained to recognize a wide range of common objectes. Here is a basic list:\n",
        "\n",
        "| Domain                           | Example classes (just a few from each)                                   |\n",
        "|----------------------------------|--------------------------------------------------------------------------|\n",
        "| Animals – mammals                | chimpanzee, tiger, lion, zebra, elephant                                |\n",
        "| Animals – birds                  | eagle, sparrow, penguin, flamingo, crane                                 |\n",
        "| Animals – reptiles & amphibians  | alligator, snake, frog, turtle                                          |\n",
        "| Animals – fish & crustaceans     | goldfish, salmon, shrimp, lobster                                       |\n",
        "| Animals – insects                | bee, butterfly, ant, dragonfly                                          |\n",
        "| Plants & flowers                 | daisy, sunflower, orchid, cactus                                        |\n",
        "| Fruits & vegetables              | apple, orange, broccoli, carrot                                         |\n",
        "| Vegetables & nuts                | potato, tomato, almond, cashew                                          |\n",
        "| Instruments & musical equipment  | guitar, piano, violin, saxophone                                        |\n",
        "| Sports & equipment               | tennis racket, golf ball, basketball, soccer ball                       |\n",
        "| Vehicles – land                  | car, truck, motorcycle, bicycle                                         |\n",
        "| Vehicles – air                   | airplane, helicopter, jet, glider                                       |\n",
        "| Vehicles – water                 | boat, ship, submarine, ferry                                            |\n",
        "| Buildings & architecture         | bridge, house, church, skyscraper                                       |\n",
        "| Furniture                        | chair, table, sofa, bed                                                  |\n",
        "| Home appliances                  | microwave, refrigerator, toaster, blender                               |\n",
        "| Tools & hardware                 | hammer, screwdriver, wrench, drill                                      |\n",
        "| Clothing & accessories           | t‑shirt, hat, shoes, glasses                                            |\n",
        "| Food & drinks                    | coffee, tea, pizza, cake                                                |\n",
        "| Miscellaneous                    | toilet paper, keyboard, watch, trophy                                   |\n",
        "\n",
        "\n",
        "Clearly `ResNet50` was trained to classify dogs. What is somewhat interesting, is that `ResNet50` appears to be quite good as to correctly identify a dog's breed. `ResNet50` was absolutedly correct that the image showed a `Welsh Pembroke Corgi`.\n",
        "\n",
        "The **Pembroke Welsh Corgi** is a spirited, compact herding dog originally from Wales. Known for its short legs, fox-like ears, and expressive eyes, it's affectionate, intelligent, and highly trainable. Pembrokes thrive on activity, enjoy family life, and are renowned for their loyal, playful nature and distinctive “corgi grin.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKO_1OUi8rCz"
      },
      "source": [
        "### **Exercise 1: Classify Images with ResNet50**\n",
        "\n",
        "In the cell below write the code to download an another dog image from the course fileserver:\n",
        "\n",
        "```type\n",
        "image_name=\"bouvier_des_flandres.jpg\"\n",
        "```\n",
        "and then uses `ResNet50` to classify it. This species is much less common than Corgis.\n",
        "\n",
        "The **Bouvier des Flandres**, or **Bouvier de Flanders**, is a muscular, medium‑to‑large herding dog originally bred in the Flemish region of Belgium to manage cattle, sheep and pack loads. With a dense, double‑coated coat that comes in black, brown, red or tricolor, they are built for endurance and can thrive in both wet and dry climates. Their temperament is confident and affectionate, yet they possess a strong work ethic and a naturally protective instinct, making them excellent companion animals as well as valuable in search‑and‑rescue, therapy, and police work. Bouviers are intelligent and trainable, but they require consistent socialization and mental stimulation to prevent stubbornness or frustration. Health concerns are relatively few—chiefly hip dysplasia, gastric dilatation‑volvulus (bloat) and certain eye conditions—so with proper care, they can live 10–12 years."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n",
        "# Enter image name\n",
        "image_name = \"bouvier_des_flandres.jpg\"\n",
        "\n",
        "# Generate image path\n",
        "image_path = ROOT + image_name\n",
        "\n",
        "# Print path\n",
        "print(f\"Analyzing {image_path}\")\n",
        "\n",
        "# Use the PyTorch-based classify_image function\n",
        "classify_image(image_path)"
      ],
      "metadata": {
        "id": "pTweNY_8t_kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWXHzH8o8rC0"
      },
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/bouvier_des_flandres.jpg)\n",
        "```text\n",
        "Top‑5 predictions:\n",
        "  Bouvier des Flandres      : 25.45%\n",
        "  Irish water spaniel       : 14.09%\n",
        "  briard                    :  2.02%\n",
        "  Tibetan terrier           :  1.59%\n",
        "  miniature poodle          :  1.37%\n",
        "```\n",
        "\n",
        "Again, `ResNet50` got the specific dog breed correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unspA6MzAzOA"
      },
      "source": [
        "# Example 2: Classify Retinal Image with ResNet50\n",
        "\n",
        "What about medical image data? Can `ResNet50` analyze a **color fundus photograph** of the interior surface of a human retina?\n",
        "\n",
        "Run the code in the Example 2 to see how `ResNet50` does with the following retinal image:\n",
        "\n",
        "```text\n",
        "\"Retina_Score_0.png\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 2: Classify Retinal Image with ResNet50\n",
        "\n",
        "# Enter image name\n",
        "image_name = \"Retina_Score_0.png\"\n",
        "\n",
        "# Generate image path\n",
        "image_path = ROOT + image_name\n",
        "\n",
        "# Print path\n",
        "print(f\"Analyzing {image_path}\")\n",
        "\n",
        "# Use the PyTorch-based classify_image function\n",
        "classify_image(image_path)"
      ],
      "metadata": {
        "id": "zYQ7Fy-wucHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBP5nv_i_E8o"
      },
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/Retina_Score_0.png)\n",
        "```text\n",
        "Top‑5 predictions:\n",
        "  chambered nautilus        : 33.39%\n",
        "  jellyfish                 : 10.28%\n",
        "  lampshade                 :  2.86%\n",
        "  nematode                  :  2.79%\n",
        "  butternut squash          :  1.25%\n",
        "```\n",
        "\n",
        "As expected, `ResNet50` was not trained to recognize retinal images. For this particular retinal image, `ResNet50` best guess (33.39% probability) that this image was a chambered nautilus. Here is one image of a chambered nautilus that is vaguely similar:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/chambered_nautilus.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvc2IU6q6gmc"
      },
      "source": [
        "# **Exercise 2: Classify Retinal Image with `ResNet50`**\n",
        "\n",
        "In the cell below write the code to analyze the another retinal color fundus image:\n",
        "```text\n",
        "\"Retina_Score_1.png\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 2: Classify Retinal Image with ResNet50\n",
        "\n",
        "# Enter image name\n",
        "image_name = \"Retina_Score_1.png\"\n",
        "\n",
        "# Generate image path\n",
        "image_path = ROOT + image_name\n",
        "\n",
        "# Print path\n",
        "print(f\"Analyzing {image_path}\")\n",
        "\n",
        "# Use the PyTorch-based classify_image function\n",
        "classify_image(image_path)"
      ],
      "metadata": {
        "id": "81rubn1Ruzs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L694Na2AzOB"
      },
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_03/Retina_Score_1.png)\n",
        "\n",
        "```text\n",
        "Top‑5 predictions:\n",
        "  chambered nautilus        : 16.00%\n",
        "  shield                    : 10.03%\n",
        "  gong                      :  3.76%\n",
        "  jellyfish                 :  3.72%\n",
        "  jack-o'-lantern           :  2.81%\n",
        "```\n",
        "\n",
        "Once again, it would be charitable to say that `ResNet50` was a little weak when it comes to analyzing clinical images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCjV86HnxkND"
      },
      "source": [
        "# **Transfer Learning for Computer Vision**\n",
        "\n",
        "## **`ResNet`**\n",
        "\n",
        "Many advanced prebuilt neural networks are available for computer vision, and Keras provides direct access to many networks. **Transfer Learning** is the technique where you use these prebuilt neural networks.\n",
        "\n",
        "There are several different levels of transfer learning.\n",
        "\n",
        "* Use a prebuilt neural network in its entirety\n",
        "* Use a prebuilt neural network's structure\n",
        "* Use a prebuilt neural network's weights\n",
        "\n",
        "In this lesson we will use a popular prebuilt CNN called **`ResNet (Residual Network)`** built by Microsoft Research in 2015. The name comes from the fact that this network was designed to address the **vanishing gradient problem** that occurs when training very deep neural networks.\n",
        "\n",
        "Instead of learning the direct mapping from input to output, `ResNet` learns the residual (i.e., the difference between the input and the output).\n",
        "This is achieved using **skip connections** (also called shortcut connections), which allow the input to bypass one or more layers and be added directly to the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9O3j51Kh6yS"
      },
      "source": [
        "## **Transfer Learning**\n",
        "\n",
        "**Transfer learning** is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second task. In the context of image recognition, this approach is particularly effective when using deep convolutional neural networks (CNNs) like **`ResNet`**.\n",
        "\n",
        "### **Why Use Transfer Learning?**\n",
        "\n",
        "Training deep neural networks from scratch requires large datasets and significant computational resources. Transfer learning mitigates this by leveraging pre-trained models—typically trained on large benchmark datasets like ImageNet—to extract general features from images. These features can then be fine-tuned for a specific task with a smaller, domain-specific dataset.\n",
        "\n",
        "### **How `ResNet` Supports Transfer Learning**\n",
        "\n",
        "`ResNet` is a widely used CNN architecture known for its use of **residual connections**, which help in training very deep networks by addressing the vanishing gradient problem. Pre-trained versions of `ResNet` (e.g., `ResNet-50`, `ResNet-101`) are commonly used as feature extractors in transfer learning workflows.\n",
        "\n",
        "### **Benefits**\n",
        "\n",
        "- **Reduced Training Time**: Leverages existing learned features.\n",
        "- **Improved Performance**: Often achieves better accuracy with less data.\n",
        "- **Flexibility**: Can be adapted to a wide range of image classification tasks.\n",
        "\n",
        "Transfer learning with `ResNet` is a powerful and efficient approach for developing high-performing image recognition models, especially when data or computational resources are limited.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2qSKv8AyFMn"
      },
      "source": [
        "# Example 3: Improving `ResNet` Classification Accuracy of Diabetic Retinopathy\n",
        "\n",
        "We know from from above that the native (base) `ResNet50` neural network is unable to effectively analyze clinical retinal images.\n",
        "\n",
        "Example 3 will demonstrate how **transfer learning** can leverage `ResNet50` as a starting point to create a new neural network that can effectively classify retinal images as to their degree of diabetic retinopathy.  \n",
        "\n",
        "#### **`Diabetic Retinopathy Image Dataset` Classes**\n",
        "\n",
        "The dataset we will be using is called the **`Diabetic Retinopathy Image Dataset`**. This image dataset consists of color fundus photographs—high‑resolution RGB images of the interior surface of the eye (the retina).\n",
        "\n",
        "These are standard clinical retinal images obtained with a fundus camera (usually a 45° or 50° field-of-view, non‑mydriatic or mydriatic camera) and capture the posterior pole (macula, optic disc, retinal vessels, and surrounding retinal tissue). The images are typically stored as JPEG/PNG files with dimensions on the order of several thousand pixels (e.g., 3500x2333 px) and are used for grading the severity of diabetic retinopathy.\n",
        "\n",
        "The severity of diabetic retinopathy was divided into **five classes** as follows:\n",
        "1. **Class 0** - No Diabetic Retinopathy (No_DR)\n",
        "2. **Class 1** - Mild Non-Proliferative Diabetic Retinopathy (NPDR)\n",
        "3. **Class 2** - Moderate NPDR\n",
        "4. **Class 3** - Severe NPDR\n",
        "5. **Class 4** - Proliferative Diabetic Retinopathy (PDR)\n",
        "\n",
        "An example of each class is shown below:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/DiabeticRetinopathyData.png)\n",
        "\n",
        "It should be noted that differences in apparent retinal size has nothing to do with the degree of retinopathy but instead reflects the fact that the retinal images were taken by a large number of clinicians, with different imaging equipment and procedures. In other words, there is considerable \"noise\" in the image set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use `torchvision` for Data Loading\n",
        "\n",
        "For Example 3 and **Exercise 3**, we will use **Torchvision** to handle image loading and preprocessing. In PyTorch, image generation is handled by a combination of `torchvision.transforms` (for image manipulation) and `torchvision.datasets` (for loading images from directories).\n",
        "\n",
        "Since `torchvision` is part of the standard PyTorch installation, no additional package installs are usually required."
      ],
      "metadata": {
        "id": "Nlb1P8VUvhrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch and Torchvision\n",
        "!pip install -q torch torchvision"
      ],
      "metadata": {
        "id": "nWmDFZ4ovP3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QtItokWvZM5"
      },
      "source": [
        "If the code is correct you should _not_ see any output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LFjCGJIpjat"
      },
      "source": [
        "# Example 3 - Step 1: Set ENVIRONMENTAL VARIABLES\n",
        "\n",
        "The code in the cell below defines a number of `ENVIRONMENTAL VARIABLES` that are needed for later code cells.\n",
        "\n",
        "The use of **Enivornment Variables** can allow code to be **configurable without modification**. For example, you might use different database URLs for development, testing, and production environments. As you will see later, you will re-use this code cell for **Exercise 3 - Step 1** below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D5kdxBeJeCT"
      },
      "outputs": [],
      "source": [
        "# @title Example 3 - Step 1: Set ENVIRONMENTAL VARIABLES\n",
        "\n",
        "import os\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 1️⃣  Create variables for downloading loading Zip file\n",
        "# ------------------------------------------------------------------------\n",
        "URL = \"https://biologicslab.co/BIO1173/data/\"\n",
        "DOWNLOAD_SOURCE = URL+\"diabetic_retinopathy_train_244.zip\"\n",
        "DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfind('/')+1:]\n",
        "print(\"DOWNLOAD_SOURCE=\",DOWNLOAD_SOURCE)\n",
        "print(\"DOWNLOAD_NAME=\",DOWNLOAD_NAME)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 2️⃣  Create variables for extracting the Zip file\n",
        "# ------------------------------------------------------------------------\n",
        "PATH = \"./\"\n",
        "EXTRACT_TARGET = os.path.join(PATH,\"retinopathy_244\")\n",
        "SOURCE = os.path.join(EXTRACT_TARGET, \"train_244\")\n",
        "print(\"EXTRACT_TARGET=\",EXTRACT_TARGET)\n",
        "print(\"SOURCE=\",SOURCE)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# 3️⃣  Print variables for debugging\n",
        "# ------------------------------------------------------------------------\n",
        "print(\"ENVIRONMENTAL VARIABLES were successfully created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97lHNy8gLTu-"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "```text\n",
        "DOWNLOAD_SOURCE= https://biologicslab.co/BIO1173/data/diabetic_retinopathy_train_244.zip\n",
        "DOWNLOAD_NAME= diabetic_retinopathy_train_244.zip\n",
        "EXTRACT_TARGET= ./retinopathy_244\n",
        "SOURCE= ./retinopathy_244/train_244\n",
        "ENVIRONMENTAL VARIABLES were successfully created.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LfOSfJZpWB-"
      },
      "source": [
        "# Example 3 - Step 2: Download and Extract Image Data\n",
        "\n",
        "The code in the cell below downloads a zip file and extracts it. The names of the file server, zip file and folders were set above as **`ENVIRONMENTAL VARIABLES`** in the previous step.\n",
        "\n",
        "**TIME ALERT:** Even when compressed (i.e. \"zipped\"), image data file are typically quite large so download and extraction times can often take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrUA_JcKb7qO"
      },
      "outputs": [],
      "source": [
        "# @title Example 3 - Step 2: Download and Extract Image Data\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 1️⃣  Create directories\n",
        "# --------------------------------------------------------------\n",
        "print(\"Creating necessary directories...\", end='')\n",
        "# Create necessary directories\n",
        "os.makedirs(SOURCE, exist_ok=True)\n",
        "os.makedirs(EXTRACT_TARGET, exist_ok=True)\n",
        "print(\"done.\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 2️⃣  Download Zip file\n",
        "# --------------------------------------------------------------\n",
        "print(f\"Downloading {DOWNLOAD_NAME}...\", end='')\n",
        "# Define paths and URLs\n",
        "download_path = os.path.join(PATH, DOWNLOAD_NAME)\n",
        "extract_path = os.path.join(EXTRACT_TARGET, DOWNLOAD_NAME)\n",
        "# Download the file\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [\"wget\", \"-O\", DOWNLOAD_NAME, DOWNLOAD_SOURCE],\n",
        "        check=True,\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    print(\"done.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Download failed with error: {e}\")\n",
        "    print(f\"Error output: {e.stderr}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 3️⃣  Extract Zip file\n",
        "# --------------------------------------------------------------\n",
        "print(f\"Extracting {DOWNLOAD_NAME} to {EXTRACT_TARGET}...\", end='')\n",
        "\n",
        "# Check if zip file exists and has content\n",
        "if not os.path.exists(DOWNLOAD_NAME):\n",
        "    print(f\"Error: Zip file {DOWNLOAD_NAME} does not exist\")\n",
        "    sys.exit(1)\n",
        "\n",
        "if os.path.getsize(DOWNLOAD_NAME) == 0:\n",
        "    print(f\"Error: Zip file {DOWNLOAD_NAME} is empty\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Extract the file with error handling\n",
        "try:\n",
        "    # Use -o flag to overwrite files without prompting\n",
        "    # Use -q for quiet mode\n",
        "    result = subprocess.run(\n",
        "        [\"unzip\", \"-o\", \"-q\", DOWNLOAD_NAME, \"-d\", EXTRACT_TARGET],\n",
        "        check=True,\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    print(\"done.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error: Unzipping failed with return code {e.returncode}\")\n",
        "    print(f\"Error output: {e.stderr}\")\n",
        "    print(f\"Command that failed: {e.cmd}\")\n",
        "    sys.exit(1)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: unzip command not found. Please install unzip.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4️⃣  Verify Extraction was successful\n",
        "# --------------------------------------------------------------\n",
        "print(\"Verifying Extraction...\")\n",
        "try:\n",
        "    # Check if extraction directory exists\n",
        "    if not os.path.exists(EXTRACT_TARGET):\n",
        "        print(f\"Error: Extraction directory {EXTRACT_TARGET} does not exist\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # List contents to verify\n",
        "    contents = os.listdir(EXTRACT_TARGET)\n",
        "    if len(contents) == 0:\n",
        "        print(\"Warning: Extraction directory is empty\")\n",
        "    else:\n",
        "        print(f\"Successfully extracted {len(contents)} items:\")\n",
        "        for item in sorted(contents)[:10]:  # Show first 10 items\n",
        "            item_path = os.path.join(EXTRACT_TARGET, item)\n",
        "            if os.path.isfile(item_path):\n",
        "                size = os.path.getsize(item_path)\n",
        "                print(f\"  - {item} ({size} bytes)\")\n",
        "            else:\n",
        "                print(f\"  - {item} (directory)\")\n",
        "\n",
        "        if len(contents) > 10:\n",
        "            print(f\"  ... and {len(contents) - 10} more items\")\n",
        "\n",
        "    # Try to get more detailed information about what was extracted\n",
        "    result = subprocess.run(\n",
        "        [\"ls\", \"-la\", EXTRACT_TARGET],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        check=False  # Don't raise exception for this command\n",
        "    )\n",
        "    if result.returncode == 0:\n",
        "        print(\"Directory contents:\")\n",
        "        print(result.stdout)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during verification: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"Extraction completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZWjAy503F8"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "```text\n",
        "Creating necessary directories...done.\n",
        "Downloading diabetic_retinopathy_train_244.zip...done.\n",
        "Extracting diabetic_retinopathy_train_244.zip to ./retinopathy_244...done.\n",
        "Verifying Extraction...\n",
        "Successfully extracted 2 items:\n",
        "  - trainLabels.csv (465317 bytes)\n",
        "  - train_244 (directory)\n",
        "Directory contents:\n",
        "total 1040\n",
        "drwxr-xr-x 3 root root   4096 Mar  1 01:53 .\n",
        "drwxr-xr-x 1 root root   4096 Mar  1 01:52 ..\n",
        "drwxr-xr-x 2 root root 581632 Mar  1 01:53 train_244\n",
        "-rw-rw---- 1 root root 465317 Sep 12 15:05 trainLabels.csv\n",
        "\n",
        "Extraction completed successfully!\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksYDMhoSzVQQ"
      },
      "source": [
        "# Example 3 - Step 3: Load Labels for the Training Set\n",
        "\n",
        "CNNs are trained by _supervised_ learning, which means that during each training step the network's prediction must be compared to a known correct answer. The label supplies this target: it allows a loss function (e.g., cross‑entropy, IoU, etc.) to be computed, which in turn provides gradients that tell the optimizer how to adjust the weights. Without a label, the loss cannot be evaluated, no gradients can be derived, and the network has no signal to improve its predictions. Thus, labels are essential for defining the training objective and enabling the network to learn from data.\n",
        "\n",
        "The file `trainLabels.csv` contains the label information for our retinal images. This file has just two columns, **image** and **level**. The `image` specifies the image's filename and from which eye the image was obtained; for example, `10_left.png`. The `level` column contains the a numerical value between 0 and 4 which indicates the serverity of diabetic retinopathy. So in this example the **_level_** is the **image label**.\n",
        "\n",
        "The code in the cell below reads the file `trainLabels.csv` and creates a Pandas dataframe called `eg_raw_df` to store the label information. A short amount of `eg_raw_df` is printed out for inspection to make sure the code worked as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y3dlgXu15Wx"
      },
      "outputs": [],
      "source": [
        "# @title Example 3 - Step 3: Load the Labels for the Training Set\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read labels and create dataframe\n",
        "eg_raw_df = pd.read_csv(\n",
        "        os.path.join(EXTRACT_TARGET,\"trainLabels.csv\"),\n",
        "        na_values=['NA', '?'])\n",
        "\n",
        "# Add file extention\n",
        "image_col = 'image'\n",
        "eg_raw_df[image_col] = eg_raw_df[image_col].astype(str) + '.png'\n",
        "\n",
        "# Print sample for verification\n",
        "eg_raw_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy1q3kKwNTtV"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image01A.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9rzoplyVBO7"
      },
      "source": [
        "# Example 3 - Step 4: Validate Images\n",
        "\n",
        "The code in the cell below checks to see if there is an actual retinal image for each image label in contained the DataFrame `eg_raw`, that was generated in the previous step.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdv0eSrGVB6y"
      },
      "outputs": [],
      "source": [
        "# @title Example 3 - Step 4: Validate Images\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Validate that all image files actually exist\n",
        "def validate_image_files(df, source_path):\n",
        "    source = Path(source_path)\n",
        "    existing_files = set()\n",
        "\n",
        "    # Get all actual files in the directory\n",
        "    for file_path in source.rglob('*'):\n",
        "        if file_path.is_file() and file_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
        "            existing_files.add(file_path.name)\n",
        "\n",
        "    # Check which files in DataFrame actually exist\n",
        "    df['file_exists'] = df['image'].apply(lambda x: x in existing_files)\n",
        "\n",
        "    print(f\"Total images in DataFrame: {len(df)}\")\n",
        "    print(f\"Images that exist: {df['file_exists'].sum()}\")\n",
        "    print(f\"Missing images: {(~df['file_exists']).sum()}\")\n",
        "\n",
        "    # Filter to only include existing files\n",
        "    valid_df = df[df['file_exists']].copy()\n",
        "    print(f\"Valid DataFrame size: {len(valid_df)}\")\n",
        "\n",
        "    return valid_df\n",
        "\n",
        "# Use it:\n",
        "eg_raw_df = validate_image_files(eg_raw_df, SOURCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL2bcF55WsuN"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "```text\n",
        "Total images in DataFrame: 35126\n",
        "Images that exist: 17448\n",
        "Missing images: 17678\n",
        "Valid DataFrame size: 17448\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyRcw-DlXoNw"
      },
      "source": [
        "# Example 3 - Step 5: Split Images into Training and Validation Sets\n",
        "\n",
        "If you judge the model only on the same data you used to update its weights, you’ll get an *optimistic* estimate of its performance – the model will look perfect on the training set but will usually fail on new images.  \n",
        "Splitting the data into **training + validation** lets you:\n",
        "\n",
        "1. **Measure true generalization** - see how the network behaves on unseen samples.  \n",
        "2. **Tune hyper‑parameters** - learning rate, number of layers, data‑augmentation policies, etc.  \n",
        "3. **Detect & prevent over‑fitting** - monitor validation loss/accuracy during training and stop or adjust when the model starts to degrade.  \n",
        "4. **Select the best model** - keep the checkpoint that had the lowest validation error.  \n",
        "\n",
        "Without a validation set, you'll be blind to over-fitting, you'll have no reliable metric for early stopping, and you'll have no principled way to pick the best architecture or hyper-parameters.\n",
        "\n",
        "The code in the cell below, splits the retinal images into a training set and a validation set. How much of the train set is used for the validation set depends on the variable `FRAC`. In the cell below, `FRAC` is set to 0.8 which means 80% of the images will be put into the training set (`eg_train_df`) and the remaining 20% will be put into the validation set (`eg_val_df`). Which images are used in each set is randomize. It should be noted that only the **image names** are being \"split\" into a training and validation pool. Later an image generator will use these two DataFrames to actually generate two image sets.\n",
        "\n",
        "After the split, the number images assigned to both sets is printed out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsYUYOm5UyLA"
      },
      "outputs": [],
      "source": [
        "# @title Example 3 - Step 5: Split Images into Training and Validation Sets\n",
        "\n",
        "# Set split fraction\n",
        "FRAC=0.8  # 80% training / 20% validation\n",
        "\n",
        "# Convert the class column to string – required for `flow_from_dataframe`\n",
        "eg_raw_df['level'] = eg_raw_df['level'].astype(str)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "#  1️⃣ Randomly split data into training and validation sets\n",
        "# ------------------------------------------------------------------\n",
        "eg_train_df = eg_raw_df.sample(frac=FRAC, random_state=42)\n",
        "eg_val_df   = eg_raw_df.drop(eg_train_df.index)\n",
        "\n",
        "# Calculate the split fraction as sanity check\n",
        "split_fraction = len(eg_train_df) / (len(eg_val_df) + len(eg_train_df))\n",
        "\n",
        "# Print out numbers\n",
        "print(f\"Training set size   : {len(eg_train_df)}\")\n",
        "print(f\"Validation set size : {len(eg_val_df)}\")\n",
        "print(f\"Calculated split fraction =\", split_fraction)\n",
        "\n",
        "# Quick sanity check\n",
        "print(\"\\nSample training rows:\")\n",
        "print(eg_train_df[['image', 'level']].head())\n",
        "\n",
        "print(\"\\nSample validation rows:\")\n",
        "print(eg_val_df[['image', 'level']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF4iuKw5KpBX"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "```text\n",
        "Training set size   : 13958\n",
        "Validation set size : 3490\n",
        "Calculated split fraction = 0.7999770747363595\n",
        "\n",
        "Sample training rows:\n",
        "                 image level\n",
        "16508   20710_left.png     0\n",
        "8057   10109_right.png     2\n",
        "23120   29217_left.png     0\n",
        "24         31_left.png     0\n",
        "12902   16217_left.png     0\n",
        "\n",
        "Sample validation rows:\n",
        "           image level\n",
        "1   10_right.png     0\n",
        "7   16_right.png     4\n",
        "10   19_left.png     0\n",
        "13  20_right.png     0\n",
        "15  21_right.png     0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL-2WIiRW81b"
      },
      "source": [
        "# Example 3 - Step 6: Final Check for Valid Images\n",
        "\n",
        "The code in the cell below does one more final check for valid image files before image transforms occur and the dataloader is created in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1BAvweOW9db"
      },
      "outputs": [],
      "source": [
        "# @title Example 3 - Step 6: Final Check for Valid Images\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Check which files are actually valid\n",
        "def check_valid_images(df, source_path):\n",
        "    valid_files = []\n",
        "    invalid_files = []\n",
        "\n",
        "    for filename in df['image']:\n",
        "        file_path = Path(source_path) / filename\n",
        "        if file_path.exists() and file_path.is_file():\n",
        "            valid_files.append(filename)\n",
        "        else:\n",
        "            invalid_files.append(filename)\n",
        "\n",
        "    print(f\"Total files: {len(df)}\")\n",
        "    print(f\"Valid files: {len(valid_files)}\")\n",
        "    print(f\"Invalid files: {len(invalid_files)}\")\n",
        "\n",
        "    # Remove invalid files from your dataframe\n",
        "    df_clean = df[df['image'].isin(valid_files)]\n",
        "    print(f\"After cleaning: {len(df_clean)} samples\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# Apply cleaning to both train and validation sets\n",
        "eg_train_df = check_valid_images(eg_train_df, SOURCE)\n",
        "eg_val_df = check_valid_images(eg_val_df, SOURCE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNezz4kbXjaJ"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "Total files: 13958\n",
        "Valid files: 13958\n",
        "Invalid files: 0\n",
        "After cleaning: 13958 samples\n",
        "Total files: 3490\n",
        "Valid files: 3490\n",
        "Invalid files: 0\n",
        "After cleaning: 3490 samples\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkj3iP2ggmiv"
      },
      "source": [
        "# Example 3 - Step 7: Create Image Transforms and DataLoaders\n",
        "\n",
        "The code in the cell below uses **torchvision.transforms** to define the sequence of augmentations and DataLoader to manage memory and batching. This separation provides more flexibility and control over the preprocessing pipeline.\n",
        "\n",
        "**IMPORTANT NOTE:**\n",
        "\n",
        "You need to be very careful to correctly set the **IMAGE SIZE**. This is especially true when performing transfer learning with pre-trained neural networks like `ResNet50`. `ResNet50` is designed to work with square images of exactly `244` pixels wide and `244` pixels high (i.e. `244 X 244`) that have 3 color channels (i.e. `RGB`) and no `alpha` channel.\n",
        "\n",
        "In the cell below, here is the code that specifies the image size:\n",
        "\n",
        "```type\n",
        "# Specify image size\n",
        "EG_IMG_W = 244\n",
        "EG_IMG_H = 244\n",
        "```\n",
        "You should also note that we set the `batch` sizes for training at this step in process:\n",
        "```text\n",
        "# Specify batch size\n",
        "EG_BATCH_TRAIN  = 64\n",
        "EG_BATCH_VAL    = 64\n",
        "```\n",
        "If we need to use a different batch size we will need to go back and re-run this cell again.\n",
        "\n",
        "You should also note that we only augment the **training images** --  the **validation images** are **not** augmented."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 3: Step 7: Create Image Transforms and DataLoaders\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 0️⃣ IMPORTANT: Specify image size and batch size\n",
        "# ------------------------------------------------------------------\n",
        "EG_IMG_W = 244\n",
        "EG_IMG_H = 244\n",
        "EG_BATCH_TRAIN = 64\n",
        "EG_BATCH_VAL = 64\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1️⃣ Define the Custom Dataset\n",
        "# ------------------------------------------------------------------\n",
        "class RetinopathyDataset(Dataset):\n",
        "    def __init__(self, dataframe, root_dir, transform=None):\n",
        "        self.df = dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        # Create a mapping for categories if they are strings\n",
        "        self.classes = sorted(self.df['level'].unique())\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.df.iloc[idx]['image'])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        # In PyTorch, labels are usually long integers (CrossEntropyLoss)\n",
        "        # rather than one-hot encoded vectors.\n",
        "        label = self.class_to_idx[self.df.iloc[idx]['level']]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2️⃣ Define Transforms (Augmentation for Train, Clean for Val)\n",
        "# ------------------------------------------------------------------\n",
        "# PyTorch ResNet normalization values\n",
        "norm_mean = [0.485, 0.456, 0.406]\n",
        "norm_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = T.Compose([\n",
        "    T.Resize((EG_IMG_H, EG_IMG_W)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(20),\n",
        "    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(norm_mean, norm_std)\n",
        "])\n",
        "\n",
        "val_transforms = T.Compose([\n",
        "    T.Resize((EG_IMG_H, EG_IMG_W)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(norm_mean, norm_std)\n",
        "])\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3️⃣ Create DataLoaders\n",
        "# ------------------------------------------------------------------\n",
        "train_dataset = RetinopathyDataset(eg_train_df, str(SOURCE), transform=train_transforms)\n",
        "val_dataset = RetinopathyDataset(eg_val_df, str(SOURCE), transform=val_transforms)\n",
        "\n",
        "eg_train_loader = DataLoader(train_dataset, batch_size=EG_BATCH_TRAIN, shuffle=True, num_workers=0)\n",
        "eg_val_loader = DataLoader(val_dataset, batch_size=EG_BATCH_VAL, shuffle=False, num_workers=0)\n",
        "\n",
        "# Sanity Check\n",
        "eg_x_train, eg_y_train = next(iter(eg_train_loader))\n",
        "eg_x_val, eg_y_val = next(iter(eg_val_loader))\n",
        "\n",
        "print(f\"TRAIN batch images: {eg_x_train.shape}\") # [Batch, Channels, H, W]\n",
        "print(f\"TRAIN batch labels: {eg_y_train.shape}\") # [Batch]\n",
        "print(f\"VAL batch images:   {eg_x_val.shape}\")\n",
        "print(f\"VAL batch labels:   {eg_y_val.shape}\")"
      ],
      "metadata": {
        "id": "u6vIt8pZ0sdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRAatHmKZb6q"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "```text\n",
        "TRAIN batch images: torch.Size([64, 3, 244, 244])\n",
        "TRAIN batch labels: torch.Size([64])\n",
        "VAL batch images:   torch.Size([64, 3, 244, 244])\n",
        "VAL batch labels:   torch.Size([64])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gh7tqrUXn8y"
      },
      "source": [
        "# Example 3 - Step 8: Check Class Distribution\n",
        "\n",
        "The code in the cell below generates a histogram showing the distribution of the 5 classes in the validation data set.\n",
        "\n",
        "#### **Why Check Class Distribution in CNN Classification?**\n",
        "\n",
        "When training a Convolutional Neural Network (CNN) for image classification, examining the distribution of classes in your dataset helps ensure that your model learns effectively and generalizes well. Here's why it's important:\n",
        "\n",
        "##### **1. Detecting Class Imbalance**\n",
        "If one class has significantly more samples than others, the model may become biased toward predicting the majority class. This can lead to:\n",
        "- High accuracy but poor performance on minority classes.\n",
        "- Misleading evaluation metrics.\n",
        "\n",
        "##### **2. Choosing the Right Metrics**\n",
        "In imbalanced datasets, accuracy alone is not a reliable metric. You may need to use:\n",
        "- Precision, recall, F1-score\n",
        "- Confusion matrix\n",
        "- ROC-AUC (for binary classification)\n",
        "\n",
        "##### **3. Designing Better Validation Strategies**\n",
        "Knowing the class distribution helps in:\n",
        "- Stratified sampling for train/test splits\n",
        "- Ensuring each class is represented in validation and test sets\n",
        "\n",
        "##### **4. Applying Corrective Techniques**\n",
        "If imbalance is detected, you can apply:\n",
        "- **Data augmentation** for minority classes\n",
        "- **Class weighting** in the loss function\n",
        "- **Oversampling** or **undersampling**\n",
        "- **Synthetic data generation** (e.g., SMOTE)\n",
        "\n",
        "##### **5. Improving Interpretability**\n",
        "Understanding class distribution helps interpret model behavior and debug issues like:\n",
        "- Why the model is misclassifying certain classes\n",
        "- Why training loss is low but validation performance is poor\n",
        "\n",
        "\n",
        "### **Best Practice**\n",
        "Always visualize class distribution before training using a bar chart or value counts.\n",
        "\n",
        "Following **Best Practice**, the code in the cell below generates a bar chart showing the class distribution before we start our training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 3 - Step 8: Check Class Distribution (PyTorch)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# In PyTorch, we can pull the labels directly from the dataset's dataframe\n",
        "# or use the class_to_idx mapping we created in the RetinopathyDataset\n",
        "eg_labels_val = val_dataset.df['level'].values\n",
        "\n",
        "# If labels are strings, map them to integers for bincount\n",
        "label_map = val_dataset.class_to_idx\n",
        "eg_labels_val_indices = [label_map[label] for label in eg_labels_val]\n",
        "\n",
        "# Count class distribution\n",
        "eg_class_counts = np.bincount(eg_labels_val_indices)\n",
        "\n",
        "# Plot distribution\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(range(len(eg_class_counts)), eg_class_counts, color='skyblue', edgecolor='black')\n",
        "\n",
        "# Add class names to the X-axis for better readability\n",
        "plt.xticks(range(len(eg_class_counts)), list(label_map.keys()))\n",
        "\n",
        "plt.xlabel(\"Class Name (Condition Severity)\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.title(\"Validation Set Class Distribution\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Print counts for verification\n",
        "for cls, idx in label_map.items():\n",
        "    print(f\"Class {idx} ({cls}): {eg_class_counts[idx]} samples\")"
      ],
      "metadata": {
        "id": "CBLtqck71Nux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXzm6mpsLfaN"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image02A.png)\n",
        "\n",
        "```text\n",
        "Class 0 (0): 1602 samples\n",
        "Class 1 (1): 516 samples\n",
        "Class 2 (2): 1055 samples\n",
        "Class 3 (3): 175 samples\n",
        "Class 4 (4): 142 samples\n",
        "```\n",
        "\n",
        "The classes are not balanced. Healthy retinal images (Class `0`) outnumbers the other classes. Unfortunately this situation is quite common in medical imaging of pathological states. The problem is that our neural network model can quickly learn that if it picks `0` as the image label, it will be correct _most_ of the time. We need to keep this imbalance in mind when we interpret the results of training our neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 3 - Step 9: Setup Neural Network and Define Training Components\n",
        "\n",
        "The code in the cell below demonstrates how to use **transfer learning** with the **ResNet50** architecture in PyTorch for a custom image classification task involving 5 classes (degrees of diabetic retinopathy).\n",
        "\n",
        "#### **What It Does**\n",
        "\n",
        "- **Loads a pretrained `ResNet50` model**: We load the model with weights trained on ImageNet.\n",
        "- **Freezes the base model's weights**: We disable gradient calculations for the pre-trained layers to retain learned features and speed up training.\n",
        "- **Modifies the \"Head\" (Final Layer)**:\n",
        "  - In PyTorch, ResNet50 already has a Global Average Pooling layer.\n",
        "  - We replace the final Fully Connected (`fc`) layer with a custom **Sequential** block:\n",
        "    - **Linear Layer**: To transition from the base features (2048) to a hidden layer.\n",
        "    - **ReLU Activation**: For non-linearity.\n",
        "    - **Dropout**: For regularization to prevent overfitting.\n",
        "    - **Final Linear Layer**: To output scores for our **5 classes**.\n",
        "- **Defines the Loss Function and Optimizer**:\n",
        "  - **CrossEntropyLoss**: The PyTorch standard for multi-class classification (combines LogSoftmax and NLLLoss).\n",
        "  - **Adam Optimizer**: Specifically configured to only update the parameters of our new \"head.\"\n",
        "\n",
        "#### **Use Case**\n",
        "\n",
        "This approach is ideal when:\n",
        "- You have limited training data.\n",
        "- You want to leverage powerful pretrained models.\n",
        "- You need to adapt a general-purpose model to a specific classification task.\n",
        "\n",
        "In the cell below, we create `ResNet50_model_244`. We first freeze all parameters in the base model. Then, we overwrite `model.fc` with our custom architecture. Finally, we move the model to the **GPU** (if available) to ensure high-performance training."
      ],
      "metadata": {
        "id": "jLJerF6K134A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 3 - Step 9: Setup Neural Network and Define Training Components\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "\n",
        "# Set the seed\n",
        "set_seed()\n",
        "\n",
        "# 1️⃣ Load the base ResNet50 model with pre-trained weights\n",
        "weights = models.ResNet50_Weights.DEFAULT\n",
        "ResNet50_model_244 = models.resnet50(weights=weights)\n",
        "\n",
        "# 2️⃣ Freeze all layers in the base model\n",
        "for param in ResNet50_model_244.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 3️⃣ Modify the \"Head\"\n",
        "# ResNet50 already has GlobalAveragePooling (called avgpool) before the fc layer.\n",
        "# We replace the original 'fc' layer with a new Sequential block.\n",
        "num_features = ResNet50_model_244.fc.in_features # Get the input dimension (2048)\n",
        "\n",
        "ResNet50_model_244.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(256, 5) # Output 5 classes for retinopathy levels\n",
        ")\n",
        "\n",
        "# 4️⃣ Move model to GPU if available\n",
        "# We use .is_available() to check if a NVIDIA GPU is present\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ResNet50_model_244 = ResNet50_model_244.to(device)\n",
        "\n",
        "# 5️⃣ Define Optimizer and Loss Function\n",
        "# Only optimize the parameters of the new 'fc' layer\n",
        "optimizer = optim.Adam(ResNet50_model_244.fc.parameters(), lr=1e-4)\n",
        "\n",
        "# CrossEntropyLoss is the PyTorch equivalent of categorical_crossentropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Model successfully moved to: {device}\")\n",
        "\n",
        "# 5️⃣ Define Optimizer and Loss Function\n",
        "# Only optimize the parameters of the new 'fc' layer (where requires_grad=True)\n",
        "optimizer = optim.Adam(ResNet50_model_244.fc.parameters(), lr=1e-4)\n",
        "\n",
        "# nn.CrossEntropyLoss in PyTorch expects raw scores (logits),\n",
        "# so we don't need a Softmax layer at the end of our model.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Model moved to: {device}\")"
      ],
      "metadata": {
        "id": "fVhcJyMO17YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy1wH-0ML_qT"
      },
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "```text\n",
        "Model successfully moved to: cuda\n",
        "Model moved to: cuda\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqiz6uT7Psx7"
      },
      "source": [
        "# Example 3 - Step 10: Train Neural Network\n",
        "\n",
        "The code in the cell below trains the neural network `ResNet50_model_244` for the number of epochs specified by the variable `EPOCHS`. A second variable, `PATIENCE`, controls the Early Stopping monitor. To keep training time reasonable the variable EPOCH has been set to 10 and the variable PATIENCE has been set to 3.\n",
        "\n",
        "**TIME ALERT!**\n",
        "\n",
        "We are analyzing a **LARGE** image dataset so training will take time. Even using Google fastest GPU hardware acceleration, it will probably take more than **1 hour to complete**. So don't start training if you can't afford to wait.\n",
        "\n",
        "**DON'T WASTE YOUR MONEY!**\n",
        "\n",
        "To get feedback on how long the training takes, the code in the cell below contains a \"timer function\". Specifically, the last line of code reads:\n",
        "\n",
        "```text\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")\n",
        "```\n",
        "This code snippet prints out how long the training required.\n",
        "\n",
        "If you weren't careful and skipped over running the cell that defined the custom function `hms_string()` at the start of this lesson, your model will train for an hour but **FAIL** at the end with an error message.\n",
        "\n",
        "Since every time you use a Colab GPU **costs you money**, don't waste your time and _money_ by running this code cell if you didn't run _all_ of the code cells above before you run the next one.\n",
        "\n",
        "**You have been warned.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 3 - Step 10: Train Neural Network\n",
        "\n",
        "import time\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1️⃣ Initialize History Dictionary\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "EPOCHS = 10\n",
        "PATIENCE = 3\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Scheduler for learning rate\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2)\n",
        "\n",
        "print(f\"-- Training started for {EPOCHS} epochs ----------------------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- TRAINING PHASE ---\n",
        "    ResNet50_model_244.train()\n",
        "    train_loss, train_correct = 0.0, 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(eg_train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = ResNet50_model_244(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        train_correct += torch.sum(preds == labels.data).item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    # --- VALIDATION PHASE ---\n",
        "    ResNet50_model_244.eval()\n",
        "    val_loss, val_correct = 0.0, 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in eg_val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = ResNet50_model_244(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_correct += torch.sum(preds == labels.data).item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    # 2️⃣ Calculate and Append Metrics to History\n",
        "    epoch_train_loss = train_loss / total_train\n",
        "    epoch_train_acc = train_correct / total_train\n",
        "    epoch_val_loss = val_loss / total_val\n",
        "    epoch_val_acc = val_correct / total_val\n",
        "\n",
        "    history['train_loss'].append(epoch_train_loss)\n",
        "    history['train_acc'].append(epoch_train_acc)\n",
        "    history['val_loss'].append(epoch_val_loss)\n",
        "    history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | \"\n",
        "          f\"Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # 3️⃣ Early Stopping & Best Model Saving\n",
        "    scheduler.step(epoch_val_loss)\n",
        "\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        # Save the best weights locally\n",
        "        torch.save(ResNet50_model_244.state_dict(), 'best_model_local.pth')\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 9️⃣ Inspect training\n",
        "# ------------------------------------------------------------------------\n",
        "print(f\"\\nTraining finished.\")\n",
        "\n",
        "# Compute elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")\n",
        "\n",
        "# Print final training statistics\n",
        "print(f\"Final training completed in {epoch+1} epochs\")"
      ],
      "metadata": {
        "id": "7luiYT6Okb3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FRIhXrVyrJf"
      },
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image03A.png)\n",
        "\n",
        "Using an older, less powerful GPU accelerator (`T4 High-RAM`) training for 10 epochs required about 20 min to complete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORsbZqckdC4"
      },
      "source": [
        "# Example 3 - Step 11: Plot Training/Validation Accuracy\n",
        "\n",
        "A **Training-versus-Validation Accuracy Plot** provides a quick visual gauge of how well a model is learning and generalizing. As training progresses, a rising training accuracy shows that the model is fitting the data, while the validation accuracy tracks performance on unseen samples.\n",
        "\n",
        "When both curves rise together and plateau, the model is likely well-balanced. If training accuracy climbs while validation accuracy lags or dips, the model is over-fitting; conversely, if both stay low, the model is under-fitting or too simple.\n",
        "\n",
        "The shape of the curves also reveals training issues—sharp jumps or oscillations can signal an inappropriate learning rate, and noisy validation performance may indicate label noise or class imbalance. Thus, the plot informs decisions about early stopping, regularisation, architecture changes, or hyper-parameter tuning.\n",
        "\n",
        "\n",
        "#### **Interpreting Common Patterns**\n",
        "\n",
        "| Curve Shape | Interpretation | Suggested Action |\n",
        "|-------------|----------------|------------------|\n",
        "| **Both curves rise together and plateau at high accuracy** | Good fit & generalisation. | Continue training if you want to squeeze a bit more. |\n",
        "| **Training rises, validation rises then drops** | Over‑fitting. | Add regularisation, dropout, data augmentation, or stop early. |\n",
        "| **Both curves rise slowly and stay low** | Under‑fitting. | Increase model capacity, train longer, or reduce regularisation. |\n",
        "| **Validation lags behind training by a fixed margin, but both improve** | Model learns but generalises less well. | Consider a larger training set, better augmentation, or a different architecture. |\n",
        "| **Validation fluctuates wildly** | High variance due to small batch or high learning rate. | Reduce learning rate, increase batch size, or use a learning‑rate scheduler. |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 3 - Step 11: Plot Training/Validation Accuracy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pull metrics from history dictionary\n",
        "eg_val_acc = [acc.cpu().item() if torch.is_tensor(acc) else acc for acc in history['val_acc']]\n",
        "eg_train_acc = [acc.cpu().item() if torch.is_tensor(acc) else acc for acc in history['train_acc']]\n",
        "\n",
        "# --- Find the epoch with the highest validation accuracy -------------\n",
        "best_epoch_idx = np.argmax(eg_val_acc)\n",
        "best_epoch_num = best_epoch_idx + 1\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(eg_val_acc, label='Validation Accuracy', marker='o')\n",
        "plt.plot(eg_train_acc, label='Training Accuracy', marker='x')\n",
        "\n",
        "# Vertical line at the best epoch\n",
        "plt.axvline(best_epoch_idx, color='r', linestyle='--',\n",
        "            label=f'Best epoch (epoch {best_epoch_num})')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training / Validation Accuracy (ResNet50)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Annotate the exact accuracy value at the best epoch\n",
        "best_eg_val_acc = eg_val_acc[best_epoch_idx]\n",
        "plt.text(best_epoch_idx, best_eg_val_acc,\n",
        "         f' {best_eg_val_acc:.4f}',\n",
        "         va='bottom', ha='left', color='r', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lutp6Qii6oYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jADKuEQrDei"
      },
      "source": [
        "If the code is correct you should see something _similar_ to the following graph.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image04A.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M2meHM83wzP"
      },
      "source": [
        "# Example 3 - Step 12: Save Model to GDrive\n",
        "\n",
        "Run the next cell to save your retrained `ResNet50_model_244` model to your GDrive.\n",
        "\n",
        "**IMPORTANT NOTE** You will be using this saved PyTorch model in a later class lesson so make sure **not** to delete it from your GDrive!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 3 - Step 10: Save Model Weights to Google Drive\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 1️⃣ Mount Google Drive (if not already done)\n",
        "# --------------------------------------------------------------\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 2️⃣ Define the names / paths\n",
        "# --------------------------------------------------------------\n",
        "model_name  = \"ResNet50_model_244\"\n",
        "gdrive_dir  = f\"/content/drive/My Drive/{model_name}\"  # Folder on Drive\n",
        "# PyTorch convention uses .pth or .pt\n",
        "gdrive_file = os.path.join(gdrive_dir, f\"{model_name}.pth\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 3️⃣ Make sure the Drive folder exists\n",
        "# --------------------------------------------------------------\n",
        "os.makedirs(gdrive_dir, exist_ok=True)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4️⃣ Save the weights *on* Drive\n",
        "# --------------------------------------------------------------\n",
        "# We save the state_dict, which is the dictionary of weights\n",
        "torch.save(ResNet50_model_244.state_dict(), gdrive_file)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 5️⃣ Verify and List\n",
        "# --------------------------------------------------------------\n",
        "if os.path.exists(gdrive_file):\n",
        "    print(f\"Success! Model weights saved to Drive: {gdrive_file}\")\n",
        "else:\n",
        "    print(\"Error: File not found. Check Drive connection.\")\n",
        "\n",
        "# List the directory contents\n",
        "!ls -lh \"/content/drive/My Drive/ResNet50_model_244\""
      ],
      "metadata": {
        "id": "Pg2f7Agq7JyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibbgwTSACd1d"
      },
      "source": [
        "If the code is correct you should see something similar to the following output\n",
        "```text\n",
        "Success! Model weights saved to Drive: /content/drive/My Drive/ResNet50_model_244/ResNet50_model_244.pth\n",
        "total 92M\n",
        "-rw------- 1 root root 92M Mar  1 02:40 ResNet50_model_244.pth\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SofmEo6JmCZz"
      },
      "source": [
        "------------------------------------------\n",
        "\n",
        "\n",
        "# **EXERCISE 3**\n",
        "\n",
        "In **Exercise 3** you are going to re-analyze the Diabetic Retinopathy data set using `ResNet101` in conjunction with larger retinal images that are 512 X 512 pixels.\n",
        "\n",
        "**`ResNet-101`** was built to give a **deeper**, **larger-receptive-field** backbone that can be trained stably thanks to residual connections. That extra depth—and the corresponding increase in receptive field and representational capacity—makes the network particularly suited to the high-resolution, high-complexity images that dominate modern clinical imaging datasets (e.g., whole-slide pathology, high‑res CT/MRI, retinal photographs).  \n",
        "\n",
        "#### **1. Depth matters for large, complex images**\n",
        "\n",
        "| Aspect                       | Why it matters in clinical imaging                                                                                     |\n",
        "|------------------------------|------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Receptive field**         | • Deeper network ⇒ larger receptive field.<br>• Captures micro‑details (nuclei, capillaries) *and* macro‑context (lobule shape). |\n",
        "| **Hierarchical feature extraction** | • Early layers learn edges & textures.<br>• Deeper layers combine them into complex motifs.<br>• Needed for multi‑level abstraction (malignant vs. normal). |\n",
        "| **Parameter efficiency**    | • Residual blocks add depth without exploding the number of weights.<br>• 101‑layer stack packs more power while staying manageable. |\n",
        "\n",
        "\n",
        "#### **2. Residual connections: the training engine that makes depth feasible**\n",
        "\n",
        "* **Vanishing / exploding gradients** are especially problematic when you try to back‑propagate through dozens of nonlinear layers. Residual connections add a *shortcut* that lets the gradient flow almost unchanged from the output back to the input, so deeper models can be trained from scratch (or fine‑tuned on a new domain) without catastrophic loss of signal.\n",
        "* In medical-image research, practitioners often *fine-tune* ImageNet‑pretrained models on limited clinical data. Because residual connections preserve gradient flow, a 101‑layer network can be adapted with fewer epochs and fewer data points than a comparable plain CNN.\n",
        "\n",
        "#### **3. ResNet‑101 and typical clinical datasets**\n",
        "\n",
        "| Dataset type              | Typical image size                                    | Typical complexity                                   | ResNet‑101 benefits                                                                                                                                                 |\n",
        "|---------------------------|-------------------------------------------------------|-------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| Whole‑slide pathology     | 10k‑40k px × 10k‑40k px (usually tiled to 512×512)   | Ultra‑fine texture + large‑scale architecture        | • Deep residual layers capture both micro‑ and macro‑features.<br>• Receptive field ≈ 400 px after 5 stages                                                      |\n",
        "| Chest CT                  | 512×512 or larger                                    | Subtle texture changes, large organs                 | • Deeper encoder gives more context for slice‑to‑slice variation                                                                                                      |\n",
        "| Retinal fundus            | 5000×5000 px (often cropped to 1024×1024)            | Fine vascular patterns + large vessels               | • Extra depth boosts sensitivity to minute capillary changes                                                                         \n",
        "> **Bottom line**: ResNet‑101 was *not* invented *solely* for medical imaging, but its architectural traits (depth, residual links, large receptive field) make it a natural fit for the high‑resolution, highly variable images found in clinical datasets.\n",
        "\n",
        "#### **4. Practical use-cases that illustrate the point**\n",
        "\n",
        "1. **Whole‑slide analysis**  \n",
        "   *U‑Net + ResNet‑101 encoder* is the standard backbone for segmentation of tumor vs. normal tissue. The encoder’s depth allows the network to produce rich multi‑scale feature maps that are essential for accurate boundary delineation.\n",
        "\n",
        "2. **Histopathology classification**  \n",
        "   A 101‑layer residual network fine‑tuned on 2048×2048 patches yields a 2‑fold improvement in AUC over a 50‑layer variant, especially on datasets where subtle nuclear pleomorphism is the key discriminant.\n",
        "\n",
        "3. **Radiology reporting**  \n",
        "   ResNet‑101 pre‑trained on ImageNet is used as the feature extractor for a downstream transformer that produces radiology reports. The larger depth improves the semantic quality of the extracted features, translating into more coherent, clinically relevant reports.\n",
        "\n",
        "#### **5. What the “larger image size” means**\n",
        "\n",
        "When people say ResNet‑101 was created “in part to analyze larger image sizes,” they're really referring to two intertwined design goals:\n",
        "\n",
        "1. **Scale-aware receptive fields** - As the input resolution grows, the network’s receptive field must grow proportionally so that each unit still sees a meaningful portion of the image. Deeper networks naturally achieve this.\n",
        "\n",
        "2. **Robust training** - High-resolution images contain many more pixels, which can amplify the effects of noise, small mis-alignments, or class imbalance. Residual learning keeps gradients stable, allowing the model to learn meaningful patterns from a massive pixel set without **over-fitting** or getting stuck.\n",
        "\n",
        "---\n",
        "\n",
        "#### **TL;DR**\n",
        "\n",
        "ResNet‑101’s 101 layers give it a large receptive field and the capacity to learn complex, hierarchical features, which is vital for interpreting the fine‑grained details and global context in large medical images. Residual connections make training such a deep network feasible, allowing it to be fine‑tuned on the often limited clinical datasets that are typical in healthcare. That combination is why the architecture was chosen, and later adopted, for many high‑resolution clinical image‑analysis tasks.\n",
        "\n",
        "#### **Image Size Comparison**\n",
        "\n",
        "You should realize that `512×512×3` image is more than **4X larger** than a `244×244×3` image. Here's the math:\n",
        "\n",
        "\\begin{aligned}\n",
        "512 \\times 512 \\times 3 &= 786{,}432 \\\\\n",
        "244 \\times 244 \\times 3 &= 178{,}608 \\\\\n",
        "\\frac{786{,}432}{178{,}608} &\\approx 4.4\n",
        "\\end{aligned}\n",
        "\n",
        "\n",
        "So, the **512x512x3** image is approximately **4.4 times larger** than the **244×244×3** image in terms of pixel data. That additional detail might be critical for \"seeing\" small differences between the images in different diabetic retinal classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyqunLqtmQLj"
      },
      "source": [
        "### **Exercise 3 - Step 1: Set ENVIRONMENTAL VARIABLES**\n",
        "\n",
        "In the cell below write the code to define a your `ENVIRONMENTAL VARIABLES`. It is important to make the all of the following changes exactly as suggested.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change this line of code\n",
        "```text\n",
        "DOWNLOAD_SOURCE = URL+\"diabetic_retinopathy_train_244.zip\"\n",
        "```\n",
        "to read\n",
        "```text\n",
        "DOWNLOAD_SOURCE = URL+\"diabetic_retinopathy_train_512.zip\"\n",
        "```\n",
        "2. Change this line of code\n",
        "```text\n",
        "EXTRACT_TARGET = os.path.join(PATH,\"retinopathy_244\")\n",
        "```\n",
        "to read\n",
        "```text\n",
        "EXTRACT_TARGET = os.path.join(PATH,\"retinopathy_512\")\n",
        "```\n",
        "3. Change this line of code\n",
        "```text\n",
        "SOURCE = os.path.join(EXTRACT_TARGET, \"train_244\")\n",
        "```\n",
        "to read\n",
        "```text\n",
        "SOURCE = os.path.join(EXTRACT_TARGET, \"train_512\")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTJS3d5tmQLj"
      },
      "outputs": [],
      "source": [
        "# @title Exercise 3 - Step 1: Set ENVIRONMENTAL VARIABLES\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ehx51EmQLj"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "DOWNLOAD_SOURCE= https://biologicslab.co/BIO1173/data/diabetic_retinopathy_train_512.zip\n",
        "DOWNLOAD_NAME= diabetic_retinopathy_train_512.zip\n",
        "EXTRACT_TARGET= ./retinopathy_512\n",
        "SOURCE= ./retinopathy_512/train_512\n",
        "ENVIRONMENTAL VARIABLES were successfully created.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owF95gW0mQLk"
      },
      "source": [
        "# **Exercise 3 - Step 2: Download and Extract Image Data**\n",
        "\n",
        "In the cell below write the code to (1) create the necessary directories, (2) download the Zip file containing the image data, (3) extract this data into the appropiate folder and (4) verify the sucess\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "You can re-use the code in Example 3 - Step 2 _without_ making any code changes.\n",
        "\n",
        "**TIME WARNING:** This Zip file is _substantially_ larger than the one you downloaded in Example 3. You can expect it to take approximately **7 minutes** to download and unzip the image files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr8eqTjtdCW1"
      },
      "outputs": [],
      "source": [
        "# @title Exercise 3 - Step 2: Download and Extract Image Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX3hmd-omQLk"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "```text\n",
        "Creating necessary directories...done.\n",
        "Downloading diabetic_retinopathy_train_512.zip...done.\n",
        "Extracting diabetic_retinopathy_train_512.zip to ./retinopathy_512...done.\n",
        "Verifying Extraction...\n",
        "Successfully extracted 2 items:\n",
        "  - trainLabels.csv (465317 bytes)\n",
        "  - train_512 (directory)\n",
        "Directory contents:\n",
        "total 1040\n",
        "drwxr-xr-x 3 root root   4096 Mar  1 02:49 .\n",
        "drwxr-xr-x 1 root root   4096 Mar  1 02:42 ..\n",
        "drwxr-xr-x 2 root root 581632 Mar  1 02:49 train_512\n",
        "-rw-rw---- 1 root root 465317 Sep 12 15:05 trainLabels.csv\n",
        "\n",
        "Extraction completed successfully!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69raKQFgmQLl"
      },
      "source": [
        "# **Exercise 3 - Step 3: Load Labels for the Training Set**\n",
        "\n",
        "In the cell below write the code to extract the label information from the file `trainLabel.csv` located in your `./retinopathy_512` folder and create a DataFrame called `ex_raw_df`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Copy the code from `Example 3 - Step 3` and then change the prefix `eg_` to `ex_` **everywhere** in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpmGEfHEmQLl"
      },
      "outputs": [],
      "source": [
        "# @title Exercise 3 - Step 3: Load Labels for the Training Set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkYpMY5MmQLl"
      },
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image05A.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxmUxVqYcpHv"
      },
      "source": [
        "# **Exercise 3 - Step 4: Validate Images**\n",
        "\n",
        "In the cell below write the code to check whether an actual retinal image file exists for each image label in the DataFrame `ex_raw`.  \n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Make sure to change the prefix `eg_` to `ex_` in the print statements are the bottom of the code copied from Example 3 - Step 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI9Y8eGEcpHv"
      },
      "outputs": [],
      "source": [
        "# @title Exercise 3 - Step 4: Validate Images\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxHv6dHQcpHw"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "Total images in DataFrame: 35126\n",
        "Images that exist: 17448\n",
        "Missing images: 17678\n",
        "Valid DataFrame size: 17448\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9FsUIsDmQLl"
      },
      "source": [
        "# **Exercise 3 - Step 5: Split Images into Training and Validation Sets**\n",
        "\n",
        "In the cell below write the code to split your retinal images into a training set and a validation set with 80% of the images going into the training set. After splitting, print out the number images in both sets and a short sample of both the training set and the validation set.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Copy the code from `Example 3 - Step 5` and change the prefix `eg_` to `ex_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63vrz9p2L3X2"
      },
      "outputs": [],
      "source": [
        "# @title Exercise 3 - Step 5: Split Images into Training and Validation Sets\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoNzAXAA4Pez"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "Training set size   : 13958\n",
        "Validation set size : 3490\n",
        "Calculated split fraction = 0.7999770747363595\n",
        "\n",
        "Sample training rows:\n",
        "                 image level\n",
        "16508   20710_left.png     0\n",
        "8057   10109_right.png     2\n",
        "23120   29217_left.png     0\n",
        "24         31_left.png     0\n",
        "12902   16217_left.png     0\n",
        "\n",
        "Sample validation rows:\n",
        "           image level\n",
        "1   10_right.png     0\n",
        "7   16_right.png     4\n",
        "10   19_left.png     0\n",
        "13  20_right.png     0\n",
        "15  21_right.png     0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urX5VTGfgG4w"
      },
      "source": [
        "# **Exercise 3 - Step 6: Final Check for Valid Images**\n",
        "\n",
        "In the cell below write the to make a final check that all the image files are valid.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Make sure to change the prefix `eg_` to `ex_` every where in the code that you copied from Example 3 - Step 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Eh0xPragHco"
      },
      "outputs": [],
      "source": [
        "# @title Exercise 3 - Step 6: Final Check for Valid Images\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb_9xI_rOt2p"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "Total files: 13958\n",
        "Valid files: 13958\n",
        "Invalid files: 0\n",
        "After cleaning: 13958 samples\n",
        "Total files: 3490\n",
        "Valid files: 3490\n",
        "Invalid files: 0\n",
        "After cleaning: 3490 samples\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSSVk-dYaLYe"
      },
      "source": [
        "# **Exercise 3: Step 7: Create Image Transforms and DataLoaders**\n",
        "\n",
        "In the cell below write the code to create the image transforms and the dataloaders.\n",
        "\n",
        "**Code Notes:**\n",
        "\n",
        "1. Is is _very_ important that you change your image size.\n",
        "\n",
        "Specifically you must change this code snippet:\n",
        "```python\n",
        "    #------------------------------------------------------------------\n",
        "    # 0️⃣ IMPORTANT: Specify image size\n",
        "    #-----------------------------------------------------------------\n",
        "    EG_IMG_W = 244\n",
        "    EG_IMG_H = 244\n",
        "```\n",
        "to read as\n",
        "```python\n",
        "    #------------------------------------------------------------------\n",
        "    # 0️⃣ IMPORTANT: Specify image size\n",
        "    #-----------------------------------------------------------------\n",
        "    EX_IMG_W = 512\n",
        "    EX_IMG_H = 512\n",
        "```\n",
        "The main objective of **Exercise 3** is to analyze larger retinal imagee (512 X 512 pixels) to see if that will improve classification accuracy.\n",
        "\n",
        "2. Change batch name\n",
        "\n",
        "Specifically you must change this code snippet:\n",
        "```python\n",
        "    # Specify batch size\n",
        "    EG_BATCH_TRAIN  = 64\n",
        "    EG_BATCH_VAL    = 64\n",
        "```\n",
        "to read as\n",
        "```python\n",
        "    # Specify batch size\n",
        "    EX_BATCH_TRAIN  = 64\n",
        "    EX_BATCH_VAL    = 64\n",
        "```\n",
        "You will also need to change these values in the code block.\n",
        "\n",
        "\n",
        "3. Make sure to change the prefix `eg_` to `ex_` as well as `EG_` to `EX_` everywhere they appear in the code block."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 3: Step 7: Create Image Transforms and DataLoaders\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-T283CC0tgYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaCyU5oz4p_C"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "TRAIN batch images: torch.Size([64, 3, 512, 512])\n",
        "TRAIN batch labels: torch.Size([64])\n",
        "VAL batch images:   torch.Size([64, 3, 512, 512])\n",
        "VAL batch labels:   torch.Size([64])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry1JMAnfmQLm"
      },
      "source": [
        "# **Exercise 3 - Step 8: Check Class Distribution**\n",
        "\n",
        "In the cell below write the code to generate a bar graph showing the number of images in each of the 5 output classes.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Copy the code from `Example 3 - Step 6` and change the prefix `eg_` to `ex_` everywhere that it occurs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 3 - Step 8: Check Class Distribution\n",
        "\n"
      ],
      "metadata": {
        "id": "qWPY0-adwcmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9hzpBna4-MB"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image07A.png)\n",
        "\n",
        "```text\n",
        "Class 0 (0): 1602 samples\n",
        "Class 1 (1): 516 samples\n",
        "Class 2 (2): 1055 samples\n",
        "Class 3 (3): 175 samples\n",
        "Class 4 (4): 142 samples\n",
        "```\n",
        "As before, the classes are not balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOOp2my2acdu"
      },
      "source": [
        "# **Exercise 3 - Step 9: Setup `ResNet101` Base Network**\n",
        "\n",
        "In the cell below write the code to setup your base network using `ResNet101` instead of `ResNet50`. Called your new model `ResNet101_model_512`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change this line of code\n",
        "```python\n",
        "    # 1️⃣ Load the base ResNet101 model with pre-trained weights\n",
        "    weights = models.ResNet50_Weights.DEFAULT\n",
        "```\n",
        "to read\n",
        "```python\n",
        "    # 1️⃣ Load the base ResNet101 model with pre-trained weights\n",
        "    weights = models.ResNet101_Weights.DEFAULT\n",
        "```\n",
        "\n",
        "2. Change this line of code\n",
        "```python\n",
        "    # 2️⃣ Freeze all layers in the base model\n",
        "    for param in ResNet50_model_244.parameters():\n",
        "```\n",
        "to read\n",
        "```python\n",
        "    # 2️⃣ Freeze all layers in the base model\n",
        "    for param in ResNet101_model_512.parameters():\n",
        "```\n",
        "\n",
        "Change `ResNet50_model_244` to `ResNet101_model_512` everywhere it occurs in the code block.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 3 - Step 9: Setup `ResNet101` Base Network\n",
        "\n"
      ],
      "metadata": {
        "id": "cZDZj7Y5xbML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMSFZmOW5k-g"
      },
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "Model successfully moved to: cuda\n",
        "Model moved to: cuda\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi1Fbo8NmQLm"
      },
      "source": [
        "# **Exercise 3 - Step 10: Train Neural Network**\n",
        "\n",
        "In the cell below write the code to train your neural network `ResNet101_model_512`.\n",
        "\n",
        "#### **Please Read this Carefully**\n",
        "\n",
        "The `ResNet101_model_512` is perhaps the largest neural network that you will be asked to train in this course. Due to the large size of the images (512x512 pixels), training this model for even `10` epochs requires close to **2 hours!** So don't start training your model unless you have enough time.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change `ResNet50_model_244` to read `ResNet101_model_512` everywhere it occurs in the code block.\n",
        "\n",
        "2. Change the prefix `eg_` to `ex_` everywhere it occurs in the code block."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 3 - Step 10: Train Neural Network\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sO9mDM5ayeRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lALh-nnSmQLm"
      },
      "source": [
        "If the code is correct, you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image08A.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pSv_2shmQLn"
      },
      "source": [
        "# **Exercise 3 - Step 11: Plot Training History**\n",
        "\n",
        "In the cell below write the code to plot the training history of your `ResNet101_model_512`.\n",
        "\n",
        "Copy the code in Example 3 - Step 9 into the cell below.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "1. Change the prefix `eg_` to `ex_` everywhere in the copied code.\n",
        "2. Change the plot title from `ResNet50` to `ResNet101`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 3 - Step 11: Plot Training History\n",
        "\n"
      ],
      "metadata": {
        "id": "3uDUB-eSDG2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjnUgGZSRYZr"
      },
      "source": [
        "If the code is correct you should see something similar to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_03/class_03_2_image09A.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9yqR3rQmQLn"
      },
      "source": [
        "# **Exercise 3 - Step 12: Save Model to GDrive**\n",
        "\n",
        "In the next cell write the code to save your `ResNet101_model_512` to your GDrive.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Change this line of code\n",
        "```python\n",
        "    model_name = \"ResNet50_model_244\" # model object name\n",
        "```\n",
        "to read\n",
        "```python\n",
        "    model_name = \"ResNet101_model_512\"  # model object name\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 3 - Step 12: Save Model to GDrive\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 1️⃣ Mount Google Drive (if not already done)\n",
        "# --------------------------------------------------------------\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 2️⃣ Define the names / paths\n",
        "# --------------------------------------------------------------\n",
        "model_name  = \"ResNet101_model_512\"\n",
        "gdrive_dir  = f\"/content/drive/My Drive/{model_name}\"  # Folder on Drive\n",
        "# PyTorch convention uses .pth or .pt\n",
        "gdrive_file = os.path.join(gdrive_dir, f\"{model_name}.pth\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 3️⃣ Make sure the Drive folder exists\n",
        "# --------------------------------------------------------------\n",
        "os.makedirs(gdrive_dir, exist_ok=True)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4️⃣ Save the weights *on* Drive\n",
        "# --------------------------------------------------------------\n",
        "# We save the state_dict, which is the dictionary of weights\n",
        "torch.save(ResNet101_model_512.state_dict(), gdrive_file)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 5️⃣ Verify and List\n",
        "# --------------------------------------------------------------\n",
        "if os.path.exists(gdrive_file):\n",
        "    print(f\"Success! Model weights saved to Drive: {gdrive_file}\")\n",
        "else:\n",
        "    print(\"Error: File not found. Check Drive connection.\")\n",
        "\n",
        "# List the directory contents\n",
        "!ls -lh \"/content/drive/My Drive/ResNet101_model_512\""
      ],
      "metadata": {
        "id": "sTUHsz5ID5Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN2bqt7V_C1S"
      },
      "source": [
        "If the code is correct you should see something similar to the following output\n",
        "```text\n",
        "Success! Model weights saved to Drive: /content/drive/My Drive/ResNet101_model_512/ResNet101_model_512.pth\n",
        "total 165M\n",
        "-rw------- 1 root root 165M Dec 29 08:31 ResNet101_model_512.pth\n",
        "-rw------- 1 root root  14K Sep 11 00:27 test_label.csv\n",
        "```\n",
        "\n",
        "Your `ResNet101_model_512` is now safely stored on your GDrive.\n",
        "\n",
        "If you wanted to, you could easily copy your model back from your GDrive into you current Google Colab directory and use it analyze new retinal fundus images, looking for signs of diabetic retinopathy. Since your model is already trained, you could analyze the retinal images from your new patient as he/she sits in your office.\n",
        "\n",
        "Always keep in mind a common saying among members of the ML/AI community:\n",
        "\n",
        "> **_“Train-once, deploy-many”_**\n",
        "\n",
        "**IMPORTANT NOTE:** Make sure **not** to erase your saved file from your GDrive. You will need to use it later in a different class lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MhC_-6ebE3l"
      },
      "source": [
        "# **Electronic Submission**\n",
        "\n",
        "When you run the code in the cell below, it will grade your Colab notebook and tell you your pending grade as it currently stands. You will be given the choice to either submit your notebook for final grading or the option to continue your work on one (or more) Exercises. You no longer have the option to upload a PDF of your Colab notebook to Canvas for grading. Grant Access to your Colab Secrets if you are asked to do so.\n",
        "\n",
        "**NOTE:** You grade on this Colab notebook will be based solely on the code in your **Exercises**. Failure to run one (or more) Examples will not affect your grade."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Electronic Submission\n",
        "\n",
        "import urllib.request\n",
        "import ssl\n",
        "import time\n",
        "\n",
        "url = \"https://biologicslab.co/BIO1173/backend_code/validate.py?v=\" + str(time.time())\n",
        "\n",
        "ctx = ssl.create_default_context()\n",
        "ctx.check_hostname = False\n",
        "ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "req = urllib.request.Request(\n",
        "    url,\n",
        "    headers={\n",
        "        \"Cache-Control\": \"no-cache, no-store, must-revalidate\",\n",
        "        \"Pragma\": \"no-cache\",\n",
        "        \"Expires\": \"0\"\n",
        "    }\n",
        ")\n",
        "\n",
        "with urllib.request.urlopen(req, context=ctx) as r:\n",
        "    exec(r.read().decode(\"utf-8\"))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "x7YjRpRGWTO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFG4NZGwRfSk"
      },
      "source": [
        "# **Additional Information**\n",
        "\n",
        "### **The Diabetic Retinopathy Dataset**\n",
        "\n",
        "The **Diabetic Retinopathy Dataset** used in this lesson was part of a 2015 Kaggle Competition.\n",
        "\n",
        "Here is a summary of the winners and what they did to win this competition.\n",
        "\n",
        "### Winner of the 2015 Kaggle Diabetic Retinopathy Detection competition\n",
        "**Team**: *o_O* (Mathis Antony & Stephan Brüggemann)  \n",
        "**Score**: 0.845 weighted quadratic‑weighted Kappa (private leaderboard)  \n",
        "**Public leaderboard**: 7th place (5.8 k K‑appa)\n",
        "\n",
        "| Item | Details | Source |\n",
        "|------|---------|--------|\n",
        "| **Winning team** | “o_O” (Mathis Antony & Stephan Brüggemann) | 5.8 k K‑appa on the private leaderboard, 7 th place on the public leaderboard【9†L18-L23】 |\n",
        "| **Overall performance** | 0.845 weighted quadratic weighted Kappa (private leaderboard) | 5.8 k K‑appa, 0.845 score【9†L19-L22】 |\n",
        "| **Core architecture** | Two custom 2‑D convolutional nets (Net A & Net B) with a **per‑patient blending network** (Table 2) | 13‑25 | 12‑15 |\n",
        "| **Training framework** | Lasagne + nolearn (Theano) | 10‑11 |\n",
        "| **Image size** | 128 × 128, 256 × 256 and 512 × 512 (large color images, cropped to remove background) | 21‑24 |\n",
        "| **Pre‑training strategy** | *First* train a small network on 128‑pixel images.  Weights are then used to initialise an intermediate‑size network (trained on 256 px) and finally a 512‑pixel network.  Orthogonal initialization for all weights. | 105‑108 |\n",
        "| **Data augmentation** | Translation, stretching, rotation, flipping, colour jitter; per‑channel zero‑mean/unit‑variance scaling; 112/224/448 output sizes for 128/256/512 input images. | 115‑121 |\n",
        "| **Class imbalance handling** | Dynamic resampling: oversample rare classes initially, then gradually reduce; resampling weights \\((1.36, 14.4, 6.64, 40.2, 49.6)\\) → \\((1,2,2,2,2)\\). | 82‑96 |\n",
        "| **Training schedule** | Nesterov momentum with a fixed learning‑rate schedule over 250 epochs; learning rates 0.003 (epoch 0) → 0.00003 (epoch 150); L2 weight decay 0.0005; dropout after convolution and dense layers. | 31‑36, 70‑73 |\n",
        "| **Loss & objective** | Mean‑squared‑error regression (output thresholded at (0.5,1.5,2.5,3.5) to obtain integer grades). | 75‑77 |\n",
        "| **Blending network** | Input: mean & std of the RMSPool layer over 50 augmentations for each eye (µ,σ) + eye‑side indicator; 8193‑input → Dense 32 → Maxout 16 → Dense 32 → Maxout 16; Adam optimiser with a schedule (5 e‑4 → 5 e‑7). | 148‑156, 158‑166 |\n",
        "| **Final ensemble** | Average of the two conv‑net predictions, blended with the patient‑level network; score 0.845 (private) vs 0.824 (no per‑patient blend). | 167‑169 |\n",
        "| **Key design choices that yielded the win** | 1. **Large input resolution** – 512 × 512 (and even 768 × 768 for 0.81 Kappa) to capture micro‑aneurysms. 2. **Stage‑wise pre‑training** – starting from 128 px to 512 px to stabilise training. 3. **Extensive data augmentation & per‑channel normalisation**. 4. **Dynamic resampling** to address class imbalance without a weighted loss. 5. **Per‑patient blending** that aggregates information from both eyes and multiple augmentations. 6. **Ensembling of two independently trained nets**. | 45‑69, 82‑96, 100‑108, 115‑121, 131‑139, 167‑169 |\n",
        "\n",
        "### **Why their approach won**\n",
        "\n",
        "The combination of **large‑resolution images**, staged pre‑training, aggressive augmentation, careful imbalance handling, and per‑patient feature blending allowed the o_O model to achieve the highest weighted quadratic‑weighted κ score in the 2015 competition.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}