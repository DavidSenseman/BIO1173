{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_03_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gCdRulV4qfn"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nekk1PCx4qfn"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgvtAeBY4qfo"
      },
      "source": [
        "**Module 3: Introduction to TensorFlow**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 3 Material\n",
        "\n",
        "* Part 3.1: Deep Learning and Neural Network Introduction\n",
        "* **Part 3.2: Using Keras to Build Regression Models**\n",
        "* Part 3.3: Using Keras to Build Classification Models\n",
        "* Part 3.4: Saving and Loading a Keras Neural Network\n",
        "* Part 3.5: Early Stopping in Keras to Prevent Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4FTlK3F4qfo"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to ```/content/drive```  and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# YOU MUST RUN THIS CELL FIRST\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your GMAIL address is visible in the output above."
      ],
      "metadata": {
        "id": "NzxK5OiJoTDO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeWypKgGCtiR"
      },
      "source": [
        "# **Using Keras to Build Regression Models**\n",
        "\n",
        "[Keras](https://keras.io/) is a **software layer** or API (application interface) that \"sits\" on top of TensorFlow. This makes buidling neural networks much easier.\n",
        "\n",
        "For example, the following code uses Keras to add one layer to a neural network called `model`:\n",
        "~~~text\n",
        "model.add(Dense(25, input_dim=x_0.shape[1], activation='relu'))\n",
        "~~~\n",
        "\n",
        "When you run this line of code, this apparently simple function, `model.add(Dense(25...)` executes a large number of hidden commands \"behind the scenes\".\n",
        "\n",
        "For example, here the code Keras uses to just define the class `SimpleDense(Layer)`:\n",
        "\n",
        "~~~text\n",
        "class SimpleDense(Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "\n",
        "    # Create the state of the layer (weights)\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            trainable=True,\n",
        "            name=\"kernel\",\n",
        "        )\n",
        "        self.bias = self.add_weight(\n",
        "            shape=(self.units,),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"bias\",\n",
        "        )\n",
        "\n",
        "    # Defines the computation\n",
        "    def call(self, inputs):\n",
        "        return ops.matmul(inputs, self.kernel) + self.bias\n",
        "\n",
        "# Instantiates the layer.\n",
        "linear_layer = SimpleDense(4)\n",
        "\n",
        "# This will also call `build(input_shape)` and create the weights.\n",
        "y = linear_layer(ops.ones((2, 2)))\n",
        "assert len(linear_layer.weights) == 2\n",
        "\n",
        "# These weights are trainable, so they're listed in `trainable_weights`:\n",
        "assert len(linear_layer.trainable_weights) == 2\n",
        "~~~\n",
        "\n",
        "Rather than asking you to create each individual layer in a network with hundreds lines of Python and Tensorflow code, you will be asked only use a handful of Keras commands.\n",
        "\n",
        "Unless you are researching entirely new structures of deep neural networks, it is unlikely that you need to program TensorFlow directly. For this class, we will usually use TensorFlow _through_ Keras, rather than directly program in TensorFlow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmiV2M2K4qfq"
      },
      "source": [
        "**Neural Network Classification and Regression**\n",
        "![Neural Network Classification and Regression](https://biologicslab.co/BIO1173/images/class_2_ann_class_reg.png \"Neural Network Classification and Regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_roAjwT4qfq"
      },
      "source": [
        "## Example 1: Simple TensorFlow/Keras Regression\n",
        "\n",
        "In [Regression Analysis](https://en.wikipedia.org/wiki/Regression_analysis), the goal is to estimate the relationships between a [dependent variable](https://en.wikipedia.org/wiki/Dependent_and_independent_variables) (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more [independent variables](https://en.wikipedia.org/wiki/Dependent_and_independent_variables)  (often called 'predictors', 'covariates', 'explanatory variables' or 'features').  \n",
        "\n",
        "With multiple independent variables, the equation for a linear regression is\n",
        "\n",
        "> $ Y_{i} = \\alpha + \\beta X_{i,1} + \\beta  X_{i,2} + \\beta X_{i,n} $\n",
        "\n",
        "where $Y_{i}$ is the **_independent_** variable and $X_{i,}$ are the **_dependent_** variables.\n",
        "\n",
        "In the words of \"machine learning\", the $y$-value is the _response variable_ that we are trying to predict, while the $X$-values are the **_features_** that we are using to predict the value of $y$. While we know already know the values for $X$ and $Y$, what we don't know is the values of the coefficients, $\\alpha$ and the $\\beta$ for each category of independent variable.\n",
        "\n",
        "\n",
        "Example 1 will show how to encode the Apple Quality dataset for regression and predict values. In particular, we will see if we can predict the ripeness of an apple based on an apples's size, weight, sweetness, crunchiness, and other features. Example 1 is divided into 3 steps to make understanding of the coding easier to follow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDYv_Gbo4qfq"
      },
      "source": [
        "### Example 1-Step 1: Read dataset and create a DataFrame.\n",
        "\n",
        "The first step in most lessons in the course will begin by downloading the dataset from the course HTTPS server, and creating a DataFrame to hold the information. The code in the cell below, downloads the Apple Quality dataset and creates a DataFrame called `ripeDF`. In most Python examples, just the letters `df` is used as the name of a dataframe. In some lessons will adopt this convention, but most of the time we will use a more descriptive name, as in this example, `ripeDF` as the name of our DataFrame.   \n",
        "\n",
        "As customary, after creating our new DataFrame, we disply the first few rows and columns to make sure the data was read correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmb3cFrUCtiR"
      },
      "outputs": [],
      "source": [
        "# Example 1-Step 1: Read data and create DataFrame\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the datafile and create DataFrame\n",
        "ripeDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/apple_quality.csv\",\n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "# Create variable for later\n",
        "ripeNum = ripeDF['A_id']\n",
        "\n",
        "# Set the max rows and max columns\n",
        "pd.set_option('display.max_rows', 8)\n",
        "pd.set_option('display.max_columns', 8)\n",
        "\n",
        "# Display the DataFrame\n",
        "display(ripeDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY0kOUYM4qfr"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![__](http://biologicslab.co/BIO1173/images/class_03/class_03_2_image01a.png)\n",
        "\n",
        "Notice that the only column that contains non-numeric values is `Quality`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AljSnzni4qfr"
      },
      "source": [
        "### Example 1-Step 2: Create Feature Vector\n",
        "\n",
        "The first step in creating a feature vector is to convert any categorical values (strings) into numerical values. All of the data in the Apple Quality dataset is numeric with the exception of the column `Quality` which has 2 categorical string values, `bad` and `good`.\n",
        "\n",
        "Instead of using One-Hot Encoding, the code below uses the Pandas `map()` method to convert the string `bad` to `0` and the string `good` to the value `1`.\n",
        "~~~text\n",
        "# Define the mapping dictionary\n",
        "mapping = {'bad': 0, 'good': 1}\n",
        "# Map the strings to integers\n",
        "ripeDF['Quality'] = ripeDF['Quality'].map(mapping)\n",
        "~~~\n",
        "The next step is the generate the X-values for the regression. In this example we want to use **ALL** of the numeric values in `ripeDF` DataFrame, **except** the values in the column `Ripeness`. This should make sense. We never want to include the Y-values with the X-values.\n",
        "\n",
        "The $X$-values are the numbers contained in the `ripeDF` DataFrame columns, `Size`, `Weight`, `Sweetness`, `Crunchiness`, `Juiciness`, `Acidity` and `Quality`. One way to extract these **values** and create a Numpy array called `ripeX` is shown in this code chunk:\n",
        "~~~text\n",
        "# Generate X-values\n",
        "ripeX = ripeDF[['Size', 'Weight', 'Sweetness', 'Crunchiness',\n",
        "       'Juiciness', 'Acidity', 'Quality']].values\n",
        "ripeX = np.asarray(ripeX).astype('float32')\n",
        "~~~\n",
        "Notice the double square brackets `[[ ]]` followed by the word `values`. The command takes the Pandas numerical values in the specified columns and **_converts_** them into a Numpy array called `ripeX`.\n",
        "\n",
        "After we generate our X-values, we need to make sure all of the values are in the correct format for Keras, which is `float32`. You should **aways** convert your X and Y values to `float32` to avoid errors.\n",
        "\n",
        "\n",
        "Now we can generate our Y-values. The Y-values are in the `Ripeness` column, and we can use a similar code chunk to extract them into a Numpy array called `ripeY` as shown in this code chunk:\n",
        "~~~text\n",
        "# Generate Y-values\n",
        "ripeY = ripeDF['Ripeness'].values\n",
        "ripeY = np.asarray(ripeY).astype('float32')\n",
        "~~~\n",
        "Again, we are using the Pandas method `.values()` to convert the numerical values in the `ripeDF` DataFrame into Numpy arrays, `ripeX` and `ripeY`. These are the X and Y values that we will use to train our neural network.\n",
        "\n",
        "The last line of code prints out the Numpy array created for the $Y$-values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UW1OOu64qfr"
      },
      "outputs": [],
      "source": [
        "# Example 1-Step 2: Create Feature Vector\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Convert strings to integers\n",
        "mapping = {'bad': 0, 'good': 1}  # define mapping\n",
        "ripeDF['Quality'] = ripeDF['Quality'].map(mapping) # map\n",
        "\n",
        "# Generate X values\n",
        "ripeX = ripeDF[['Size', 'Weight', 'Sweetness', 'Crunchiness',\n",
        "       'Juiciness', 'Acidity', 'Quality']].values\n",
        "ripeX = np.asarray(ripeX).astype('float32')\n",
        "\n",
        "# Generate Y values\n",
        "ripeY = ripeDF['Ripeness'].values\n",
        "ripeY = np.asarray(ripeY).astype('float32')\n",
        "\n",
        "# Print Y values\n",
        "print(ripeY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ1FzP0r4qfr"
      },
      "source": [
        "If your code is correct, you should see that following output:\n",
        "~~~text\n",
        "[ 0.3298398   0.86753008 -0.03803333 ...  4.76385918  0.21448838\n",
        " -0.77657147]\n",
        "~~~\n",
        "\n",
        "If you compare the Numpy array printed above for the $y$-values, you will see that they are exactly the same as the `Ripeness` values that were printed out in Example 1-Step 1 above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBpmQVeJ4qfr"
      },
      "source": [
        "### Example 1-Step 3: Construct, compile and train the neural network\n",
        "\n",
        "The last step is to build our neural network, compile and train it. Using Keras these steps are relatively easy -- once you understand what the various commands mean.\n",
        "\n",
        "We begin by telling Keras what kind of model we want. There are actually several different types of neural networks. In this example we will using a **Feedforward Neural Network (FNN)** which is the simplest type, where information flows from the input layer directly through hidden layers to the output layer without loops. Keras refers to FNNs as \"Sequential\".\n",
        "\n",
        "~~~text\n",
        "# Specify the model type\n",
        "ripeModel = Sequential()\n",
        "~~~\n",
        "For our `ripeModel` model, we are going to have 3 layers:\n",
        "\n",
        "* one input layer\n",
        "* one hidden layer\n",
        "* one output layer\n",
        "\n",
        "As your might expect, we need to add each layer in the correct order, starting with the input layer.\n",
        "\n",
        "The input layer is also considered a hidden layer, so it is referred to as `Hidden 1`somewhat special since it needs to have one neuron for each X-value. The line of code for adding the input layer is:\n",
        "~~~text\n",
        "ripeModel.add(Input(shape=(ripeX.shape[1],)))  # Input layer\n",
        "~~~\n",
        "Pay particular attention to the argument `input_dim=ripeX.shape[1]`. This argument tells Keras exactly how many input neurons should be put into the input layer. When students in the course build neural networks using \"copy-and-paste\", they often forget to change this parameter which will prevent their model from training.\n",
        "\n",
        "In this example there are `7` different inputs: 'Size', 'Weight', 'Sweetness', 'Crunchiness','Juiciness', 'Acidity', and 'Quality' so the value of `input_dim=ripeX.shape[1]` would be `7`.\n",
        "\n",
        "Next, we tell Keras that we want a **_second hidden layer_** with 10 neurons. The code for adding the second hidden layer with 10 neurons is:\n",
        "\n",
        "~~~text\n",
        "ripeModel.add(Dense(10, activation='relu'))  # Hidden 2\n",
        "~~~\n",
        "The argument `Dense` tells Keras that we want **_every_** neuron in the second hidden layer to be connected to **_every_** neuron in the next layer -- the output layer.\n",
        "\n",
        "The output layer is where we will \"find\" our answer. In a regression model, there is only a **_single neuron_** in the output layer. The numerical prediction of `Ripeness` for a given apple, ($Y$) is the numerical value in this one output neuron.  In other words, the numerical value in the output neuron, at the end of training, will predict the `Ripeness` of a particular apple given its 'Size', 'Weight', 'Sweetness', 'Crunchiness','Juiciness', 'Acidity', and 'Quality'.\n",
        "\n",
        "~~~text\n",
        "ripeModel.add(Dense(1)) # Output\n",
        "~~~\n",
        "Notice that we don't specify an activation type for the output layer, since this the last neuron in the sequence.\n",
        "\n",
        "Once we have specified all of the different layers that we want in our model, the next step is to **_Compile_** the model. The compile step sets up the framework for your model. It involves:\n",
        "\n",
        "* Checking for format errors.\n",
        "* Defining the loss function, which quantifies how well the model’s predictions match the actual target values.\n",
        "* Choosing an optimizer (such as stochastic gradient descent) or setting the learning rate.\n",
        "* Selecting metrics to evaluate the model’s performance during training.\n",
        "\n",
        "In our model, we will select the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) as the loss function, and 'adam' as the optimizer. The Adam optimizer is a popular algorithm used in deep learning that helps adjust the parameters of a neural network in real-time to improve its accuracy and speed. Adam stands for _Adaptive Moment Estimation_, which means that it adapts the learning rate of each parameter based on its historical gradients and momentum.\n",
        "\n",
        "~~~text\n",
        "# Complile model\n",
        "ripeModel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "~~~\n",
        "\n",
        "The next line of code uses the Keras `model.summary()` function to print out a summary of our model.\n",
        "~~~text\n",
        "# Print model (optional)\n",
        "ripeModel.summary()\n",
        "~~~\n",
        "This step is optional.\n",
        "\n",
        "The last line of code \"fits the model to the data\". This is computerspeak for training the model.\n",
        "\n",
        "~~~text\n",
        "# Train model\n",
        "ripeModel.fit(ripeX,ripeY,verbose=2,epochs=100)\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7JhweUP4qfs"
      },
      "outputs": [],
      "source": [
        "# Example 1-Step 3: Buid the neural network\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# Specify the model type as sequential\n",
        "ripeModel = Sequential()\n",
        "\n",
        "# Add layers\n",
        "ripeModel.add(Input(shape=(ripeX.shape[1],)))  # Input layer\n",
        "ripeModel.add(Dense(64, activation='relu'))    # Hidden 1\n",
        "ripeModel.add(Dense(10, activation='relu'))    # Hidden 2\n",
        "ripeModel.add(Dense(1))  # Output layer\n",
        "\n",
        "# Complile model\n",
        "ripeModel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Print model\n",
        "ripeModel.summary()\n",
        "\n",
        "# Train model\n",
        "ripeModel.fit(ripeX,ripeY,verbose=2,epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q4Cfoo94qfs"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "~~~text\n",
        "\n",
        "Model: \"sequential_18\"\n",
        "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
        "┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n",
        "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
        "│ dense_37 (Dense)                     │ (None, 64)                  │             512 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dense_38 (Dense)                     │ (None, 10)                  │             650 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dense_39 (Dense)                     │ (None, 1)                   │              11 │\n",
        "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
        " Total params: 1,173 (4.58 KB)\n",
        " Trainable params: 1,173 (4.58 KB)\n",
        " Non-trainable params: 0 (0.00 B)\n",
        "Epoch 1/100\n",
        "125/125 - 1s - 9ms/step - loss: 2.4290\n",
        "Epoch 2/100\n",
        "125/125 - 0s - 2ms/step - loss: 1.7537\n",
        "Epoch 3/100\n",
        "125/125 - 0s - 2ms/step - loss: 1.5671\n",
        "Epoch 4/100\n",
        "125/125 - 0s - 2ms/step - loss: 1.4324\n",
        "Epoch 5/100\n",
        "125/125 - 0s - 2ms/step - loss: 1.3570\n",
        "\n",
        "........................\n",
        "\n",
        "Epoch 95/100\n",
        "125/125 - 0s - 2ms/step - loss: 0.6189\n",
        "Epoch 96/100\n",
        "125/125 - 0s - 2ms/step - loss: 0.6410\n",
        "Epoch 97/100\n",
        "125/125 - 0s - 2ms/step - loss: 0.6214\n",
        "Epoch 98/100\n",
        "125/125 - 0s - 3ms/step - loss: 0.6280\n",
        "Epoch 99/100\n",
        "125/125 - 1s - 5ms/step - loss: 0.6262\n",
        "Epoch 100/100\n",
        "125/125 - 0s - 2ms/step - loss: 0.6237\n",
        "<keras.src.callbacks.history.History at 0x7bcdefa27880>\n",
        "\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCcxJf324qfs"
      },
      "source": [
        "-----------------------------------\n",
        "\n",
        "## **\"Fit\" the Model?**\n",
        "\n",
        "In machine learning the term, \"fit the model\", means to **_train_** the model using the numerical values in our dataset. During training, the model **_learns_** from the data. In essence, the model makes a prediction of what it \"thinks\" is the correct answer, and then adjusts its trainable parameters (weights and biases) in an effort to make its predictions more accurate (i.e. minimize the loss function).\n",
        "\n",
        "The **fit step** involves:\n",
        "\n",
        "* Forward passes (feeding input data through the network).\n",
        "* Backward passes (calculating derivatives using backpropagation).\n",
        "* Updating weights based on gradients to improve predictions.\n",
        "\n",
        "The fit step is by far the most _computationally_ demanding step. This is where GPU's and TPU's are used to speed up the training. With relatively small neural networks like this one, a relatively modern laptop will come with a **central processing unit (CPU)** that can handled all the computations involved in a training a small neural network in a reasonable time period. Increasing, newer laptops are being equipped with \"AI chips\" that can speed of training by a significant amout.\n",
        "\n",
        "The command:\n",
        "~~~text\n",
        "# Fit the model to the data\n",
        "model_0.fit(x_0,y_0,verbose=2,epochs=100)\n",
        "~~~\n",
        "has the following 4 arguments:\n",
        "\n",
        "* the $X$-values\n",
        "* the $Y$-values\n",
        "* the level of _verbosity_ (how much feedback should be printed out during training)\n",
        "* the number of `epochs`\n",
        "\n",
        "In the example above, the number of epoch is set to `100`. An epoch means training the neural network with all the training data for one complete cycle. During an epoch, all of the data is used exactly once. A forward pass and a backward pass together are counted as one pass.\n",
        "\n",
        "With the verbosity set to `2`, Keras will print out the loss value, the number of milliseconds the epoch required, and the time per step.\n",
        "\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW58FX0q4qfs"
      },
      "source": [
        "Notice that the **loss value** decreases from `3.3706` after the 1st epoch (`Epoch 1/100`),\n",
        "~~~text\n",
        "Epoch 1/100\n",
        "125/125 - 1s - 9ms/step - loss: 2.4290\n",
        "~~~\n",
        "to less than a third of that amount, `0.7313` after the last epoch (`Epoch 100/100`)\n",
        "~~~text\n",
        "Epoch 100/100\n",
        "125/125 - 0s - 2ms/step - loss: 0.6237\n",
        "~~~\n",
        "This decrease in loss is due to the neural network **_learning_**. After each epoch, the network makes slight adjustments in the network's _trainable parameters_ (i.e. biases and connection weights), and runs the complete dataset through the model again to see if the updated parameters do a better job of predicting the `Ripeness` of each apple in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuuprWbX4qfs"
      },
      "source": [
        "## **Exercise 1: Simple Tensorflow Regression**\n",
        "\n",
        "For **Exercise 1** you are to build the same Feed Forward Neural network (FNN) demonstrated in Example 1. However, this time your goal will be to build a regression neural network that can predict the `Acidity` of an apple. In other words, the column `Acidity` will be your response variable (Y-values).\n",
        "\n",
        "As was done in Example 1, **Exercise 3** has been divided into 3 steps:\n",
        "\n",
        "1. Read dataset and create a DataFrame\n",
        "2. Create Feature Vector\n",
        "3. Construct, compile and train the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPvj3pJv4qfs"
      },
      "source": [
        "### **Exercise 1-Step 1: Read dataset and create a DataFrame**\n",
        "\n",
        "In the cell below, write the code to create a DataFrame called `acidDF` by reading the Apple Quality dataset from the course HTTPS server. Set the dislay options to show `8` rows and `8` columns, and then display your `acidDF` DataFrame.\n",
        "\n",
        "Make sure to change the code in Example 1 to the following:\n",
        "\n",
        "~~~text\n",
        "# Create variable for later\n",
        "acidNum = acidDF['A_id']\n",
        "~~~\n",
        "\n",
        "Otherwise, you will get an incorrect answer later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMjqaoGT4qfs"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1-Step 1 here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUjrJDGL4qft"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "![__](http://biologicslab.co/BIO1173/images/class_03/class_03_2_image02a.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X791TFF34qft"
      },
      "source": [
        "### **Exercise 1-Step 2: Create Feature Vector**\n",
        "\n",
        "In the cell below, create a Feature Vector for your regression model. Start by mapping the categorical values in the column `Quality` to integers as was demonstrated in Example 1-Step 2.\n",
        "\n",
        "Then generate your X-values. Remember that your model will be predicting `Acidity` so you **don't** want to include the values with your other X-values. You can use this code chunk to generate your X-values:\n",
        "\n",
        "~~~text\n",
        "# Generate X values\n",
        "acidX = acidDF[['Size', 'Weight', 'Sweetness', 'Crunchiness',\n",
        "       'Juiciness', 'Ripeness', 'Quality']].values\n",
        "acidX = np.asarray(acidX).astype('float32')\n",
        "~~~\n",
        "As you can see, the string `Acidity` has been replaced by the string `Ripeness`.\n",
        "\n",
        "You can use this code chunk to generate your Y-values:\n",
        "\n",
        "~~~text\n",
        "# Generate Y values\n",
        "acidY = acidDF['Acidity'].values\n",
        "acidY = np.asarray(acidY).astype('float32')\n",
        "~~~\n",
        "\n",
        "As a check, print out the first 10 values in your Numpy array `acidY`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "j-_YHNQ14qft"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1-Step 2 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiJcTzuQ4qft"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "~~~text\n",
        "[-0.49159048 -0.72280937  2.62163647 ... -1.33461139 -2.22971981\n",
        "  1.59979646]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMNR57km4qft"
      },
      "source": [
        "### **Exercise 1-Step 3: Construct, compile and train the neural network**\n",
        "\n",
        "In the cell below, contruct a regression neural network called `acidModel` by \"copy-and-paste\" the code in Example 1-Step 3.\n",
        "\n",
        "Don't forget to change the value of the argument `input_dim` in the input layer. Your model should read:\n",
        "~~~text\n",
        "acidModel.add(Dense(25, input_dim=acidX.shape[1],\n",
        "                        activation='relu'))  # Hidden 1\n",
        "~~~\n",
        "As mentioned above, students in this course often forget to change this variable causing the model not to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vyByb5J74qft"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1-Step 3 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqwPQ76j4qft"
      },
      "source": [
        "If your code is correct, your output should start with something similar to the following:\n",
        "\n",
        "~~~text\n",
        "Model: \"sequential_17\"\n",
        "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
        "┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n",
        "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
        "│ dense_34 (Dense)                     │ (None, 64)                  │             512 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dense_35 (Dense)                     │ (None, 10)                  │             650 │\n",
        "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
        "│ dense_36 (Dense)                     │ (None, 1)                   │              11 │\n",
        "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
        " Total params: 1,173 (4.58 KB)\n",
        " Trainable params: 1,173 (4.58 KB)\n",
        " Non-trainable params: 0 (0.00 B)\n",
        "Epoch 1/100\n",
        "125/125 - 1s - 10ms/step - loss: 3.6186\n",
        "Epoch 2/100\n",
        "125/125 - 0s - 3ms/step - loss: 2.8457\n",
        "Epoch 3/100\n",
        "125/125 - 0s - 3ms/step - loss: 2.5317\n",
        "Epoch 4/100\n",
        "125/125 - 0s - 2ms/step - loss: 2.3947\n",
        "Epoch 5/100\n",
        "125/125 - 0s - 2ms/step - loss: 2.2933\n",
        "Epoch 6/100\n",
        "125/125 - 0s - 3ms/step - loss: 2.2158\n",
        "\n",
        "\n",
        "\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUex_Zm74qfu"
      },
      "source": [
        "and end with something similar to the following:\n",
        "~~~text\n",
        "Epoch 95/100\n",
        "125/125 - 1s - 5ms/step - loss: 0.9980\n",
        "Epoch 96/100\n",
        "125/125 - 0s - 2ms/step - loss: 0.9712\n",
        "Epoch 97/100\n",
        "125/125 - 0s - 2ms/step - loss: 0.9829\n",
        "Epoch 98/100\n",
        "125/125 - 0s - 3ms/step - loss: 0.9880\n",
        "Epoch 99/100\n",
        "125/125 - 1s - 5ms/step - loss: 0.9779\n",
        "Epoch 100/100\n",
        "125/125 - 0s - 2ms/step - loss: 0.9732\n",
        "<keras.src.callbacks.history.History at 0x7bcdf04266e0>\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roUAphar4qfu"
      },
      "source": [
        "In the particular example, your `acidModel` started with an error rate (`loss:`) equal to `3.6186` after the first epoch. At the end of `100` epochs, the model's prediction of the acidity level had improved substantially, with a loss equal to only `0.9732`.\n",
        "\n",
        "Due to the random nature that Keras initializes weights and biases, your output will likely be somewhat different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82O2rvqiCtiR"
      },
      "source": [
        "## Introduction to Neural Network Hyperparameters\n",
        "\n",
        "If you look at the above code, you will see that the neural network contains four layers. The first layer is the input layer because it contains the **input_dim** parameter that the programmer sets to be the number of inputs the dataset has. The network needs one input neuron for every column in the data set (including dummy variables).  \n",
        "\n",
        "There are also several hidden layers, with 25 and 10 neurons each. You might be wondering how the programmer chose these numbers. Selecting a hidden neuron structure is one of the most common questions about neural networks. Unfortunately, there is no right answer. These are hyperparameters. They are settings that can affect neural network performance, yet there are no clearly defined means of setting them.\n",
        "\n",
        "In general, more hidden neurons mean more capability to fit complex problems. However, too many neurons can lead to overfitting and lengthy training times. Too few can lead to underfitting the problem and will sacrifice accuracy. Also, how many layers you have is another hyperparameter. In general, more layers allow the neural network to perform more of its feature engineering and data preprocessing. But this also comes at the expense of training times and the risk of overfitting. In general, you will see that neuron counts start larger near the input layer and tend to shrink towards the output layer in a triangular fashion.\n",
        "\n",
        "Some techniques use machine learning to optimize these values. These will be discussed later in this course.\n",
        "\n",
        "## Controlling the Amount of Output\n",
        "\n",
        "The program produces one line of output for each training epoch. You can eliminate this output by setting the verbose setting of the fit command:\n",
        "\n",
        "* **verbose=0** - No progress output (use with Jupyter if you do not want output).\n",
        "* **verbose=1** - Display progress bar, does not work well with Jupyter.\n",
        "* **verbose=2** - Summary progress output (use with Jupyter if you want to know the loss at each epoch).\n",
        "\n",
        "## Regression Prediction\n",
        "\n",
        "Next, we will perform actual predictions. The program assigns these predictions to the **pred** variable. For Example 5, these will be predictions of apple **Ripeness** from the neural network; For Exercise 5, these will be predictions of apple **Quality** from the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg2kJGVd4qfu"
      },
      "source": [
        "### Example 2: Use Model to Make Predictions\n",
        "\n",
        "The code in the cell below uses Keras' `model.predict()` function to predict the `Ripeness` of each of the 4000 apples in the Apple Quality dataset based on its 'Size', 'Weight', 'Sweetness', 'Crunchiness','Juiciness', 'Acidity', and 'Quality'. The predictions are stored in a variable called `pred`.\n",
        "\n",
        "Keep in mind that these `Ripeness` **_predictions_** are being made by the neural network model, `ripeModel` after it was trained ('fitted') to the dataset for 100 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbErLyX_CtiR",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Example 2: Predict the Ripeness of each apple in the dataset\n",
        "\n",
        "\n",
        "# Use model to make Ripeness predictions\n",
        "ripePred = ripeModel.predict(ripeX)\n",
        "\n",
        "# Print shape of pred\n",
        "print(f\"Shape of ripePred: {ripePred.shape}\")\n",
        "\n",
        "# Print first 10 predictions\n",
        "print(ripePred[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lurjQhln4qfv"
      },
      "source": [
        "If your code is correct you should see something similar to the following output:\n",
        "~~~text\n",
        "125/125 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step\n",
        "Shape of ripePred: (4000, 1)\n",
        "[[-0.17432438]\n",
        " [ 0.6893991 ]\n",
        " [ 0.19412594]\n",
        " [-3.3996456 ]\n",
        " [-1.4036642 ]\n",
        " [ 1.2713469 ]\n",
        " [-1.7385985 ]\n",
        " [ 1.4339687 ]\n",
        " [ 3.6715446 ]\n",
        " [ 2.8603623 ]]\n",
        "~~~\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRf1AzDq4qfv"
      },
      "source": [
        "### **Exercise 2: Use model to make predictions**\n",
        "\n",
        "In the cell below, use Keras' `model.predict()` function to predict the `Acidity` of each of the 4000 apples in the Apple Quality dataset based on its 'Size', 'Weight', 'Sweetness', 'Crunchiness','Juiciness', 'Ripeness' and `Quality`. Store these  predictions in a variable called `acidPred`.\n",
        "\n",
        "Print out the shape of `acidPred` and your model's first `10` acidity predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dS8vVmaB4qfv"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukhp7jnF4qfv"
      },
      "source": [
        "If your code is correct you should see something similar to the following output:\n",
        "~~~text\n",
        "125/125 [==============================] - 0s 2ms/step\n",
        "Shape of acidPred: (4000, 1)\n",
        "[[-0.96637136]\n",
        " [ 0.5449684 ]\n",
        " [ 2.9393399 ]\n",
        " [ 1.3829033 ]\n",
        " [ 1.7506608 ]\n",
        " [-2.8392882 ]\n",
        " [ 3.751297  ]\n",
        " [-1.6653696 ]\n",
        " [-2.7585254 ]\n",
        " [-1.591665  ]]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTwGsgUf4qfy"
      },
      "source": [
        "### Example 3: Determine the accuracy of the model's predictions\n",
        "\n",
        "An obvious question is how good are the neural network's predictions?  Since we know the correct `Ripeness` for each apple in the dataset, we can measure how close each neural network prediction was to the actual value.\n",
        "\n",
        "A common measure in regression analysis is the [Root-mean-square error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation).\n",
        "\n",
        "RMSE measures of the differences between predicted values and true values.\n",
        "\n",
        "The code in the cell below computes the RMSE of the `Ripeness` predictions made by `ripeModel` with the actual `Ripeness` values in the Apple Quality dataset, which are stored in the array `ripeY`. The RMSE is stored in a new variable called `score`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrY0Vs9oCtiR"
      },
      "outputs": [],
      "source": [
        "# Example 3: Determine the RMSE for model_0\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "# Measure RMSE error\n",
        "score = np.sqrt(metrics.mean_squared_error(ripePred,ripeY))\n",
        "\n",
        "# Print out the RSME\n",
        "print(f\"Final score (RMSE) for ripeModel: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzgRfYak4qfy"
      },
      "source": [
        "If your code is correct, you should see something similar to the following output:\n",
        "~~~text\n",
        "Final score (RMSE) for ripeModel: 0.8224384784698486\n",
        "~~~\n",
        "So what does this RMSE value mean?\n",
        "\n",
        "The answer is somewhat complicated. What can be said with certainty, is that an RMSE value equal to `0` would indicate `100%` perfect predictions. However, that rarely happens with real data.\n",
        "\n",
        "We also know that RMSE will always be non-negative (i.e., positive) since it is the _square_ of two numbers.\n",
        "\n",
        "However, beyond that, interpreting the meaning of RMSE is not straightforward. In general, a lower RMSE is better than a higher one. However, comparisons across different types of data would be invalid because the magnitude of the RSME is dependent on the scale of the numbers used. In other words, you will get a larger RSME when trying to predict a bigger number than a smaller one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3yqZwhnCtiS"
      },
      "source": [
        "### Example 4: Compare predictions to actual values\n",
        "\n",
        "The best way to get a sense of how accurate were the predictions of the `ripeModel` is to print out the actual values next to the model's predictive values.\n",
        "\n",
        "The code in the cell below uses a `for` loop to print out the first `10` comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl7hv8NnCtiS",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Example 4: Print out predictions and actual values\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# set the print option to 4 decimal places\n",
        "np.set_printoptions(formatter={'float': '{:.4f}'.format})\n",
        "\n",
        "# For loop for printing values\n",
        "for i in range(10):\n",
        "    print(f\"{i+1}. Apple:{ripeNum[i]}\"\n",
        "         + f\"  Ripeness: {ripeY[i]}\"\n",
        "         + f\"  Predicted: {ripePred[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPl1_Wzd4qfz"
      },
      "source": [
        "If your code is correct you should see an output that is similar to the following:\n",
        "\n",
        "~~~text\n",
        "1. Apple:0  Ripeness: 0.3298397958278656  Predicted: [0.3509]\n",
        "2. Apple:1  Ripeness: 0.867530107498169  Predicted: [0.5495]\n",
        "3. Apple:2  Ripeness: -0.03803332895040512  Predicted: [0.1906]\n",
        "4. Apple:3  Ripeness: -3.4137613773345947  Predicted: [-3.3089]\n",
        "5. Apple:4  Ripeness: -1.303849458694458  Predicted: [-1.6270]\n",
        "6. Apple:5  Ripeness: 1.9146158695220947  Predicted: [2.0036]\n",
        "7. Apple:6  Ripeness: -1.8474167585372925  Predicted: [-1.4015]\n",
        "8. Apple:7  Ripeness: 0.9744378328323364  Predicted: [1.3325]\n",
        "9. Apple:8  Ripeness: 4.080920696258545  Predicted: [3.7945]\n",
        "10. Apple:9  Ripeness: 1.620856761932373  Predicted: [3.3389]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47RO5gB64qfz"
      },
      "source": [
        "By inspection, we can see that the `ripeModel` is doing a reasonable job, but definitely **not** a perfect job, of predicting the `Ripeness` of an individual apple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj37avaR4qfz"
      },
      "source": [
        "### **Exercise 3: Determine the quality of the model's predictions**\n",
        "\n",
        "Write the code in the cell below, to compute the [Root-mean-square error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) for apple `Acidity` predicted by your `acidModel`. Print out the RSME for your `acidModel`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acFowhko4qfz"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChnT0xnt4qfz"
      },
      "source": [
        "If your code is correct you should see something similar to the following output:\n",
        "~~~text\n",
        "Final score (RMSE) for acidModel: 2.863641276499843\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAPdvYMY4qfz"
      },
      "source": [
        "### **Exercise 4: Compare predictions to actual values**\n",
        "\n",
        "In the cell below write the code to print out predictions made by your `acidModel` for the `Acidity` of the first 10 apples as well as their actual `Acidity` values, side-by-side. Don't forget to change the variable `ripeNum` to `acidNum`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6zpBDl54qfz"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JTUJ6xD4qfz"
      },
      "source": [
        "If your code is correct you should see an output that is something similar to the following:\n",
        "~~~text\n",
        "1. Apple:0  Acidity: -0.4915904700756073  Predicted: [0.8089]\n",
        "2. Apple:1  Acidity: -0.722809374332428  Predicted: [0.8771]\n",
        "3. Apple:2  Acidity: 2.621636390686035  Predicted: [1.6307]\n",
        "4. Apple:3  Acidity: 0.7907232046127319  Predicted: [0.1031]\n",
        "5. Apple:4  Acidity: 0.5019840598106384  Predicted: [-1.5020]\n",
        "6. Apple:5  Acidity: -2.981523275375366  Predicted: [6.7723]\n",
        "7. Apple:6  Acidity: 2.414170503616333  Predicted: [-2.2504]\n",
        "8. Apple:7  Acidity: -1.4701250791549683  Predicted: [-0.4674]\n",
        "9. Apple:8  Acidity: -4.8719048500061035  Predicted: [2.6443]\n",
        "10. Apple:9  Acidity: 2.185607671737671  Predicted: [3.5449]\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6su0lVUk4qf0"
      },
      "source": [
        "Compared to `ripeModel`, your `acidModel` seems to be able to make more accurate predictions. This is consistent with the observation that the RSME for your `acidModel` was smaller than the RMSE for the `ripeModel` created in Example 1.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTpDXtvE4qf0"
      },
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_03_2.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Poly-A Tail**\n",
        "\n",
        "\n",
        "## **BASIC**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/7/7b/AtariBASIC.png)\n",
        "\n",
        "\n",
        "**BASIC (Beginners' All-purpose Symbolic Instruction Code)** is a family of general-purpose, high-level programming languages designed for ease of use. The original version was created by John G. Kemeny and Thomas E. Kurtz at Dartmouth College in 1963. They wanted to enable students in non-scientific fields to use computers. At the time, nearly all computers required writing custom software, which only scientists and mathematicians tended to learn.\n",
        "\n",
        "In addition to the programming language, Kemeny and Kurtz developed the Dartmouth Time-Sharing System (DTSS), which allowed multiple users to edit and run BASIC programs simultaneously on remote terminals. This general model became popular on minicomputer systems like the PDP-11 and Data General Nova in the late 1960s and early 1970s. Hewlett-Packard produced an entire computer line for this method of operation, introducing the HP2000 series in the late 1960s and continuing sales into the 1980s. Many early video games trace their history to one of these versions of BASIC.\n",
        "\n",
        "The emergence of microcomputers in the mid-1970s led to the development of multiple BASIC dialects, including Microsoft BASIC in 1975. Due to the tiny main memory available on these machines, often 4 KB, a variety of Tiny BASIC dialects were also created. BASIC was available for almost any system of the era, and became the de facto programming language for home computer systems that emerged in the late 1970s. These PCs almost always had a BASIC interpreter installed by default, often in the machine's firmware or sometimes on a ROM cartridge.\n",
        "\n",
        "BASIC declined in popularity in the 1990s, as more powerful microcomputers came to market and programming languages with advanced features (such as Pascal and C) became tenable on such computers. By then, most nontechnical personal computer users relied on pre-written applications rather than writing their own programs. In 1991, Microsoft released Visual Basic, combining an updated version of BASIC with a visual forms builder. This reignited use of the language and \"VB\" remains a major programming language in the form of VB.NET, while a hobbyist scene for BASIC more broadly continues to exist.\n",
        "\n",
        "**Origin**\n",
        "\n",
        "John G. Kemeny was the chairman of the Dartmouth College Mathematics Department. Based largely on his reputation as an innovator in math teaching, in 1959 the college won an Alfred P. Sloan Foundation award for \\$500,000 to build a new department building. Thomas E. Kurtz had joined the department in 1956, and from the 1960s Kemeny and Kurtz agreed on the need for programming literacy among students outside the traditional STEM fields. Kemeny later noted that \"Our vision was that every student on campus should have access to a computer, and any faculty member should be able to use a computer in the classroom whenever appropriate. It was as simple as that.\"\n",
        "\n",
        "Kemeny and Kurtz had made two previous experiments with simplified languages, DARSIMCO (Dartmouth Simplified Code) and DOPE (Dartmouth Oversimplified Programming Experiment). These did not progress past a single freshman class. New experiments using Fortran and ALGOL followed, but Kurtz concluded these languages were too tricky for what they desired. As Kurtz noted, Fortran had numerous oddly formed commands, notably an \"almost impossible-to-memorize convention for specifying a loop: DO 100, I = 1, 10, 2. Is it '1, 10, 2' or '1, 2, 10', and is the comma after the line number required or not?\"\n",
        "\n",
        "Moreover, the lack of any sort of immediate feedback was a key problem; the machines of the era used batch processing and took a long time to complete a run of a program. While Kurtz was visiting MIT, John McCarthy suggested that time-sharing offered a solution; a single machine could divide up its processing time among many users, giving them the illusion of having a (slow) computer to themselves.[8] Small programs would return results in a few seconds. This led to increasing interest in a system using time-sharing and a new language specifically for use by non-STEM students.\n",
        "\n",
        "Kemeny wrote the first version of BASIC. The acronym BASIC comes from the name of an unpublished paper by Thomas Kurtz.The new language was heavily patterned on FORTRAN II; statements were one-to-a-line, numbers were used to indicate the target of loops and branches, and many of the commands were similar or identical to Fortran. However, the syntax was changed wherever it could be improved. For instance, the difficult to remember DO loop was replaced by the much easier to remember FOR I = 1 TO 10 STEP 2, and the line number used in the DO was instead indicated by the NEXT I. Likewise, the cryptic IF statement of Fortran, whose syntax matched a particular instruction of the machine on which it was originally written, became the simpler IF I=5 THEN GOTO 100. These changes made the language much less idiosyncratic while still having an overall structure and feel similar to the original FORTRAN.\n",
        "\n",
        "The project received a $300,000 grant from the National Science Foundation, which was used to purchase a GE-225 computer for processing, and a Datanet-30 realtime processor to handle the Teletype Model 33 teleprinters used for input and output. A team of a dozen undergraduates worked on the project for about a year, writing both the DTSS system and the BASIC compiler.[7] The first version BASIC language was released on 1 May 1964.[10][11]\n",
        "\n",
        "Initially, BASIC concentrated on supporting straightforward mathematical work, with matrix arithmetic support from its initial implementation as a batch language, and character string functionality being added by 1965. Usage in the university rapidly expanded, requiring the main CPU to be replaced by a GE-235,[7] and still later by a GE-635. By the early 1970s there were hundreds of terminals connected to the machines at Dartmouth, some of them remotely.\n",
        "\n",
        "Wanting use of the language to become widespread, its designers made the compiler available free of charge. In the 1960s, software became a chargeable commodity; until then, it was provided without charge as a service with expensive computers, usually available only to lease. They also made it available to high schools in the Hanover, New Hampshire, area and regionally throughout New England on Teletype Model 33 and Model 35 teleprinter terminals connected to Dartmouth via dial-up phone lines, and they put considerable effort into promoting the language. In the following years, as other dialects of BASIC appeared, Kemeny and Kurtz's original BASIC dialect became known as Dartmouth BASIC.\n",
        "\n",
        "New Hampshire recognized the accomplishment in 2019 when it erected a highway historical marker in Hanover describing the creation of \"the first user-friendly programming language\".[12]"
      ],
      "metadata": {
        "id": "Z5bWC592gvcH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rjylbkWOhn0p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}