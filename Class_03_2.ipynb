{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEEW13uCtiJ"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/master/Class_03_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BIO 1173: Intro Computational Biology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Module 3: Introduction to TensorFlow**\n",
    "\n",
    "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Integrative Biology](https://sciences.utsa.edu/integrative-biology/), [UTSA](https://www.utsa.edu/)\n",
    "\n",
    "\n",
    "### Module 3 Material\n",
    "\n",
    "* Part 3.1: Deep Learning and Neural Network Introduction\n",
    "* **Part 3.2: Using Keras to Build Regression Models**\n",
    "* Part 3.3: Using Keras to Build Classification Models\n",
    "* Part 3.4: Saving and Loading a Keras Neural Network\n",
    "* Part 3.5: Early Stopping in Keras to Prevent Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpVJpj2DCtiM"
   },
   "source": [
    "## Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wU1AMzx8CtiM",
    "outputId": "24a17c67-4563-471f-9955-dfadd0f171d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Setup\n",
    "\n",
    "Run the next code cell to load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current working directory is : C:\\Users\\David\\BIO1173\\Class_03_2\n",
      "Disk usage(total=4000108531712, used=999028875264, free=3001079656448)\n",
      "TensorFlow version: 2.10.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# You MUST run this code cell first\n",
    "\n",
    "# Import TensorFlow modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Import Keras modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# Import scikit-learn metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "# Import other needed packages\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "os.environ['tf.compat.v1.logging.set_verbosity'] = '1'\n",
    "import shutil\n",
    "path = '/'\n",
    "memory = shutil.disk_usage(path)\n",
    "dirpath = os.getcwd()\n",
    "\n",
    "# Print out diagnostics\n",
    "print(\"Your current working directory is : \" + dirpath)\n",
    "print(\"Disk\", memory)\n",
    "print(\"TensorFlow version:\", tf.version.VERSION)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeWypKgGCtiR"
   },
   "source": [
    "# Part 3.2: Using Keras to Build Regression Models\n",
    "\n",
    "[Keras](https://keras.io/) is a **_software layer_** or API (application interface) that \"sits\" on top of TensorFlow. This makes buidling neural networks much easier. \n",
    "\n",
    "For example, the following code uses Keras to add one layer to a neural network called `model`:\n",
    "~~~text\n",
    "model.add(Dense(25, input_dim=x_0.shape[1], activation='relu'))\n",
    "~~~\n",
    "\n",
    "When you run this line of code, this apparently simple function, `model.add(Dense(25...)` executes a large number of hidden commands \"behind the scenes\". \n",
    "\n",
    "For example, here the code Keras uses to just define the class `SimpleDense(Layer)`:\n",
    "\n",
    "~~~text\n",
    "class SimpleDense(Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    # Create the state of the layer (weights)\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"bias\",\n",
    "        )\n",
    "\n",
    "    # Defines the computation\n",
    "    def call(self, inputs):\n",
    "        return ops.matmul(inputs, self.kernel) + self.bias\n",
    "\n",
    "# Instantiates the layer.\n",
    "linear_layer = SimpleDense(4)\n",
    "\n",
    "# This will also call `build(input_shape)` and create the weights.\n",
    "y = linear_layer(ops.ones((2, 2)))\n",
    "assert len(linear_layer.weights) == 2\n",
    "\n",
    "# These weights are trainable, so they're listed in `trainable_weights`:\n",
    "assert len(linear_layer.trainable_weights) == 2\n",
    "~~~\n",
    "\n",
    "Rather than asking you to create each individual layer in a network with hundreds lines of Python and Tensorflow code, you will be asked only use a handful of Keras commands. \n",
    "\n",
    "Unless you are researching entirely new structures of deep neural networks, it is unlikely that you need to program TensorFlow directly. For this class, we will usually use TensorFlow _through_ Keras, rather than directly program in TensorFlow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple TensorFlow/Keras Regression\n",
    "\n",
    "In [Regression Analysis](https://en.wikipedia.org/wiki/Regression_analysis), the goal is to estimate the relationships between a [dependent variable](https://en.wikipedia.org/wiki/Dependent_and_independent_variables) (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more [independent variables](https://en.wikipedia.org/wiki/Dependent_and_independent_variables)  (often called 'predictors', 'covariates', 'explanatory variables' or 'features').  \n",
    "\n",
    "With multiple independent variables, the equation for a linear regression is \n",
    "\n",
    "> $ Y_{i} = \\alpha + \\beta X_{i,1} + \\beta  X_{i,2} + \\beta X_{i,n} $\n",
    "\n",
    "where $Y_{i}$ is the **_independent_** variable and $X_{i,}$ are the **_dependent_** variables. \n",
    "\n",
    "In the words of \"machine learning\", the $y$-value is the _response variability_ that we are trying to predict, while the $X$-values are the **_features_** that we are using to predict the value of $y$. While we know already know the values for $X$ and $Y$, what we don't know is the values of the coefficients, $\\alpha$ and the $\\beta$ for each category of independent variable. \n",
    "\n",
    "\n",
    "Example 1 shows how to encode the Apple Quality dataset for regression and predict values. We will see if we can predict the ripeness of an apple based on an apples's size, weight, sweetness, crunchiness, and other features. Example 1 is divided into 3 steps to make understanding of the coding easier to follow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5-Step 1: Read dataset and create a DataFrame.\n",
    "\n",
    "The first step in most lessons in the course will begin by downloading the dataset from the course HTTPS server, and creating a DataFrame to hold the information. The code in the cell below, downloads the Apple Quality dataset and creates a DataFrame called `ripeDF`. In most Python examples, just the letters `df` is used as the name of a dataframe. In some lessons will adopt this convention, but most of the time we will use a more descriptive name, as in this example, `ripeDF` as the name of our DataFrame.   \n",
    "\n",
    "As customary, after creating our new DataFrame, we disply the first few rows and columns to make sure the data was read correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmb3cFrUCtiR",
    "outputId": "7afe0626-d793-42c2-e8e2-a9c64a4e580f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_id</th>\n",
       "      <th>Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>...</th>\n",
       "      <th>Juiciness</th>\n",
       "      <th>Ripeness</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-3.970049</td>\n",
       "      <td>-2.512336</td>\n",
       "      <td>5.346330</td>\n",
       "      <td>...</td>\n",
       "      <td>1.844900</td>\n",
       "      <td>0.329840</td>\n",
       "      <td>-0.491590</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.195217</td>\n",
       "      <td>-2.839257</td>\n",
       "      <td>3.664059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853286</td>\n",
       "      <td>0.867530</td>\n",
       "      <td>-0.722809</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.292024</td>\n",
       "      <td>-1.351282</td>\n",
       "      <td>-1.738429</td>\n",
       "      <td>...</td>\n",
       "      <td>2.838636</td>\n",
       "      <td>-0.038033</td>\n",
       "      <td>2.621636</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.657196</td>\n",
       "      <td>-2.271627</td>\n",
       "      <td>1.324874</td>\n",
       "      <td>...</td>\n",
       "      <td>3.637970</td>\n",
       "      <td>-3.413761</td>\n",
       "      <td>0.790723</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>3996</td>\n",
       "      <td>-0.293118</td>\n",
       "      <td>1.949253</td>\n",
       "      <td>-0.204020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024523</td>\n",
       "      <td>-1.087900</td>\n",
       "      <td>1.854235</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>3997</td>\n",
       "      <td>-2.634515</td>\n",
       "      <td>-2.138247</td>\n",
       "      <td>-2.440461</td>\n",
       "      <td>...</td>\n",
       "      <td>2.199709</td>\n",
       "      <td>4.763859</td>\n",
       "      <td>-1.334611</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>3998</td>\n",
       "      <td>-4.008004</td>\n",
       "      <td>-1.779337</td>\n",
       "      <td>2.366397</td>\n",
       "      <td>...</td>\n",
       "      <td>2.161435</td>\n",
       "      <td>0.214488</td>\n",
       "      <td>-2.229720</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>3999</td>\n",
       "      <td>0.278540</td>\n",
       "      <td>-1.715505</td>\n",
       "      <td>0.121217</td>\n",
       "      <td>...</td>\n",
       "      <td>1.266677</td>\n",
       "      <td>-0.776571</td>\n",
       "      <td>1.599796</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      A_id      Size    Weight  Sweetness  ...  Juiciness  Ripeness   Acidity  \\\n",
       "0        0 -3.970049 -2.512336   5.346330  ...   1.844900  0.329840 -0.491590   \n",
       "1        1 -1.195217 -2.839257   3.664059  ...   0.853286  0.867530 -0.722809   \n",
       "2        2 -0.292024 -1.351282  -1.738429  ...   2.838636 -0.038033  2.621636   \n",
       "3        3 -0.657196 -2.271627   1.324874  ...   3.637970 -3.413761  0.790723   \n",
       "...    ...       ...       ...        ...  ...        ...       ...       ...   \n",
       "3996  3996 -0.293118  1.949253  -0.204020  ...   0.024523 -1.087900  1.854235   \n",
       "3997  3997 -2.634515 -2.138247  -2.440461  ...   2.199709  4.763859 -1.334611   \n",
       "3998  3998 -4.008004 -1.779337   2.366397  ...   2.161435  0.214488 -2.229720   \n",
       "3999  3999  0.278540 -1.715505   0.121217  ...   1.266677 -0.776571  1.599796   \n",
       "\n",
       "      Quality  \n",
       "0        good  \n",
       "1        good  \n",
       "2         bad  \n",
       "3        good  \n",
       "...       ...  \n",
       "3996     good  \n",
       "3997      bad  \n",
       "3998     good  \n",
       "3999     good  \n",
       "\n",
       "[4000 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 5-Step 1: Read data and create DataFrame\n",
    "\n",
    "# Read the datafile and create DataFrame\n",
    "ripeDF = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/apple_quality.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "# Create variable for later\n",
    "appleNum = ripeDF['A_id']\n",
    "\n",
    "# Set the max rows and max columns\n",
    "pd.set_option('display.max_rows', 8)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(ripeDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "![__](http://biologicslab.co/BIO1173/images/class_03_2_Exm1.png)\n",
    "\n",
    "Notice that the only column that contains non-numeric values is `Quality`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1-Step 2: Create Feature Vector\n",
    "\n",
    "The first step in creating a feature vector is to convert any categorical values (strings) into numerical values. All of the data in the Apple Quality dataset is numeric with the exception of the column `Quality` which has 2 categorical string values, `bad` and `good`. \n",
    "\n",
    "Instead of using One-Hot Encoding, the code below uses the Pandas `map()` method to convert the string `bad` to `0` and the string `good` to the value `1`. \n",
    "~~~text\n",
    "# Define the mapping dictionary\n",
    "mapping = {'bad': 0, 'good': 1}\n",
    "# Map the strings to integers\n",
    "ripeDF['Quality'] = ripeDF['Quality'].map(mapping)\n",
    "~~~\n",
    "The next step is the generate the X-values for the regression. In this example we want to use **ALL** of the numeric values in `ripeDF` DataFrame, **except** the values in the column `Ripeness`. This should make sense. We never want to include the Y-values with the X-values.\n",
    "\n",
    "The $X$-values are the numbers contained in the `ripeDF` DataFrame columns, `Size`, `Weight`, `Sweetness`, `Crunchiness`, `Juiciness`, `Acidity` and `Quality`. One way to extract these **values** and create a Numpy array called `ripeX` is shown in this code chunk: \n",
    "~~~text\n",
    "# Generate X-values\n",
    "ripeX = ripeDF[['Size', 'Weight', 'Sweetness', 'Crunchiness',\n",
    "       'Juiciness', 'Acidity', 'Quality']].values\n",
    "ripeX = np.asarray(ripeX).astype('float32')\n",
    "~~~\n",
    "Notice the double square brackets `[[ ]]` followed by the word `values`. The command takes the Pandas numerical values in the specified columns and **_converts_** them into a Numpy array called `ripeX`. \n",
    "\n",
    "After we generate our X-values, we need to make sure all of the values are in the correct format for Keras, which is `float32`. You should **aways** convert your X and Y values to `float32` to avoid errors. \n",
    "\n",
    "\n",
    "Now we can generate our Y-values. The Y-values are in the `Ripeness` column, and we can use a similar code chunk to extract them into a Numpy array called `ripeY` as shown in this code chunk:\n",
    "~~~text\n",
    "# Generate Y-values\n",
    "ripeY = ripeDF['Ripeness'].values\n",
    "ripeY = np.asarray(ripeY).astype('float32')\n",
    "~~~\n",
    "Again, we are using the Pandas method `.values()` to convert the numerical values in the `ripeDF` DataFrame into Numpy arrays, `ripeX` and `ripeY`. These are the X and Y values that we will use to train our neural network.\n",
    "\n",
    "The last line of code prints out the Numpy array created for the $Y$-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmb3cFrUCtiR",
    "outputId": "7afe0626-d793-42c2-e8e2-a9c64a4e580f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3298398   0.8675301  -0.03803333 ...  4.7638593   0.21448839\n",
      " -0.77657145]\n"
     ]
    }
   ],
   "source": [
    "# Example 1-Step 2: Create Feature Vector\n",
    "\n",
    "# Convert strings to integers\n",
    "mapping = {'bad': 0, 'good': 1}  # define mapping\n",
    "ripeDF['Quality'] = ripeDF['Quality'].map(mapping) # map\n",
    "\n",
    "# Generate X values\n",
    "ripeX = ripeDF[['Size', 'Weight', 'Sweetness', 'Crunchiness',\n",
    "       'Juiciness', 'Acidity', 'Quality']].values\n",
    "ripeX = np.asarray(ripeX).astype('float32')\n",
    "\n",
    "# Generate Y values\n",
    "ripeY = ripeDF['Ripeness'].values\n",
    "ripeY = np.asarray(ripeY).astype('float32')\n",
    "\n",
    "# Print Y values\n",
    "print(ripeY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see that following output:\n",
    "~~~text\n",
    "[ 0.3298398   0.86753008 -0.03803333 ...  4.76385918  0.21448838\n",
    " -0.77657147]\n",
    "~~~\n",
    "\n",
    "If you compare the Numpy array printed above for the $y$-values, you will see that they are exactly the same as the `Ripeness` values that were printed out in Example 1-Step 1 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1-Step 3: Construct, compile and train the neural network\n",
    "\n",
    "The last step is to build our neural network, compile and train it. Using Keras these steps are relatively easy -- once you understand what the various commands mean.\n",
    "\n",
    "We begin by telling Keras what kind of model we want. There are actually several different types of neural networks. In this example we will using a **Feedforward Neural Network (FNN)** which is the simplest type, where information flows from the input layer directly through hidden layers to the output layer without loops. Keras refers to FNNs as \"Sequential\".\n",
    "\n",
    "~~~text\n",
    "# Specify the model type\n",
    "ripeModel = Sequential()\n",
    "~~~\n",
    "For our `ripeModel` model, we are going to have 3 layers:\n",
    "\n",
    "* one input layer\n",
    "* one hidden layer\n",
    "* one output layer \n",
    "\n",
    "As your might expect, we need to add each layer in the correct order, starting with the input layer. \n",
    "\n",
    "The input layer is also considered a hidden layer, so it is referred to as `Hidden 1`somewhat special since it needs to have one neuron for each X-value. The line of code for adding the input layer is: \n",
    "~~~text\n",
    "ripeModel.add(Dense(25, input_dim=ripeX.shape[1], \n",
    "                        activation='relu'))  # Hidden 1 \n",
    "~~~\n",
    "Pay particular attention to the argument `input_dim=ripeX.shape[1]`. This argument tells Keras exactly how many input neurons should be put into the input layer. When students in the course build neural networks using \"copy-and-paste\", they often forget to change this parameter which will prevent their model from training. \n",
    "\n",
    "In this example there are `7` different inputs: 'Size', 'Weight', 'Sweetness', 'Crunchiness','Juiciness', 'Acidity', and 'Quality' so the value of `input_dim=ripeX.shape[1]` would be `7`.\n",
    "\n",
    "Next, we tell Keras that we want a **_second hidden layer_** with 10 neurons. The code for adding the second hidden layer with 10 neurons is:\n",
    "\n",
    "~~~text\n",
    "ripeModel.add(Dense(10, activation='relu'))  # Hidden 2\n",
    "~~~\n",
    "The argument `Dense` tells Keras that we want **_every_** neuron in the second hidden layer to be connected to **_every_** neuron in the next layer -- the output layer.\n",
    "\n",
    "The output layer is where we will \"find\" our answer. In a regression model, there is only a **_single neuron_** in the output layer. The numerical prediction of `Ripeness` for a given apple, ($Y$) is the numerical value in this one output neuron.  In other words, the numerical value in the output neuron, at the end of training, will predict the `Ripeness` of a particular apple given its 'Size', 'Weight', 'Sweetness', 'Crunchiness','Juiciness', 'Acidity', and 'Quality'. \n",
    "\n",
    "~~~text\n",
    "ripeModel.add(Dense(1)) # Output\n",
    "~~~\n",
    "Notice that we don't specify an activation type for the output layer, since this the last neuron in the sequence.\n",
    "\n",
    "Once we have specified all of the different layers that we want in our model, the next step is to **_Compile_** the model. The compile step sets up the framework for your model. It involves:\n",
    "\n",
    "* Checking for format errors.\n",
    "* Defining the loss function, which quantifies how well the model’s predictions match the actual target values.\n",
    "* Choosing an optimizer (such as stochastic gradient descent) or setting the learning rate.\n",
    "* Selecting metrics to evaluate the model’s performance during training.\n",
    "\n",
    "In our model, we will select the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) as the loss function, and 'adam' as the optimizer. The Adam optimizer is a popular algorithm used in deep learning that helps adjust the parameters of a neural network in real-time to improve its accuracy and speed. Adam stands for _Adaptive Moment Estimation_, which means that it adapts the learning rate of each parameter based on its historical gradients and momentum. \n",
    "\n",
    "~~~text\n",
    "# Complile model\n",
    "ripeModel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "~~~\n",
    "\n",
    "The next line of code uses the Keras `model.summary()` function to print out a summary of our model.\n",
    "~~~text\n",
    "# Print model (optional)\n",
    "ripeModel.summary()\n",
    "~~~\n",
    "This step is optional. \n",
    "\n",
    "The last line of code \"fits the model to the data\". This is computerspeak for training the model.\n",
    "\n",
    "~~~text\n",
    "# Train model\n",
    "ripeModel.fit(ripeX,ripeY,verbose=2,epochs=100)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 25)                200       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                260       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 471\n",
      "Trainable params: 471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "125/125 - 1s - loss: 2.9913 - 593ms/epoch - 5ms/step\n",
      "Epoch 2/100\n",
      "125/125 - 0s - loss: 2.0743 - 322ms/epoch - 3ms/step\n",
      "Epoch 3/100\n",
      "125/125 - 0s - loss: 1.7320 - 326ms/epoch - 3ms/step\n",
      "Epoch 4/100\n",
      "125/125 - 0s - loss: 1.5860 - 324ms/epoch - 3ms/step\n",
      "Epoch 5/100\n",
      "125/125 - 0s - loss: 1.4896 - 328ms/epoch - 3ms/step\n",
      "Epoch 6/100\n",
      "125/125 - 0s - loss: 1.4013 - 370ms/epoch - 3ms/step\n",
      "Epoch 7/100\n",
      "125/125 - 0s - loss: 1.3366 - 391ms/epoch - 3ms/step\n",
      "Epoch 8/100\n",
      "125/125 - 0s - loss: 1.2780 - 396ms/epoch - 3ms/step\n",
      "Epoch 9/100\n",
      "125/125 - 0s - loss: 1.2224 - 397ms/epoch - 3ms/step\n",
      "Epoch 10/100\n",
      "125/125 - 0s - loss: 1.1768 - 408ms/epoch - 3ms/step\n",
      "Epoch 11/100\n",
      "125/125 - 0s - loss: 1.1398 - 411ms/epoch - 3ms/step\n",
      "Epoch 12/100\n",
      "125/125 - 0s - loss: 1.1057 - 414ms/epoch - 3ms/step\n",
      "Epoch 13/100\n",
      "125/125 - 0s - loss: 1.0814 - 411ms/epoch - 3ms/step\n",
      "Epoch 14/100\n",
      "125/125 - 0s - loss: 1.0517 - 415ms/epoch - 3ms/step\n",
      "Epoch 15/100\n",
      "125/125 - 0s - loss: 1.0337 - 412ms/epoch - 3ms/step\n",
      "Epoch 16/100\n",
      "125/125 - 0s - loss: 1.0184 - 411ms/epoch - 3ms/step\n",
      "Epoch 17/100\n",
      "125/125 - 0s - loss: 1.0003 - 411ms/epoch - 3ms/step\n",
      "Epoch 18/100\n",
      "125/125 - 0s - loss: 0.9837 - 411ms/epoch - 3ms/step\n",
      "Epoch 19/100\n",
      "125/125 - 0s - loss: 0.9688 - 413ms/epoch - 3ms/step\n",
      "Epoch 20/100\n",
      "125/125 - 0s - loss: 0.9608 - 416ms/epoch - 3ms/step\n",
      "Epoch 21/100\n",
      "125/125 - 0s - loss: 0.9463 - 414ms/epoch - 3ms/step\n",
      "Epoch 22/100\n",
      "125/125 - 0s - loss: 0.9357 - 414ms/epoch - 3ms/step\n",
      "Epoch 23/100\n",
      "125/125 - 0s - loss: 0.9246 - 417ms/epoch - 3ms/step\n",
      "Epoch 24/100\n",
      "125/125 - 0s - loss: 0.9131 - 414ms/epoch - 3ms/step\n",
      "Epoch 25/100\n",
      "125/125 - 0s - loss: 0.9200 - 415ms/epoch - 3ms/step\n",
      "Epoch 26/100\n",
      "125/125 - 0s - loss: 0.9007 - 417ms/epoch - 3ms/step\n",
      "Epoch 27/100\n",
      "125/125 - 0s - loss: 0.8887 - 416ms/epoch - 3ms/step\n",
      "Epoch 28/100\n",
      "125/125 - 0s - loss: 0.8830 - 417ms/epoch - 3ms/step\n",
      "Epoch 29/100\n",
      "125/125 - 0s - loss: 0.8833 - 419ms/epoch - 3ms/step\n",
      "Epoch 30/100\n",
      "125/125 - 0s - loss: 0.8719 - 416ms/epoch - 3ms/step\n",
      "Epoch 31/100\n",
      "125/125 - 0s - loss: 0.8668 - 417ms/epoch - 3ms/step\n",
      "Epoch 32/100\n",
      "125/125 - 0s - loss: 0.8593 - 413ms/epoch - 3ms/step\n",
      "Epoch 33/100\n",
      "125/125 - 0s - loss: 0.8559 - 415ms/epoch - 3ms/step\n",
      "Epoch 34/100\n",
      "125/125 - 0s - loss: 0.8479 - 415ms/epoch - 3ms/step\n",
      "Epoch 35/100\n",
      "125/125 - 0s - loss: 0.8439 - 413ms/epoch - 3ms/step\n",
      "Epoch 36/100\n",
      "125/125 - 0s - loss: 0.8382 - 416ms/epoch - 3ms/step\n",
      "Epoch 37/100\n",
      "125/125 - 0s - loss: 0.8366 - 417ms/epoch - 3ms/step\n",
      "Epoch 38/100\n",
      "125/125 - 0s - loss: 0.8322 - 420ms/epoch - 3ms/step\n",
      "Epoch 39/100\n",
      "125/125 - 0s - loss: 0.8279 - 415ms/epoch - 3ms/step\n",
      "Epoch 40/100\n",
      "125/125 - 0s - loss: 0.8227 - 416ms/epoch - 3ms/step\n",
      "Epoch 41/100\n",
      "125/125 - 0s - loss: 0.8254 - 419ms/epoch - 3ms/step\n",
      "Epoch 42/100\n",
      "125/125 - 0s - loss: 0.8195 - 420ms/epoch - 3ms/step\n",
      "Epoch 43/100\n",
      "125/125 - 0s - loss: 0.8086 - 419ms/epoch - 3ms/step\n",
      "Epoch 44/100\n",
      "125/125 - 0s - loss: 0.8091 - 419ms/epoch - 3ms/step\n",
      "Epoch 45/100\n",
      "125/125 - 0s - loss: 0.8079 - 421ms/epoch - 3ms/step\n",
      "Epoch 46/100\n",
      "125/125 - 0s - loss: 0.8018 - 418ms/epoch - 3ms/step\n",
      "Epoch 47/100\n",
      "125/125 - 0s - loss: 0.8015 - 414ms/epoch - 3ms/step\n",
      "Epoch 48/100\n",
      "125/125 - 0s - loss: 0.7943 - 417ms/epoch - 3ms/step\n",
      "Epoch 49/100\n",
      "125/125 - 0s - loss: 0.7899 - 416ms/epoch - 3ms/step\n",
      "Epoch 50/100\n",
      "125/125 - 0s - loss: 0.7885 - 419ms/epoch - 3ms/step\n",
      "Epoch 51/100\n",
      "125/125 - 0s - loss: 0.7911 - 418ms/epoch - 3ms/step\n",
      "Epoch 52/100\n",
      "125/125 - 0s - loss: 0.7867 - 419ms/epoch - 3ms/step\n",
      "Epoch 53/100\n",
      "125/125 - 0s - loss: 0.7840 - 423ms/epoch - 3ms/step\n",
      "Epoch 54/100\n",
      "125/125 - 0s - loss: 0.7898 - 417ms/epoch - 3ms/step\n",
      "Epoch 55/100\n",
      "125/125 - 0s - loss: 0.7776 - 421ms/epoch - 3ms/step\n",
      "Epoch 56/100\n",
      "125/125 - 0s - loss: 0.7818 - 419ms/epoch - 3ms/step\n",
      "Epoch 57/100\n",
      "125/125 - 0s - loss: 0.7816 - 424ms/epoch - 3ms/step\n",
      "Epoch 58/100\n",
      "125/125 - 0s - loss: 0.7774 - 427ms/epoch - 3ms/step\n",
      "Epoch 59/100\n",
      "125/125 - 0s - loss: 0.7699 - 419ms/epoch - 3ms/step\n",
      "Epoch 60/100\n",
      "125/125 - 0s - loss: 0.7715 - 418ms/epoch - 3ms/step\n",
      "Epoch 61/100\n",
      "125/125 - 0s - loss: 0.7657 - 422ms/epoch - 3ms/step\n",
      "Epoch 62/100\n",
      "125/125 - 0s - loss: 0.7677 - 421ms/epoch - 3ms/step\n",
      "Epoch 63/100\n",
      "125/125 - 0s - loss: 0.7683 - 420ms/epoch - 3ms/step\n",
      "Epoch 64/100\n",
      "125/125 - 0s - loss: 0.7593 - 419ms/epoch - 3ms/step\n",
      "Epoch 65/100\n",
      "125/125 - 0s - loss: 0.7617 - 416ms/epoch - 3ms/step\n",
      "Epoch 66/100\n",
      "125/125 - 0s - loss: 0.7589 - 419ms/epoch - 3ms/step\n",
      "Epoch 67/100\n",
      "125/125 - 0s - loss: 0.7651 - 419ms/epoch - 3ms/step\n",
      "Epoch 68/100\n",
      "125/125 - 0s - loss: 0.7564 - 420ms/epoch - 3ms/step\n",
      "Epoch 69/100\n",
      "125/125 - 0s - loss: 0.7569 - 423ms/epoch - 3ms/step\n",
      "Epoch 70/100\n",
      "125/125 - 0s - loss: 0.7599 - 422ms/epoch - 3ms/step\n",
      "Epoch 71/100\n",
      "125/125 - 0s - loss: 0.7547 - 423ms/epoch - 3ms/step\n",
      "Epoch 72/100\n",
      "125/125 - 0s - loss: 0.7484 - 424ms/epoch - 3ms/step\n",
      "Epoch 73/100\n",
      "125/125 - 0s - loss: 0.7456 - 411ms/epoch - 3ms/step\n",
      "Epoch 74/100\n",
      "125/125 - 0s - loss: 0.7477 - 411ms/epoch - 3ms/step\n",
      "Epoch 75/100\n",
      "125/125 - 0s - loss: 0.7522 - 412ms/epoch - 3ms/step\n",
      "Epoch 76/100\n",
      "125/125 - 0s - loss: 0.7471 - 412ms/epoch - 3ms/step\n",
      "Epoch 77/100\n",
      "125/125 - 0s - loss: 0.7474 - 420ms/epoch - 3ms/step\n",
      "Epoch 78/100\n",
      "125/125 - 0s - loss: 0.7489 - 416ms/epoch - 3ms/step\n",
      "Epoch 79/100\n",
      "125/125 - 0s - loss: 0.7427 - 412ms/epoch - 3ms/step\n",
      "Epoch 80/100\n",
      "125/125 - 0s - loss: 0.7383 - 411ms/epoch - 3ms/step\n",
      "Epoch 81/100\n",
      "125/125 - 0s - loss: 0.7379 - 413ms/epoch - 3ms/step\n",
      "Epoch 82/100\n",
      "125/125 - 0s - loss: 0.7371 - 412ms/epoch - 3ms/step\n",
      "Epoch 83/100\n",
      "125/125 - 0s - loss: 0.7365 - 413ms/epoch - 3ms/step\n",
      "Epoch 84/100\n",
      "125/125 - 0s - loss: 0.7322 - 416ms/epoch - 3ms/step\n",
      "Epoch 85/100\n",
      "125/125 - 0s - loss: 0.7310 - 419ms/epoch - 3ms/step\n",
      "Epoch 86/100\n",
      "125/125 - 0s - loss: 0.7353 - 423ms/epoch - 3ms/step\n",
      "Epoch 87/100\n",
      "125/125 - 0s - loss: 0.7313 - 412ms/epoch - 3ms/step\n",
      "Epoch 88/100\n",
      "125/125 - 0s - loss: 0.7250 - 410ms/epoch - 3ms/step\n",
      "Epoch 89/100\n",
      "125/125 - 0s - loss: 0.7263 - 416ms/epoch - 3ms/step\n",
      "Epoch 90/100\n",
      "125/125 - 0s - loss: 0.7241 - 413ms/epoch - 3ms/step\n",
      "Epoch 91/100\n",
      "125/125 - 0s - loss: 0.7266 - 421ms/epoch - 3ms/step\n",
      "Epoch 92/100\n",
      "125/125 - 0s - loss: 0.7225 - 421ms/epoch - 3ms/step\n",
      "Epoch 93/100\n",
      "125/125 - 0s - loss: 0.7230 - 411ms/epoch - 3ms/step\n",
      "Epoch 94/100\n",
      "125/125 - 0s - loss: 0.7228 - 406ms/epoch - 3ms/step\n",
      "Epoch 95/100\n",
      "125/125 - 0s - loss: 0.7201 - 414ms/epoch - 3ms/step\n",
      "Epoch 96/100\n",
      "125/125 - 0s - loss: 0.7195 - 412ms/epoch - 3ms/step\n",
      "Epoch 97/100\n",
      "125/125 - 0s - loss: 0.7196 - 415ms/epoch - 3ms/step\n",
      "Epoch 98/100\n",
      "125/125 - 0s - loss: 0.7287 - 416ms/epoch - 3ms/step\n",
      "Epoch 99/100\n",
      "125/125 - 0s - loss: 0.7190 - 416ms/epoch - 3ms/step\n",
      "Epoch 100/100\n",
      "125/125 - 0s - loss: 0.7169 - 415ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e2b58bcbe0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 5-Step 3: Buid the neural network \n",
    "\n",
    "# Specify the model type as sequential\n",
    "ripeModel = Sequential()\n",
    "\n",
    "# Construct model\n",
    "ripeModel.add(Dense(25, input_dim=ripeX.shape[1], \n",
    "                        activation='relu'))  # Hidden 1\n",
    "ripeModel.add(Dense(10, activation='relu'))  # Hidden 2\n",
    "ripeModel.add(Dense(1)) # Output\n",
    "\n",
    "# Complile model\n",
    "ripeModel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Print model\n",
    "ripeModel.summary()\n",
    "\n",
    "# Train model\n",
    "ripeModel.fit(ripeX,ripeY,verbose=2,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "~~~text\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " dense (Dense)               (None, 25)                200       \n",
    "                                                                 \n",
    " dense_1 (Dense)             (None, 10)                260       \n",
    "                                                                 \n",
    " dense_2 (Dense)             (None, 1)                 11        \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 471\n",
    "Trainable params: 471\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Epoch 1/100\n",
    "125/125 - 1s - loss: 2.8682 - 602ms/epoch - 5ms/step\n",
    "Epoch 2/100\n",
    "125/125 - 0s - loss: 2.0818 - 327ms/epoch - 3ms/step\n",
    "Epoch 3/100\n",
    "125/125 - 0s - loss: 1.7714 - 332ms/epoch - 3ms/step\n",
    "Epoch 4/100\n",
    "125/125 - 0s - loss: 1.5415 - 324ms/epoch - 3ms/step\n",
    "Epoch 5/100\n",
    "125/125 - 0s - loss: 1.4121 - 324ms/epoch - 3ms/step\n",
    "\n",
    "........................\n",
    "\n",
    "Epoch 95/100\n",
    "125/125 - 0s - loss: 0.7104 - 461ms/epoch - 4ms/step\n",
    "Epoch 96/100\n",
    "125/125 - 0s - loss: 0.7077 - 465ms/epoch - 4ms/step\n",
    "Epoch 97/100\n",
    "125/125 - 0s - loss: 0.7116 - 466ms/epoch - 4ms/step\n",
    "Epoch 98/100\n",
    "125/125 - 0s - loss: 0.7080 - 497ms/epoch - 4ms/step\n",
    "Epoch 99/100\n",
    "125/125 - 0s - loss: 0.7100 - 463ms/epoch - 4ms/step\n",
    "Epoch 100/100\n",
    "125/125 - 0s - loss: 0.7053 - 430ms/epoch - 3ms/step\n",
    "\n",
    "<keras.callbacks.History at 0x1d8ba2baa30>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "\n",
    "## **\"Fit\" the Model?**\n",
    "\n",
    "In machine learning the term, \"fit the model\", means to **_train_** the model using the numerical values in our dataset. During training, the model **_learns_** from the data. In essence, the model makes a prediction of what it \"thinks\" is the correct answer, and then adjusts its trainable parameters (weights and biases) in an effort to make its predictions more accurate (i.e. minimize the loss function).\n",
    "\n",
    "The **fit step** involves:\n",
    "\n",
    "* Forward passes (feeding input data through the network).\n",
    "* Backward passes (calculating derivatives using backpropagation).\n",
    "* Updating weights based on gradients to improve predictions.\n",
    "\n",
    "The fit step is by far the most _computationally_ demanding step. This is where GPU's and TPU's are used to speed up the training. With relatively small neural networks like this one, a relatively modern laptop will come with a **_central processing unit (CPU)_** that can handled all the computations involved in a training a small neural network in a reasonable time period. Increasing, newer laptops are being equipped with \"AI chips\" that can speed of training by a significant amout. \n",
    "\n",
    "The command:\n",
    "~~~text\n",
    "# Fit the model to the data\n",
    "model_0.fit(x_0,y_0,verbose=2,epochs=100)\n",
    "~~~\n",
    "has the following 4 arguments: \n",
    "\n",
    "* the $X$-values\n",
    "* the $Y$-values\n",
    "* the level of _verbosity_ (how much feedback should be printed out during training)\n",
    "* the number of `epochs`\n",
    "\n",
    "In the example above, the number of epoch is set to `100`. An epoch means training the neural network with all the training data for one complete cycle. During an epoch, all of the data is used exactly once. A forward pass and a backward pass together are counted as one pass. \n",
    "\n",
    "With the verbosity set to `2`, Keras will print out the loss value, the number of milliseconds the epoch required, and the time per step. \n",
    "\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the **loss value** decreases from `3.3706` after the 1st epoch (`Epoch 1/100`), \n",
    "~~~text\n",
    "Epoch 1/100\n",
    "125/125 - 1s - loss: 2.4560 - 966ms/epoch - 8ms/step\n",
    "~~~\n",
    "to less than a third of that amount, `0.7313` after the last epoch (`Epoch 100/100`)\n",
    "~~~text\n",
    "Epoch 100/100\n",
    "125/125 - 0s - loss: 0.7313 - 374ms/epoch - 3ms/step\n",
    "~~~\n",
    "This decrease in loss is due to the neural network **_learning_**. After each epoch, the network makes slight adjustments in the network's _trainable parameters_ (i.e. biases and connection weights), and runs the complete dataset through the model again to see if the updated parameters do a better job of predicting the `Ripeness` of each apple in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 1: Simple Tensorflow Regression**\n",
    "\n",
    "For **Exercise 1** you are to build the same Feed Forward Neural network (FNN) demonstrated in Example 1. However, this time your goal will be to build a regression neural network that can predict the `Acidity` of an apple. In other words, the column `Acidity` will be your response variable (Y-values). \n",
    "\n",
    "As was done in Example 1, **Exercise 3** has been divided into 3 steps:\n",
    "\n",
    "1. Read dataset and create a DataFrame\n",
    "2. Create Feature Vector\n",
    "3. Construct, compile and train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1-Step 1: Read dataset and create a DataFrame**\n",
    "\n",
    "In the cell below, write the code to create a DataFrame called `acidDF` by reading the Apple Quality dataset from the course HTTPS server. Set the dislay options to show `8` rows and `8` columns, and then display your `acidDF` DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmb3cFrUCtiR",
    "outputId": "7afe0626-d793-42c2-e8e2-a9c64a4e580f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_id</th>\n",
       "      <th>Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>...</th>\n",
       "      <th>Juiciness</th>\n",
       "      <th>Ripeness</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-3.970049</td>\n",
       "      <td>-2.512336</td>\n",
       "      <td>5.346330</td>\n",
       "      <td>...</td>\n",
       "      <td>1.844900</td>\n",
       "      <td>0.329840</td>\n",
       "      <td>-0.491590</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.195217</td>\n",
       "      <td>-2.839257</td>\n",
       "      <td>3.664059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853286</td>\n",
       "      <td>0.867530</td>\n",
       "      <td>-0.722809</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.292024</td>\n",
       "      <td>-1.351282</td>\n",
       "      <td>-1.738429</td>\n",
       "      <td>...</td>\n",
       "      <td>2.838636</td>\n",
       "      <td>-0.038033</td>\n",
       "      <td>2.621636</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.657196</td>\n",
       "      <td>-2.271627</td>\n",
       "      <td>1.324874</td>\n",
       "      <td>...</td>\n",
       "      <td>3.637970</td>\n",
       "      <td>-3.413761</td>\n",
       "      <td>0.790723</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>3996</td>\n",
       "      <td>-0.293118</td>\n",
       "      <td>1.949253</td>\n",
       "      <td>-0.204020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024523</td>\n",
       "      <td>-1.087900</td>\n",
       "      <td>1.854235</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>3997</td>\n",
       "      <td>-2.634515</td>\n",
       "      <td>-2.138247</td>\n",
       "      <td>-2.440461</td>\n",
       "      <td>...</td>\n",
       "      <td>2.199709</td>\n",
       "      <td>4.763859</td>\n",
       "      <td>-1.334611</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>3998</td>\n",
       "      <td>-4.008004</td>\n",
       "      <td>-1.779337</td>\n",
       "      <td>2.366397</td>\n",
       "      <td>...</td>\n",
       "      <td>2.161435</td>\n",
       "      <td>0.214488</td>\n",
       "      <td>-2.229720</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>3999</td>\n",
       "      <td>0.278540</td>\n",
       "      <td>-1.715505</td>\n",
       "      <td>0.121217</td>\n",
       "      <td>...</td>\n",
       "      <td>1.266677</td>\n",
       "      <td>-0.776571</td>\n",
       "      <td>1.599796</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      A_id      Size    Weight  Sweetness  ...  Juiciness  Ripeness   Acidity  \\\n",
       "0        0 -3.970049 -2.512336   5.346330  ...   1.844900  0.329840 -0.491590   \n",
       "1        1 -1.195217 -2.839257   3.664059  ...   0.853286  0.867530 -0.722809   \n",
       "2        2 -0.292024 -1.351282  -1.738429  ...   2.838636 -0.038033  2.621636   \n",
       "3        3 -0.657196 -2.271627   1.324874  ...   3.637970 -3.413761  0.790723   \n",
       "...    ...       ...       ...        ...  ...        ...       ...       ...   \n",
       "3996  3996 -0.293118  1.949253  -0.204020  ...   0.024523 -1.087900  1.854235   \n",
       "3997  3997 -2.634515 -2.138247  -2.440461  ...   2.199709  4.763859 -1.334611   \n",
       "3998  3998 -4.008004 -1.779337   2.366397  ...   2.161435  0.214488 -2.229720   \n",
       "3999  3999  0.278540 -1.715505   0.121217  ...   1.266677 -0.776571  1.599796   \n",
       "\n",
       "      Quality  \n",
       "0        good  \n",
       "1        good  \n",
       "2         bad  \n",
       "3        good  \n",
       "...       ...  \n",
       "3996     good  \n",
       "3997      bad  \n",
       "3998     good  \n",
       "3999     good  \n",
       "\n",
       "[4000 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Insert your code for Exercise 5-Step 1 here:\n",
    "\n",
    "acidDF = pd.read_csv(\n",
    "    \"https://biologicslab.co/BIO1173/data/apple_quality.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "# Set the max rows and max columns\n",
    "pd.set_option('display.max_rows', 8)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(acidDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see the following output:\n",
    "\n",
    "![__](http://biologicslab.co/BIO1173/images/class_03_2_Exm1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1-Step 2: Create Feature Vector**\n",
    "\n",
    "In the cell below, create a Feature Vector for your regression model. Start by mapping the categorical values in the column `Quality` to integers as was demonstrated in Example 1-Step 2. \n",
    "\n",
    "Then generate your X-values. Remember that your model will be predicting `Acidity` so you **don't** want to include the values with your other X-values. You can use this code chunk to generate your X-values:\n",
    "\n",
    "~~~text\n",
    "# Generate X values\n",
    "acidX = acidDF[['Size', 'Weight', 'Sweetness', 'Crunchiness',\n",
    "       'Juiciness', 'Ripeness', 'Quality']].values\n",
    "acidX = np.asarray(acidX).astype('float32')\n",
    "~~~\n",
    "As you can see, the string `Acidity` has been replaced by the string `Ripeness`. \n",
    "\n",
    "You can use this code chunk to generate your Y-values:\n",
    "\n",
    "~~~text\n",
    "# Generate Y values\n",
    "acidY = acidDF['Acidity'].values\n",
    "acidY = np.asarray(acidY).astype('float32')\n",
    "~~~\n",
    "\n",
    "As a check, print out the first 10 values in your Numpy array `acidY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.49159047 -0.7228094   2.6216364  ... -1.3346114  -2.2297199\n",
      "  1.5997964 ]\n"
     ]
    }
   ],
   "source": [
    "# Insert your code for Exercise 5-Step 2 here\n",
    "\n",
    "# Convert strings to integers\n",
    "mapping = {'bad': 0, 'good': 1}  # define mapping\n",
    "acidDF['Quality'] = acidDF['Quality'].map(mapping) # map\n",
    "\n",
    "# Generate X values\n",
    "acidX = acidDF[['Size', 'Weight', 'Sweetness', 'Crunchiness',\n",
    "       'Juiciness', 'Ripeness', 'Quality']].values\n",
    "acidX = np.asarray(acidX).astype('float32')\n",
    "\n",
    "# Generate Y values\n",
    "acidY = acidDF['Acidity'].values\n",
    "acidY = np.asarray(acidY).astype('float32')\n",
    "\n",
    "# Print Y values\n",
    "print(acidY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "~~~text\n",
    "[-0.49159048 -0.72280937  2.62163647 ... -1.33461139 -2.22971981\n",
    "  1.59979646]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1-Step 3: Construct, compile and train the neural network**\n",
    "\n",
    "In the cell below, contruct a regression neural network called `acidModel` by \"copy-and-paste\" the code in Example 1-Step 3.\n",
    "\n",
    "Don't forget to change the value of the argument `input_dim` in the input layer. Your model should read:\n",
    "~~~text\n",
    "acidModel.add(Dense(25, input_dim=acidX.shape[1], \n",
    "                        activation='relu'))  # Hidden 1\n",
    "~~~\n",
    "As mentioned above, students in this course often forget to change this variable causing the model not to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 25)                200       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                260       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 471\n",
      "Trainable params: 471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "125/125 - 1s - loss: 4.0237 - 745ms/epoch - 6ms/step\n",
      "Epoch 2/100\n",
      "125/125 - 0s - loss: 3.4108 - 422ms/epoch - 3ms/step\n",
      "Epoch 3/100\n",
      "125/125 - 0s - loss: 3.0480 - 417ms/epoch - 3ms/step\n",
      "Epoch 4/100\n",
      "125/125 - 0s - loss: 2.7907 - 414ms/epoch - 3ms/step\n",
      "Epoch 5/100\n",
      "125/125 - 0s - loss: 2.5917 - 417ms/epoch - 3ms/step\n",
      "Epoch 6/100\n",
      "125/125 - 0s - loss: 2.4395 - 418ms/epoch - 3ms/step\n",
      "Epoch 7/100\n",
      "125/125 - 0s - loss: 2.3286 - 418ms/epoch - 3ms/step\n",
      "Epoch 8/100\n",
      "125/125 - 0s - loss: 2.2305 - 493ms/epoch - 4ms/step\n",
      "Epoch 9/100\n",
      "125/125 - 0s - loss: 2.1648 - 422ms/epoch - 3ms/step\n",
      "Epoch 10/100\n",
      "125/125 - 0s - loss: 2.1074 - 414ms/epoch - 3ms/step\n",
      "Epoch 11/100\n",
      "125/125 - 0s - loss: 2.0640 - 409ms/epoch - 3ms/step\n",
      "Epoch 12/100\n",
      "125/125 - 0s - loss: 2.0229 - 411ms/epoch - 3ms/step\n",
      "Epoch 13/100\n",
      "125/125 - 0s - loss: 1.9918 - 412ms/epoch - 3ms/step\n",
      "Epoch 14/100\n",
      "125/125 - 0s - loss: 1.9536 - 414ms/epoch - 3ms/step\n",
      "Epoch 15/100\n",
      "125/125 - 0s - loss: 1.9385 - 415ms/epoch - 3ms/step\n",
      "Epoch 16/100\n",
      "125/125 - 0s - loss: 1.9033 - 412ms/epoch - 3ms/step\n",
      "Epoch 17/100\n",
      "125/125 - 0s - loss: 1.8820 - 415ms/epoch - 3ms/step\n",
      "Epoch 18/100\n",
      "125/125 - 0s - loss: 1.8504 - 415ms/epoch - 3ms/step\n",
      "Epoch 19/100\n",
      "125/125 - 0s - loss: 1.8322 - 412ms/epoch - 3ms/step\n",
      "Epoch 20/100\n",
      "125/125 - 0s - loss: 1.8047 - 415ms/epoch - 3ms/step\n",
      "Epoch 21/100\n",
      "125/125 - 0s - loss: 1.7866 - 415ms/epoch - 3ms/step\n",
      "Epoch 22/100\n",
      "125/125 - 0s - loss: 1.7713 - 416ms/epoch - 3ms/step\n",
      "Epoch 23/100\n",
      "125/125 - 0s - loss: 1.7539 - 413ms/epoch - 3ms/step\n",
      "Epoch 24/100\n",
      "125/125 - 0s - loss: 1.7384 - 414ms/epoch - 3ms/step\n",
      "Epoch 25/100\n",
      "125/125 - 0s - loss: 1.7210 - 413ms/epoch - 3ms/step\n",
      "Epoch 26/100\n",
      "125/125 - 0s - loss: 1.7101 - 411ms/epoch - 3ms/step\n",
      "Epoch 27/100\n",
      "125/125 - 0s - loss: 1.6996 - 413ms/epoch - 3ms/step\n",
      "Epoch 28/100\n",
      "125/125 - 0s - loss: 1.6880 - 412ms/epoch - 3ms/step\n",
      "Epoch 29/100\n",
      "125/125 - 0s - loss: 1.6742 - 411ms/epoch - 3ms/step\n",
      "Epoch 30/100\n",
      "125/125 - 0s - loss: 1.6649 - 415ms/epoch - 3ms/step\n",
      "Epoch 31/100\n",
      "125/125 - 0s - loss: 1.6486 - 413ms/epoch - 3ms/step\n",
      "Epoch 32/100\n",
      "125/125 - 0s - loss: 1.6456 - 414ms/epoch - 3ms/step\n",
      "Epoch 33/100\n",
      "125/125 - 0s - loss: 1.6267 - 412ms/epoch - 3ms/step\n",
      "Epoch 34/100\n",
      "125/125 - 0s - loss: 1.6333 - 412ms/epoch - 3ms/step\n",
      "Epoch 35/100\n",
      "125/125 - 0s - loss: 1.6149 - 411ms/epoch - 3ms/step\n",
      "Epoch 36/100\n",
      "125/125 - 0s - loss: 1.6041 - 413ms/epoch - 3ms/step\n",
      "Epoch 37/100\n",
      "125/125 - 0s - loss: 1.5990 - 414ms/epoch - 3ms/step\n",
      "Epoch 38/100\n",
      "125/125 - 0s - loss: 1.5906 - 413ms/epoch - 3ms/step\n",
      "Epoch 39/100\n",
      "125/125 - 0s - loss: 1.5786 - 415ms/epoch - 3ms/step\n",
      "Epoch 40/100\n",
      "125/125 - 0s - loss: 1.5667 - 412ms/epoch - 3ms/step\n",
      "Epoch 41/100\n",
      "125/125 - 0s - loss: 1.5590 - 412ms/epoch - 3ms/step\n",
      "Epoch 42/100\n",
      "125/125 - 0s - loss: 1.5548 - 411ms/epoch - 3ms/step\n",
      "Epoch 43/100\n",
      "125/125 - 0s - loss: 1.5504 - 416ms/epoch - 3ms/step\n",
      "Epoch 44/100\n",
      "125/125 - 0s - loss: 1.5286 - 415ms/epoch - 3ms/step\n",
      "Epoch 45/100\n",
      "125/125 - 0s - loss: 1.5222 - 412ms/epoch - 3ms/step\n",
      "Epoch 46/100\n",
      "125/125 - 0s - loss: 1.5253 - 412ms/epoch - 3ms/step\n",
      "Epoch 47/100\n",
      "125/125 - 0s - loss: 1.5117 - 413ms/epoch - 3ms/step\n",
      "Epoch 48/100\n",
      "125/125 - 0s - loss: 1.4845 - 421ms/epoch - 3ms/step\n",
      "Epoch 49/100\n",
      "125/125 - 0s - loss: 1.4776 - 415ms/epoch - 3ms/step\n",
      "Epoch 50/100\n",
      "125/125 - 0s - loss: 1.4775 - 413ms/epoch - 3ms/step\n",
      "Epoch 51/100\n",
      "125/125 - 0s - loss: 1.4597 - 415ms/epoch - 3ms/step\n",
      "Epoch 52/100\n",
      "125/125 - 0s - loss: 1.4455 - 411ms/epoch - 3ms/step\n",
      "Epoch 53/100\n",
      "125/125 - 0s - loss: 1.4495 - 416ms/epoch - 3ms/step\n",
      "Epoch 54/100\n",
      "125/125 - 0s - loss: 1.4344 - 415ms/epoch - 3ms/step\n",
      "Epoch 55/100\n",
      "125/125 - 0s - loss: 1.4208 - 418ms/epoch - 3ms/step\n",
      "Epoch 56/100\n",
      "125/125 - 0s - loss: 1.4150 - 416ms/epoch - 3ms/step\n",
      "Epoch 57/100\n",
      "125/125 - 0s - loss: 1.4145 - 413ms/epoch - 3ms/step\n",
      "Epoch 58/100\n",
      "125/125 - 0s - loss: 1.4063 - 414ms/epoch - 3ms/step\n",
      "Epoch 59/100\n",
      "125/125 - 0s - loss: 1.3930 - 414ms/epoch - 3ms/step\n",
      "Epoch 60/100\n",
      "125/125 - 0s - loss: 1.3791 - 415ms/epoch - 3ms/step\n",
      "Epoch 61/100\n",
      "125/125 - 0s - loss: 1.3752 - 416ms/epoch - 3ms/step\n",
      "Epoch 62/100\n",
      "125/125 - 0s - loss: 1.3812 - 413ms/epoch - 3ms/step\n",
      "Epoch 63/100\n",
      "125/125 - 0s - loss: 1.3620 - 411ms/epoch - 3ms/step\n",
      "Epoch 64/100\n",
      "125/125 - 0s - loss: 1.3497 - 412ms/epoch - 3ms/step\n",
      "Epoch 65/100\n",
      "125/125 - 0s - loss: 1.3469 - 410ms/epoch - 3ms/step\n",
      "Epoch 66/100\n",
      "125/125 - 0s - loss: 1.3421 - 414ms/epoch - 3ms/step\n",
      "Epoch 67/100\n",
      "125/125 - 0s - loss: 1.3376 - 412ms/epoch - 3ms/step\n",
      "Epoch 68/100\n",
      "125/125 - 0s - loss: 1.3192 - 413ms/epoch - 3ms/step\n",
      "Epoch 69/100\n",
      "125/125 - 0s - loss: 1.3158 - 414ms/epoch - 3ms/step\n",
      "Epoch 70/100\n",
      "125/125 - 0s - loss: 1.3111 - 410ms/epoch - 3ms/step\n",
      "Epoch 71/100\n",
      "125/125 - 0s - loss: 1.3035 - 414ms/epoch - 3ms/step\n",
      "Epoch 72/100\n",
      "125/125 - 0s - loss: 1.3010 - 413ms/epoch - 3ms/step\n",
      "Epoch 73/100\n",
      "125/125 - 0s - loss: 1.2921 - 411ms/epoch - 3ms/step\n",
      "Epoch 74/100\n",
      "125/125 - 0s - loss: 1.2846 - 413ms/epoch - 3ms/step\n",
      "Epoch 75/100\n",
      "125/125 - 0s - loss: 1.2780 - 411ms/epoch - 3ms/step\n",
      "Epoch 76/100\n",
      "125/125 - 0s - loss: 1.2752 - 413ms/epoch - 3ms/step\n",
      "Epoch 77/100\n",
      "125/125 - 0s - loss: 1.2642 - 411ms/epoch - 3ms/step\n",
      "Epoch 78/100\n",
      "125/125 - 0s - loss: 1.2529 - 412ms/epoch - 3ms/step\n",
      "Epoch 79/100\n",
      "125/125 - 0s - loss: 1.2524 - 413ms/epoch - 3ms/step\n",
      "Epoch 80/100\n",
      "125/125 - 0s - loss: 1.2494 - 413ms/epoch - 3ms/step\n",
      "Epoch 81/100\n",
      "125/125 - 0s - loss: 1.2327 - 417ms/epoch - 3ms/step\n",
      "Epoch 82/100\n",
      "125/125 - 0s - loss: 1.2342 - 414ms/epoch - 3ms/step\n",
      "Epoch 83/100\n",
      "125/125 - 0s - loss: 1.2311 - 413ms/epoch - 3ms/step\n",
      "Epoch 84/100\n",
      "125/125 - 0s - loss: 1.2309 - 417ms/epoch - 3ms/step\n",
      "Epoch 85/100\n",
      "125/125 - 0s - loss: 1.2141 - 414ms/epoch - 3ms/step\n",
      "Epoch 86/100\n",
      "125/125 - 0s - loss: 1.2184 - 412ms/epoch - 3ms/step\n",
      "Epoch 87/100\n",
      "125/125 - 0s - loss: 1.2091 - 409ms/epoch - 3ms/step\n",
      "Epoch 88/100\n",
      "125/125 - 0s - loss: 1.2108 - 413ms/epoch - 3ms/step\n",
      "Epoch 89/100\n",
      "125/125 - 0s - loss: 1.1965 - 411ms/epoch - 3ms/step\n",
      "Epoch 90/100\n",
      "125/125 - 0s - loss: 1.2026 - 412ms/epoch - 3ms/step\n",
      "Epoch 91/100\n",
      "125/125 - 0s - loss: 1.2026 - 412ms/epoch - 3ms/step\n",
      "Epoch 92/100\n",
      "125/125 - 0s - loss: 1.1893 - 412ms/epoch - 3ms/step\n",
      "Epoch 93/100\n",
      "125/125 - 0s - loss: 1.1925 - 418ms/epoch - 3ms/step\n",
      "Epoch 94/100\n",
      "125/125 - 0s - loss: 1.1784 - 415ms/epoch - 3ms/step\n",
      "Epoch 95/100\n",
      "125/125 - 0s - loss: 1.1768 - 421ms/epoch - 3ms/step\n",
      "Epoch 96/100\n",
      "125/125 - 0s - loss: 1.1816 - 410ms/epoch - 3ms/step\n",
      "Epoch 97/100\n",
      "125/125 - 0s - loss: 1.1743 - 413ms/epoch - 3ms/step\n",
      "Epoch 98/100\n",
      "125/125 - 0s - loss: 1.1666 - 414ms/epoch - 3ms/step\n",
      "Epoch 99/100\n",
      "125/125 - 0s - loss: 1.1723 - 413ms/epoch - 3ms/step\n",
      "Epoch 100/100\n",
      "125/125 - 0s - loss: 1.1639 - 418ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e2b82bbac0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert your code for Exercise 1-Step 3 here\n",
    "\n",
    "# Specify the model type as sequential\n",
    "acidModel = Sequential()\n",
    "\n",
    "# Build model\n",
    "acidModel.add(Dense(25, input_dim=acidX.shape[1], \n",
    "                        activation='relu'))  # Hidden 1 \n",
    "acidModel.add(Dense(10, activation='relu'))  # Hidden 2\n",
    "acidModel.add(Dense(1))  # Output\n",
    "\n",
    "# Complile model\n",
    "acidModel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Print model summary\n",
    "acidModel.summary()\n",
    "\n",
    "# Train model\n",
    "acidModel.fit(acidX,acidY,verbose=2,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, your output should start with something similar to the following:\n",
    "\n",
    "~~~text\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " dense_3 (Dense)             (None, 25)                200       \n",
    "                                                                 \n",
    " dense_4 (Dense)             (None, 10)                260       \n",
    "                                                                 \n",
    " dense_5 (Dense)             (None, 1)                 11        \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 471\n",
    "Trainable params: 471\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Epoch 1/100\n",
    "125/125 - 1s - loss: 4.2960 - 746ms/epoch - 6ms/step\n",
    "Epoch 2/100\n",
    "125/125 - 0s - loss: 3.5910 - 415ms/epoch - 3ms/step\n",
    "Epoch 3/100\n",
    "125/125 - 0s - loss: 3.2880 - 473ms/epoch - 4ms/step\n",
    "Epoch 4/100\n",
    "125/125 - 0s - loss: 3.0075 - 415ms/epoch - 3ms/step\n",
    "Epoch 5/100\n",
    "125/125 - 0s - loss: 2.7870 - 409ms/epoch - 3ms/step\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and end with something similar to the following:\n",
    "~~~text\n",
    "Epoch 95/100\n",
    "125/125 - 0s - loss: 1.1699 - 451ms/epoch - 4ms/step\n",
    "Epoch 96/100\n",
    "125/125 - 0s - loss: 1.1745 - 453ms/epoch - 4ms/step\n",
    "Epoch 97/100\n",
    "125/125 - 1s - loss: 1.1708 - 508ms/epoch - 4ms/step\n",
    "Epoch 98/100\n",
    "125/125 - 0s - loss: 1.1617 - 464ms/epoch - 4ms/step\n",
    "Epoch 99/100\n",
    "125/125 - 0s - loss: 1.1697 - 414ms/epoch - 3ms/step\n",
    "Epoch 100/100\n",
    "125/125 - 0s - loss: 1.1617 - 411ms/epoch - 3ms/step\n",
    "\n",
    "<keras.callbacks.History at 0x1811106fdf0>\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the particular example, your `acidModel` started with an error rate (`loss:`) equal to `4.296` after the first epoch. At the end of `100` epochs, the model's prediction of the acidity level had improved substantially, with a loss equal to only `1.1617`.\n",
    "\n",
    "Due to the random nature that Keras initializes weights and biases, your output will likely be somewhat different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82O2rvqiCtiR"
   },
   "source": [
    "## Introduction to Neural Network Hyperparameters\n",
    "\n",
    "If you look at the above code, you will see that the neural network contains four layers. The first layer is the input layer because it contains the **input_dim** parameter that the programmer sets to be the number of inputs the dataset has. The network needs one input neuron for every column in the data set (including dummy variables).  \n",
    "\n",
    "There are also several hidden layers, with 25 and 10 neurons each. You might be wondering how the programmer chose these numbers. Selecting a hidden neuron structure is one of the most common questions about neural networks. Unfortunately, there is no right answer. These are hyperparameters. They are settings that can affect neural network performance, yet there are no clearly defined means of setting them.\n",
    "\n",
    "In general, more hidden neurons mean more capability to fit complex problems. However, too many neurons can lead to overfitting and lengthy training times. Too few can lead to underfitting the problem and will sacrifice accuracy. Also, how many layers you have is another hyperparameter. In general, more layers allow the neural network to perform more of its feature engineering and data preprocessing. But this also comes at the expense of training times and the risk of overfitting. In general, you will see that neuron counts start larger near the input layer and tend to shrink towards the output layer in a triangular fashion. \n",
    "\n",
    "Some techniques use machine learning to optimize these values. These will be discussed later in this course.\n",
    "\n",
    "## Controlling the Amount of Output\n",
    "\n",
    "The program produces one line of output for each training epoch. You can eliminate this output by setting the verbose setting of the fit command:\n",
    "\n",
    "* **verbose=0** - No progress output (use with Jupyter if you do not want output).\n",
    "* **verbose=1** - Display progress bar, does not work well with Jupyter.\n",
    "* **verbose=2** - Summary progress output (use with Jupyter if you want to know the loss at each epoch).\n",
    "\n",
    "## Regression Prediction\n",
    "\n",
    "Next, we will perform actual predictions. The program assigns these predictions to the **pred** variable. For Example 5, these will be predictions of apple **Ripeness** from the neural network; For Exercise 5, these will be predictions of apple **Quality** from the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Use model to make predictions\n",
    "\n",
    "The code in the cell below uses Keras' `model.predict()` function to predict the `Ripeness` of each of the 4000 apples in the Apple Quality dataset based on its 'Size', 'Weight', 'Sweetness', 'Crunchiness','Juiciness', 'Acidity', and 'Quality'. The predictions are stored in a variable called `pred`. \n",
    "\n",
    "Keep in mind that these `Ripeness` **_predictions_** are being made by the neural network model, `ripeModel` after it was trained ('fitted') to the dataset for 100 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbErLyX_CtiR",
    "outputId": "9173c10d-d0e9-43c0-9bb7-cb14d082e97a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 1ms/step\n",
      "Shape of ripePred: (4000, 1)\n",
      "[[ 0.30069673]\n",
      " [ 0.87126034]\n",
      " [-0.06779067]\n",
      " [-3.7803648 ]\n",
      " [-1.5543201 ]\n",
      " [ 1.640624  ]\n",
      " [-1.6265161 ]\n",
      " [ 1.6308248 ]\n",
      " [ 3.7290533 ]\n",
      " [ 2.7232528 ]]\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Predict the Ripeness of each apple in the dataset\n",
    "\n",
    "# Use model to make Ripeness predictions\n",
    "ripePred = ripeModel.predict(ripeX)\n",
    "\n",
    "# Print shape of pred\n",
    "print(f\"Shape of ripePred: {ripePred.shape}\")\n",
    "\n",
    "# Print first 10 predictions\n",
    "print(ripePred[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:\n",
    "~~~text\n",
    "125/125 [==============================] - 0s 2ms/step\n",
    "Shape of ripePred: (4000, 1)\n",
    "[[ 0.41057712]\n",
    " [ 0.8406092 ]\n",
    " [ 0.03273106]\n",
    " [-3.8475904 ]\n",
    " [-1.5179614 ]\n",
    " [ 1.9175235 ]\n",
    " [-1.9796481 ]\n",
    " [ 1.3494562 ]\n",
    " [ 4.5986714 ]\n",
    " [ 2.5490847 ]]\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2: Use model to make predictions**\n",
    "\n",
    "In the cell below, use Keras' `model.predict()` function to predict the `Acidity` of each of the 4000 apples in the Apple Quality dataset based on its 'Size', 'Weight', 'Sweetness', 'Crunchiness','Juiciness', 'Ripeness' and `Quality`. Store these  predictions in a variable called `acidPred`. \n",
    "\n",
    "Print out the shape of `acidPred` and your model's first `10` acidity predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 1ms/step\n",
      "Shape of acidPred: (4000, 1)\n",
      "[[-2.0719798]\n",
      " [ 0.900121 ]\n",
      " [ 3.8747263]\n",
      " [ 1.8134047]\n",
      " [ 0.7781885]\n",
      " [-2.3871996]\n",
      " [ 2.6942391]\n",
      " [-2.086415 ]\n",
      " [-3.2495368]\n",
      " [-1.493188 ]]\n"
     ]
    }
   ],
   "source": [
    "# Insert your code for Exercise 2 here\n",
    "\n",
    "acidPred = acidModel.predict(acidX)\n",
    "print(f\"Shape of acidPred: {acidPred.shape}\")\n",
    "print(acidPred[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:\n",
    "~~~text\n",
    "125/125 [==============================] - 0s 2ms/step\n",
    "Shape of acidPred: (4000, 1)\n",
    "[[-0.96637136]\n",
    " [ 0.5449684 ]\n",
    " [ 2.9393399 ]\n",
    " [ 1.3829033 ]\n",
    " [ 1.7506608 ]\n",
    " [-2.8392882 ]\n",
    " [ 3.751297  ]\n",
    " [-1.6653696 ]\n",
    " [-2.7585254 ]\n",
    " [-1.591665  ]]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Determine the accuracy of the model's predictions\n",
    "\n",
    "An obvious question is how good are the neural network's predictions?  Since we know the correct `Ripeness` for each apple in the dataset, we can measure how close each neural network prediction was to the actual value.\n",
    "\n",
    "A common measure in regression analysis is the [Root-mean-square error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation).\n",
    "\n",
    "RMSE measures of the differences between predicted values and true values. \n",
    "\n",
    "The code in the cell below computes the RMSE of the `Ripeness` predictions made by `ripeModel` with the actual `Ripeness` values in the Apple Quality dataset, which are stored in the array `ripeY`. The RMSE is stored in a new variable called `score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrY0Vs9oCtiR",
    "outputId": "f1c6ef77-b636-4ac0-df4d-0439977eb6eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE) for ripeModel: 0.8483414053916931\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Determine the RMSE for model_0\n",
    "\n",
    "# Measure RMSE error\n",
    "score = np.sqrt(metrics.mean_squared_error(ripePred,ripeY))\n",
    "\n",
    "# Print out the RSME\n",
    "print(f\"Final score (RMSE) for ripeModel: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see something similar to the following output:\n",
    "~~~text\n",
    "Final score (RMSE) for ripeModel: 0.8224384784698486\n",
    "~~~\n",
    "So what does this RMSE value mean? \n",
    "\n",
    "The answer is somewhat complicated. What can be said with certainty, is that an RMSE value equal to `0` would indicate `100%` perfect predictions. However, that rarely happens with real data.\n",
    "\n",
    "We also know that RMSE will always be non-negative (i.e., positive) since it is the _square_ of two numbers.\n",
    "\n",
    "However, beyond that, interpreting the meaning of RMSE is not straightforward. In general, a lower RMSE is better than a higher one. However, comparisons across different types of data would be invalid because the magnitude of the RSME is dependent on the scale of the numbers used. In other words, you will get a larger RSME when trying to predict a bigger number than a smaller one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3yqZwhnCtiS"
   },
   "source": [
    "### Example 4: Compare predictions to actual values\n",
    "\n",
    "The best way to get a sense of how accurate were the predictions of the `ripeModel` is to print out the actual values next to the model's predictive values. \n",
    "\n",
    "The code in the cell below uses a `for` loop to print out the first `10` comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tl7hv8NnCtiS",
    "outputId": "a4dd79ee-8af5-4727-b595-f938814266d9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Apple:0  Ripeness: 0.3298397958278656  Predicted: [0.3007]\n",
      "2. Apple:1  Ripeness: 0.867530107498169  Predicted: [0.8713]\n",
      "3. Apple:2  Ripeness: -0.03803332895040512  Predicted: [-0.0678]\n",
      "4. Apple:3  Ripeness: -3.4137613773345947  Predicted: [-3.7804]\n",
      "5. Apple:4  Ripeness: -1.303849458694458  Predicted: [-1.5543]\n",
      "6. Apple:5  Ripeness: 1.9146158695220947  Predicted: [1.6406]\n",
      "7. Apple:6  Ripeness: -1.8474167585372925  Predicted: [-1.6265]\n",
      "8. Apple:7  Ripeness: 0.9744378328323364  Predicted: [1.6308]\n",
      "9. Apple:8  Ripeness: 4.080920696258545  Predicted: [3.7291]\n",
      "10. Apple:9  Ripeness: 1.620856761932373  Predicted: [2.7233]\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Print out predictions and actual values\n",
    "\n",
    "# set the print option to 4 decimal places\n",
    "np.set_printoptions(formatter={'float': '{:.4f}'.format})\n",
    "\n",
    "# For loop for printing values\n",
    "for i in range(10):\n",
    "    print(f\"{i+1}. Apple:{appleNum[i]}\" \n",
    "         + f\"  Ripeness: {ripeY[i]}\"\n",
    "         + f\"  Predicted: {ripePred[i]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see an output that is similar to the following:\n",
    "\n",
    "~~~text\n",
    "1. Apple:0  Ripeness: 0.3298397958278656  Predicted: [0.4106]\n",
    "2. Apple:1  Ripeness: 0.867530107498169  Predicted: [0.8406]\n",
    "3. Apple:2  Ripeness: -0.03803332895040512  Predicted: [0.0327]\n",
    "4. Apple:3  Ripeness: -3.4137613773345947  Predicted: [-3.8476]\n",
    "5. Apple:4  Ripeness: -1.303849458694458  Predicted: [-1.5180]\n",
    "6. Apple:5  Ripeness: 1.9146158695220947  Predicted: [1.9175]\n",
    "7. Apple:6  Ripeness: -1.8474167585372925  Predicted: [-1.9796]\n",
    "8. Apple:7  Ripeness: 0.9744378328323364  Predicted: [1.3495]\n",
    "9. Apple:8  Ripeness: 4.080920696258545  Predicted: [4.5987]\n",
    "10. Apple:9  Ripeness: 1.620856761932373  Predicted: [2.5491]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspection, we can see that the `ripeModel` is doing a reasonable job, but definitely **not** a perfect job, of predicting the `Ripeness` of an individual apple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3: Determine the quality of the model's predictions**\n",
    "\n",
    "Write the code in the cell below, to compute the [Root-mean-square error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) for apple `Acidity` predicted by your `acidModel`. Print out the RSME for your `acidModel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrY0Vs9oCtiR",
    "outputId": "f1c6ef77-b636-4ac0-df4d-0439977eb6eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE) for acidModel: 1.0666450262069702\n"
     ]
    }
   ],
   "source": [
    "# Insert your code for Exercise 3 here\n",
    "\n",
    "# Measure RMSE error\n",
    "score = np.sqrt(metrics.mean_squared_error(acidPred,acidY))\n",
    "\n",
    "# Print out the RSME\n",
    "print(f\"Final score (RMSE) for acidModel: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see something similar to the following output:\n",
    "~~~text\n",
    "Final score (RMSE) for acidModel: 1.084747914279273\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3yqZwhnCtiS"
   },
   "source": [
    "### **Exercise 4: Compare predictions to actual values**\n",
    "\n",
    "In the cell below write the code to print out predictions made by your `acidModel` for the `Acidity` of the first 10 apples as well as their actual `Acidity` values, side-by-side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tl7hv8NnCtiS",
    "outputId": "a4dd79ee-8af5-4727-b595-f938814266d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Apple:0  Acidity: -0.4915904700756073  Predicted: [-2.0720]\n",
      "2. Apple:1  Acidity: -0.722809374332428  Predicted: [0.9001]\n",
      "3. Apple:2  Acidity: 2.621636390686035  Predicted: [3.8747]\n",
      "4. Apple:3  Acidity: 0.7907232046127319  Predicted: [1.8134]\n",
      "5. Apple:4  Acidity: 0.5019840598106384  Predicted: [0.7782]\n",
      "6. Apple:5  Acidity: -2.981523275375366  Predicted: [-2.3872]\n",
      "7. Apple:6  Acidity: 2.414170503616333  Predicted: [2.6942]\n",
      "8. Apple:7  Acidity: -1.4701250791549683  Predicted: [-2.0864]\n",
      "9. Apple:8  Acidity: -4.8719048500061035  Predicted: [-3.2495]\n",
      "10. Apple:9  Acidity: 2.185607671737671  Predicted: [-1.4932]\n"
     ]
    }
   ],
   "source": [
    "# Insert your code for Exercise 4 here\n",
    "\n",
    "# Set print precision\n",
    "np.set_printoptions(formatter={'float': '{:.4f}'.format})\n",
    "\n",
    "# For loop for printing values\n",
    "for i in range(10):\n",
    "    print(f\"{i+1}. Apple:{appleNum[i]}\" \n",
    "         + f\"  Acidity: {acidY[i]}\"\n",
    "         + f\"  Predicted: {acidPred[i]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct you should see an output that is something similar to the following:\n",
    "~~~text\n",
    "1. Apple:0  Acidity: -0.4915904700756073  Predicted: [-1.1459]\n",
    "2. Apple:1  Acidity: -0.722809374332428  Predicted: [0.6893]\n",
    "3. Apple:2  Acidity: 2.621636390686035  Predicted: [3.5310]\n",
    "4. Apple:3  Acidity: 0.7907232046127319  Predicted: [1.1122]\n",
    "5. Apple:4  Acidity: 0.5019840598106384  Predicted: [0.6371]\n",
    "6. Apple:5  Acidity: -2.981523275375366  Predicted: [-3.5875]\n",
    "7. Apple:6  Acidity: 2.414170503616333  Predicted: [2.1825]\n",
    "8. Apple:7  Acidity: -1.4701250791549683  Predicted: [-1.7862]\n",
    "9. Apple:8  Acidity: -4.8719048500061035  Predicted: [-3.1646]\n",
    "10. Apple:9  Acidity: 2.185607671737671  Predicted: [-1.0773]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to `ripeModel`, your `acidModel` seems to be able to make more accurate predictions. This is consistent with the observation that the RSME for your `acidModel` was smaller than the RMSE for the `ripeModel` created in Example 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lesson Turn-in**\n",
    "\n",
    "When you have completed all of the code cells, and run them in sequential order (the last code cell should be number 14), use the **File --> Print.. --> Save to PDF** to generate a PDF of your JupyterLab notebook. Save your PDF as `Class_03_2.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
