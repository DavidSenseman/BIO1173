{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* **Part 4.1: Introduction to Large Language Models (LLMs)**\n",
        "* Part 4.2: Chatbots\n",
        "* Part 4.3: Image Generation with StableDiffusion\n",
        "* Part 4.4: Agentic AI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **You MUST Obtain Your Gemini API Key Now!**\n",
        "\n",
        "In order to run the code in the next few lessons, you will need to obtain a Google `Gemini API key` and install your key in the `Secrets` location in your Google Colab notebook. It is important to key your `Gemini API key` secret. If anyone learns your key, they can use it costing you a lot of money. Instructions for obtaining your API Key and how to store in your Colab Secrets was previously published as an class Announcement on Canvas.  "
      ],
      "metadata": {
        "id": "xvyn29wc6BsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test Your `GEMINI_API_KEY`**\n",
        "\n",
        "To see if your `GEMINI_API_KEY` is correctly setup, run the next code cell."
      ],
      "metadata": {
        "id": "59XFG5rWAwp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Check if API key is properly loaded\n",
        "try:\n",
        "    # 1. Get the key from Secrets\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "    # 2. Set it as an environment variable\n",
        "    os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "\n",
        "    print(\"API key loaded and environment variable set successfully!\")\n",
        "    print(f\"Key length: {len(GOOGLE_API_KEY)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading API key: {e}\")\n",
        "    print(\"Please set your API key in Google Colab:\")\n",
        "    print(\"1. Go to Secrets in the left sidebar (key icon)\")\n",
        "    print(\"2. Create a new secret named 'GOOGLE_API_KEY'\")\n",
        "    print(\"3. Paste your GOOGLE API key and toggle 'Notebook access' on\")"
      ],
      "metadata": {
        "id": "VoCuPqqW7Vau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 1 You may see this message when you run this cell:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image08C.png)\n",
        "\n",
        "If you do see this popup just click on `Grant access`.\n",
        "\n",
        "\n",
        "* 2. If your `GEMINI_API_KEY` is correctly installed you should see something _similar_ to the following output.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image09C.png)\n",
        "\n",
        "* 3. However, if you see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image10C.png)\n",
        "\n",
        "You will need to correct the error before you can continue. Ask your Instructor or TA for help if you can resolve the error yourself."
      ],
      "metadata": {
        "id": "KCkgqZivDg6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to Large Language Models (LLMs)**\n",
        "\n",
        "Run the next cell to see short introduction to Large Language Models (LLMs). This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "ejrc2-ZrHtRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"wjZofJX0v4M\"\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\">\n",
        "</iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "Jc_icH4HpAw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Large Language Models (LLMs)**\n",
        "\n",
        "**Large Language Models (LLMs)** such as `Gemini` have brought AI into mainstream use. LLMs allow regular users to interact with AI using natural language. Most of these language models require extreme processing capabilities and hardware. Because of this, application programming interfaces (APIs) accessed through the Internet are becoming common entry points for these models. One of the most compelling features of services like Gemini is their availability as an API. But before we dive into the depths of coding and integration, let's understand what an API is and its significance in the AI domain.\n",
        "\n",
        "API stands for **Application Programming Interface**. Think of it as a bridge or a messenger that allows two different software applications to communicate. In the context of AI and machine learning, APIs often allow developers to access a particular model or service without having to house the model on their local machine. This technique can be beneficial when the model in question, like `Gemini`, is large and resource-intensive.\n",
        "\n",
        "\n",
        "\n",
        "In the realm of AI, APIs have several distinct advantages:\n",
        "\n",
        "**1. Scalability:** Since the actual model runs on external servers, developers don't need to worry about scaling infrastructure.\n",
        "**2. Maintenance:** You get to use the latest and greatest version of the model without constantly updating your local copy.\n",
        "**3. Cost-Effective:** Leveraging external computational resources can be more cost-effective than maintaining high-end infrastructure locally, especially for sporadic or one-off tasks.\n",
        "**4. Ease of Use:** Instead of diving into the nitty-gritty details of model implementation and optimization, developers can directly utilize its capabilities with a few lines of code.\n",
        "\n",
        "In this section, we won't be running the neural network computations locally. We will use the new Google Gen AI SDK to communicate with the `Gemini API` to access and harness the abilities of the latest Gemini models. The actual execution of the neural network code happens on `Google servers`, bringing forth a unique synergy of Python's flexibility and Gemini's multimodal mastery. (NOTE: The physical location of these servers is managed by Google Cloud).\n",
        "\n",
        "In this section, we will make use of the `Google Gemini API`. Further information on this API can be found here:\n",
        "\n",
        "* [Google AI Studio Login/Registration](https://aistudio.google.com/)\n",
        "* [Gemini API Overview](https://ai.google.dev/gemini-api/docs)\n",
        "* [Gemini Python SDK Reference](https://ai.google.dev/api/python/google/genai)\n",
        "* [Google Gen AI SDK GitHub](https://github.com/googleapis/python-genai)\n",
        "* [Gemini API Cookbook](https://github.com/google-gemini/cookbook)\n",
        "* [LangChain for Gemini](https://python.langchain.com/docs/integrations/chat/google_generative_ai/)"
      ],
      "metadata": {
        "id": "1kYC5hDZTxQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing LangChain to use the Google Gen AI SDK**\n",
        "\n",
        "As we delve deeper into the intricacies of deep learning, it's crucial to understand that the tools and platforms we use are as versatile as the concepts themselves. When it comes to accessing Gemini, a state-of-the-art multimodal AI model developed by Google, there are two predominant pathways:\n",
        "\n",
        "**Direct API Access using Python's HTTP Capabilities:** Python, with its rich library ecosystem, provides utilities like `requests` to directly communicate with APIs over HTTP. This method involves crafting the necessary REST API calls, handling JSON responses, and error checking, giving the developer granular control over the process.\n",
        "\n",
        "**Using the Official Google Gen AI SDK:** Google offers an official Python SDK (often via the `google-genai` or `google-generativeai` packages) that simplifies the process of integrating with Gemini and other Google AI services. This library abstracts many of the intricacies and boilerplate steps of direct API access, offering a streamlined and user-friendly approach to interacting with the model.\n",
        "\n",
        "Each approach has its advantages. `Direct API access` provides a more hands-on, granular approach, allowing developers to intimately understand the intricacies of each API call. On the other hand, using the `Google Gen AI SDK` can accelerate development, reduce potential errors, and allow for a more straightforward integration, especially for those new to API interactions.\n",
        "\n",
        "We will make use of the `Gemini API` through a library called `LangChain`. `LangChain` is a framework designed to simplify the creation of applications using LLMs. As a language model integration framework, `LangChain's` use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis. `LangChain` allows you to quickly change between different underlying LLMs—switching from OpenAI to Gemini, for example—with minimal code changes."
      ],
      "metadata": {
        "id": "rMJAU0uyUTHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `langchain`\n",
        "\n",
        "Run the code in the next cell to install the `langchain`modules."
      ],
      "metadata": {
        "id": "5Y1nGVRB8lDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_google_genai"
      ],
      "metadata": {
        "id": "izfrSbMY9NXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image19F.png)\n",
        "\n",
        "Don't worry about the error message."
      ],
      "metadata": {
        "id": "nyeI8RWE40Zr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List Current Google Gemini Models\n",
        "\n",
        "Run the code in the next cell to generate a table showing the various models that you can currently access using your Gemini API Key."
      ],
      "metadata": {
        "id": "k1Btp-3gHSZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List Current Google Gemini Models\n",
        "\n",
        "from google import genai\n",
        "import os\n",
        "\n",
        "# --- 1. SETUP & AUTHENTICATION ---\n",
        "try:\n",
        "    # Use the environment variable that was already set\n",
        "    api_key = os.environ['GOOGLE_API_KEY']\n",
        "\n",
        "    # Create a client with the key\n",
        "    client = genai.Client(api_key=api_key)\n",
        "    print(\"Authentication successful.\\n\")\n",
        "\n",
        "except KeyError:\n",
        "    print(\"API key not found. Please run the setup cell first.\")\n",
        "\n",
        "# --- 2. PRINT HEADER ---\n",
        "print(f\"{'Model Name':<60} {'Display Name':<40}\")\n",
        "print(\"-\" * 130)\n",
        "\n",
        "# --- 3. PRINT ROWS ---\n",
        "try:\n",
        "    for m in client.models.list():\n",
        "        print(f\"{m.name:<60} {m.display_name:<40}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError listing models: {e}\")\n"
      ],
      "metadata": {
        "id": "96LdjFt-6WGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Current Gemini API Models (January 2026)**\n",
        "\n",
        "As of January 2026, the Gemini API primarily utilizes the **Gemini 2.0** and **Gemini 2.5** series.\n",
        "\n",
        "#### **1. Gemini 2.5 Series (Newest Flagship)**\n",
        "*   **`gemini-2.5-pro`**: The most advanced reasoning model with enhanced \"thinking\" capabilities for complex coding, math, and multimodal tasks. Includes a **1M token context window**.\n",
        "*   **`gemini-2.5-flash`**: A fast, efficient model that also supports thinking capabilities. Great balance of speed and intelligence for most tasks.\n",
        "*   **`gemini-2.5-flash-lite-preview`**: An ultra-efficient version designed for high-throughput tasks at lower cost.\n",
        "\n",
        "#### **2. Gemini 2.0 Series (Stable Production)**\n",
        "*   **`gemini-2.0-flash`**: Fast and versatile model optimized for diverse, everyday tasks. Good for production use cases requiring speed.\n",
        "*   **`gemini-2.0-flash-lite`**: Lightweight version for cost-sensitive, high-volume applications.\n",
        "\n",
        "#### **3. Specialized Models**\n",
        "*   **`gemini-2.0-flash-live-001`**: Optimized for the Live API to support real-time, bidirectional voice and video interactions.\n",
        "*   **`gemini-2.5-pro-preview-tts`**: Specialized text-to-speech model for generating expressive, natural audio output.\n",
        "*   **`gemini-2.5-flash-preview-native-audio`**: Native audio generation capabilities for multimodal applications.\n",
        "\n",
        "#### **4. Legacy Models**\n",
        "*   **`gemini-1.5-pro`** / **`gemini-1.5-flash`**: Still available but recommended to migrate to 2.0 or 2.5 series for improved performance.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Vrm9DeIv7QxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt Engineering**\n",
        "\n",
        "When working with a large language model (LLM) like ChatGPT or Gemini, the **prompt** serves as the foundation for interaction. It is the input or instruction provided to the model, guiding it to generate relevant and useful outputs.\n",
        "\n",
        "**1. Role of the Prompt**\n",
        "- **Instructional Guide**: The prompt shapes what the model does. Whether it's answering a question, completing a task, or writing creatively, the prompt provides the necessary context.  \n",
        "- **Boundary Setter**: A well-crafted prompt can define the scope of the task, ensuring the response is focused and doesn't deviate from the topic.  \n",
        "- **Task Optimizer**: By providing clear and concise instructions, the prompt ensures that the LLM generates responses that align with user expectations.\n",
        "\n",
        "**2. Importance of the Prompt**\n",
        "- **Determines Quality of Output**: The quality of the model's response depends heavily on the clarity and specificity of the prompt. A vague prompt can lead to irrelevant or incomplete answers, while a precise one produces accurate and valuable results.\n",
        "- **Customizable Interactions**: Prompts allow users to adapt the model to different scenarios—such as summarization, translation, or brainstorming—making it versatile and dynamic.  \n",
        "- **Reduces Ambiguity**: A good prompt minimizes room for misunderstanding, helping the model interpret the task as intended.  \n",
        "\n",
        "**3. Iterative Improvement**\n",
        "Working with LLMs is often an _iterative_  process. If the initial response isn't quite right, the user can refine the prompt, adding more detail or constraints to guide the model toward the desired result. Instead of starting over from scratch, you just edit the prompt and try it again.\n",
        "\n",
        "The prompt isn't just the input—it’s the bridge between the user’s needs and the model’s capabilities. Mastering prompt design is key to fully leveraging the potential of an LLM like Gemini or ChatGPT.\n"
      ],
      "metadata": {
        "id": "XG5KgzC0dCxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Basic Query to LangChain\n",
        "\n",
        "We begin by writing a **prompt**, to ask (query) `Gemini` a simple question: \"What are the 5 most prestigous medical schools in the USA?\".\n",
        "\n",
        "The Python code in the cell below interacts with `Gemini 2.0 flash` using `LangChain` to retrieve our answer. Since Gemini can be a wordy, we tell the LLM to only provide a list using this prompt:\n",
        "\n",
        "```python\n",
        "# Specify question\n",
        "question = \"Provide just a list of five most prestigious medical schools in the US?\"\n",
        "```\n",
        "Crafting a prompt so that the LLM provides the information you want is called `prompt engineering`.\n",
        "\n",
        "**NOTE:** This cell will not run if you do not have a valid GEMINI_API_KEY already installed in Google Colab's `Secrets`.\n"
      ],
      "metadata": {
        "id": "TTSOCfbAqc-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Basic Query (Client-Side Limit)\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n",
        "# Specify question\n",
        "question = \"Provide just a list of five most prestigious medical schools in the US?\"\n",
        "print(f\"Question: {question}\\n\")\n",
        "\n",
        "# 1. Setup\n",
        "gemini_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Specify which model to use\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    google_api_key=gemini_key,\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# 2. Define our Hard Limit (in characters)\n",
        "MAX_CHARS = 500\n",
        "\n",
        "# 3. Submit question to LLM\n",
        "try:\n",
        "    streamed_text = \"\"\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "    # 3. Stream and Monitor\n",
        "    for chunk in llm.stream(question):\n",
        "        streamed_text += chunk.content\n",
        "\n",
        "        # Update the display\n",
        "        display_handle.update(Markdown(streamed_text))\n",
        "\n",
        "        # Check if we have exceeded our manual limit\n",
        "        if len(streamed_text) > MAX_CHARS:\n",
        "            # Append a notice so the user knows why it stopped\n",
        "            display_handle.update(Markdown(streamed_text + \"...\\n\\n**(Stopped by Client-Side Limit)**\"))\n",
        "            break # Forcefully exit the loop\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during query: {e}\")"
      ],
      "metadata": {
        "id": "JQxdSwJcfvzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01F.png)"
      ],
      "metadata": {
        "id": "K7V2Z3bOrqS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the response from `LangChain` is in regular English, complete with formatting. While the formatting may make it easier to read, we often have to parse the results given to us by LLMs.\n",
        "\n",
        "Later, we will see that `LangChain` can help with this as well. You will also notice that we specified a value of `0` for **temperature**; this instructs the LLM to be less creative with its responses and more consistent. Because we are working primarily with data extraction in this section, a low temperature will give us more consistent results.\n",
        "\n",
        "In `LangChain` (specifically for Gemini models), the temperature parameter typically ranges from **0.0** to **2.0**. The temperature controls the randomness of the model's output:\n",
        "\n",
        "* **Low Temperature (e.g., 0.0):** Produces more deterministic and focused responses, ideal for tasks requiring precision.\n",
        "\n",
        "* **High Temperature (e.g., 1.0 - 2.0):** Generates more creative and diverse outputs, useful for brainstorming or creative writing.\n",
        "\n",
        "If you're working with `LangChain` and `Gemini models`, you can set the temperature when initializing the model or during runtime."
      ],
      "metadata": {
        "id": "7qWYVPwEVhOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Basic Query to LangChain**\n",
        "\n",
        "For **Exercise 1** think about a subject for a `Top Five List` that **you** find interesting and see what response you get back from `ChatGTP`.\n",
        "\n",
        "Feel free to change the **temperature** of your request if you want a more _creative_ response from `LangChain`. There are no \"right\" or \"wrong\" answers here as long as your code works."
      ],
      "metadata": {
        "id": "MvfJCF6wr-lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x_tQt3ejr-lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I am interested in guitar players, I asked  for a list of the 5 greatest guitart players of all time.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image02F.png)\n",
        "\n",
        "You output will be different depending up your question."
      ],
      "metadata": {
        "id": "r24AGvJYr-lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Working with Prompts\n",
        "\n",
        "As mentioned above, interactions with LLMs is typically accomplished using `prompts`and that there is a whole new field called **Prompt Engineering** that focuses on designing, refining, and optimizing prompts to maximize the effectiveness and relevance of outputs generated by large language models (LLMs) like Gemini 2.5 Pro, Gemini Flash, and others.\n",
        "\n",
        "In Example 2, we will \"engineer\" a prompt that will have `Gemini` translate text from French to English. We don't need to tell Gemini what language of our text is, it is smart enough to figure it out by itself.\n",
        "\n",
        "In this example, we will just be using normal Python `f-strings` to build the prompt."
      ],
      "metadata": {
        "id": "V3XBiKXLWB2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Working with Prompts\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Define our Hard Limit (in characters)\n",
        "MAX_CHARS = 500\n",
        "\n",
        "# Define text and style\n",
        "text = \"\"\"Laissez les bons temps rouler\"\"\"  # What we want translated\n",
        "style = \"American English\"\n",
        "\n",
        "# Build prompt\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "# Uncomment next line to print prompt\n",
        "# print(f\"Prompt: {prompt}\\n\")\n",
        "\n",
        "# 1. Setup API Key\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# 2. Initialize the Gemini Model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=api_key\n",
        ")\n",
        "\n",
        "# 3. Submit question to LLM\n",
        "try:\n",
        "    streamed_text = \"\"\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "    # 3. Stream and Monitor\n",
        "    for chunk in llm.stream(prompt):\n",
        "        streamed_text += chunk.content\n",
        "\n",
        "        # Update the display\n",
        "        display_handle.update(Markdown(streamed_text))\n",
        "\n",
        "        # Check if we have exceeded our manual limit\n",
        "        if len(streamed_text) > MAX_CHARS:\n",
        "            # Append a notice so the user knows why it stopped\n",
        "            display_handle.update(Markdown(streamed_text + \"...\\n\\n**(Stopped by Client-Side Limit)**\"))\n",
        "            break # Forcefully exit the loop\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during query: {e}\")"
      ],
      "metadata": {
        "id": "U_fa3Pck-z-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "Let the good times roll.\n",
        "```"
      ],
      "metadata": {
        "id": "WySFzrpTtyHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------\n",
        "\n",
        "**Why does the code Uses Triple Quotes?**\n",
        "\n",
        "The code in the cell above uses triple double quotes (\"\"\") for the prompt string to allow for clean, multi-line formatting and to include special characters, such as backticks (```) and placeholders ({style} and {text}).\n",
        "\n",
        "~~~text\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\"\"\"\n",
        "~~~\n",
        "\n",
        "-------------------------\n"
      ],
      "metadata": {
        "id": "0lPEtXSNh42d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Working with Prompts**\n",
        "\n",
        "In the cell below, use `Gemini-2.0-flash`  to translate the German expression: \"Ein Prosit der Gemütlichkeit\" into English.\n"
      ],
      "metadata": {
        "id": "uq2WA-AYuVR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n"
      ],
      "metadata": {
        "id": "kzkL7ARauVR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image03F.png)"
      ],
      "metadata": {
        "id": "VGza68NEuVR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dynamic Prompts**\n",
        "\n",
        "A **dynamic prompt** is a flexible and adaptive input designed for interaction with language models (LLMs) like `Gemini`, where placeholders or variables are used to customize the prompt based on context or user-provided information.\n",
        "\n",
        "This approach allows for reusability, personalization, and automation, ensuring that the output is tailored to specific needs without rewriting the entire prompt.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Key Characteristics of a Dynamic Prompt**\n",
        "1. **Variables**:\n",
        "   - Dynamic prompts include placeholders for variables, like `{name}`, `{style}`, or `{text}`, which can be filled with different values at runtime.\n",
        "   - For example:\n",
        "     ```python\n",
        "     prompt = f\"Translate this text: {text} into {language}.\"\n",
        "     ```\n",
        "     Here, `{text}` and `{language}` can be dynamically replaced by the desired input values.\n",
        "2. **Context-Aware**:\n",
        "   - They adapt to the context, such as the user’s preferences, conversation history, or specific tasks.\n",
        "   - For instance, a dynamic prompt for summarization might consider the length of the desired output: \"Summarize the following article in less than {words} words.\"\n",
        "3. **Reusable Templates**:\n",
        "   - Instead of hardcoding individual tasks, dynamic prompts use templates that can be applied across multiple scenarios by simply replacing values.\n",
        "   - Example template:\n",
        "     ```python\n",
        "     eg_template_text = \"\"\"Write a {tone} response to the following message:\n",
        "     message: {user_message}\"\"\"\n",
        "     ```\n",
        "4. **Personalization**:\n",
        "   - Dynamic prompts can be personalized based on user inputs or profiles, enhancing user experience. For example:\n",
        "     ```python\n",
        "     f\"Hi {name}, here’s the weather forecast for {city}!\"\n",
        "     ```\n",
        "\n",
        "#### **Why Are Dynamic Prompts Important?**\n",
        "\n",
        "- **Efficiency**: They save time by enabling template reuse.\n",
        "- **Scalability**: Useful for applications needing to handle diverse inputs.\n",
        "- **Adaptability**: They produce tailored outputs depending on the specific context or task.\n",
        "- **User Experience**: Personalization through dynamic prompts improves user satisfaction.\n",
        "\n",
        "---\n",
        "\n",
        "Dynamic prompts are at the heart of effective interactions with LLMs, making them more versatile, context-aware, and user-specific."
      ],
      "metadata": {
        "id": "azkr5E3Cii37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Create and Use a Dynamic Prompt\n",
        "\n",
        "The code in the cell below creates a **Dynamic Prompt**. The process has been broken down into 5 steps to make it easier to understand the process. The key step is to define the `source text` using this code:\n",
        "\n",
        "```python\n",
        "source_text = \"千里之行，始于足下。\"\n",
        "```\n",
        "\n",
        "By simply changing the `source_text` you can translate almost any language into English.\n",
        "\n",
        "In this example, the prefix `eg_` has been added to several variable names to keep them separate from similar variables that you will need to create in the exercises.\n",
        "\n",
        "To keep the output manageable, an output limit of 500 characters has been added."
      ],
      "metadata": {
        "id": "Wn-e4R3BuntR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Create and Use a Dynamic Prompt\n",
        "\n",
        "# Initialize Client\n",
        "from google import genai\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# 1. Setup Client\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Error checking API key: {e}\")\n",
        "\n",
        "# 2. Define Variables\n",
        "target_style = \"American English\"\n",
        "source_text = \"千里之行，始于足下。\"\n",
        "\n",
        "# 3. Build the Prompt\n",
        "eg_prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {target_style}. \\\n",
        "text: ```{source_text}```\n",
        "\"\"\"\n",
        "\n",
        "print(f\"--- Prompt Sent to Model ---\\n{eg_prompt}\\n\")\n",
        "\n",
        "# 4. Define our Hard Limit\n",
        "MAX_CHARS = 500\n",
        "\n",
        "# 5. Stream the response\n",
        "streamed_text = \"\"\n",
        "display_handle = display(Markdown(\"Waiting for stream...\"), display_id=True)\n",
        "\n",
        "try:\n",
        "    # Use client.models.generate_content_stream for streaming\n",
        "    response_stream = client.models.generate_content_stream(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        contents=eg_prompt\n",
        "    )\n",
        "\n",
        "    for chunk in response_stream:\n",
        "        # In the new SDK, chunk.text provides the text content directly\n",
        "        if chunk.text:\n",
        "            streamed_text += chunk.text\n",
        "\n",
        "            # Check if we've exceeded the limit\n",
        "            if len(streamed_text) >= MAX_CHARS:\n",
        "                final_output = streamed_text[:MAX_CHARS] + \"...\\n\\n**(Stopped by Client-Side Limit)**\"\n",
        "                display_handle.update(Markdown(final_output))\n",
        "                break\n",
        "\n",
        "            # Update the live display with the current text\n",
        "            display_handle.update(Markdown(streamed_text))\n",
        "\n",
        "except Exception as e:\n",
        "    # Fixed the syntax error here\n",
        "    print(f\"Error during query: {e}\")"
      ],
      "metadata": {
        "id": "Rcmj7lpyuKqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image04F.png)"
      ],
      "metadata": {
        "id": "nn-lK6Hf3vMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This newly constructed prompt can now perform the intended task of translation."
      ],
      "metadata": {
        "id": "31sPO7R7GHv5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TphfSQIf5su_"
      },
      "source": [
        "### **Exercise 3 - Create and Use a Dynamic Prompt**\n",
        "\n",
        "In the cell below, write the code to translate this text \"Президент Трамп, русская Родина сдаётся\" into English.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n"
      ],
      "metadata": {
        "id": "u-P2IMX1xXlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image05F.png)"
      ],
      "metadata": {
        "id": "TBUb3qckxXlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Memory\n",
        "\n",
        "Human minds have both **long-term memory** and **short-term memory**. Long-term memory is what the human has learned throughout their lifetime. Short-term memory is what a human has only recently discovered in the last minute or so. For humans, learning is converting short-term memory into long-term memory that we will retain.\n",
        "\n",
        "This process works somewhat differently for a LLM. Long-term memory was the weight of the neural network when it was initially trained or finetuned. Short-term memory is additional information that we wish the LLM to retain from previous prompts. For example, if the first prompt is \"My name is David\", the LLM will likely tell you hello and repeat your name. However, the LLM will not know the answer if the second prompt is \"What is my name.\" without adding a memory component.\n",
        "\n",
        "These memory objects, which LangChain provides, provide a sort of short-term memory. It is important to note that these objects are not affecting the long-term memory of the LLM, and once you discard the memory object, the LLM will forget. Additionally, the memory object can only hold so much information; newer information may replace older information once it is filled.\n",
        "\n",
        "One important point to remember is that LLM's only have their input prompt. To provide such memory, these objects are appending anything we wish the LLM to remember to the input prompt. This section will see two ways to augment the prompt with previous information: a buffer and a summary. The buffer prepends a script of the last conversation up to this point. The summary approach keeps a consistently updated summary paragraph of the conversation.m and short-term memory. **Long-term memory** is what the human has learned throughout their lifetime. **Short-term memory** is what a human has only recently discovered in the last minute or so. For humans, learning is converting short-term memory into long-term memory that we will retain.\n",
        "\n",
        "This process works somewhat differently for an LLM.\n",
        "\n",
        "* **Long-term memory** was the weight of the neural network when it was initially trained or finetuned.\n",
        "* **Short-term memory** is additional information that we wish the LLM to retain from previous prompts in the current session.\n",
        "\n",
        "For example, if the first prompt is *\"My name is David\"*, the LLM will likely tell you hello and repeat your name. However, the LLM will not know the answer if the second prompt is *\"What is my name?\"* without adding a memory component.\n",
        "\n",
        "### How the Google GenAI SDK handles this\n",
        "\n",
        "In the **Google GenAI SDK**, this short-term memory is handled by the **Chat Session** (`client.chats.create`). It is important to note that this session history does not affect the *long-term* memory (weights) of the LLM; once you restart the runtime or discard the chat object, the LLM will forget the conversation.\n",
        "\n",
        "One important point to remember is that LLMs technically only have their current input prompt. To provide \"memory,\" the SDK automatically appends the entire history of the conversation to your new prompt behind the scenes.\n",
        "\n",
        "**The Context Window Advantage**\n",
        "Unlike older libraries that required complex \"Buffer\" or \"Summary\" objects to manage limited space, the Gemini models used by this SDK have massive **Context Windows** (up to 1-2 million tokens). This allows the SDK to simply retain the full \"Buffer\" of your conversation history without needing to summarize or truncate it as frequently."
      ],
      "metadata": {
        "id": "QV_l3Co4BRHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Conversation Bots (Google GenAI SDK)\n",
        "\n",
        "A **Custom Conversation Bot** is an AI-powered system built using the `google-genai` library that engages in natural language conversations while strictly adhering to a specific persona or domain.\n",
        "\n",
        "In the code below, we will create a **Renal Assistant**—a bot customized specifically for nephrology that can \"see\" medical images and remember patient symptoms.\n",
        "\n",
        "### Key Features in the New SDK\n",
        "\n",
        "* **System Instructions (Persona):**\n",
        "    Instead of complex prompt templates, we use the `system_instruction` parameter to define exactly how the bot behaves (e.g., *\"You are a Physician's Assistant,\" \"Do not diagnose\"*). This instruction persists throughout the entire conversation.\n",
        "\n",
        "* **Native Multimodality:**\n",
        "    Modern custom bots are not limited to text. As demonstrated with our `analyze_local_image` function, the bot can seamlessly process text and images (like lab results or kidney scans) in the same message to provide context-aware analysis.\n",
        "\n",
        "* **Automatic Session Memory:**\n",
        "    The SDK's `client.chats.create` object automatically manages the conversation history. It remembers the user's name, previous symptoms, and uploaded images without the developer needing to manually store or summarize the text.\n",
        "\n",
        "### Common Use Cases\n",
        "* **Healthcare Triage:** (e.g., Our Renal Health Assistant)\n",
        "* **Customer Support:** Answering FAQs based on specific company policies.\n",
        "* **Educational Tutoring:** A history tutor that roleplays as a historical figure.\n",
        "* **Technical Analysis:** debugging code or analyzing charts uploaded by the user.\n",
        "\n",
        "### Summary\n",
        "Custom conversation bots using the Google GenAI SDK are flexible, intelligent tools. By combining **System Instructions** with **Multimodal capabilities**, developers can rapidly create specialized assistants that are far more capable than generic chatbots."
      ],
      "metadata": {
        "id": "HSzKX4yRCL1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4A: Create a Custom Conversation Bot\n",
        "\n",
        "In Example 4 we are going to create a custom conversational bot named \"AI Physician's Assistant\" that is designed to help screen phone calls to a medical clinic.\n",
        "\n",
        "The most important step is the creation of a **Persona** for the Custom Bot. Here is the code used to create the Persona (System Instructions) for our AI Physician's Assistant:\n",
        "\n",
        "```python\n",
        "\n",
        "# 2. Define the Persona (System Instruction)\n",
        "system_instruction_text = \"\"\"\n",
        "You are a medical AI Physician's Assistant specializing in Nephrology.\n",
        "Your role is to have a friendly conversation with a patient.\n",
        "\n",
        "Guidance:\n",
        "- ONLY discuss topics related to renal (kidney) health issues.\n",
        "- If the user asks about other medical conditions (e.g., heart, lungs), politely redirect them to a general practitioner.\n",
        "- If the user asks non-medical questions, steer them back to kidney health.\n",
        "- Ask follow-up questions to better understand symptoms (e.g., color of urine, pain levels).\n",
        "- IMPORTANT: Do not provide definitive medical diagnoses or prescribe medication.\n",
        "- ALWAYS recommend seeing a doctor for official treatment.\n",
        "\"\"\"\n",
        "```\n",
        "Notice that an important part of creating a Persona is providing `Guidance` so that the Custom Bot has a solid framework in which to work. In this particular example we have created the Custom Bot that is specialized in **nephrology**, which is the clinical aspects of kidney function.\n",
        "\n",
        "As in previous examples, the prefix \"eg_\" has been added to key variables to separate the Bot in the example form the Bot you will make in the next exercise."
      ],
      "metadata": {
        "id": "JkSpisR5t7Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4A: Create a Custom Conversation Bot\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# 1. Setup Client and Key\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Error: Please set your GOOGLE_API_KEY in the Secrets tab. Details: {e}\")\n",
        "\n",
        "# 2. Define the Persona (System Instruction)\n",
        "eg_system_instruction_text = \"\"\"\n",
        "You are a medical AI Physician's Assistant specializing in Nephrology.\n",
        "Your role is to have a friendly conversation with a patient.\n",
        "\n",
        "Guidance:\n",
        "- ONLY discuss topics related to renal (kidney) health issues.\n",
        "- If the user asks about other medical conditions (e.g., heart, lungs), politely redirect them to a general practitioner.\n",
        "- If the user asks non-medical questions, steer them back to kidney health.\n",
        "- Ask follow-up questions to better understand symptoms (e.g., color of urine, pain levels).\n",
        "- IMPORTANT: Do not provide definitive medical diagnoses or prescribe medication.\n",
        "- ALWAYS recommend seeing a doctor for official treatment.\n",
        "\"\"\"\n",
        "\n",
        "# 3. Create the Chat Session\n",
        "eg_chat = client.chats.create(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=eg_system_instruction_text,\n",
        "        temperature=0.7,\n",
        "        max_output_tokens=500\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"AI Physician's Assistant is ready.\")\n",
        "\n",
        "# --- HELPER FUNCTION TO CHAT ---\n",
        "def eg_send_message_to_assistant(user_input):\n",
        "    \"\"\"Sends a message to the active chat session and prints the response.\"\"\"\n",
        "    try:\n",
        "        # The SDK automatically appends this to history\n",
        "        response = eg_chat.send_message(user_input)\n",
        "\n",
        "        # Display results nicely in Colab\n",
        "        print(f\"\\nUser: {user_input}\")\n",
        "        print(\"Physician's Assistant:\")\n",
        "        display(Markdown(response.text))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "OzBV_47edAGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "```text\n",
        "AI Physician's Assistant is ready.\n",
        "```"
      ],
      "metadata": {
        "id": "4dmpavl_zAhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4B: Test the Custom Bot\n",
        "\n",
        "The code in the cell below sends several messages to our `AI Physician Assistant` to test the Custom Bot's ability to automously handle a call from a patient seeking medical advice."
      ],
      "metadata": {
        "id": "UkdBkIeGzMXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4B: Test the Custom Bot\n",
        "\n",
        "\n",
        "# Create and send messages to the bot\n",
        "\n",
        "# Message 1: Relevant topic\n",
        "eg_send_message_to_assistant(\"Doc, I am having trouble peeing.\")\n",
        "\n",
        "# Message 2: Testing memory (The bot should remember the previous context)\n",
        "eg_send_message_to_assistant(\"It hurts a little bit, and it happens mostly at night.\")\n",
        "\n",
        "# Message 3: Testing the guardrails (Irrelevant topic)\n",
        "eg_send_message_to_assistant(\"Can you also check why my knee hurts?\")"
      ],
      "metadata": {
        "id": "tGQd5Gwxf5Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image06F.png)"
      ],
      "metadata": {
        "id": "MVLRycz90a1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4A: Create a Custom Conversation Bot**\n",
        "\n",
        "For **Exercise 4A** create a custom conversational bot named \"AI Physician's Assistant\" that is designed to help screen phone calls to a medical clinic. Assign your `Physician's Assistant` any medical specialty you want -- except don't use `nephrology` since this was already used in Example 4.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Change the prefix 'eg_' to 'ex_' as required to keep your variables separated from similar variables in the examples."
      ],
      "metadata": {
        "id": "Dtq3kdR91CR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4A here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IndxVUjW1CR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "```text\n",
        "AI Physician's Assistant is ready.\n",
        "```"
      ],
      "metadata": {
        "id": "rhtZUyEi1CSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4B: Test the Custom Bot**\n",
        "\n",
        "In the cell below, write the code to send at least 4 messages to your `AI Physician's Assistant` to test his/her ability to automously handle a call from a patient seeking medical advice. Make sure that at least 3 questions are appropiate for the medical specialty that you have assigned to your Custom Bot."
      ],
      "metadata": {
        "id": "iwi23RpW1CSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4B: Test the Custom Bot\n",
        "\n"
      ],
      "metadata": {
        "id": "ihvaGRo81CSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your output will dependent on the medical specialty you have selected for your AI Physician Assistant and the specific questions that you created.\n"
      ],
      "metadata": {
        "id": "0aqAfYeH1CSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLMs Have Different Functions**\n",
        "\n",
        "LLMs can also be distinguished by their specialized functions, which are often achieved through fine-tuning.\n",
        "\n",
        "* **Base/Foundation LLMs:** These are the initial LLMs trained on a massive, diverse, and unlabeled dataset. They have a broad understanding of language but are not inherently designed for instruction following.\n",
        "  * **Example:** Google's PaLM before instruction-tuning.\n",
        "\n",
        "* **Instruct Models:** Fine-tuned to follow specific instructions or prompts. This training makes them more useful for direct task completion.\n",
        "* **Chat Models:** A specialized type of instruct model, further fine-tuned using conversational data to perform well in dialogue-based interactions.\n",
        "* **Code Models:** Trained on extensive datasets of code to perform programming-related tasks, like generating, summarizing, and debugging code.\n",
        "* **Multimodal Models:** Can process and generate content across multiple data types, such as text, images, and audio. They combine different encoding modalities to understand and act on complex prompts.\n",
        "  * **Example:** Google's Gemini 2.5 Pro is a multimodal model that can process text, images, audio, and video.\n",
        "* **Hybrid Models:** Combine the strengths of different models and techniques. A common hybrid approach uses a powerful LLM for reasoning alongside a Retrieval-Augmented Generation (RAG) system for accessing up-to-date, authoritative information from an external knowledge base.\n"
      ],
      "metadata": {
        "id": "nLvOafQl_U1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Image Analysis with Nano Banana Pro**\n",
        "\n",
        "**Nano Banana Pro** is a state-of-the-art multimodal AI model that can process and reason with images, text, and audio. It can be applied to analyze biological and medical images by leveraging its ability to \"think with\" visual content, rather than just interpreting it. However, as a smaller model, it is generally better suited for specific, high-volume tasks and as part of a larger, more precise analytical pipeline, rather than for delivering definitive, complex diagnoses on its own.\n",
        "\n",
        "#### **Applications in Biological Imaging**\n",
        "\n",
        "Nano Banana Pro is useful for tasks that benefit from its speed, efficiency, and understanding of visual patterns in biological contexts.\n",
        "* **Microscopy image analysis:** The model can be used for classification tasks, such as differentiating between wild-type and mutated cells, or identifying certain features in tissue samples. It can generate descriptive captions for images, which is valuable for data curation.\n",
        "* **Pattern and texture analysis:** In diagnostics, AI can analyze subtle patterns in biological samples that may be hard for humans to detect. For example, AI can analyze drying patterns in tears to detect signs of dry-eye disease or in blood to help screen for conditions like leukemia.\n",
        "* **Data filtering and preprocessing:** Due to its cost-effectiveness, GPT-4o-mini can be used in data pipelines to filter and curate large sets of biological images. This helps democratize access to high-quality training data for more powerful, domain-specific models.\n",
        "\n",
        "#### **Applications in Medical Imaging**\n",
        "\n",
        "In medical imaging, GPT-4o-mini can act as a component within a larger workflow, performing initial screening and high-throughput tasks, though it has limitations for definitive clinical decision-making.\n",
        "\n",
        "* **High-volume screening:** The model can be used for high-volume, low-cost tasks like summarizing daily patient messages related to imaging or extracting structured data from scanned medical forms.\n",
        "* **Patient data analysis:** It can help healthcare professionals analyze patient data by identifying patterns in images when paired with contextual information. This can assist in decision-making or provide support for treatment plans.\n",
        "* **Preliminary assessment (with limitations):** Studies have shown that multimodal models can identify anatomical regions, modalities, and sometimes pathologies in images like CT scans and X-rays. However, these studies also highlight that such models can produce \"hallucinations\" or inaccuracies, especially with complex interpretations. The \"all or nothing\" accuracy of GPT-4o in radiology demonstrates that it should not be used for definitive diagnoses.\n",
        "* **Modular pipelines:** For more robust analysis, Nano Banana Pro can serve as an initial filter in a multi-step diagnostic process. The image can then be sent to a human expert or a more specialized model for final verification.\n"
      ],
      "metadata": {
        "id": "EOZG2cVePfd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5A: Download Image\n",
        "\n",
        "In Example 5 we are going to use `gpt-4o-mini` to analyze a clinical image. The first step will be to download the image, `kidney_scan.jpg` that is stored on the course fileserver.\n",
        "\n",
        "The code in the cell below downloads the image and provides\n",
        "verification that the image was found and sucessfully downloaded to our Colab notebook.\n"
      ],
      "metadata": {
        "id": "_drb79eB2n_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5A: Image Analysis\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Define the URL and the filename you want to save it as\n",
        "image_url = \"https://biologicslab.co/BIO1173/images/class_04/eg_medical_image.png\"\n",
        "eg_filename = \"eg_medical_image.png\"\n",
        "\n",
        "print(f\"Attempting to download from: {image_url}\")\n",
        "\n",
        "try:\n",
        "    # 1. Get the content from the URL\n",
        "    response = requests.get(image_url)\n",
        "\n",
        "    # 2. Check if the request was successful (Status Code 200)\n",
        "    if response.status_code == 200:\n",
        "        # 3. Write the content to a file in your Colab directory\n",
        "        with open(eg_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"✅ Success! Saved as '{eg_filename}' in current directory.\")\n",
        "\n",
        "        # Optional: Verify it exists by listing the directory\n",
        "        if os.path.exists(eg_filename):\n",
        "            print(f\"File verified at: {os.path.abspath(eg_filename)}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"❌ Failed to download. Status code: {response.status_code}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "wnv964UN5ah4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image14F.png)\n"
      ],
      "metadata": {
        "id": "6IRkzl47-SXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5B: Analyze the Image\n",
        "\n",
        "\n",
        "The code in the cell analyzes the image.\n"
      ],
      "metadata": {
        "id": "dBHeLA2S-46d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5B: Analyze the Image\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from google import genai\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# 1. Setup Client (Global)\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Error checking API key: {e}\")\n",
        "\n",
        "# 2. Define the Detailed Prompt\n",
        "eg_image_analysis_prompt = \"\"\"\n",
        "You are analyzing a histology image of a section of a healthy human small intestine wall.\n",
        "\n",
        "Provide your analysis in the following format:\n",
        "1. Identify the basic structures present in the view.\n",
        "2. Distinguish between the different layers of the intestinal wall (Mucosa, Submucosa, Muscularis Externa, Serosa) if visible.\n",
        "3. Point out specific cell types/structures such as Villi, Crypts of Lieberkühn, Goblet Cells, Enterocytes, and Lamina Propria.\n",
        "4. Describe the basic function of each identified structure.\n",
        "5. Explain the tissue organization of the small intestine in the context of form and function (absorption and digestion).\n",
        "\n",
        "Histological Image Analysis Response:\n",
        "\"\"\"\n",
        "\n",
        "# 3. Define the image filename\n",
        "eg_filename = \"eg_medical_image.png\"\n",
        "\n",
        "def eg_analyze_histology(eg_filename):\n",
        "    \"\"\"\n",
        "    Loads an image and sends it to Gemini with the specific histology prompt.\n",
        "    \"\"\"\n",
        "    # Check file existence\n",
        "    if not os.path.exists(eg_filename):\n",
        "        print(f\"Error: '{eg_filename}' not found.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Open the image using PIL\n",
        "        img = Image.open(eg_filename)\n",
        "        print(f\"Loading image: {eg_filename}\")\n",
        "        display(img) # Display thumbnail\n",
        "\n",
        "        print(\"\\nPhysician's Assistant is analyzing histology...\")\n",
        "\n",
        "        # Send Request (Native SDK handles the image directly - no Base64 needed!)\n",
        "        response = client.models.generate_content(\n",
        "            model=\"nano-banana-pro-preview\",\n",
        "            contents=[eg_image_analysis_prompt, img]\n",
        "        )\n",
        "\n",
        "        # Display Result\n",
        "        print(\"-\" * 40)\n",
        "        display(Markdown(response.text))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Run the analysis\n",
        "eg_analyze_histology(eg_filename)\n"
      ],
      "metadata": {
        "id": "rIiS8dKeA0i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image20F.png)\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image21F.png)\n"
      ],
      "metadata": {
        "id": "fva_sapXE4Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5A: Download Image**\n",
        "\n",
        "In the cell below, write the code to use Nano Banana Pro to analyze a clinical image called `ex_medical_image.png` that is stored on the course fileserver.\n",
        "\n"
      ],
      "metadata": {
        "id": "tPJ_BblJHshG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5A here\n",
        "\n"
      ],
      "metadata": {
        "id": "0CIyfHW6HTU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image13F.png)\n"
      ],
      "metadata": {
        "id": "O977bxcpHshH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5B: Image Analysis**\n",
        "\n",
        "Write the code to send your image to Nano Banana Pro for analysis.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Use this prompt:\n",
        "```python\n",
        "# 2. Define the Detailed Prompt\n",
        "ex_image_analysis_prompt = \"\"\"\n",
        "You are analyzing a histology image of a section of a healthy human kidney.\n",
        "\n",
        "Provide your analysis in the following format:\n",
        "1. Identify the basic structures present in the view.\n",
        "2. Distinguish between the Cortex and Medulla if visible.\n",
        "3. Point out specific cell types/structures such as Glomeruli, Proximal/Distal Tubules, and Collecting Ducts.\n",
        "4. Describe the basic function of each identified structure.\n",
        "5. Explain the tissue organization of the kidney in the context of form and function (filtration and reabsorption).\n",
        "\n",
        "Histological Image Analysis Response:\n",
        "\"\"\"\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "QwH6ppmZHSno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5B here\n",
        "\n"
      ],
      "metadata": {
        "id": "vK1kr1ntHTU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image15F.png)\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image16F.png)\n"
      ],
      "metadata": {
        "id": "WMp0TODSHTU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play this YouTube video to learn more about Nano Banana Pro and its ability to generate **infographics**."
      ],
      "metadata": {
        "id": "CcYRSzKGuJJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"hbfrZriq0f8?si=OFa081eI_qsGUBSn\"\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen\n",
        "  referrerpolicy=\"strict-origin-when-cross-origin\">\n",
        "</iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "KHf2KzxNuKJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is an Infographic?**\n",
        "\n",
        "An **infographic** (information graphic) is a visual representation of information, data, or knowledge intended to present complex information quickly and clearly. It combines imagery, charts, and minimal text to tell a story or explain a process.\n",
        "\n",
        "#### **Why are Infographics Useful for Technical Research?**\n",
        "\n",
        "In the context of scientific publishing and education, infographics (often called **Visual Abstracts**) serve several critical functions:\n",
        "\n",
        "* **Rapid Comprehension:** The human brain processes visuals 60,000 times faster than text. An infographic allows a reader to understand the core findings of a paper in seconds rather than minutes.\n",
        "* **Simplification of Complexity:** Technical papers often contain dense jargon and complex methodologies. An infographic forces the author to distill this down to the most essential \"take-home\" message.\n",
        "* **Visualizing Workflows:** For studies involving multi-step processes (like the *GUARDIAN* study's screening workflow or the *Neuron-Cancer* mechanism), a flowchart is often clearer than a text description.\n",
        "* **Broader Accessibility:** Visual summaries make high-level research accessible to students, the media, and the general public, effectively bridging the gap between academia and the real world.\n",
        "* **Increased Citations:** Studies suggest that research papers promoted with visual abstracts on social media receive significantly more engagement and citations than those without."
      ],
      "metadata": {
        "id": "_fabROymBzS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Create an Infographic from a Research Article\n",
        "\n",
        "For Example 6 we are going to use Google's `Nano Banana Pro` to generate an infographic for a recently published scientific research article. This example has been divided into two steps to make the code easier to follow."
      ],
      "metadata": {
        "id": "fHE311e1zZcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6A: Generate Summary of the Research Paper\n",
        "\n",
        "In the first step, we will download the research paper, \"NerverCancerTransfer.pdf\" from the course file server. We will then upload it to Nano Banana Pro so it can \"read\" the paper and generate a summary of the paper's contents. In the next step, we will let Nano Banana Pro use this summary to construct an infographic."
      ],
      "metadata": {
        "id": "Cka55VOLEBVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6A: Generate Summary of the Research Paper\n",
        "\n",
        "import requests\n",
        "import os\n",
        "from google import genai\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Define name of the research paper\n",
        "RESEARCH_PAPER=\"NerveCancerTransfer.pdf\"\n",
        "\n",
        "# 1. Setup Client\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "    print(\"API client initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up API client: {e}\")\n",
        "\n",
        "# 2. Download research paper from fileserver\n",
        "pdf_url = \"https://biologicslab.co/BIO1173/images/class_04/NerveCancerTransfer.pdf\"\n",
        "eg_pdf_filename = \"NerveCancerTransfer.pdf\"\n",
        "\n",
        "# Download PDF (Silent if already exists)\n",
        "if not os.path.exists(eg_pdf_filename):\n",
        "    try:\n",
        "        r = requests.get(pdf_url)\n",
        "        with open(eg_pdf_filename, 'wb') as f: f.write(r.content)\n",
        "    except: pass\n",
        "\n",
        "# 3. Upload the PDF to Gemini\n",
        "print(f\"Uploading {eg_pdf_filename} to Gemini...\", end=\"\")\n",
        "uploaded_file = client.files.upload(file=eg_pdf_filename)\n",
        "print(\"done.\")\n",
        "print(f\"Upload complete: {uploaded_file.name}\")\n",
        "\n",
        "# 4. Define the summary prompt\n",
        "summary_prompt = \"\"\"\n",
        "Read this research paper and provide a detailed summary focusing on:\n",
        "\n",
        "1. What is this study?\n",
        "2. What is its main purpose?\n",
        "3. What are the key findings or outcomes?\n",
        "\n",
        "Keep the summary under 1500 words but include all key details needed to create\n",
        "an accurate infographic about this specific study.\n",
        "\"\"\"\n",
        "\n",
        "# 5. Generate summary using Gemini\n",
        "print(\"Analyzing your research paper...\",end=\"\")\n",
        "eg_summary_response = client.models.generate_content(\n",
        "    model=\"nano-banana-pro-preview\",\n",
        "    contents=[uploaded_file, summary_prompt]\n",
        ")\n",
        "\n",
        "print(\"done.\")\n",
        "\n",
        "# 6. Print Summary (optional)\n",
        "eg_paper_summary = eg_summary_response.text\n",
        "# Uncomment next lines to print out research summary\n",
        "# print(\"\\nPaper Summary:\")\n",
        "# print(\"-\" * 40)\n",
        "# display(Markdown(paper_summary))\n"
      ],
      "metadata": {
        "id": "dk6YCpikD5uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image32F.png)"
      ],
      "metadata": {
        "id": "8QXg-YY4OSSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6B: Generate Infographic Summary\n",
        "\n",
        "Now that we have a detailed summary of our research paper, we can let Nano Banana Pro read the summary and based on what it finds, generate an infographic that provides a visual summary the research paper."
      ],
      "metadata": {
        "id": "I4Mt0PGlF2KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6B: Generate Infographic Summary\n",
        "\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# 1. Define the Infographic Prompt (let Nano Banana Pro decide the content)\n",
        "infographic_prompt = f\"\"\"\n",
        "You are a scientific illustrator. Read the following research paper summary and\n",
        "create an educational infographic that best represents the key concepts and workflow\n",
        "described in the paper.\n",
        "\n",
        "RESEARCH PAPER SUMMARY:\n",
        "{eg_paper_summary}\n",
        "\n",
        "Create a clear, visually appealing educational infographic based on this research paper.\n",
        "Include appropriate labels and text to explain the key steps or concepts.\n",
        "\n",
        "Style: Clean, modern scientific illustration suitable for a biology textbook.\n",
        "\"\"\"\n",
        "\n",
        "# 2. Define the output filename\n",
        "eg_output_filename = \"eg_infographic.png\"\n",
        "\n",
        "def eg_generate_infographic(prompt, output_file):\n",
        "    \"\"\"\n",
        "    Generates an infographic using Nano Banana Pro.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Generating infographic based on the uploaded research article...\")\n",
        "        print(\"(This may take 30-60 seconds)\\n\")\n",
        "\n",
        "        # Send request to Gemini Image Generation\n",
        "        response = client.models.generate_content(\n",
        "            model=\"nano-banana-pro-preview\",\n",
        "            contents=prompt,\n",
        "            config={\n",
        "                \"response_modalities\": [\"image\", \"text\"],\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Extract and save the image\n",
        "        for part in response.candidates[0].content.parts:\n",
        "            if hasattr(part, 'inline_data') and part.inline_data:\n",
        "                # Get the raw image bytes\n",
        "                image_bytes = part.inline_data.data\n",
        "\n",
        "                # Check if it's already bytes or needs decoding\n",
        "                if isinstance(image_bytes, str):\n",
        "                    image_bytes = base64.b64decode(image_bytes)\n",
        "\n",
        "                if len(image_bytes) < 100:\n",
        "                    print(\"Error: Received empty/broken image bytes.\")\n",
        "                    return None\n",
        "\n",
        "                # Open image from bytes\n",
        "                image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "                # Display the image\n",
        "                print(\"Generated Infographic:\")\n",
        "                print(\"-\" * 40)\n",
        "                display(image)\n",
        "\n",
        "                # Save the image\n",
        "                image.save(output_file)\n",
        "                print(f\"\\nSaved as: {output_file}\")\n",
        "                return image\n",
        "\n",
        "        print(\"No image was generated in the response.\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Run the infographic generation\n",
        "infographic = eg_generate_infographic(infographic_prompt, eg_output_filename)\n"
      ],
      "metadata": {
        "id": "3I2fm9A1F23w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image33F.png)"
      ],
      "metadata": {
        "id": "rf3j-L2wOvux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6A: Generate Summary of the Research Paper**\n",
        "\n",
        "For **Exericse 6A** you are to write the code to generate a summary of a different research article called \"GUARDIAN_study.pdf\" that is also available on the course file server.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Change the prefix `eg_` to `ex_`."
      ],
      "metadata": {
        "id": "uSnIvMG-PdzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6A here\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SfoL6D0MwmeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see xomethong _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image34F.png)"
      ],
      "metadata": {
        "id": "hzPmIGh_Pdza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6B: Generate Infographic**\n",
        "\n",
        "In the next code cell write the Python code to use Nano Banana Pro to generate an infographic based on the summary you created in the previous step (**Exercise 6A**)."
      ],
      "metadata": {
        "id": "DrE2w8-mPdza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6B here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FSpQYmYwPdza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image36F.png)"
      ],
      "metadata": {
        "id": "igXxxUVhPdza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Turning Photos into Cartoon Images**\n",
        "\n",
        "In this part of this lesson we demonstrate how to convert a photorealtistic image into a cartoon image.\n",
        "\n",
        "### **Why Convert a Photorealistic Image into a Cartoon Image?**\n",
        "\n",
        "Converting a photorealistic image into a cartoon image can serve a variety of creative, practical, and communicative purposes. Here are some common reasons:\n",
        "\n",
        "#### **1. Artistic Expression**\n",
        "- Stylizes images to emphasize features or emotions.\n",
        "- Transforms mundane photos into engaging artwork.\n",
        "\n",
        "#### **2. Branding and Marketing**\n",
        "- Cartoon images are memorable and approachable.\n",
        "- Ideal for logos, mascots, and promotional materials.\n",
        "\n",
        "#### **3. Social Media and Content Creation**\n",
        "- Popular for avatars, thumbnails, and storytelling.\n",
        "- Helps creators stand out with a consistent visual identity.\n",
        "\n",
        "#### **4. Privacy and Anonymity**\n",
        "- Obscures identities while retaining personality.\n",
        "- Useful for online profiles and public presentations.\n",
        "\n",
        "#### **5. Educational and Instructional Use**\n",
        "- Simplified visuals make complex subjects more accessible.\n",
        "- Common in infographics, children’s books, and tutorials.\n",
        "\n",
        "#### **6. Entertainment and Media**\n",
        "- Widely used in animation, comics, and video games.\n",
        "- Enables exaggerated expressions and imaginative scenarios.\n"
      ],
      "metadata": {
        "id": "6tYdQmStD3-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 7: Create Cartoon Image from Photograph\n",
        "\n",
        "The code in the cell below demonstrates how to turn an actual image of two women working in a biology lab into a cartoon‑style image using Nano Banana Pro.\n",
        "\n",
        "By altering the prompt, you can achieve different cartoon effects.\n",
        "\n",
        "#### **Here are the Top 3 Cartoon Styles:**\n",
        "\n",
        "**1. The \"Disney / Pixar\" 3D Style**\n",
        "Best for: Making people or pets look friendly and expressive.\n",
        "\n",
        "```text\n",
        "Prompt:\n",
        "\n",
        "A cute 3D rendered character in the style of a\n",
        "modern Pixar movie. Big expressive eyes, soft smooth lighting,\n",
        "vibrant colors, 4k resolution, 3D blender render.\n",
        "Style: Disney/Pixar 3D render\n",
        "\n",
        "```\n",
        "\n",
        "**2. The \"Anime / Manga\" Style**\n",
        "Best for: Action shots or dramatic portraits.\n",
        "\n",
        "```text\n",
        "Prompt:\n",
        "\n",
        "High-quality anime style illustration,\n",
        "Studio Ghibli art style. cel-shaded, vibrant colors,\n",
        "clean lines, highly detailed background.\n",
        "Style: Anime Manga\n",
        "\n",
        "```\n",
        "\n",
        "**3. The \"Flat Vector\" Style (Great for Presentations)**\n",
        "Best for: Icons, symbols, or professional graphics\n",
        "\n",
        "```text\n",
        "Prompt:\n",
        "\n",
        "Flat vector art illustration, minimal design,\n",
        "bold solid colors, no gradients, white background,\n",
        "thick black outlines.\n",
        "Style: Flat Vector\n",
        "\n",
        "```\n",
        "\n",
        "The example in the cell below, uses the **Anime/Manga Style**."
      ],
      "metadata": {
        "id": "lJIKDndhrXgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7: Generate a Cartoon Image\n",
        "\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "from PIL import Image\n",
        "import requests\n",
        "from google import genai\n",
        "from google.colab import userdata\n",
        "from IPython.display import display\n",
        "\n",
        "# Define name of the original photo\n",
        "ORIGINAL_PHOTO = \"BioLab2.jpg\"\n",
        "\n",
        "# 1. Setup Client\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "    print(\"API client initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up API client: {e}\")\n",
        "\n",
        "# 2. Download photo from fileserver\n",
        "image_url = \"https://biologicslab.co/BIO1173/images/class_04/BioLab2.jpg\"\n",
        "\n",
        "# Download file\n",
        "try:\n",
        "    print(f\"Downloading from: {image_url}\")\n",
        "    r = requests.get(image_url, timeout=30)\n",
        "\n",
        "    if r.status_code == 200:\n",
        "        with open(ORIGINAL_PHOTO, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "        file_size = len(r.content) / 1024\n",
        "        print(f\"Downloaded: {ORIGINAL_PHOTO} ({file_size:.2f} KB)\")\n",
        "    else:\n",
        "        print(f\"Download failed. Status code: {r.status_code}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Download error: {e}\")\n",
        "\n",
        "# 3. Load the image with PIL\n",
        "print(f\"Loading {ORIGINAL_PHOTO}...\")\n",
        "original_image = Image.open(ORIGINAL_PHOTO)\n",
        "print(f\"Image size: {original_image.size}\")\n",
        "print(\"Original Image:\")\n",
        "display(original_image)\n",
        "\n",
        "# 4. Define the cartoon prompt\n",
        "eg_cartoon_prompt = \"\"\"\n",
        "Generate a 3D Pixar-style cartoon version of this image.\n",
        "Focus on big expressive eyes and soft, fluffy fur texture.\n",
        "Bright, happy lighting.\n",
        "Style: Disney/Pixar 3D render.\n",
        "\"\"\"\n",
        "\n",
        "# 5. Define the output filename\n",
        "eg_output_filename = \"eg_cartoon_image.png\"\n",
        "\n",
        "def eg_generate_cartoon(image, prompt, output_file):\n",
        "    \"\"\"\n",
        "    Generates a cartoon version of an image using Nano Banana Pro.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Generating cartoon version of the image...\")\n",
        "        print(\"(This may take 30-60 seconds)\\n\")\n",
        "\n",
        "        # Send request to Gemini Image Generation with the PIL image directly\n",
        "        response = client.models.generate_content(\n",
        "            model=\"nano-banana-pro-preview\",\n",
        "            contents=[image, prompt],\n",
        "            config={\n",
        "                \"response_modalities\": [\"image\", \"text\"],\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Extract and save the image\n",
        "        for part in response.candidates[0].content.parts:\n",
        "            if hasattr(part, 'inline_data') and part.inline_data:\n",
        "                # Get the raw image bytes\n",
        "                image_bytes = part.inline_data.data\n",
        "\n",
        "                # Check if it's already bytes or needs decoding\n",
        "                if isinstance(image_bytes, str):\n",
        "                    image_bytes = base64.b64decode(image_bytes)\n",
        "\n",
        "                if len(image_bytes) < 100:\n",
        "                    print(\"Error: Received empty/broken image bytes.\")\n",
        "                    return None\n",
        "\n",
        "                # Open image from bytes\n",
        "                result_image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "                # Display the image\n",
        "                print(\"Generated Cartoon Image:\")\n",
        "                print(\"-\" * 40)\n",
        "                display(result_image)\n",
        "\n",
        "                # Save the image\n",
        "                result_image.save(output_file)\n",
        "                print(f\"\\nSaved as: {output_file}\")\n",
        "                return result_image\n",
        "\n",
        "        print(\"No image was generated in the response.\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Run the cartoon generation (pass the PIL image directly)\n",
        "eg_cartoon_image = eg_generate_cartoon(original_image, eg_cartoon_prompt, eg_output_filename)\n"
      ],
      "metadata": {
        "id": "aIOO0XQE5XuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image38F.png)\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image39F.png)"
      ],
      "metadata": {
        "id": "ob7B02c55WuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7: Create Cartoon Image from Photograph**\n",
        "\n",
        "In the cell below, write the code to create a cartoon image from the same photograph used in Example 7, but change the cartoon style to either **Anime/Manga Style** or **Flat Vector Style**.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Copy-and-paste the code from Example 7 into the cell below. Modify the prompt to change the style using the prompt examples cited in Example 7."
      ],
      "metadata": {
        "id": "yx9OK59NDHLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 7 here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wGadWsOz_P9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your output will depend upon the cartoon style you selected for **Exercise 7**"
      ],
      "metadata": {
        "id": "Jk7Xhe0GrXgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embedding Layers in PyTorch**\n",
        "\n",
        "[Embedding Layers](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) are a handy feature of PyTorch that allows the program to automatically insert additional information into the data flow of your neural network. An embedding layer would automatically allow you to insert vectors in the place of word indexes.  \n",
        "\n",
        "Programmers often use embedding layers with Natural Language Processing (NLP); however, you can use these layers when you wish to insert a lengthier vector in an index value place. In some ways, you can think of an embedding layer as dimension expansion. However, the hope is that these additional dimensions provide more information to the model and provide a better score."
      ],
      "metadata": {
        "id": "HrvnI_Bc5FsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this demonstration of Embedding Layer, we are break the code into 3 blocks (steps).\n",
        "\n",
        "## Overview: Learning Medical Term Embeddings with Neural Networks\n",
        "\n",
        "This code demonstrates a fundamental concept in modern AI and natural language processing: **word embeddings**. Embeddings are numerical representations of words (or concepts) that capture semantic relationships in a way that computers can process and learn from.\n",
        "\n",
        "### What Are Embeddings?\n",
        "\n",
        "In traditional computing, words are just strings of characters with no inherent meaning. The word \"heart\" has no mathematical relationship to \"lung\" or \"kidney.\" However, in medicine and biology, we know these terms are semantically related—they're all organs in the human body.\n",
        "\n",
        "**Embeddings solve this problem** by representing each word as a vector (a list of numbers) in a multi-dimensional space. Words with similar meanings or relationships end up close together in this space, while unrelated words are far apart.\n",
        "\n",
        "### What This Code Does\n",
        "\n",
        "#### Step 1: Define the Medical Vocabulary\n",
        "The code creates a list of 22 common medical terms including organs (heart, lung, kidney, liver, brain), body components (bone, muscle, skin, blood, nerve), pathogens (virus, bacteria), and medical conditions (infection, fever, diabetes, hypertension, stroke, cancer, allergy).\n",
        "\n",
        "#### Step 2: Create the Embedding Layer\n",
        "Using PyTorch, the code creates an **embedding layer** that will learn to represent each of the 22 medical terms as a 6-dimensional vector. Initially, these vectors are random numbers with no meaningful relationships.\n",
        "\n",
        "```python\n",
        "embedding_layer = nn.Embedding(num_embeddings=22, embedding_dim=6)\n"
      ],
      "metadata": {
        "id": "Xan6g8XzGjv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Example - Step 1\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textwrap\n",
        "WIDTH = 80  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "\n",
        "# Create a medical vocabulary (common terms in medicine)\n",
        "medical_terms = [\n",
        "    'heart', 'lung', 'kidney', 'liver', 'brain', 'stomach',\n",
        "    'bone', 'muscle', 'skin', 'blood', 'nerve', 'virus',\n",
        "    'bacteria', 'infection', 'fever', 'pain', 'diabetes',\n",
        "    'hypertension', 'stroke', 'cancer', 'allergy', 'immune'\n",
        "]\n",
        "\n",
        "print(f\"Medical vocabulary: {len(medical_terms)} terms\")\n",
        "wrapped_text = textwrap.fill(f\"Medical terms: {medical_terms}\", width=WIDTH)\n",
        "print(wrapped_text)\n",
        "\n",
        "# Create embedding layer - learning representations of medical terms\n",
        "embedding_layer = nn.Embedding(num_embeddings=len(medical_terms), embedding_dim=6)\n",
        "optimizer = optim.Adam(embedding_layer.parameters(), lr=0.01)\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Define some semantic relationships (in real scenarios, this would come from data)\n",
        "training_pairs = [\n",
        "    # (term1_index, term2_index) - terms that are anatomically related\n",
        "    (0, 6),  # heart vs bone\n",
        "    (1, 7),  # lung vs muscle\n",
        "    (2, 8),  # kidney vs skin\n",
        "    (3, 9),  # liver vs blood\n",
        "    (4, 10), # brain vs nerve\n",
        "    (11, 12), # virus vs bacteria\n",
        "    (13, 14), # infection vs fever\n",
        "    (15, 16), # pain vs diabetes\n",
        "    (17, 18), # hypertension vs stroke\n",
        "    (19, 20), # cancer vs allergy\n",
        "]\n",
        "\n",
        "# Create target embeddings that represent relationships\n",
        "target_embeddings = torch.zeros(len(medical_terms), 6)\n",
        "\n",
        "print(\"\\nTraining Embeddings to Reveal Medical Relationships...\")\n",
        "\n",
        "# Training loop - this simulates the neural network learning medical concepts\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # For each training pair, we want similar embeddings for related terms\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (term1_idx, term2_idx) in enumerate(training_pairs):\n",
        "        # Get embeddings for both terms\n",
        "        emb1 = embedding_layer(torch.tensor([term1_idx]))\n",
        "        emb2 = embedding_layer(torch.tensor([term2_idx]))\n",
        "\n",
        "        # Simple loss: make embeddings of related terms more similar\n",
        "        loss = torch.norm(emb1 - emb2)  # Distance between embeddings\n",
        "        total_loss += loss\n",
        "\n",
        "    # Average loss over all pairs\n",
        "    avg_loss = total_loss / len(training_pairs)\n",
        "\n",
        "    # Backpropagation (this is where the \"learning\" happens!)\n",
        "    avg_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {avg_loss.item():.4f}\")\n",
        "\n",
        "print(\"\\nTraining Complete!\")\n",
        "print(\"The neural network has learned how to represent medical terms in a meaningful way.\")\n",
        "\n",
        "# Get final embeddings\n",
        "final_embeddings = embedding_layer.weight.data.detach().numpy()"
      ],
      "metadata": {
        "id": "R_RH2CgjGk6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image29C.png)"
      ],
      "metadata": {
        "id": "sxNxyOU2JT9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Example - Step 2\n",
        "\n",
        "##### **Overview: Visualizing Medical Term Embeddings with PCA**\n",
        "\n",
        "This code takes the 6-dimensional embeddings learned in Step 1 and creates a 2D visualization that reveals how the neural network has organized medical terms based on their semantic relationships.\n",
        "\n",
        "##### **The Challenge: Visualizing High-Dimensional Data**\n",
        "\n",
        "The embeddings from Step 1 represent each medical term as a 6-dimensional vector. Humans cannot visualize 6 dimensions directly, so we need a technique to reduce the dimensionality while preserving the important relationships between terms.\n",
        "\n",
        "#### **What This Code Does**\n",
        "\n",
        "**Step 1: Install and Import Libraries**\n",
        "The code installs `adjustText`, a library that automatically repositions overlapping text labels in plots to improve readability. It also imports visualization tools including matplotlib for plotting and scikit-learn's PCA for dimensionality reduction.\n",
        "\n",
        "**Step 2: Extract the Learned Embeddings**\n",
        "```python\n",
        "final_embeddings = embedding_layer.weight.data.detach().numpy()\n",
        "```\n",
        "**Step 3: Apply Principal Component Analysis (PCA)**\n",
        "\n",
        "```python\n",
        "pca = PCA(n_components=2)\n",
        "embedded_2d = pca.fit_transform(final_embeddings)\n",
        "```\n",
        "**PCA (Principal Component Analysis)** is a mathematical technique that reduces high-dimensional data to fewer dimensions while preserving as much variance (information) as possible. Here, it compresses 6 dimensions down to 2 dimensions that can be plotted on a standard x-y graph.\n",
        "\n",
        "The code also reports the explained variance ratio, which tells us how much of the original information is retained in the 2D representation. Higher values (closer to 1.0) mean less information is lost during compression.\n",
        "\n",
        "**Step 4: Create the Scatter Plot**\n",
        "The code creates a visualization where:\n",
        "\n",
        "* Each medical term is represented as a blue dot in 2D space\n",
        "* The position of each dot reflects its learned embedding (after PCA transformation)\n",
        "* Terms that the neural network learned are \"similar\" will appear close together\n",
        "* Terms that are \"different\" will appear far apart\n",
        "\n",
        "**Step 5: Add Readable Labels**\n",
        "Each point is labeled with its medical term using white background boxes for readability:\n",
        "```python\n",
        "bbox=dict(boxstyle='round,pad=0.2', facecolor='white', edgecolor='none', alpha=0.8)\n",
        "```\n",
        "**Step 6: Draw Semantic Group Connections**\n",
        "The code defines four semantic groups of related medical terms:\n",
        "\n",
        "Organs: heart, lung, kidney, liver, brain\n",
        "Body structures: bone, muscle, skin\n",
        "Pathology: virus, bacteria, infection, fever\n",
        "Diseases: diabetes, hypertension, stroke, cancer\n",
        "Dashed lines connect terms within each group, making it easy to see whether the neural network successfully learned to cluster related concepts together.\n",
        "\n",
        "**Step 7: Adjust Text Labels**\n",
        "```python\n",
        "adjust_text(texts, ax=ax, arrowprops=dict(...))\n",
        "```\n",
        "\n",
        "The adjust_text function automatically moves labels that would overlap, drawing small gray arrows connecting each label to its corresponding point. This ensures all terms remain readable even in crowded regions of the plot.\n",
        "\n",
        "**Interpreting the Results**\n",
        "When you run this visualization, you should observe:\n",
        "\n",
        "**Clustering:** Related medical terms (like organs) should appear near each other\n",
        "**Separation:** Unrelated terms (like \"heart\" vs \"virus\") should be far apart\n",
        "**Semantic Structure:** The overall layout reveals how the neural network \"understands\" relationships between medical concepts\n",
        "\n",
        "This type of visualization is commonly used to evaluate embedding quality in NLP research, medical informatics, and AI model development.\n",
        "\n"
      ],
      "metadata": {
        "id": "DyJTIT6tSU0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Example - Step 2\n",
        "\n",
        "!pip install adjustText > /dev/null\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from adjustText import adjust_text\n",
        "\n",
        "# Get final embeddings\n",
        "final_embeddings = embedding_layer.weight.data.detach().numpy()\n",
        "\n",
        "# Visualize the embeddings using PCA (reducing 6D to 2D for visualization)\n",
        "pca = PCA(n_components=2)\n",
        "embedded_2d = pca.fit_transform(final_embeddings)\n",
        "\n",
        "print(f\"\\nPCA Analysis:\")\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.3f}\")\n",
        "\n",
        "# Create visualization\n",
        "fig, ax = plt.subplots(figsize=(9, 7.5))\n",
        "\n",
        "# Plot the embeddings in 2D space\n",
        "ax.scatter(embedded_2d[:, 0], embedded_2d[:, 1], s=100, alpha=0.7, c='blue')\n",
        "\n",
        "# Create labels with white background for readability\n",
        "texts = []\n",
        "for i, (term, (x, y)) in enumerate(zip(medical_terms, embedded_2d)):\n",
        "    texts.append(ax.text(x, y, term, fontsize=12, ha='center', va='center',\n",
        "                         bbox=dict(boxstyle='round,pad=0.2', facecolor='white',\n",
        "                                   edgecolor='none', alpha=0.8)))\n",
        "\n",
        "# Draw connections between related terms BEFORE adjusting text\n",
        "anatomical_groups = [\n",
        "    ['heart', 'lung', 'kidney', 'liver', 'brain'],   # Organ system\n",
        "    ['bone', 'muscle', 'skin'],                      # Body structure\n",
        "    ['virus', 'bacteria', 'infection', 'fever'],     # Pathology\n",
        "    ['diabetes', 'hypertension', 'stroke', 'cancer'] # Disease types\n",
        "]\n",
        "\n",
        "for group in anatomical_groups:\n",
        "    group_indices = [medical_terms.index(term) for term in group if term in medical_terms]\n",
        "    if len(group_indices) > 1:\n",
        "        group_positions = embedded_2d[group_indices]\n",
        "        ax.plot(group_positions[:, 0], group_positions[:, 1],\n",
        "                alpha=0.5, linewidth=1, linestyle='--')\n",
        "\n",
        "# Adjust text with constraints to keep labels near points\n",
        "adjust_text(texts,\n",
        "            ax=ax,\n",
        "            arrowprops=dict(arrowstyle='-', color='gray', alpha=0.5, shrinkA=10, shrinkB=5),\n",
        "            expand_points=(2.0, 2.0),\n",
        "            force_points=(1.0, 1.0))\n",
        "\n",
        "\n",
        "ax.set_title('Clustering of Terms in Embedding Space')\n",
        "ax.set_xlabel('First Principal Component')\n",
        "ax.set_ylabel('Second Principal Component')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3a8a7o17OTxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image40F.png)"
      ],
      "metadata": {
        "id": "sUQnbSOJWy0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analysis: Medical Term Embedding Visualization**\n",
        "\n",
        "#### **PCA Statistics**\n",
        "- **First Principal Component**: Explains 43.8% of the variance\n",
        "- **Second Principal Component**: Explains 19.4% of the variance\n",
        "- **Total Explained Variance**: 63.3%\n",
        "\n",
        "This means the 2D visualization captures about two-thirds of the information from the original 6-dimensional embeddings. The remaining 36.7% of the variance exists in dimensions we cannot see in this plot.\n",
        "\n",
        "#### **Observed Clustering Patterns**\n",
        "\n",
        "###### **Successfully Learned Relationships**\n",
        "\n",
        "1. **Brain and Nerve** (bottom right): These terms are tightly clustered together, which makes biological sense—the brain is part of the nervous system, and nerves transmit signals to and from the brain.\n",
        "\n",
        "2. **Virus and Bacteria** (bottom left): These pathogens are positioned very close together, reflecting their shared role as infectious agents that cause disease.\n",
        "\n",
        "3. **Hypertension and Stroke** (center): These cardiovascular-related terms are adjacent, correctly capturing the medical relationship where hypertension is a major risk factor for stroke.\n",
        "\n",
        "4. **Lung and Muscle** (center right): These are positioned near each other, both being essential components of the respiratory/movement systems.\n",
        "\n",
        "5. **Pain, Fever, and Diabetes** (lower center): These symptom/condition terms cluster together in the lower portion of the plot.\n",
        "\n",
        "6. **Cancer and Allergy** (upper left): Both represent immune system dysregulation—cancer involves uncontrolled cell growth that evades immune detection, while allergies are immune overreactions.\n",
        "\n",
        "#### Interesting Observations\n",
        "\n",
        "- **Blood and Liver** appear close together, which reflects their biological relationship (the liver filters and processes blood).\n",
        "\n",
        "- **Heart and Bone** are positioned near each other in the lower left, likely because they were paired during training.\n",
        "\n",
        "- **Immune** stands alone on the far right, separated from other terms. This may indicate the model learned it as a distinct concept, or it could benefit from more training connections.\n",
        "\n",
        "- **Stomach** appears isolated at the top, suggesting the model didn't have strong training pairs connecting it to other digestive or organ terms.\n",
        "\n",
        "#### **Limitations**\n",
        "\n",
        "The dashed lines connecting semantic groups (organs, body structures, pathology, diseases) show that while some related terms cluster well, others are spread across the embedding space. This is expected because:\n",
        "\n",
        "1. The training pairs were limited and somewhat arbitrary\n",
        "2. Only 1,000 training epochs were used\n",
        "3. The 6-dimensional embedding space is relatively small\n",
        "4. Real medical relationships are complex and multidimensional\n",
        "\n",
        "#### **Conclusion**\n",
        "\n",
        "The visualization demonstrates that even with simple training data and a small neural network, **embeddings can capture meaningful semantic relationships** between medical terms. In production systems like clinical NLP tools, embeddings are trained on millions of medical documents, resulting in much richer and more accurate representations of medical knowledge.\n"
      ],
      "metadata": {
        "id": "BgF3LZ3XXY5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Example - Step 3\n",
        "\n",
        "Finally, the code in the cell below performs a similarity analysis on the embedded terms and prints out the various relationships discovered by the analysis."
      ],
      "metadata": {
        "id": "MmSeSduAKUet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Example - Step 3\n",
        "\n",
        "# Demonstrate similarity calculations\n",
        "print(\"\\nSimilarity Analysis:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Calculate cosine similarities between some key terms\n",
        "heart_idx = medical_terms.index('heart')\n",
        "lung_idx = medical_terms.index('lung')\n",
        "brain_idx = medical_terms.index('brain')\n",
        "virus_idx = medical_terms.index('virus')\n",
        "\n",
        "heart_emb = final_embeddings[heart_idx]\n",
        "lung_emb = final_embeddings[lung_idx]\n",
        "brain_emb = final_embeddings[brain_idx]\n",
        "virus_emb = final_embeddings[virus_idx]\n",
        "\n",
        "# Calculate similarities\n",
        "sim_heart_lung = cosine_similarity([heart_emb], [lung_emb])[0][0]\n",
        "sim_heart_brain = cosine_similarity([heart_emb], [brain_emb])[0][0]\n",
        "sim_heart_virus = cosine_similarity([heart_emb], [virus_emb])[0][0]\n",
        "\n",
        "print(f\"Heart ↔ Lung similarity: {sim_heart_lung:.3f}\")\n",
        "print(f\"Heart ↔ Brain similarity: {sim_heart_brain:.3f}\")\n",
        "print(f\"Heart ↔ Virus similarity: {sim_heart_virus:.3f}\")\n",
        "\n",
        "# Show how embeddings can be used for medical applications\n",
        "print(\"Medical Applications of These Embeddings:\")\n",
        "print(\"=\" * 40)\n",
        "print(\"1. Disease Diagnosis: Finding similar symptoms and conditions\")\n",
        "print(\"2. Drug Discovery: Identifying molecular relationships\")\n",
        "print(\"3. Medical Literature Analysis: Understanding concept relationships\")\n",
        "print(\"4. Clinical Decision Support: Recommending treatments based on similarity\")\n",
        "\n",
        "# Demonstrate how to use the learned embeddings\n",
        "print(\"\\nPractical Example:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "def find_similar_terms(target_term, top_n=3):\n",
        "    \"\"\"Find most similar terms to a given medical term\"\"\"\n",
        "    if target_term not in medical_terms:\n",
        "        return f\"Term '{target_term}' not found in vocabulary\"\n",
        "\n",
        "    target_idx = medical_terms.index(target_term)\n",
        "    target_embedding = final_embeddings[target_idx]\n",
        "\n",
        "    similarities = []\n",
        "    for i, term in enumerate(medical_terms):\n",
        "        if i != target_idx:\n",
        "            similarity = cosine_similarity([target_embedding], [final_embeddings[i]])[0][0]\n",
        "            similarities.append((term, similarity))\n",
        "\n",
        "    # Sort by similarity and return top N\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_n]\n",
        "\n",
        "# Test with a few examples\n",
        "print(f\"Most similar terms to 'heart':\")\n",
        "for term, sim in find_similar_terms('heart'):\n",
        "    print(f\"  {term}: {sim:.3f}\")\n",
        "\n",
        "print(f\"\\nMost similar terms to 'cancer':\")\n",
        "for term, sim in find_similar_terms('cancer'):\n",
        "    print(f\"  {term}: {sim:.3f}\")\n",
        "\n",
        "# Show the embedding matrix\n",
        "print(f\"\\nEmbedding Matrix (first 5 terms):\")\n",
        "print(\"Each row represents a medical term's learned embedding vector\")\n",
        "print(final_embeddings[:5])\n",
        "\n",
        "print(\"\\nKey Takeaway:\")\n",
        "print(\"Medical embeddings learn to represent not just words, but their meanings and relationships.\")\n",
        "print(\"This is how AI systems understand medical concepts - by learning patterns from vast amounts of medical data.\")"
      ],
      "metadata": {
        "id": "aesskQi0KVTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image41F.png)"
      ],
      "metadata": {
        "id": "i5Hj_EkpZC8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "These numbers have no inherent meaning individually—their power comes from the **relationships** between vectors.\n",
        "\n",
        "#### **Real-World Medical Applications**\n",
        "\n",
        "This similarity-based approach powers many healthcare AI systems:\n",
        "\n",
        "1. **Disease Diagnosis**: Find conditions with similar symptom profiles\n",
        "2. **Drug Discovery**: Identify molecules with similar properties to known drugs\n",
        "3. **Medical Literature Search**: Retrieve papers about semantically related topics\n",
        "4. **Clinical Decision Support**: Recommend treatments based on similar patient cases\n",
        "5. **Medical Coding**: Match clinical notes to standardized diagnosis codes\n",
        "\n",
        "#### **Key Takeaway**\n",
        "\n",
        "Embeddings transform words from arbitrary symbols into **mathematical objects** that capture meaning. By measuring distances and similarities in embedding space, AI systems can reason about medical concepts in ways that support diagnosis, treatment, and research—all without explicitly programming medical knowledge into the system.\n"
      ],
      "metadata": {
        "id": "rqjsn1z7YZ-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-In**\n",
        "\n",
        "When you have completed and run all of the code cells, use the `File --> Print.. --> Microsoft Print to PDF` to generate your PDF if you are running `MS Windows`. If you have a Mac, use the `File --> Print.. --> Save as PDF`\n",
        "\n",
        "In either case, save your PDF as Copy of Class_04_4.lastname.pdf where lastname is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "r6uhybFv7HuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **UNIVAC**\n",
        "\n",
        "![___](https://upload.wikimedia.org/wikipedia/commons/2/2f/Univac_I_Census_dedication.jpg)\n",
        "\n",
        "**UNIVAC (Universal Automatic Computer)** was a line of electronic digital stored-program computers starting with the products of the Eckert–Mauchly Computer Corporation. Later the name was applied to a division of the Remington Rand company and successor organizations.\n",
        "\n",
        "The BINAC, built by the Eckert–Mauchly Computer Corporation, was the first general-purpose computer for commercial use, but it was not a success. The last UNIVAC-badged computer was produced in 1986.\n",
        "\n",
        "**UNIVAC Sperry Rand label**\n",
        "\n",
        "J. Presper Eckert and John Mauchly built the ENIAC (Electronic Numerical Integrator and Computer) at the University of Pennsylvania's Moore School of Electrical Engineering between 1943 and 1946. A 1946 patent rights dispute with the university led Eckert and Mauchly to depart the Moore School to form the Electronic Control Company, later renamed Eckert–Mauchly Computer Corporation (EMCC), based in Philadelphia, Pennsylvania. That company first built a computer called BINAC (BINary Automatic Computer) for Northrop Aviation (which was little used, or perhaps not at all). Afterwards, the development of UNIVAC began in April 1946.[1] UNIVAC was first intended for the Bureau of the Census, which paid for much of the development, and then was put in production.\n",
        "\n",
        "With the death of EMCC's chairman and chief financial backer Henry L. Straus in a plane crash on October 25, 1949, EMCC was sold to typewriter, office machine, electric razor, and gun maker Remington Rand on February 15, 1950. Eckert and Mauchly now reported to Leslie Groves, the retired army general who had previously managed building The Pentagon and led the Manhattan Project.\n",
        "\n",
        "The most famous UNIVAC product was the UNIVAC I mainframe computer of 1951, which became known for predicting the outcome of the U.S. presidential election the following year: this incident is noteworthy because the computer correctly predicted an Eisenhower landslide over Adlai Stevenson, whereas the final Gallup poll had Eisenhower winning the popular vote 51–49 in a close contest.\n",
        "\n",
        "The prediction led CBS's news boss in New York, Siegfried Mickelson, to believe the computer was in error, and he refused to allow the prediction to be read. Instead, the crew showed some staged theatrics that suggested the computer was not responsive, and announced it was predicting 8–7 odds for an Eisenhower win (the actual prediction was 100–1 in his favour).\n",
        "\n",
        "When the predictions proved true—Eisenhower defeated Stevenson in a landslide, with UNIVAC coming within 3.5% of his popular vote total and four votes of his Electoral College total—Charles Collingwood, the on-air announcer, announced that they had failed to believe the earlier prediction.\n",
        "\n",
        "The United States Army requested a UNIVAC computer from Congress in 1951. Colonel Wade Heavey explained to the Senate subcommittee that the national mobilization planning involved multiple industries and agencies: \"This is a tremendous calculating process...there are equations that can not be solved by hand or by electrically operated computing machines because they involve millions of relationships that would take a lifetime to figure out.\" Heavey told the subcommittee it was needed to help with mobilization and other issues similar to the invasion of Normandy that were based on the relationships of various groups.\n",
        "\n",
        "The UNIVAC was manufactured at Remington Rand's former Eckert-Mauchly Division plant on W Allegheny Avenue in Philadelphia, Pennsylvania. Remington Rand also had an engineering research lab in Norwalk, Connecticut, and later bought Engineering Research Associates (ERA) in St. Paul, Minnesota. In 1953 or 1954 Remington Rand merged their Norwalk tabulating machine division, the ERA \"scientific\" computer division, and the UNIVAC \"business\" computer division into a single division under the UNIVAC name. This severely annoyed those who had been with ERA and with the Norwalk laboratory.\n",
        "\n",
        "In 1955 Remington Rand merged with Sperry Corporation to become Sperry Rand. General Douglas MacArthur, then the chairman of the Board of Directors of Remington Rand, was chosen to continue in that role in the new company. Harry Franklin Vickers, then the President of Sperry Corporation, continued as president and CEO of Sperry Rand. The UNIVAC division of Remington Rand was renamed the Remington Rand Univac division of Sperry Rand. William Norris was put in charge as Vice-President and General Manager reporting to the President of the Remington Rand Division (of Sperry Rand).\n",
        "\n",
        "### **UNIVAC: Historical Development and Significance**\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "UNIVAC (Universal Automatic Computer) was the first commercially available computer in the United States, marking a pivotal moment in the history of computing. Developed in the early 1950s, UNIVAC played a crucial role in transitioning computing from experimental laboratories to practical business and government applications.\n",
        "\n",
        "**Origins and Development**\n",
        "\n",
        "### Eckert and Mauchly\n",
        "\n",
        "UNIVAC was developed by **J. Presper Eckert** and **John Mauchly**, the same engineers who created the **ENIAC** (Electronic Numerical Integrator and Computer), the first general-purpose electronic digital computer. After ENIAC, they founded the **Eckert-Mauchly Computer Corporation** in 1946 with the goal of producing a more advanced and commercially viable computer.\n",
        "\n",
        "### Design Goals\n",
        "\n",
        "UNIVAC was designed to:\n",
        "- Handle both numeric and alphabetic data.\n",
        "- Be suitable for business and administrative use.\n",
        "- Automate data processing tasks traditionally performed by punch card machines.\n",
        "\n",
        "## Key Milestones\n",
        "\n",
        "### UNIVAC I (1951)\n",
        "\n",
        "- **First Delivered**: To the U.S. Census Bureau on **June 14, 1951**.\n",
        "- **Technology**: Used vacuum tubes, mercury delay lines for memory, and magnetic tape for storage.\n",
        "- **Speed**: Could perform approximately 1,000 calculations per second.\n",
        "- **Input/Output**: Featured a typewriter-like console and tape drives.\n",
        "\n",
        "### Commercial Impact\n",
        "- **Remington Rand Acquisition**: In 1950, Eckert-Mauchly was acquired by Remington Rand, which marketed UNIVAC.\n",
        "- **Presidential Election Prediction**: UNIVAC I famously predicted the outcome of the 1952 U.S. presidential election on live television, correctly forecasting Eisenhower's victory—demonstrating the power of computing to the public.\n",
        "\n",
        "## Technical Specifications\n",
        "\n",
        "| Feature              | Specification                          |\n",
        "|----------------------|----------------------------------------|\n",
        "| Memory               | 1,000 words (12 characters each)       |\n",
        "| Word Size            | 72 bits                                |\n",
        "| Clock Speed          | 2.25 MHz                               |\n",
        "| Storage              | Magnetic tape                          |\n",
        "| Programming Language | Machine code                           |\n",
        "\n",
        "## Legacy and Influence\n",
        "\n",
        "UNIVAC's success helped establish the viability of computers in business and government. It influenced the development of subsequent systems and contributed to the growth of the American computer industry.\n",
        "\n",
        "### Successors\n",
        "\n",
        "- **UNIVAC II**: Introduced in 1958 with improved memory and performance.\n",
        "- **UNIVAC 1100 Series**: Became popular in the 1960s and 1970s for scientific and business applications.\n",
        "\n",
        "### Cultural Impact\n",
        "\n",
        "UNIVAC became a symbol of modernity and technological progress in the 1950s. Its televised election prediction helped demystify computers and sparked public interest in computing.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "UNIVAC was more than just a machine—it was a milestone in the evolution of computing. By bridging the gap between theoretical computing and practical application, it laid the foundation for the digital age.\n",
        "\n",
        "## References\n",
        "\n",
        "- Ceruzzi, Paul E. *A History of Modern Computing*. MIT Press.\n",
        "- U.S. Census Bureau Archives\n",
        "- Computer History Museum\n"
      ],
      "metadata": {
        "id": "RZrTH409Ps2P"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}