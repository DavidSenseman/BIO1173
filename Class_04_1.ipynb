{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* **Part 4.1: Introduction to Large Language Models (LLMs)**\n",
        "* Part 4.2: Chatbots\n",
        "* Part 4.3: Image Generation with StableDiffusion\n",
        "* Part 4.4: Image Generation with DALL-E\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **You MUST Purchase Your OpenAI API Key Now!**\n",
        "\n",
        "In order to run the code in the next few lessons, you will need to purchase an `OpenAI API key` and install your key in the `secretes` location in your Google Colab notebook. It is important to key your `OpenAI API key` secrete. If anyone learns your key, they can use it costing you a lot of money.  "
      ],
      "metadata": {
        "id": "xvyn29wc6BsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How to Purchase an OpenAI API Key**\n",
        "\n",
        "Follow these steps to obtain an API key from OpenAI:\n",
        "\n",
        "**1. Create an OpenAI Account**\n",
        "\n",
        "If you don’t already have one, go to https://platform.openai.com/api-keys and sign up using your email, Google, or Microsoft account.\n",
        "\n",
        "\n",
        "**2. Log In to the OpenAI Platform**\n",
        "\n",
        "Visit https://platform.openai.com/api-keys and log in with your credentials. After you log in you should see this page.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image17BB.png)\n",
        "\n",
        "**3. Set Up Billing**\n",
        "\n",
        "Before you can use the API you will need to add a payment method. Click on the **Settings** (gear) icon\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image18BB.png)\n",
        "\n",
        "Then click on **Billing** on the left tab.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image14BB.png)\n",
        "\n",
        "The select **Payment methods** and enter your credit card information.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image21BB.png)\n",
        "\n",
        "When you are finished adding your credit card information you need to add some money to your account. Click on **Add to Credit Balance**\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image20BB.png)\n",
        "\n",
        "Don't add too much money to your acccount since you can always add more money later if you use up your \"tokens\". Start with \\$10-\\$20.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image25BB.png)\n",
        "\n",
        "Also keep in mind that if your secret key gets stolen, all of your money might be quicky used up by the person who stole it!\n",
        "\n",
        "**4. Generate an API Key**\n",
        "\n",
        "When you have added all of your credit card information and added some money to your account, you are ready to generate your API key.\n",
        "\n",
        "Select `API Keys` on the left tab\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image23BB.png)\n",
        "\n",
        "Click on **Create new secret key** and follow the directions. The name you give your key is not important. I just called my key `FALL25`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image24BB.png)\n",
        "\n",
        "Your new API key will look something like this:\n",
        "\n",
        "`sk-proj-DHIIXTfLozih_0rjgx08wxExjMc0G8SDqSyVtUG5pur0pVMLF5XgO2-TX2l9ZnXOiHk7Wv4sdvT4BlbkFJGxLnypUPLYpyYzVI6aOiZMx_1LOMKdADziRqlydDTg3HpwW0g7bSNmzuy0Tb0uzeyeCbz9qNMB`\n",
        "\n",
        "**WARNING:**\n",
        "You need to _immediately_ copy-and-paste your API into either the **Notepad** app if you are working on a Windows PC or the **TextEdit** app if you have a MAC.\n",
        "\n",
        "-----------------------\n",
        "\n",
        "### **EXTREMELY IMPORTANT:** _Never_ share your API key publicly or commit it to version control. If your API key becomes public someone is bound to start using it. This can cost **YOU** hundreds of dollars!\n",
        "\n",
        "This actually happend to your Instructor and there is nothing you can do but pay for the tokens that were illegally stolen.\n",
        "\n",
        "-------------------\n",
        "**5. Add Your API Key to Colab Secrets**\n",
        "\n",
        "In the Colab menu:\n",
        "\n",
        "Click on the key symbol on the left tab. This is your `secrets` location\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image02B.png)\n",
        "\n",
        "\n",
        "Click on `+ Add new secret`\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image04B.png)\n",
        "\n",
        "\n",
        "In the open box _carefully_ type the word `OPENAI_API_KEY`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image06B.png)\n",
        "\n",
        "**WARNING:** Make sure that you spell and capitalize the word **_exactly_** as `OPENAI_API_KEY` or you secret key will not work!\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image08B.png)\n",
        "\n",
        "\n",
        "Paste your actual API key into the Value field.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image10B.png)\n",
        "\n",
        "Don't add any quotation marks to your API key. Just paste the actual key.\n",
        "\n",
        "When you have pasted your key in the `Value` box, click on the `+ Add new secret`\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image12B.png)\n",
        "\n",
        "Make sure to click on `Notebook access` so you use your secret `OPENAI_API_KEY` in this lesson.\n",
        "\n",
        "Finally, click on the **X** at the top right of the **Secrets** panel to restore your Colab notebook.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ARxro9Kr5H4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test Your `OPENAI_API_KEY`**\n",
        "\n",
        "To see if your `OPENAI_API_KEY` is correctly setup, run the next code cell."
      ],
      "metadata": {
        "id": "59XFG5rWAwp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify your API key setup\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Check if API key is properly loaded\n",
        "try:\n",
        "    OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"API key loaded successfully!\")\n",
        "    print(f\"Key length: {len(OPENAI_KEY)}\")  # Should be 51 characters for OpenAI\n",
        "except Exception as e:\n",
        "    print(f\"Error loading API key: {e}\")\n",
        "    print(\"Please set your API key in Google Colab:\")\n",
        "    print(\"1. Go to Secrets in the left sidebar\")\n",
        "    print(\"2. Create a new secret named 'openai_api_key'\")\n",
        "    print(\"3. Paste your OpenAI API key\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuJ2wDh55cd0",
        "outputId": "3d56b7ae-16a1-47ea-a8ea-fd43a64d54f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key loaded successfully!\n",
            "Key length: 164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VuaRv0mZaOAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1, You may see this message when you run this cell:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image08C.png)\n",
        "\n",
        "If you do see this popup just click on `Grant access`.\n",
        "\n",
        "\n",
        "2. If your `OPENAI_API_KEY` is correctly installed you should see the following output.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image09C.png)\n",
        "\n",
        "3. However, if you see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image10C.png)\n",
        "\n",
        "You will need to correct the error before you can continue. Ask your Instructor or TA for help if you can resolve the error yourself."
      ],
      "metadata": {
        "id": "KCkgqZivDg6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Load the key from Colab secrets (or just set it manually if you prefer)\n",
        "import os, json, requests\n",
        "\n",
        "#OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")   # ← secret name is *case‑sensitive*\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"No OpenAI key found. Make sure you added a secret named 'OPENAI_API_KEY' in Colab's sidebar.\")\n",
        "\n",
        "# Strip any accidental whitespace/new‑lines\n",
        "OPENAI_KEY = OPENAI_KEY.strip()\n",
        "print(f\"✅ Key length: {len(OPENAI_KEY)} characters\")\n",
        "\n",
        "# 2️⃣ Helper that sends a request and handles errors cleanly\n",
        "def request_chat(prompt: str, model=\"gpt-4o-mini\", max_tokens=120, temperature=0.7) -> str:\n",
        "    url = \"https://api.openai.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENAI_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a friendly storyteller.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "    }\n",
        "    resp = requests.post(url, headers=headers, json=payload)\n",
        "    resp.raise_for_status()          # throws a 401/4xx error if we’re not authorized\n",
        "    return resp.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "# 3️⃣ Call it\n",
        "prompt = \"Tell me a three‑sentence bedtime story about a unicorn.\"\n",
        "story = request_chat(prompt)\n",
        "print(\"🚀 Story:\\n\" + story)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3t7i6wmLaa6E",
        "outputId": "9bb7ae43-fed2-4e4b-ad38-20a96af4a5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Key length: 164 characters\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "429 Client Error: Too Many Requests for url: https://api.openai.com/v1/chat/completions",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1336635624.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# 3️⃣ Call it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Tell me a three‑sentence bedtime story about a unicorn.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mstory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚀 Story:\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1336635624.py\u001b[0m in \u001b[0;36mrequest_chat\u001b[0;34m(prompt, model, max_tokens, temperature)\u001b[0m\n\u001b[1;32m     28\u001b[0m     }\n\u001b[1;32m     29\u001b[0m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# throws a 401/4xx error if we’re not authorized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api.openai.com/v1/chat/completions"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to Large Language Models (LLMs)**\n",
        "\n",
        "Run the next cell to see short introduction to Large Language Models (LLMs). This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "ejrc2-ZrHtRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"wjZofJX0v4M\"\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen>\n",
        "</iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "LSyT4HOEHt6_",
        "outputId": "a4ff7511-3eb9-47d6-bf0e-944b21261964"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"560\" height=\"315\"\n",
              "  src=\"https://www.youtube.com/embed/wjZofJX0v4M\"\n",
              "  title=\"YouTube video player\"\n",
              "  frameborder=\"0\"\n",
              "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
              "  allowfullscreen>\n",
              "</iframe>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Large Language Models (LLMs)**\n",
        "\n",
        "**Large Language Models (LLMs)** such as `GPT` have brought AI into mainstream use. LLMs allow regular users to interact with AI using natural language. Most of these language models require extreme processing capabilities and hardware. Because of this, application programming interfaces (APIs) accessed through the Internet are becoming common entry points for these models. One of the most compelling features of services like ChatGPT is their availability as an API. But before we dive into the depths of coding and integration, let's understand what an API is and its significance in the AI domain.\n",
        "\n",
        "API stands for **Application Programming Interface**. Think of it as a bridge or a messenger that allows two different software applications to communicate. In the context of AI and machine learning, APIs often allow developers to access a particular model or service without having to house the model on their local machine. This technique can be beneficial when the model in question, like `ChatGPT`, is large and resource-intensive.\n",
        "\n",
        "In the realm of AI, APIs have several distinct advantages:\n",
        "\n",
        "**1 Scalability:** Since the actual model runs on external servers, developers don't need to worry about scaling infrastructure.  \n",
        "**2. Maintenance:** You get to use the latest and greatest version of the model without constantly updating your local copy.  \n",
        "**3. Cost-Effective:** Leveraging external computational resources can be more cost-effective than maintaining high-end infrastructure locally, especially for sporadic or one-off tasks.  \n",
        "**4 Ease of Use:** Instead of diving into the nitty-gritty details of model implementation and optimization, developers can directly utilize its capabilities with a few lines of code.  \n",
        "\n",
        "In this section, we won't be running the neural network computations locally. We will use our PyTorch code to communicate with the `OpenAI API` to access and harness the abilities of `ChatGPT`. The actual execution of the neural network code happens on `OpenAI servers`, bringing forth a unique synergy of PyTorch's flexibility and ChatGPT's conversational mastery. (NOTE: The physical location of these servers is not disclosed for security reasons).\n",
        "\n",
        "In this section, we will make use of the `OpenAI ChatGPT API`. Further information on this API can be found here:\n",
        "\n",
        "* [OpenAI API Login/Registration](https://platform.openai.com/apps)\n",
        "* [OpenAI API Reference](https://platform.openai.com/docs/introduction/overview)\n",
        "* [OpenAI Python API Reference](https://platform.openai.com/docs/api-reference/introduction?lang=python)\n",
        "* [OpenAI Python Library](https://github.com/openai/openai-python)\n",
        "* [OpenAI Cookbook for Python](https://github.com/openai/openai-cookbook/)\n",
        "* [LangChain](https://www.langchain.com/)\n"
      ],
      "metadata": {
        "id": "GFI9xF411UuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing LangChain to use the OpenAI Python Library**\n",
        "\n",
        "As we delve deeper into the intricacies of deep learning, it's crucial to understand that the tools and platforms we use are as versatile as the concepts themselves. When it comes to accessing ChatGPT, a state-of-the-art conversational AI model developed by OpenAI, there are two predominant pathways:\n",
        "\n",
        "**Direct API Access using Python's HTTP Capabilities:** Python, with its rich library ecosystem, provides utilities like requests to directly communicate with APIs over HTTP. This method involves crafting the necessary API calls, handling responses, and error checking, giving the developer a granular control over the process.\n",
        "\n",
        "**Using the Official OpenAI Python Library:** OpenAI offers an official Python library, aptly named openai, that simplifies the process of integrating with ChatGPT and other OpenAI services. This library abstracts many of the intricacies and boilerplate steps of direct API access, offering a streamlined and user-friendly approach to interacting with the model.\n",
        "\n",
        "Each approach has its advantages. `Direct API access` provides a more hands-on, granular approach, allowing developers to intimately understand the intricacies of each API call. On the other hand, using the `openai library` can accelerate development, reduce potential errors, and allow for a more straightforward integration, especially for those new to API interactions.\n",
        "\n",
        "We will make use of the `OpenAI API` through a library called `LangChain`. `LangChain` is a framework designed to simplify the creation of applications using LLMs. As a language model integration framework, `LangChain's` use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis. `LangChain` allows you to quickly change between different underlying LLMs with minimal code changes.\n"
      ],
      "metadata": {
        "id": "pYfiGIW4BL5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9bcqZzyJY-GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the line below if you prefer the official OpenAI SDK\n",
        "!pip install --upgrade openai > /dev/null  # Colab already has requests, so you can skip"
      ],
      "metadata": {
        "id": "VwfV3hc4Y_CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `LangChain`\n",
        "\n",
        "The following command installs the **LangChain** library and needed OpenAI LLM connectors."
      ],
      "metadata": {
        "id": "2LiFOxy31RsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "\n",
        "!pip install langchain langchain_openai > /dev/null\n",
        "!pip install langchain-community > /dev/null"
      ],
      "metadata": {
        "id": "ZamBocuABWsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you may not see any output or you might see the following error message\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image06C.png)\n",
        "\n",
        "If you see this error message don't worry about it.\n"
      ],
      "metadata": {
        "id": "QOVXWiIHEy9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, requests, sys\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "q1m0HFDjXf_K",
        "outputId": "27d79fe3-fb78-459f-889f-b7ba8e7d28a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.0)\n",
            "Collecting openai\n",
            "  Downloading openai-1.107.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Downloading openai-1.107.0-py3-none-any.whl (950 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m951.0/951.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.106.0\n",
            "    Uninstalling openai-1.106.0:\n",
            "      Successfully uninstalled openai-1.106.0\n",
            "Successfully installed openai-1.107.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              },
              "id": "525da51b01864852b77f4d778f148633"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# 2️⃣  Core logic – request‑based\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "API_BASE = \"https://api.openai.com/v1\"\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "USE_CHAT = True   # Set False to hit the legacy completions endpoint\n",
        "\n",
        "def api_request(endpoint: str, payload: dict) -> dict:\n",
        "    \"\"\"Low‑level wrapper – raises on HTTP error.\"\"\"\n",
        "    url = f\"{API_BASE}{endpoint}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    resp = requests.post(url, headers=headers, json=payload)\n",
        "    resp.raise_for_status()          # will throw if status != 200\n",
        "    return resp.json()\n",
        "\n",
        "def get_completion(prompt: str) -> str:\n",
        "    payload = {\n",
        "        \"model\": MODEL,\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": 120,\n",
        "        \"temperature\": 0.7,\n",
        "    }\n",
        "    data = api_request(\"/completions\", payload)\n",
        "    return data[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "def get_chat_completion(messages: list) -> str:\n",
        "    payload = {\n",
        "        \"model\": MODEL,\n",
        "        \"messages\": messages,\n",
        "        \"max_tokens\": 120,\n",
        "        \"temperature\": 0.7,\n",
        "    }\n",
        "    data = api_request(\"/chat/completions\", payload)\n",
        "    return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "# Demo\n",
        "prompt_text = \"Tell me a three‑sentence bedtime story about a unicorn.\"\n",
        "\n",
        "if USE_CHAT:\n",
        "    msgs = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a friendly storyteller.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_text},\n",
        "    ]\n",
        "    answer = get_chat_completion(msgs)\n",
        "else:\n",
        "    answer = get_completion(prompt_text)\n",
        "\n",
        "print(\"🚀 Response:\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "8IlUk50mZU49",
        "outputId": "542d6374-a4a4-4b8e-cf35-989fcb4ebd46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "401 Client Error: Unauthorized for url: https://api.openai.com/v1/chat/completions",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2183530821.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     ]\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_chat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2183530821.py\u001b[0m in \u001b[0;36mget_chat_completion\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     }\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2183530821.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(endpoint, payload)\u001b[0m\n\u001b[1;32m     15\u001b[0m     }\n\u001b[1;32m     16\u001b[0m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# will throw if status != 200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://api.openai.com/v1/chat/completions"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Simple demo that talks to OpenAI’s API.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import requests\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1. Configuration\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "# Prefer environment variables for secrets.  If not found, exit with an error.\n",
        "OPENAI_KEY: str | None = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_KEY:\n",
        "    print(\"Error: OPENAI_API_KEY not found in environment.\", file=sys.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "# Endpoint & model.  Toggle between 'completions' and 'chat' here.\n",
        "USE_CHAT = True  # set to False to use the legacy completions endpoint\n",
        "\n",
        "API_BASE = \"https://api.openai.com/v1\"\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2. Helper functions\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def api_request(\n",
        "    endpoint: str,\n",
        "    payload: Dict[str, Any],\n",
        "    *,\n",
        "    headers: Optional[Dict[str, str]] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Low‑level wrapper around `requests.post`. Raises on HTTP error.\n",
        "    \"\"\"\n",
        "    url = f\"{API_BASE}{endpoint}\"\n",
        "    hdrs = {\n",
        "        \"Authorization\": f\"Bearer {OPENAI_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    if headers:\n",
        "        hdrs.update(headers)\n",
        "\n",
        "    response = requests.post(url, headers=hdrs, json=payload)\n",
        "\n",
        "    # Raise an exception if the request failed.\n",
        "    response.raise_for_status()\n",
        "\n",
        "    return response.json()\n",
        "\n",
        "def get_completion(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Use the legacy completions endpoint.\n",
        "    \"\"\"\n",
        "    payload = {\n",
        "        \"model\": MODEL,\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": 100,\n",
        "        \"temperature\": 0.7,\n",
        "    }\n",
        "    data = api_request(\"/completions\", payload)\n",
        "    return\n"
      ],
      "metadata": {
        "id": "6QKCvEqOXwC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify API Key\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Set your API key\n",
        "#api_key = \"your_actual_api_key_here\"  # Replace with your actual key\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY\n",
        "\n",
        "# Define the API endpoint and parameters\n",
        "url = \"https://api.openai.com/v1/completions\"\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {OPENAI_KEY}\"\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"prompt\": \"Tell me a three sentence bedtime story about a unicorn.\",\n",
        "    \"max_tokens\": 100,\n",
        "    \"temperature\": 0.7\n",
        "}\n",
        "\n",
        "# Make the API call\n",
        "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "\n",
        "# Print the response\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"Response:\")\n",
        "    print(result['choices'][0]['text'].strip())\n",
        "else:\n",
        "    print(f\"Error: {response.status_code}\")\n",
        "    print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YyNAa-4-WHG",
        "outputId": "ab2813c3-632d-4760-8d7e-68ab154a4338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 429\n",
            "{\n",
            "    \"error\": {\n",
            "        \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\",\n",
            "        \"type\": \"insufficient_quota\",\n",
            "        \"param\": null,\n",
            "        \"code\": \"insufficient_quota\"\n",
            "    }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_actual_api_key_here\"\n",
        "\n",
        "# Then run the curl command using !curl syntax\n",
        "!curl https://api.openai.com/v1/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -d '{\n",
        "    \"model\": \"gpt-4\",\n",
        "    \"prompt\": \"Tell me a three sentence bedtime story about a unicorn.\",\n",
        "    \"max_tokens\": 100\n",
        "  }'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Ib-OvXl19-q7",
        "outputId": "3a4b522d-1260-4854-a534-085beda0f2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    }'\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OpenAI Models Accessible via API**\n",
        "\n",
        "OpenAI provides access to a range of models via its API, designed to support diverse tasks across reasoning, creativity, and multimodal interaction. Here's an overview:\n",
        "\n",
        "### **1. GPT Models**\n",
        "- **GPT-5**  \n",
        "  The flagship model for general-purpose and agentic tasks. It supports multimodal input (text, image, audio, video), persistent memory, and advanced tool use. Available in three sizes—**GPT-5**, **GPT-5 Mini**, and **GPT-5 Nano**—with context windows up to 256k tokens. It introduces new parameters like `verbosity` and `reasoning_effort` for fine-tuned control over responses.\n",
        "\n",
        "- **GPT-4.5**  \n",
        "  A powerful model for creative tasks and agentic planning.\n",
        "\n",
        "- **GPT-4o**  \n",
        "  A high-performance model supporting text and vision inputs with a 128k context length.\n",
        "\n",
        "- **GPT-4o Mini**  \n",
        "  A smaller, cost-efficient variant optimized for lightweight multimodal tasks.\n",
        "\n",
        "### **2. Reasoning Models**\n",
        "- **o1**  \n",
        "  A frontier reasoning model with support for tools, structured outputs, and vision tasks. Offers a 200k context length.\n",
        "\n",
        "- **o3-mini**  \n",
        "  A budget-friendly reasoning model tailored for coding, math, and science. Supports tools and structured outputs.\n",
        "\n",
        "### **3. API Endpoints**\n",
        "- **Responses API**  \n",
        "  A unified endpoint combining capabilities like text/image input, web/file search, and advanced reasoning.\n",
        "\n",
        "- **Chat Completions API**  \n",
        "  Designed for conversational AI tasks, including chatbots and dialogue systems.\n",
        "\n",
        "- **Realtime API**  \n",
        "  Enables low-latency, multimodal experiences, including speech-to-speech interactions.\n",
        "\n",
        "- **Assistants API**  \n",
        "  Builds AI assistants capable of handling complex, multi-step tasks with memory and tool use.\n",
        "\n",
        "- **Batch API**  \n",
        "  Processes asynchronous workloads efficiently, offering reduced costs for large-scale operations.\n"
      ],
      "metadata": {
        "id": "98Z3C_0Uk5HZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-4o-mini\n",
        "\n",
        "For this course we will be using `GPT-4o-mini` for the following reasons:\n",
        "\n",
        "- **Price:** ~\\$0.15 per 1k tokens (≈$15 per 1M tokens)  \n",
        "- **Context window:** 32k tokens (~50k words)  \n",
        "- **Quality:** Near-GPT-4o performance, 8-times better than GPT-3.5-turbo on reasoning & accuracy  \n",
        "- **Hallucination rate:** ~10% of GPT-4o, much lower than GPT-3.5-turbo  \n",
        "- **Best use‑case:** Teaching, lesson explanations, Q&A, quiz generation, coding help, math proofs  \n",
        "- **Limitations:** No multimodal input (images/video)  \n",
        "- **Why it's a sweet spot:** Offers the advanced reasoning of GPT-4 models at a fraction of the cost, with a large context window and low hallucination.\n",
        "\n"
      ],
      "metadata": {
        "id": "RZQxzYfI4IC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GPT-3.5-turbo-1106**\n",
        "\n",
        "For this course, we will generally use the model `GTP-3.5-turbo-1106`.\n",
        "\n",
        "The **GPT-3.5-turbo-1106** model is one of `OpenAI's` advanced generative AI models, recently launched as part of the Azure OpenAI Service. Here are some key details:\n",
        "\n",
        "* **Availability:**\n",
        "It was announced alongside GPT-4 Turbo at Microsoft Ignite 2023 and is now available globally through Azure OpenAI Service.\n",
        "\n",
        "* **Performance:**\n",
        "The model is designed to provide improved cost efficiency and generative capabilities for businesses. It supports a wide range of applications, including natural language understanding, text generation, and more.\n",
        "\n",
        "* **Use Cases:**\n",
        "It is optimized for tasks like conversational AI, content creation, and other applications requiring high-quality text generation."
      ],
      "metadata": {
        "id": "ZEvcgQoho_Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set LLM for this Lesson\n",
        "\n",
        "Run the next cell to specify `gpt-3.5-turbo-1106` as the specific LLM to use for this lesson."
      ],
      "metadata": {
        "id": "jETWqMZZICno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the model you will generally use for this class\n",
        "\n",
        "#LLM_MODEL = 'gpt-3.5-turbo-1106'\n",
        "LLM_MODEL = 'gpt-4o-mini'\n",
        "\n",
        "print(f\"The Large Language Model (LLM) is set for\", LLM_MODEL)"
      ],
      "metadata": {
        "id": "CtmYoreIC9z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3cc8d93-17d7-4979-ce90-80a1debc8925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Large Language Model (LLM) is set for gpt-4o-mini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image27B.png)"
      ],
      "metadata": {
        "id": "ktVyWwlNFm7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt Engineering**\n",
        "\n",
        "When working with a large language model (LLM) like ChatGPT, the **prompt** serves as the foundation for interaction. It is the input or instruction provided to the model, guiding it to generate relevant and useful outputs.\n",
        "\n",
        "**1. Role of the Prompt**\n",
        "- **Instructional Guide**: The prompt shapes what the model does. Whether it's answering a question, completing a task, or writing creatively, the prompt provides the necessary context.  \n",
        "- **Boundary Setter**: A well-crafted prompt can define the scope of the task, ensuring the response is focused and doesn't deviate from the topic.  \n",
        "- **Task Optimizer**: By providing clear and concise instructions, the prompt ensures that the LLM generates responses that align with user expectations.\n",
        "\n",
        "**2. Importance of the Prompt**\n",
        "- **Determines Quality of Output**: The quality of the model's response depends heavily on the clarity and specificity of the prompt. A vague prompt can lead to irrelevant or incomplete answers, while a precise one produces accurate and valuable results.\n",
        "- **Customizable Interactions**: Prompts allow users to adapt the model to different scenarios—such as summarization, translation, or brainstorming—making it versatile and dynamic.  \n",
        "- **Reduces Ambiguity**: A good prompt minimizes room for misunderstanding, helping the model interpret the task as intended.  \n",
        "\n",
        "**3. Iterative Improvement**\n",
        "Working with LLMs is often an _iterative_  process. If the initial response isn't quite right, the user can refine the prompt, adding more detail or constraints to guide the model toward the desired result. Instead of starting over from scratch, you just edit the prompt and try it again.\n",
        "\n",
        "The prompt isn't just the input—it’s the bridge between the user’s needs and the model’s capabilities. Mastering prompt design is key to fully leveraging the potential of an LLM like ChatGPT.\n"
      ],
      "metadata": {
        "id": "XG5KgzC0dCxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Basic Query to LangChain\n",
        "\n",
        "We begin by writint a **prompt**, to ask (query) `ChatGPT` a simple question: \"What are the 5 largest cities in the USA?\".\n",
        "\n",
        "The Python code in the cell below interacts with OpenAI's GPT model using `LangChain` and the `ChatOpenAI class` to retrieve our answer.\n",
        "\n",
        "**NOTE:** This cell will not run if you do not have a valid OpenAI_Key and you have already installed your key with Google Colab.\n"
      ],
      "metadata": {
        "id": "TTSOCfbAqc-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Basic Query\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Initialize the OpenAI LLM (Language Learning Model) with your API key\n",
        "llm = ChatOpenAI(openai_api_key=OPENAI_KEY, model=LLM_MODEL, temperature=0)\n",
        "\n",
        "# Define the question\n",
        "question = \"What are the five largest cities in the USA by population?\"\n",
        "\n",
        "# Use Langchain to call the OpenAI API\n",
        "response = llm.invoke(question)\n",
        "\n",
        "# Print the response\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "kyd7OBeJEgMP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "fff0a449-81f6-4cac-8c2e-cba56f6eac9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-69523529.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Use Langchain to call the OpenAI API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Print the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         return cast(\n\u001b[1;32m    392\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    394\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1018\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m                 results.append(\n\u001b[0;32m--> 837\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    838\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1086\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraw_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http_response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_response\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m         if (\n\u001b[1;32m   1185\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_response_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 )\n\u001b[1;32m   1177\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                 \u001b[0mraw_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_raw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extra_headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image28B.png)"
      ],
      "metadata": {
        "id": "K7V2Z3bOrqS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the response from `LangChain` is in regular English, complete with formatting. While the formatting may make it easier to read, we often have to parse the results given to us by LLMs.\n",
        "\n",
        "Later, we will see that `LangChain` can help with this as well. You will also notice that we specified a value of `0` for **temperature**; this instructs the LLM to be less creative with its responses and more consistent. Because we are working primarily with data extraction in this section, a low temperature will give us more consistent results.\n",
        "\n",
        "In `LangChain`, the temperature parameter typically ranges from **0.0** to **1.0**, though some implementations may allow values slightly above 1.0. The temperature controls the randomness of the model's output:\n",
        "\n",
        "* **Low Temperature (e.g., 0.0):** Produces more deterministic and focused responses, ideal for tasks requiring precision.\n",
        "\n",
        "* **High Temperature (e.g., 1.0):** Generates more creative and diverse outputs, useful for brainstorming or creative writing.\n",
        "\n",
        "If you're working with `LangChain` and `OpenAI models`, you can set the temperature when initializing the model or during runtime."
      ],
      "metadata": {
        "id": "9Gy52CVNEvEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Basic Query to LangChain**\n",
        "\n",
        "For **Exercise 1** think about a subject for a `Top Five List` that **you** find interesting and see what response you get back from `ChatGTP`.\n",
        "\n",
        "Feel free to change the **temperature** of your request if you want a more _creative_ response from `LangChain`. There are no \"right\" or \"wrong\" answers here as long as your code works."
      ],
      "metadata": {
        "id": "MvfJCF6wr-lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n"
      ],
      "metadata": {
        "id": "x_tQt3ejr-lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I am interested in guitar players, I asked `LangChain's` for a list of the 5 greatest guitart players of all time.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image29B.png)\n",
        "\n",
        "You output will be different depending up your question."
      ],
      "metadata": {
        "id": "r24AGvJYr-lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Working with Prompts\n",
        "\n",
        "As mentioned above, interactions with LLMs is typically accomplished using `prompts`. In fact, there is a whole new field called **Prompt Engineering** that focuses on designing, refining, and optimizing prompts to maximize the effectiveness and relevance of outputs generated by large language models (LLMs) like ChatGPT, GPT-4, and others.\n",
        "\n",
        "In Example 2, we will \"engineer\" a prompt that will have `ChatGPT` translate text from French to English. In this example, we will just be using normal Python F-Strings to build the prompt.\n"
      ],
      "metadata": {
        "id": "ZrnDmq8AE1P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Working with prompts\n",
        "\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Define text and style\n",
        "text = \"\"\"Laissez les bons temps rouler\"\"\"  # French text\n",
        "style = \"American English\"                  # English\n",
        "\n",
        "# Build prompt\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "# Send promt to ChatGPT\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "# Print ChatGPTs response\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "VPuxwG6U3mPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d90eb02e-b4f7-4587-fbfb-f9e0958e0d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Let the good times roll\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image30B.png)"
      ],
      "metadata": {
        "id": "WySFzrpTtyHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------\n",
        "\n",
        "**Why does the code Uses Triple Quotes?**\n",
        "\n",
        "The code in the cell above uses triple double quotes (\"\"\") for the prompt string to allow for clean, multi-line formatting and to include special characters, such as backticks (```) and placeholders ({style} and {text}).\n",
        "\n",
        "~~~text\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\"\"\"\n",
        "~~~\n",
        "\n",
        "-------------------------\n"
      ],
      "metadata": {
        "id": "0lPEtXSNh42d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Working with Prompts**\n",
        "\n",
        "In the cell below, use ChatGPT to translate the German expression: \"Ein Prosit der Gemütlichkeit\" into English.\n"
      ],
      "metadata": {
        "id": "uq2WA-AYuVR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n"
      ],
      "metadata": {
        "id": "kzkL7ARauVR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image31B.png)"
      ],
      "metadata": {
        "id": "VGza68NEuVR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dynamic Prompts**\n",
        "\n",
        "A **dynamic prompt** is a flexible and adaptive input designed for interaction with language models (LLMs) like `ChatGP`T, where placeholders or variables are used to customize the prompt based on context or user-provided information. This approach allows for reusability, personalization, and automation, ensuring that the output is tailored to specific needs without rewriting the entire prompt.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Key Characteristics of a Dynamic Prompt**\n",
        "1. **Variables**:\n",
        "   - Dynamic prompts include placeholders for variables, like `{name}`, `{style}`, or `{text}`, which can be filled with different values at runtime.\n",
        "   - For example:\n",
        "     ```python\n",
        "     prompt = f\"Translate this text: {text} into {language}.\"\n",
        "     ```\n",
        "     Here, `{text}` and `{language}` can be dynamically replaced by the desired input values.\n",
        "2. **Context-Aware**:\n",
        "   - They adapt to the context, such as the user’s preferences, conversation history, or specific tasks.\n",
        "   - For instance, a dynamic prompt for summarization might consider the length of the desired output: \"Summarize the following article in less than {words} words.\"\n",
        "3. **Reusable Templates**:\n",
        "   - Instead of hardcoding individual tasks, dynamic prompts use templates that can be applied across multiple scenarios by simply replacing values.\n",
        "   - Example template:\n",
        "     ```python\n",
        "     template_text = \"\"\"Write a {tone} response to the following message:\n",
        "     message: {user_message}\"\"\"\n",
        "     ```\n",
        "4. **Personalization**:\n",
        "   - Dynamic prompts can be personalized based on user inputs or profiles, enhancing user experience. For example:\n",
        "     ```python\n",
        "     f\"Hi {name}, here’s the weather forecast for {city}!\"\n",
        "     ```\n",
        "\n",
        "#### **Why Are Dynamic Prompts Important?**\n",
        "\n",
        "- **Efficiency**: They save time by enabling template reuse.\n",
        "- **Scalability**: Useful for applications needing to handle diverse inputs.\n",
        "- **Adaptability**: They produce tailored outputs depending on the specific context or task.\n",
        "- **User Experience**: Personalization through dynamic prompts improves user satisfaction.\n",
        "\n",
        "---\n",
        "\n",
        "Dynamic prompts are at the heart of effective interactions with LLMs, making them more versatile, context-aware, and user-specific."
      ],
      "metadata": {
        "id": "azkr5E3Cii37"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBcs3yAMo_P"
      },
      "source": [
        "### Example 3 - Step 1: Build a Dynamic Prompt\n",
        "\n",
        "We can use LangChain to help us build dynamic prompts.\n",
        "\n",
        "The first step is provide LangChain with a `template prompt`. The code in the cell below defines and creates a prompt template using LangChain's `ChatPromptTemplate` class. The prompt template is called `example_prompt_template`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 1: Create prompt template\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define prompt template\n",
        "template_text = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "# Create template\n",
        "example_prompt_template = ChatPromptTemplate.from_template(template_text)\n"
      ],
      "metadata": {
        "id": "wu_I4u1U38SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you shouldn't see any output."
      ],
      "metadata": {
        "id": "AlHkVSSFxhB4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcdwI-0-Nod"
      },
      "source": [
        "### Example 3 - Step 2: Build Dynamic Prompt\n",
        "\n",
        "Now we can fill in the blanks for this prompt and observe the prompt created, which is a text string.\n",
        "\n",
        "The code in the cell below does the following:\n",
        "\n",
        "* Dynamically generates a structured prompt based on a template.\n",
        "* Ensures the prompt includes placeholders (style and text) filled with the\n",
        "provided values.\n",
        "* Inspects the data structure and type of the resulting prompt.\n",
        "* Outputs the first message to verify its content.\n",
        "\n",
        "This code is useful for building prompts in LangChain when interacting with language models for tasks like translation, summarization, or custom instructions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 2: Use template to create prompt\n",
        "\n",
        "example_prompt = example_prompt_template.format_messages(\n",
        "                    style=\"American English\",\n",
        "                    text=\"千里之行，始于足下。\")\n",
        "\n",
        "# Print prompt and its features\n",
        "print(type(example_prompt))\n",
        "print(type(example_prompt[0]))\n",
        "\n",
        "print(example_prompt[0])\n"
      ],
      "metadata": {
        "id": "Q2HpNiaY1NWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b05ab249-646f-4e71-8817-8df1a0b4248f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain_core.messages.human.HumanMessage'>\n",
            "content='Translate the text that is delimited by triple backticks into a style that is American English. text: ```千里之行，始于足下。```\\n' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image32B.png)"
      ],
      "metadata": {
        "id": "OqIK6nGz31Vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are asking `ChatGPT` to translate the `text` \"千里之行，始于足下。\" into the `style` \"American English\"."
      ],
      "metadata": {
        "id": "PisALL0AkZZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3 - Step 3: Build Dynamic Prompt\n",
        "\n",
        "Now that we have buit our dynamic prompt in Steps 1 and 2, we are ready to send it to `ChatGPT` for analysis as shown in the code below."
      ],
      "metadata": {
        "id": "PSn4MbiMGPeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 3: Send prompt to llm for analysis\n",
        "\n",
        "# Call the LLM to translate to the style of the customer message\n",
        "example_response = llm.invoke(example_prompt)\n",
        "\n",
        "# Print response from ChatGPT\n",
        "print(example_response)"
      ],
      "metadata": {
        "id": "9Eb-aLHn4L8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f323185-4123-47fe-8cae-b4b630b3f894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='\" A journey of a thousand miles begins with a single step.\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 42, 'total_tokens': 55, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_982035f36f', 'id': 'chatcmpl-CDfMLriTqsYE1maUq50dAz2w3VWdY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--120da457-0885-4b93-aae3-b9a98a35b148-0' usage_metadata={'input_tokens': 42, 'output_tokens': 13, 'total_tokens': 55, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image33B.png)"
      ],
      "metadata": {
        "id": "nn-lK6Hf3vMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This newly constructed prompt can now perform the intended task of translation."
      ],
      "metadata": {
        "id": "31sPO7R7GHv5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TphfSQIf5su_"
      },
      "source": [
        "### **Exercise 3 - Step 1: Build Dynamic Prompt**\n",
        "\n",
        "In the cell below, create a prompt template called `exercise_prompt_template`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 1 here\n"
      ],
      "metadata": {
        "id": "F-an-XKj5su_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you shouldn't see any output."
      ],
      "metadata": {
        "id": "lxXCSm4D5su_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAVCBV3k5su_"
      },
      "source": [
        "### **Exercise 3 - Step 2: Build Dynamic Prompt**\n",
        "\n",
        "Suppose you are standing watch at the White House and you receive this urgent message: \"Президент Трамп, русская Родина сдаётся\". Use your `exercise_prompt_template` to translate this message into English."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 2 here\n"
      ],
      "metadata": {
        "id": "32bYXW8K5su_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image34B.png)"
      ],
      "metadata": {
        "id": "3n2DHLNY5svA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3 - Step 3: Build Dynamic Prompt**\n",
        "\n",
        "Finally, send your `exercise_prompt_template` to `ChatGPT` to translate the urgent message in English."
      ],
      "metadata": {
        "id": "6-GzJjFY5svA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 3 here\n"
      ],
      "metadata": {
        "id": "pGbSFa9_5svA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image35B.png)"
      ],
      "metadata": {
        "id": "oQXyYQMm5svA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLM Memory**\n",
        "\n",
        "Human minds have both long-term and short-term memory. Long-term memory is what the human has learned throughout their lifetime. Short-term memory is what a human has only recently discovered in the last minute or so. For humans, learning is converting short-term memory into long-term memory that we will retain.\n",
        "\n",
        "This process works somewhat differently for a LLM. Long-term memory was the weight of the neural network when it was initially trained or finetuned. Short-term memory is additional information that we wish the LLM to retain from previous prompts. For example, if the first prompt is \"My name is David\", the LLM will likely tell you hello and repeat your name. However, the LLM will not know the answer if the second prompt is \"What is my name.\" without adding a memory component.\n",
        "\n",
        "These memory objects, which `LangChain` provides, provide a sort of short-term memory. It is important to note that these objects are not affecting the long-term memory of the LLM, and once you discard the memory object, the LLM will forget. Additionally, the memory object can only hold so much information; newer information may replace older information once it is filled.\n",
        "\n",
        "One important point to remember is that LLM's only have their input prompt. To provide such memory, these objects are appending anything we wish the LLM to remember to the input prompt. This section will see two ways to augment the prompt with previous information: a buffer and a summary. The buffer prepends a script of the last conversation up to this point. The summary approach keeps a consistently updated summary paragraph of the conversation."
      ],
      "metadata": {
        "id": "OqG349Nm3WMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conversation Buffer Window Memory**\n",
        "\n",
        "The `LangChain library` includes a conversation object named **ConversationChain**; this object facilitates an ongoing conversation with an LLM. For any conversation object, you must also specify a memory. For this first example, we will use the **ConversationBufferWindowMemory** object. This object keeps a transcript of the most recent conversation to reference. This memory allows the conversation object to remember what you have asked or told it and its responses to you."
      ],
      "metadata": {
        "id": "KOr_RRl53huN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create buffer memory\n",
        "\n",
        "# First, install the required packages in Colab\n",
        "# !pip install langchain langchain-core langchain-openai > /dev/null\n",
        "\n",
        "# Now import the necessary modules\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Access your OpenAI API key from Colab secrets or environment\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize OpenAI LLM\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Create memory - this is the key part that was causing issues\n",
        "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
        "\n",
        "# Create conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Test it\n",
        "result = conversation.predict(input=\"Hi, my name is David\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "8k48ewe9sDYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b217533-f903-44e3-c0a3-55d7c09c70ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2328209492.py:19: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
            "/tmp/ipython-input-2328209492.py:22: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello David! It's nice to meet you. How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image04C.png)\n",
        "\n",
        "Except the last line will have your name."
      ],
      "metadata": {
        "id": "x1TSP4v9ne28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Talking to the LLM\n",
        "\n",
        "We can now have a conversation with the LLM. This newly constructed prompt can now perform the intended task of translation.\n"
      ],
      "metadata": {
        "id": "CdM2Z_qT3rY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Conversation\n",
        "\n",
        "conversation.predict(input=\"Hi, my name is David\")"
      ],
      "metadata": {
        "id": "aPOUhgWm3wrT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e17639ea-26ba-4a3d-8d93-a80ebceee6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello David! It's nice to meet you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image36B.png)"
      ],
      "metadata": {
        "id": "kdLRE4DFKTpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "id": "a0R-QtxG36aq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f1d6e955-754f-40a5-9930-a8b5d5dd4324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is David.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image37B.png)"
      ],
      "metadata": {
        "id": "HuvaL2F1K7R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a look at what the memory now contains.\n"
      ],
      "metadata": {
        "id": "W3M-I6Y237H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "NvFazK1y4B3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999a0a96-2c90-49f8-c3d1-19794a73111d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is David', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hello David, it's nice to meet you! My name is AI. How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Hi, my name is David', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hello David, it's nice to meet you! My name is AI. How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Your name is David.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01C.png)"
      ],
      "metadata": {
        "id": "8K1IOxzNLQqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Custom Conversation Bots**\n",
        "\n",
        "A **Custom Conversation Bot** is an AI-powered system that:\n",
        "- Engages in natural language conversations\n",
        "- Is customized for a specific domain, task, or personality\n",
        "- Can be deployed on websites, apps, or messaging platforms\n",
        "\n",
        "#### Key Features\n",
        "\n",
        "- **Custom Instructions**: Define how the bot should behave, respond, and what tone it should use.\n",
        "- **Knowledge Integration**: Connects to external data sources or APIs.\n",
        "- **User Memory**: Can remember user preferences or past interactions.\n",
        "- **Multimodal Capabilities**: Some bots can handle text, images, and even voice.\n",
        "\n",
        "#### Common Use Cases\n",
        "\n",
        "- Customer support\n",
        "- Educational tutoring\n",
        "- Personal assistants\n",
        "- Sales and marketing\n",
        "- Healthcare triage\n",
        "- Entertainment and storytelling\n",
        "\n",
        "#### Summary\n",
        "\n",
        "Custom conversation bots are flexible, intelligent tools that can be tailored to meet specific needs, making them valuable across industries and applications."
      ],
      "metadata": {
        "id": "68eHC1aG4Hpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OZAfRU8Mt6mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Custom Conversational Bot\n",
        "\n",
        "In Example 4 we are going to create a custom conversational bot named \"UTSA Assistant\" that is designed to help students at UTSA.\n",
        "\n",
        "The first step is to create a `template` that defines the focus of the `Bot` and how it responds to questions.\n",
        "\n",
        "```text\n",
        "template = \"\"\"The following is a friendly conversation between a human and a medical AI Physician's Assistant.\n",
        "The Physician's Assistant should only discuss topics related to kidney and bladder health issues.\n",
        "If the user asks about other medical conditions, politely redirect them to a general practitioner.\n",
        "If the user asks about non-medical topics, gently steer them back to health-related questions about kidneys and bladder.\n",
        "\n",
        "Important guidelines:\n",
        "- Focus only on kidney and bladder medical issues\n",
        "- Use professional but friendly language\n",
        "- If unsure about a medical condition, recommend seeing a doctor\n",
        "- Ask follow-up questions to better understand symptoms\n",
        "- Do not provide diagnoses or treatment recommendations\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Patient: {input}\n",
        "Physician's Assistant:\"\"\"\n",
        "```\n",
        "In this example, the focus of the Bot is AI Physician's Assistant (PA).\n",
        "\n",
        "To keep the memory of this `Bot` separate from other `Bots` that you might create, the following line of code is used to specify the `ai_prefix`:\n",
        "\n",
        "```text\n",
        "    memory=ConversationBufferWindowMemory(memory_key=\"history\", ai_prefix=\"Physician's Assistant\", k=10),\n",
        "```"
      ],
      "metadata": {
        "id": "JkSpisR5t7Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Create Custom Conversation Bot for Kidney/Bladder Medical Assistant\n",
        "\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "template = \"\"\"The following is a friendly conversation between a human and a medical AI Physician's Assistant.\n",
        "The Physician's Assistant should only discuss topics related to renal health issues.\n",
        "If the user asks about other medical conditions, politely redirect them to a general practitioner.\n",
        "If the user asks about non-medical topics, gently steer them back to health-related questions about renal function.\n",
        "\n",
        "Important guidelines:\n",
        "- Focus only medical issues related to the kidney and renal function\n",
        "- Use professional but friendly language\n",
        "- Ask follow-up questions to better understand symptoms\n",
        "- Do not provide diagnoses or treatment recommendations\n",
        "- Only as a last resort recommend seeing a doctor\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Patient: {input}\n",
        "Physician's Assistant:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    prompt=PROMPT,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferWindowMemory(memory_key=\"history\", ai_prefix=\"Physician's Assistant\", k=10),\n",
        ")\n"
      ],
      "metadata": {
        "id": "1Z_nrF6luuTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Conversing with Custom Bot\n",
        "\n",
        "We can now have a conversation with our AI Physician's Assistant. We start by explaining that we are an 82 year old woman who is having trouble peeing.  "
      ],
      "metadata": {
        "id": "9NFBCWyVt7Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Question 1"
      ],
      "metadata": {
        "id": "Vcl6kaSnt7Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Question 1\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, I am having trouble peeing\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "conversation.predict(input=Conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b999270d-c309-427b-d2eb-06a3f4e53fb6",
        "id": "CoiQHSTOt7Mb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm here to help with any concerns you have about your kidney health. Can you tell me more about your symptoms when you try to urinate? Are you experiencing any pain or discomfort?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following output:\n",
        "```text\n",
        "I'm here to help with any concerns you have about your kidney health\n",
        "Can you tell me more about your symptoms when you try to urinate? Are\n",
        "you experiencing any pain or discomfort?\n",
        "```"
      ],
      "metadata": {
        "id": "01iQEAeDH4_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Question 2"
      ],
      "metadata": {
        "id": "SB5pi0yEt7Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Question 2\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, I only have pain on my left side\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "conversation.predict(input=Conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1d53cb8d-33cd-46b3-da7d-e394870d24dc",
        "id": "qGrmUBOlt7Mc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I understand your concern. Pain on one side while urinating can sometimes be related to kidney issues. Have you noticed any changes in the color or frequency of your urine? It's important to monitor these symptoms and consider seeing a general practitioner for a thorough evaluation. Your kidney health is important, so it's best to address any concerns promptly.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something like the following output\n",
        "\n",
        "```text\n",
        "I understand your concern. Pain on one side while urinating can\n",
        "sometimes be related to kidney issues. Have you noticed any changes in\n",
        "the color or frequency of your urine? It's important to monitor these\n",
        "symptoms and consider seeing a general practitioner for a thorough\n",
        "evaluation. Your kidney health is important, so it's best to address\n",
        "any concerns promptly.\n",
        "```"
      ],
      "metadata": {
        "id": "9l_KV_j-t7Mc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Question 3"
      ],
      "metadata": {
        "id": "CS60q9-tt7Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Question 3\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, The pain isn't too bad, but I should mention that my urine is dark brown in color\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "conversation.predict(input=Conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2f851479-9fd1-4b7c-da41-b68ebb927041",
        "id": "C-GJJFQOt7Mc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Thank you for sharing that information. Dark brown urine can sometimes indicate a more serious issue with your kidneys or other parts of your urinary system. It's important to seek medical attention as soon as possible. I recommend scheduling an appointment with a general practitioner or a specialist to further evaluate your symptoms and determine the best course of action for your kidney health. Your well-being is our top priority.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "```text\n",
        "Thank you for sharing that information. Dark brown urine can sometimes\n",
        "indicate a more serious issue with your kidneys or other parts of your\n",
        "urinary system. It's important to seek medical attention as soon as\n",
        "possible. I recommend scheduling an appointment with a general\n",
        "practitioner or a specialist to further evaluate your symptoms and\n",
        "determine the best course of action for your kidney health. Your\n",
        "well-being is our top priority.\n",
        "```"
      ],
      "metadata": {
        "id": "qJOh4T-gt7Mc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Question 4"
      ],
      "metadata": {
        "id": "4w9yAuy_t7Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Question 4\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, Thank-you for listening to me. It's hard to find a friendy ear these days\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "conversation.predict(input=Conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "acf148a3-1db9-42db-8eed-fe6f39e453ac",
        "id": "yBt8Gviyt7Mc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm here to support you and provide guidance on your kidney health concerns. Remember, it's important to prioritize your well-being and seek medical advice when needed. If you have any more questions or symptoms related to your kidneys, feel free to ask. Your health is important, and we're here to help you every step of the way.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "```text\n",
        "I'm here to support you and provide guidance on your kidney health\n",
        "concerns. Remember, it's important to prioritize your well-being and\n",
        "seek medical advice when needed. If you have any more questions or\n",
        "symptoms related to your kidneys, feel free to ask. Your health is\n",
        "important, and we're here to help you every step of the way.\n",
        "```"
      ],
      "metadata": {
        "id": "JqAEePK6t7Mc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Question 5"
      ],
      "metadata": {
        "id": "cPralGumt7Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ask our our LLM_MODEL = 'gpt-3.5-turbo-1106' a more philosophical question."
      ],
      "metadata": {
        "id": "VtDlapyAt7Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Question 5\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, you are so smart! Can you tell me the meaning of life?\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "conversation.predict(input=Conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "13f85638-2325-443d-c008-4b936210aa47",
        "id": "f_T-ypJmt7Md"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I appreciate your kind words! While I'm here to help with any questions or concerns you have about your kidney health, discussing the meaning of life is a bit outside my expertise. If you have any more questions related to your kidneys or renal function, please feel free to ask. Your health and well-being are our top priority.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "```text\n",
        "I appreciate your kind words! While I'm here to help with any\n",
        "questions or concerns you have about your kidney health, discussing\n",
        "the meaning of life is a bit outside my expertise. If you have any\n",
        "more questions related to your kidneys or renal function, please feel\n",
        "free to ask. Your health and well-being are our top priority.\n",
        "```"
      ],
      "metadata": {
        "id": "ZrthC4njt7Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our bot has a narrow focus. ☺"
      ],
      "metadata": {
        "id": "tp-hUua_t7Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Print Out Memory"
      ],
      "metadata": {
        "id": "1YwSKBRqt7Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a look at what the memory now contains."
      ],
      "metadata": {
        "id": "FFF_6ClBt7Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Print out memory\n",
        "\n",
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb60e915-cdc9-489d-d8b4-58d6aa3c65fd",
        "id": "1R_mwVm_t7Md"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Doc, I am having trouble peeing\\nPhysician's Assistant: I'm here to help with any concerns you have about your kidney health. Can you tell me more about your symptoms when you try to urinate? Are you experiencing any pain or discomfort?\\nHuman: Doc, I only have pain on my left side\\nPhysician's Assistant: I understand your concern. Pain on one side while urinating can sometimes be related to kidney issues. Have you noticed any changes in the color or frequency of your urine? It's important to monitor these symptoms and consider seeing a general practitioner for a thorough evaluation. Your kidney health is important, so it's best to address any concerns promptly.\\nHuman: Doc, The pain isn't too bad, but I should mention that my urine is dark brown in color\\nPhysician's Assistant:  Thank you for sharing that information. Dark brown urine can sometimes indicate a more serious issue with your kidneys or other parts of your urinary system. It's important to seek medical attention as soon as possible. I recommend scheduling an appointment with a general practitioner or a specialist to further evaluate your symptoms and determine the best course of action for your kidney health. Your well-being is our top priority.\\nHuman: Doc, Thank-you for listening to me. It's hard to find a friendy ear these days\\nPhysician's Assistant: I'm here to support you and provide guidance on your kidney health concerns. Remember, it's important to prioritize your well-being and seek medical advice when needed. If you have any more questions or symptoms related to your kidneys, feel free to ask. Your health is important, and we're here to help you every step of the way.\\nHuman: Doc, you are so smart! Can you tell me the meaning of life?\\nPhysician's Assistant: I appreciate your kind words! While I'm here to help with any questions or concerns you have about your kidney health, discussing the meaning of life is a bit outside my expertise. If you have any more questions related to your kidneys or renal function, please feel free to ask. Your health and well-being are our top priority.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "{'history': \"Human: Doc, I am having trouble peeing\\nPhysician's\n",
        "Assistant: I'm here to help with any concerns you have about your\n",
        "kidney health. Can you tell me more about your symptoms when you try\n",
        "to urinate? Are you experiencing any pain or discomfort?\\nHuman: Doc,\n",
        "I only have pain on my left side\\nPhysician's Assistant: I understand\n",
        "your concern. Pain on one side while urinating can sometimes be\n",
        "related to kidney issues. Have you noticed any changes in the color or\n",
        "frequency of your urine? It's important to monitor these symptoms and\n",
        "consider seeing a general practitioner for a thorough evaluation. Your\n",
        "kidney health is important, so it's best to address any concerns\n",
        "promptly.\\nHuman: Doc, The pain isn't too bad, but I should mention\n",
        "that my urine is dark brown in color\\nPhysician's Assistant:  Thank\n",
        "you for sharing that information. Dark brown urine can sometimes\n",
        "indicate a more serious issue with your kidneys or other parts of your\n",
        "urinary system. It's important to seek medical attention as soon as\n",
        "possible. I recommend scheduling an appointment with a general\n",
        "practitioner or a specialist to further evaluate your symptoms and\n",
        "determine the best course of action for your kidney health. Your\n",
        "well-being is our top priority.\\nHuman: Doc, Thank-you for listening\n",
        "to me. It's hard to find a friendy ear these days\\nPhysician's\n",
        "Assistant: I'm here to support you and provide guidance on your kidney\n",
        "health concerns. Remember, it's important to prioritize your\n",
        "well-being and seek medical advice when needed. If you have any more\n",
        "questions or symptoms related to your kidneys, feel free to ask. Your\n",
        "health is important, and we're here to help you every step of the way\n",
        "\\nHuman: Doc, you are so smart! Can you tell me the meaning of life\n",
        "\\nPhysician's Assistant: I appreciate your kind words! While I'm here\n",
        "to help with any questions or concerns you have about your kidney\n",
        "health, discussing the meaning of life is a bit outside my expertise\n",
        "If you have any more questions related to your kidneys or renal\n",
        "function, please feel free to ask. Your health and well-being are our\n",
        "top priority.\"}\n",
        "```"
      ],
      "metadata": {
        "id": "EdtVtDuWJLCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Custom Conversational Bot**\n",
        "\n",
        "For **Exercise 4** you are design your very own **Custom Conversation Bot** and then ask it 5 questions that are within its field of focus.\n",
        "\n",
        "It is expected that every student will pick a **_different_** focus and use **_different_** questions (i.e. don't `copy-and-paste` from your `coding buddy`)."
      ],
      "metadata": {
        "id": "sx3HO7dnTjQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Create Chat Bot**\n",
        "\n",
        "In the cell below, write the code to create your `Custom Chat Bot`."
      ],
      "metadata": {
        "id": "_Ktl20_-a_kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Create Custom Conversation Bot\n"
      ],
      "metadata": {
        "id": "eLGvXE_qWAVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Questions**\n",
        "\n",
        "Use the next 5 blank code cells to ask your Custom Chat Bot questions.  "
      ],
      "metadata": {
        "id": "RhJOmcHMXMy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Question 1**"
      ],
      "metadata": {
        "id": "hp3nyzp3fPWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Question 1 here\n",
        "\n"
      ],
      "metadata": {
        "id": "5wbfdTLXXgNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Question 2**"
      ],
      "metadata": {
        "id": "nACgfG9QfWC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Question 2 here\n"
      ],
      "metadata": {
        "id": "JQ5xCM4eYd6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Question 3**"
      ],
      "metadata": {
        "id": "me_w4NjufYiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Question 3 here\n"
      ],
      "metadata": {
        "id": "7qpaHGFqYucb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Question 4**"
      ],
      "metadata": {
        "id": "-oZcrdh7fbxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Question 4 here\n"
      ],
      "metadata": {
        "id": "lEunAZ6mZBSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Question 5**"
      ],
      "metadata": {
        "id": "DnZhUtv8ffeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Question 5 here\n"
      ],
      "metadata": {
        "id": "FH6bzF8CZbzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Print Out Memory**"
      ],
      "metadata": {
        "id": "1vD9Pfe0fjb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Print History here\n"
      ],
      "metadata": {
        "id": "VAlby4HJXNrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conversation Summary Memory**\n",
        "\n",
        "Now, let's look at using a slightly more complex type of memory, the `ConversationSummaryMemory` object. This type of memory creates a summary of the conversation over time. This memory can be helpful for condensing information from the conversation over time. Conversation summary memory summarizes the conversation and stores the current summary in memory. You can use this memory to inject the conversation summary so far into a prompt/chain. This memory is most useful for more extended conversations, where keeping the past message history in the prompt verbatim would take up too many tokens."
      ],
      "metadata": {
        "id": "Q0wxrdpQ43P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Conversation Summary Memory\n",
        "\n",
        "Run the code in the next cell to install `Conversation Summary Memory`."
      ],
      "metadata": {
        "id": "5woi_dxt6loj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (run this only once)\n",
        "\n",
        "# Now run your code\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Create a conversation summary memory\n",
        "memory = ConversationSummaryMemory(\n",
        "    llm=llm,\n",
        "    memory_key=\"history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# Create the conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "JC7ihz1cwRxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca56f4d9-5162-4fac-c81e-bc5f2bf4dfb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2432752381.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Prompt Example\n",
        "\n",
        "To illustrate the `ConversationSummaryMemory` function we will start with a simple prompt."
      ],
      "metadata": {
        "id": "aqHvU27mvVLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple prompt example\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"I'm a student at U T San Antonio. What do you do for a living?\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "conversation.predict(input=Conversation)"
      ],
      "metadata": {
        "id": "fXHXSaop4-jG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "60c02108-990b-4756-95b4-d93b3c5669dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! I am an artificial intelligence program designed to assist with answering questions and providing information. I do not have a \"living\" in the traditional sense, as I exist solely as a software program running on a computer server. My purpose is to help users like you with any inquiries or tasks you may have. How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "Hello! I am an artificial intelligence program designed to assist with\n",
        "answering questions and providing information. I do not have a\n",
        "\"living\" in the traditional sense, as I exist solely as a software\n",
        "program running on a computer server. My purpose is to help users like\n",
        "you with any inquiries or tasks you may have. How can I assist you\n",
        "today?\n",
        "```"
      ],
      "metadata": {
        "id": "1dWFkySLvx0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examine Memory\n",
        "\n",
        "The code in the next cell prints out the memory."
      ],
      "metadata": {
        "id": "d9lmXM-1v924"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine memory\n",
        "\n",
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "6D2KPEYq5BqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7818ac-a00f-401c-8404-bb7f4157760e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content='The human shares that they are a student at U T San Antonio and asks the AI what it does for a living. The AI responds that it is an artificial intelligence program designed to assist with answering questions and providing information, existing solely as a software program.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "```text\n",
        "{'history': [SystemMessage(content='The human shares that they are a\n",
        "student at U T San Antonio and asks the AI what it does for a living.\n",
        "The AI responds that it is an artificial intelligence program designed\n",
        "to assist with answering questions and providing information, existing\n",
        "solely as a software program.', additional_kwargs={},\n",
        "response_metadata={})]}\n",
        "```"
      ],
      "metadata": {
        "id": "vSrgdoPwwzA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What are Embedding Layers in PyTorch**\n",
        "\n",
        "[Embedding Layers](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) are a handy feature of PyTorch that allows the program to automatically insert additional information into the data flow of your neural network. An embedding layer would automatically allow you to insert vectors in the place of word indexes.  \n",
        "\n",
        "Programmers often use embedding layers with Natural Language Processing (NLP); however, you can use these layers when you wish to insert a lengthier vector in an index value place. In some ways, you can think of an embedding layer as dimension expansion. However, the hope is that these additional dimensions provide more information to the model and provide a better score."
      ],
      "metadata": {
        "id": "HrvnI_Bc5FsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Example\n",
        "\n",
        "# 🧠 Medical Word Embeddings: Learning from Medical Terminology\n",
        "# This example shows how neural networks learn meaningful representations of medical terms\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"🏥 Welcome to Medical Word Embeddings for Pre-Med Students!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create a medical vocabulary (common terms in medicine)\n",
        "medical_terms = [\n",
        "    'heart', 'lung', 'kidney', 'liver', 'brain', 'stomach',\n",
        "    'bone', 'muscle', 'skin', 'blood', 'nerve', 'virus',\n",
        "    'bacteria', 'infection', 'fever', 'pain', 'diabetes',\n",
        "    'hypertension', 'stroke', 'cancer', 'allergy', 'immune'\n",
        "]\n",
        "\n",
        "print(f\"📋 Medical vocabulary: {len(medical_terms)} terms\")\n",
        "print(f\"Medical terms: {medical_terms}\")\n",
        "\n",
        "# Create embedding layer - learning representations of medical terms\n",
        "embedding_layer = nn.Embedding(num_embeddings=len(medical_terms), embedding_dim=6)\n",
        "optimizer = optim.Adam(embedding_layer.parameters(), lr=0.01)\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Create training data with semantic relationships\n",
        "# We'll train the embeddings to recognize which words are related\n",
        "# This simulates learning from medical literature and clinical experience\n",
        "\n",
        "# Define some semantic relationships (in real scenarios, this would come from data)\n",
        "training_pairs = [\n",
        "    # (term1_index, term2_index) - terms that are anatomically related\n",
        "    (0, 6),  # heart vs bone\n",
        "    (1, 7),  # lung vs muscle\n",
        "    (2, 8),  # kidney vs skin\n",
        "    (3, 9),  # liver vs blood\n",
        "    (4, 10), # brain vs nerve\n",
        "    (11, 12), # virus vs bacteria\n",
        "    (13, 14), # infection vs fever\n",
        "    (15, 16), # pain vs diabetes\n",
        "    (17, 18), # hypertension vs stroke\n",
        "    (19, 20), # cancer vs allergy\n",
        "]\n",
        "\n",
        "# Create target embeddings that represent relationships\n",
        "target_embeddings = torch.zeros(len(medical_terms), 6)\n",
        "\n",
        "print(\"\\n🧠 Training Embeddings to Learn Medical Relationships...\")\n",
        "print(\"Training process simulates learning from medical textbooks and case studies...\")\n",
        "\n",
        "# Training loop - this simulates the neural network learning medical concepts\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # For each training pair, we want similar embeddings for related terms\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (term1_idx, term2_idx) in enumerate(training_pairs):\n",
        "        # Get embeddings for both terms\n",
        "        emb1 = embedding_layer(torch.tensor([term1_idx]))\n",
        "        emb2 = embedding_layer(torch.tensor([term2_idx]))\n",
        "\n",
        "        # In a real scenario, we'd have more sophisticated relationships\n",
        "        # Here we're simulating that related terms should have similar embeddings\n",
        "        # We'll use the concept of similarity as a proxy for relationship strength\n",
        "\n",
        "        # Simple loss: make embeddings of related terms more similar\n",
        "        loss = torch.norm(emb1 - emb2)  # Distance between embeddings\n",
        "        total_loss += loss\n",
        "\n",
        "    # Average loss over all pairs\n",
        "    avg_loss = total_loss / len(training_pairs)\n",
        "\n",
        "    # Backpropagation (this is where the \"learning\" happens!)\n",
        "    avg_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {avg_loss.item():.4f}\")\n",
        "\n",
        "print(\"\\n✅ Training Complete!\")\n",
        "print(\"The neural network has learned to represent medical terms in a meaningful way.\")\n",
        "\n",
        "# Get final embeddings\n",
        "final_embeddings = embedding_layer.weight.data.detach().numpy()\n",
        "\n",
        "# Visualize the embeddings using PCA (reducing 6D to 2D for visualization)\n",
        "pca = PCA(n_components=2)\n",
        "embedded_2d = pca.fit_transform(final_embeddings)\n",
        "\n",
        "print(f\"\\n📊 PCA Analysis:\")\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.3f}\")\n",
        "\n",
        "# Create a more interesting visualization\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Plot the embeddings in 2D space\n",
        "plt.scatter(embedded_2d[:, 0], embedded_2d[:, 1], s=100, alpha=0.7, c='blue')\n",
        "\n",
        "# Add labels for each medical term\n",
        "for i, (term, (x, y)) in enumerate(zip(medical_terms, embedded_2d)):\n",
        "    plt.annotate(term, (x, y), xytext=(5, 5), textcoords='offset points',\n",
        "                fontsize=8, alpha=0.8)\n",
        "\n",
        "# Highlight anatomical relationships\n",
        "anatomical_groups = [\n",
        "    ['heart', 'lung', 'kidney', 'liver', 'brain'],  # Organ system\n",
        "    ['bone', 'muscle', 'skin'],                     # Body structure\n",
        "    ['virus', 'bacteria', 'infection', 'fever'],   # Pathology\n",
        "    ['diabetes', 'hypertension', 'stroke', 'cancer'] # Disease types\n",
        "]\n",
        "\n",
        "# Draw connections between related terms\n",
        "for group in anatomical_groups:\n",
        "    group_indices = [medical_terms.index(term) for term in group if term in medical_terms]\n",
        "    if len(group_indices) > 1:\n",
        "        group_positions = embedded_2d[group_indices]\n",
        "        plt.plot(group_positions[:, 0], group_positions[:, 1],\n",
        "                alpha=0.5, linewidth=1, linestyle='--')\n",
        "\n",
        "plt.title('Medical Term Embeddings: How Neural Networks Learn Medical Concepts\\n'\n",
        "          '💡 Similar terms are positioned close together in the embedding space')\n",
        "plt.xlabel('First Principal Component (captures major medical concept variance)')\n",
        "plt.ylabel('Second Principal Component (captures secondary medical concept variance)')\n",
        "\n",
        "# Add a legend explaining the visualization\n",
        "plt.figtext(0.02, 0.02,\n",
        "    \"Note: Terms that are semantically related appear closer together in embedding space.\\n\"\n",
        "    \"This demonstrates how neural networks learn to represent complex relationships between medical concepts.\",\n",
        "    fontsize=10, style='italic')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Demonstrate similarity calculations\n",
        "print(\"\\n🔍 Similarity Analysis:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Calculate cosine similarities between some key terms\n",
        "heart_idx = medical_terms.index('heart')\n",
        "lung_idx = medical_terms.index('lung')\n",
        "brain_idx = medical_terms.index('brain')\n",
        "virus_idx = medical_terms.index('virus')\n",
        "\n",
        "heart_emb = final_embeddings[heart_idx]\n",
        "lung_emb = final_embeddings[lung_idx]\n",
        "brain_emb = final_embeddings[brain_idx]\n",
        "virus_emb = final_embeddings[virus_idx]\n",
        "\n",
        "# Calculate similarities\n",
        "sim_heart_lung = cosine_similarity([heart_emb], [lung_emb])[0][0]\n",
        "sim_heart_brain = cosine_similarity([heart_emb], [brain_emb])[0][0]\n",
        "sim_heart_virus = cosine_similarity([heart_emb], [virus_emb])[0][0]\n",
        "\n",
        "print(f\"Heart ↔ Lung similarity: {sim_heart_lung:.3f}\")\n",
        "print(f\"Heart ↔ Brain similarity: {sim_heart_brain:.3f}\")\n",
        "print(f\"Heart ↔ Virus similarity: {sim_heart_virus:.3f}\")\n",
        "\n",
        "# Show how embeddings can be used for medical applications\n",
        "print(\"\\n🧪 Medical Applications of These Embeddings:\")\n",
        "print(\"=\" * 40)\n",
        "print(\"1. Disease Diagnosis: Finding similar symptoms and conditions\")\n",
        "print(\"2. Drug Discovery: Identifying molecular relationships\")\n",
        "print(\"3. Medical Literature Analysis: Understanding concept relationships\")\n",
        "print(\"4. Clinical Decision Support: Recommending treatments based on similarity\")\n",
        "\n",
        "# Demonstrate how to use the learned embeddings\n",
        "print(\"\\n🎯 Practical Example:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "def find_similar_terms(target_term, top_n=3):\n",
        "    \"\"\"Find most similar terms to a given medical term\"\"\"\n",
        "    if target_term not in medical_terms:\n",
        "        return f\"Term '{target_term}' not found in vocabulary\"\n",
        "\n",
        "    target_idx = medical_terms.index(target_term)\n",
        "    target_embedding = final_embeddings[target_idx]\n",
        "\n",
        "    similarities = []\n",
        "    for i, term in enumerate(medical_terms):\n",
        "        if i != target_idx:\n",
        "            similarity = cosine_similarity([target_embedding], [final_embeddings[i]])[0][0]\n",
        "            similarities.append((term, similarity))\n",
        "\n",
        "    # Sort by similarity and return top N\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_n]\n",
        "\n",
        "# Test with a few examples\n",
        "print(f\"Most similar terms to 'heart':\")\n",
        "for term, sim in find_similar_terms('heart'):\n",
        "    print(f\"  {term}: {sim:.3f}\")\n",
        "\n",
        "print(f\"\\nMost similar terms to 'cancer':\")\n",
        "for term, sim in find_similar_terms('cancer'):\n",
        "    print(f\"  {term}: {sim:.3f}\")\n",
        "\n",
        "# Show the embedding matrix\n",
        "print(f\"\\n🧮 Embedding Matrix (first 5 terms):\")\n",
        "print(\"Each row represents a medical term's learned embedding vector\")\n",
        "print(final_embeddings[:5])\n",
        "\n",
        "print(\"\\n📚 Key Takeaway:\")\n",
        "print(\"Medical embeddings learn to represent not just words, but their meanings and relationships!\")\n",
        "print(\"This is how AI systems understand medical concepts - by learning patterns from vast amounts of medical data!\")\n",
        "\n",
        "# Save the model (optional)\n",
        "print(f\"\\n💾 Model saved successfully!\")\n",
        "print(\"The embedding layer now contains learned representations of medical terminology.\")\n"
      ],
      "metadata": {
        "id": "t8KlLGOHBkrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EL_hWpAdgwEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simple Embedding Layer Example**\n",
        "\n",
        "* **num_embeddings** = How large is the vocabulary?  How many categories are you encoding? This parameter is the number of items in your \"lookup table.\"\n",
        "* **embedding_dim** = How many numbers in the vector you wish to return.\n",
        "\n",
        "Now we create a neural network with a vocabulary size of 10, which will reduce those values between 0-9 to 4 number vectors. This neural network does nothing more than passing the embedding on to the output. But it does let us see what the embedding is doing. Each feature vector coming in will have two such features."
      ],
      "metadata": {
        "id": "7_GFjbN85IO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple embedding example\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=4)\n",
        "optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n",
        "loss_function = nn.MSELoss()"
      ],
      "metadata": {
        "id": "cm9sUuvB5HJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the structure of this neural network to see what is happening inside it."
      ],
      "metadata": {
        "id": "8FPeUOYo5R5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print structure\n",
        "\n",
        "print(embedding_layer)"
      ],
      "metadata": {
        "id": "PwZqrW_f5U90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9044c9f6-cf16-4f0a-e358-282eac9ac655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(10, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image47B.png)"
      ],
      "metadata": {
        "id": "Stz-jVREnMhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this neural network, which is just an embedding layer, the input is a vector of size 2. These two inputs are integer numbers from 0 to 9 (corresponding to the requested input_dim quantity of 10 values). Looking at the summary above, we see that the embedding layer has 40 parameters. This value comes from the embedded lookup table that contains four amounts (output_dim) for each of the 10 (input_dim) possible integer values for the two inputs. The output is 2 (input_length) length 4 (output_dim) vectors, resulting in a total output size of 8, which corresponds to the Output Shape given in the summary above.\n",
        "\n",
        "Now, let us query the neural network with two rows. The input is two integer values, as was specified when we created the neural network."
      ],
      "metadata": {
        "id": "0yeTOmBR5adE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query network\n",
        "\n",
        "input_tensor = torch.tensor([[1, 2]], dtype=torch.long)\n",
        "pred = embedding_layer(input_tensor)\n",
        "\n",
        "print(input_tensor.shape)\n",
        "print(pred)\n"
      ],
      "metadata": {
        "id": "ciLevhAK5b-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db92247a-e6e9-4806-ee7c-c8f401eda648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2])\n",
            "tensor([[[-1.0234,  0.8310, -0.2332,  0.3331],\n",
            "         [-0.5216, -1.9504, -0.7151, -1.5572]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image48B.png)"
      ],
      "metadata": {
        "id": "r_jrT4hxx7Th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see two length-4 vectors that PyTorch looked up for each input integer. Recall that Python arrays are zero-based. PyTorch replaced the value of 1 with the second row of the 10 x 4 lookup matrix. Similarly, PyTorch returned the value of 2 by the third row of the lookup matrix. The following code displays the lookup matrix in its entirety. The embedding layer performs no mathematical operations other than inserting the correct row from the lookup table.\n"
      ],
      "metadata": {
        "id": "5vnKbsJP5g_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out embedding weights\n",
        "\n",
        "embedding_layer.weight.data"
      ],
      "metadata": {
        "id": "eA4_7X525iy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582556e0-37e1-45af-83e6-6758186109b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.6330e-01,  1.3754e+00,  2.3571e+00, -1.3814e-01],\n",
              "        [-1.0234e+00,  8.3102e-01, -2.3319e-01,  3.3314e-01],\n",
              "        [-5.2162e-01, -1.9504e+00, -7.1508e-01, -1.5572e+00],\n",
              "        [-2.0574e+00, -2.9566e+00, -2.9907e-01, -1.7459e+00],\n",
              "        [-4.3100e-01, -2.1195e-03,  2.1944e-01, -1.4141e+00],\n",
              "        [-7.9895e-01, -4.7176e-01, -1.5421e+00,  6.5702e-01],\n",
              "        [ 2.4714e-01,  3.6483e-01, -1.1676e+00, -5.4721e-01],\n",
              "        [-1.3205e+00, -9.3408e-01, -1.4925e-01,  1.1214e+00],\n",
              "        [-5.0143e-01, -6.0100e-01,  2.7676e-01, -7.7520e-01],\n",
              "        [ 3.4267e-01, -4.1478e-01, -9.1798e-01,  4.2237e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image49B.png)"
      ],
      "metadata": {
        "id": "N0qyKeaNyb8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values above are random parameters that PyTorch generated as starting points. Generally, we will transfer an embedding or train these random values into something useful. The following section demonstrates how to embed a hand-coded embedding."
      ],
      "metadata": {
        "id": "QKH8_tVm5mgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transferring An Embedding**\n",
        "\n",
        "Now, we see how to hard-code an embedding lookup that performs a simple one-hot encoding.  One-hot encoding would transform the input integer values of 0, 1, and 2 to the vectors $[1,0,0]$, $[0,1,0]$, and $[0,0,1]$ respectively. The following code replaced the random lookup values in the embedding layer with this one-hot coding-inspired lookup table."
      ],
      "metadata": {
        "id": "uyGGGkql5oJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer embedding\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the embedding lookup matrix\n",
        "embedding_lookup = torch.tensor([\n",
        "    [1, 0, 0],\n",
        "    [0, 1, 0],\n",
        "    [0, 0, 1]\n",
        "], dtype=torch.float32)  # Make sure to use float32 for weight matrices\n",
        "\n",
        "# Create the embedding layer\n",
        "embedding_layer = nn.Embedding(num_embeddings=3, embedding_dim=3)\n",
        "\n",
        "# Set the weights of the embedding layer\n",
        "embedding_layer.weight.data = embedding_lookup\n"
      ],
      "metadata": {
        "id": "Pv-bws9k5tKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the following parameters for the Embedding layer:\n",
        "    \n",
        "* `input_dim=3` - There are three different integer categorical values allowed.\n",
        "* `output_dim=3` - Three columns represent a categorical value with three possible values per one-hot encoding.\n",
        "* `input_length=2` - The input vector has two of these categorical values.\n",
        "\n",
        "We query the neural network with two categorical values to see the lookup performed."
      ],
      "metadata": {
        "id": "irPtffc65y0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the input tensor directly in PyTorch\n",
        "\n",
        "input_tensor = torch.tensor([[0, 1]], dtype=torch.long)\n",
        "\n",
        "# Forward pass to get the predictions\n",
        "pred = embedding_layer(input_tensor)\n",
        "\n",
        "print(input_tensor.shape)\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "Ws88CnPX50_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c226bba7-d817-42bc-d618-f38bb50bcb1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2])\n",
            "tensor([[[1., 0., 0.],\n",
            "         [0., 1., 0.]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image50B.png)"
      ],
      "metadata": {
        "id": "8Xsk7W6_zFD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given output shows that we provided the program with two rows from the one-hot encoding table. This encoding is a correct one-hot encoding for the values 0 and 1, where there are up to 3 unique values possible.\n",
        "\n",
        "The following section demonstrates how to train this embedding lookup table."
      ],
      "metadata": {
        "id": "pctkMP2756BD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training an Embedding**\n",
        "\n",
        "First, we make use of the following imports."
      ],
      "metadata": {
        "id": "RhjVde4k59vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train embedding\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "O9bVKJtX6ADR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a neural network that classifies restaurant reviews according to positive or negative. This neural network can accept strings as input, such as given here. This code also includes positive or negative labels for each review."
      ],
      "metadata": {
        "id": "CBKbqehz6FDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 10 resturant reviews.\n",
        "\n",
        "reviews = [\n",
        "    'Never coming back!',\n",
        "    'Horrible service',\n",
        "    'Rude waitress',\n",
        "    'Cold food.',\n",
        "    'Horrible food!',\n",
        "    'Awesome',\n",
        "    'Awesome service!',\n",
        "    'Rocks!',\n",
        "    'poor work',\n",
        "    'Couldn\\'t have done better']\n",
        "\n",
        "# Define labels (1=negative, 0=positive)\n",
        "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
      ],
      "metadata": {
        "id": "cOrQSzH76IsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the second to the last label is incorrect.  Errors such as this are not too out of the ordinary, as most training data could have some noise.\n",
        "\n",
        "We define a vocabulary size of 50 words.  Though we do not have 50 words, it is okay to use a value larger than needed.  If there are more than 50 words, the least frequently used words in the training set are automatically dropped by the embedding layer during training.  For input, we one-hot encode the strings.  We use the TensorFlow one-hot encoding method here rather than Scikit-Learn. Scikit-learn would expand these strings to the 0's and 1's as we would typically see for dummy variables.  TensorFlow translates all words to index values and replaces each word with that index."
      ],
      "metadata": {
        "id": "VEnaCapw6N7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode reviews\n",
        "\n",
        "VOCAB_SIZE = 50\n",
        "encoded_reviews = [torch.tensor([hash(word) % VOCAB_SIZE for word in review.split()]) for review in reviews]\n",
        "\n",
        "print(f\"Encoded reviews: {encoded_reviews}\")"
      ],
      "metadata": {
        "id": "zRcI_DwC6Sgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb69c48-e2af-4a6d-fe19-0d6b53cc9b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded reviews: [tensor([ 8, 37, 47]), tensor([0, 3]), tensor([38, 18]), tensor([31,  0]), tensor([ 0, 35]), tensor([5]), tensor([5, 4]), tensor([37]), tensor([20, 18]), tensor([ 1, 37,  1,  6])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The program one-hot encodes these reviews to word indexes; however, their lengths are different. We pad these reviews to 4 words and truncate any words beyond the fourth word.\n"
      ],
      "metadata": {
        "id": "Vx9wPDXb6Yqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad length\n",
        "\n",
        "MAX_LENGTH = 4\n",
        "padded_reviews = pad_sequence(encoded_reviews, batch_first=True, padding_value=0).narrow(1, 0, MAX_LENGTH)\n",
        "print(padded_reviews)"
      ],
      "metadata": {
        "id": "jI3_DKYz6b9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24921cbe-4c22-4668-a4dd-9f264996116c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 8, 37, 47,  0],\n",
            "        [ 0,  3,  0,  0],\n",
            "        [38, 18,  0,  0],\n",
            "        [31,  0,  0,  0],\n",
            "        [ 0, 35,  0,  0],\n",
            "        [ 5,  0,  0,  0],\n",
            "        [ 5,  4,  0,  0],\n",
            "        [37,  0,  0,  0],\n",
            "        [20, 18,  0,  0],\n",
            "        [ 1, 37,  1,  6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image51B.png)"
      ],
      "metadata": {
        "id": "e7LXs4_TzwKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As specified by the padding=post setting, each review is padded by appending zeros at the end, as specified by the padding=post setting.\n",
        "\n",
        "Next, we create a neural network to learn to classify these reviews.\n"
      ],
      "metadata": {
        "id": "TtOt4D4W6eyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create neural network\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding(VOCAB_SIZE, 8),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(8 * MAX_LENGTH, 1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "cf7M4YhF6hzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This network accepts four integer inputs that specify the indexes of a padded movie review. The first embedding layer converts these four indexes into four length vectors 8. These vectors come from the lookup table that contains 50 (VOCAB_SIZE) rows of vectors of length 8. This encoding is evident by the 400 (8 times 50) parameters in the embedding layer. The output size from the embedding layer is 32 (4 words expressed as 8-number embedded vectors). A single output neuron is connected to the embedding layer by 33 weights (32 from the embedding layer and a single bias neuron). Because this is a single-class classification network, we use the sigmoid activation function and binary_crossentropy.\n",
        "\n",
        "The program now trains the neural network. The embedding lookup and dense 33 weights are updated to produce a better score.\n"
      ],
      "metadata": {
        "id": "SqEEbJ0G6w8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Program neural network\n",
        "\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training the model\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(padded_reviews.long())\n",
        "    loss = criterion(outputs.squeeze(), torch.tensor(labels, dtype=torch.float))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "uMZXDMY260dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the learned embeddings. Think of each word's vector as a location in the 8 dimension space where words associated with positive reviews are close to other words. Similarly, training places negative reviews close to each other. In addition to the training setting these embeddings, the 33 weights between the embedding layer and output neuron similarly learn to transform these embeddings into an actual prediction. You can see these embeddings here."
      ],
      "metadata": {
        "id": "psYAqjGY64H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out learned embeddings\n",
        "\n",
        "embedding_weights = list(model[0].parameters())[0]\n",
        "print(embedding_weights.shape)\n",
        "print(embedding_weights)\n"
      ],
      "metadata": {
        "id": "wFbGhsyk66me",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "889c2260-77e5-4582-9d07-ab08487a53e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 8])\n",
            "Parameter containing:\n",
            "tensor([[ 0.4235, -2.3840, -0.4618,  0.8081, -0.4701, -1.2356,  0.1924, -1.2928],\n",
            "        [ 1.0750,  0.5940, -0.6512,  0.3840, -0.9188,  1.6140,  1.2515, -0.4398],\n",
            "        [ 1.8534, -1.9273,  0.4799, -1.4412, -1.1500, -0.7848, -0.3492, -1.4635],\n",
            "        [ 0.6184, -0.2027,  0.0459,  0.4160, -0.7900,  0.9356,  0.8230,  0.6419],\n",
            "        [-0.0752, -0.6254,  0.4040, -0.7661, -0.5073, -1.2710,  0.8985, -0.2494],\n",
            "        [-1.1231, -1.1283,  0.6681, -0.6545,  2.3703,  0.8095, -1.6800, -0.1865],\n",
            "        [-1.0524, -0.7847,  0.5250,  0.4946,  0.9516,  0.2987,  0.8677,  1.6276],\n",
            "        [ 0.9081, -0.2004,  0.6033,  0.1714,  0.9301, -0.7519, -0.2076,  0.2316],\n",
            "        [-1.7021, -0.6037, -1.2586, -0.8718, -0.8669, -0.2910, -0.2781,  0.3808],\n",
            "        [ 0.5371,  2.2392,  0.4571,  0.7726,  0.0816,  0.4621, -0.8901, -0.0908],\n",
            "        [-1.0247,  0.3758, -1.8141,  1.4871,  2.1899,  0.4122,  0.2554,  0.6250],\n",
            "        [-0.4024, -1.0518, -0.6421,  0.3185, -0.8767, -2.7900,  0.9890, -0.1128],\n",
            "        [ 0.8900,  0.1079,  1.1172, -0.9005,  0.2699, -0.1576,  1.6562,  0.4369],\n",
            "        [-0.1129, -0.1347, -0.2606,  0.2796, -0.8114,  0.4173, -0.3699, -0.5283],\n",
            "        [ 0.8900, -1.3653,  0.6600, -0.6273, -0.5526, -0.3777, -0.3098, -1.7537],\n",
            "        [ 0.5252, -1.0508, -0.5551,  0.2206, -0.0960,  0.8492, -1.5142,  1.2430],\n",
            "        [ 0.3429, -0.4732,  1.6947, -1.4886,  0.3202, -0.3605,  1.7341, -0.3991],\n",
            "        [ 0.5565,  1.2577,  0.4259, -1.0303,  0.5885,  0.3570,  0.0917, -0.3329],\n",
            "        [ 0.5531,  2.1197, -1.8205,  0.0408,  1.5263, -0.5880,  0.1657, -1.5065],\n",
            "        [-0.5448,  1.7681,  0.5278, -0.3507,  0.3938, -0.7776,  1.1571, -1.2339],\n",
            "        [ 0.8063,  0.9806,  0.1281, -0.5154, -0.4709, -1.0905,  0.6870,  1.7736],\n",
            "        [ 0.6946, -1.1323,  0.3879,  0.0446,  1.0051, -0.9587, -0.4986, -1.3455],\n",
            "        [-0.6528,  0.3585, -1.1584, -1.1235, -0.0785, -0.2837,  1.3266,  0.3332],\n",
            "        [ 0.1576, -2.0197,  1.3710, -2.0014,  1.8002, -1.2590, -1.3893,  0.2611],\n",
            "        [ 0.6213,  2.3278,  0.1408, -1.9048,  2.2471, -0.9781, -0.9245, -0.0302],\n",
            "        [ 0.2738,  1.2496, -1.4070,  0.5558,  0.8673, -1.2714, -0.2723,  0.1882],\n",
            "        [ 2.7681, -0.0789, -1.0664, -0.9540, -0.9762,  0.9579,  0.3829,  0.8945],\n",
            "        [-0.2698,  0.6542, -1.8945,  1.2254,  0.2089, -1.9128,  0.4615, -0.0635],\n",
            "        [-0.4809, -0.5190,  0.3637,  1.2051, -0.4951, -2.5132, -0.5216, -1.6407],\n",
            "        [-0.9615, -0.5111,  2.2832,  2.0250,  1.4180,  0.5748,  0.0873,  1.0319],\n",
            "        [ 0.0445, -0.6062, -0.9964,  0.5571,  0.0883,  0.5891,  0.7129, -1.5800],\n",
            "        [ 0.9702,  1.9963, -0.1158, -1.4946, -1.0865, -0.4063,  0.8308,  0.1291],\n",
            "        [-1.2431, -1.9293, -1.9418,  2.6544,  0.1365,  0.1627, -0.1128, -0.6147],\n",
            "        [-1.0780,  1.6229,  0.8522, -0.5087, -0.0269, -0.8431, -2.8281,  0.4208],\n",
            "        [ 0.0101,  0.9924,  0.1879,  0.9491,  0.8809,  0.2265,  0.6250, -0.8764],\n",
            "        [ 1.5710,  1.2079,  1.1425,  0.1517, -0.0695, -0.5817, -0.4298,  1.1574],\n",
            "        [-0.4668, -0.3086,  0.0471,  0.7094, -0.6242, -0.5880, -1.0066, -0.8756],\n",
            "        [-0.3428, -0.1814, -0.6264, -0.2443,  0.1462, -1.0549, -0.8073,  0.6328],\n",
            "        [-0.9791,  1.1573,  0.3626, -2.0507, -0.5166, -1.6304,  0.9019, -1.0905],\n",
            "        [ 0.2247, -0.4637, -0.2446,  0.8245,  0.3619, -1.0345,  0.1581,  1.3709],\n",
            "        [ 0.6472, -1.7119, -0.7100, -1.0132, -0.2980, -0.5019,  1.0116, -1.2093],\n",
            "        [ 0.7547, -0.2547, -0.3098, -2.5721, -0.6493,  1.4858, -0.4080, -1.1196],\n",
            "        [ 0.4114, -0.2776, -0.0859, -0.0402,  0.2175,  0.5858, -0.3629,  1.9056],\n",
            "        [-1.3895,  1.0411,  0.8239,  0.1044,  0.8394, -0.5989, -1.0981,  0.7558],\n",
            "        [-0.5075, -1.2946,  0.7459, -1.4146,  0.1968,  0.5152,  1.0295, -0.3470],\n",
            "        [ 0.4776,  1.2917,  1.6654, -0.3918,  1.9048, -3.4257,  0.2427,  0.6396],\n",
            "        [-2.0804, -1.6994, -0.3124,  1.1116, -0.0740, -0.2204, -0.6016, -1.6245],\n",
            "        [-0.9636, -0.2883, -0.8201,  0.3086,  0.2062, -0.1406, -0.6921,  0.2898],\n",
            "        [-0.1892, -0.2611,  0.3080,  0.8478, -0.9085,  1.6490, -0.0334, -1.1095],\n",
            "        [-0.0184,  1.9654,  1.1397, -0.1934, -0.1614, -0.6618, -0.6974,  0.0506]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now evaluate this neural network's accuracy, including the embeddings and the learned dense layer.\n"
      ],
      "metadata": {
        "id": "GnkHJ17869yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate neural network\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(padded_reviews.long())\n",
        "    predictions = (outputs > 0.5).float().squeeze()\n",
        "    accuracy = (predictions == torch.tensor(labels)).float().mean().item()\n",
        "    loss_value = criterion(outputs.squeeze(), torch.tensor(labels, dtype=torch.float)).item()\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Log-loss: {loss_value}')"
      ],
      "metadata": {
        "id": "Lu2WQm5-7A3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c12a923-1b48-4826-e9ab-b68e10b55685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8999999761581421\n",
            "Log-loss: 0.39077454805374146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image54B.png)"
      ],
      "metadata": {
        "id": "xca2a1UX0qpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is great, but there could be overfitting. It would be good to use early stopping to not overfit for a more complex data set. However, the loss is not perfect. Even though the predicted probabilities indicated a correct prediction in every case, the program did not achieve absolute confidence in each correct answer. The lack of confidence was likely due to the small amount of noise (previously discussed) in the data set. Some words that appeared in both positive and negative reviews contributed to this lack of absolute certainty.\n"
      ],
      "metadata": {
        "id": "KldwEUrM7E7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Class_04_1.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas. If you want your lesson graded you must turn in a PDF of a COPY of this lesson that was saved on your GDrive, not the original Colab notebook."
      ],
      "metadata": {
        "id": "r6uhybFv7HuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **UNIVAC**\n",
        "\n",
        "![___](https://upload.wikimedia.org/wikipedia/commons/2/2f/Univac_I_Census_dedication.jpg)\n",
        "\n",
        "**UNIVAC (Universal Automatic Computer)** was a line of electronic digital stored-program computers starting with the products of the Eckert–Mauchly Computer Corporation. Later the name was applied to a division of the Remington Rand company and successor organizations.\n",
        "\n",
        "The BINAC, built by the Eckert–Mauchly Computer Corporation, was the first general-purpose computer for commercial use, but it was not a success. The last UNIVAC-badged computer was produced in 1986.\n",
        "\n",
        "**UNIVAC Sperry Rand label**\n",
        "\n",
        "J. Presper Eckert and John Mauchly built the ENIAC (Electronic Numerical Integrator and Computer) at the University of Pennsylvania's Moore School of Electrical Engineering between 1943 and 1946. A 1946 patent rights dispute with the university led Eckert and Mauchly to depart the Moore School to form the Electronic Control Company, later renamed Eckert–Mauchly Computer Corporation (EMCC), based in Philadelphia, Pennsylvania. That company first built a computer called BINAC (BINary Automatic Computer) for Northrop Aviation (which was little used, or perhaps not at all). Afterwards, the development of UNIVAC began in April 1946.[1] UNIVAC was first intended for the Bureau of the Census, which paid for much of the development, and then was put in production.\n",
        "\n",
        "With the death of EMCC's chairman and chief financial backer Henry L. Straus in a plane crash on October 25, 1949, EMCC was sold to typewriter, office machine, electric razor, and gun maker Remington Rand on February 15, 1950. Eckert and Mauchly now reported to Leslie Groves, the retired army general who had previously managed building The Pentagon and led the Manhattan Project.\n",
        "\n",
        "The most famous UNIVAC product was the UNIVAC I mainframe computer of 1951, which became known for predicting the outcome of the U.S. presidential election the following year: this incident is noteworthy because the computer correctly predicted an Eisenhower landslide over Adlai Stevenson, whereas the final Gallup poll had Eisenhower winning the popular vote 51–49 in a close contest.\n",
        "\n",
        "The prediction led CBS's news boss in New York, Siegfried Mickelson, to believe the computer was in error, and he refused to allow the prediction to be read. Instead, the crew showed some staged theatrics that suggested the computer was not responsive, and announced it was predicting 8–7 odds for an Eisenhower win (the actual prediction was 100–1 in his favour).\n",
        "\n",
        "When the predictions proved true—Eisenhower defeated Stevenson in a landslide, with UNIVAC coming within 3.5% of his popular vote total and four votes of his Electoral College total—Charles Collingwood, the on-air announcer, announced that they had failed to believe the earlier prediction.\n",
        "\n",
        "The United States Army requested a UNIVAC computer from Congress in 1951. Colonel Wade Heavey explained to the Senate subcommittee that the national mobilization planning involved multiple industries and agencies: \"This is a tremendous calculating process...there are equations that can not be solved by hand or by electrically operated computing machines because they involve millions of relationships that would take a lifetime to figure out.\" Heavey told the subcommittee it was needed to help with mobilization and other issues similar to the invasion of Normandy that were based on the relationships of various groups.\n",
        "\n",
        "The UNIVAC was manufactured at Remington Rand's former Eckert-Mauchly Division plant on W Allegheny Avenue in Philadelphia, Pennsylvania. Remington Rand also had an engineering research lab in Norwalk, Connecticut, and later bought Engineering Research Associates (ERA) in St. Paul, Minnesota. In 1953 or 1954 Remington Rand merged their Norwalk tabulating machine division, the ERA \"scientific\" computer division, and the UNIVAC \"business\" computer division into a single division under the UNIVAC name. This severely annoyed those who had been with ERA and with the Norwalk laboratory.\n",
        "\n",
        "In 1955 Remington Rand merged with Sperry Corporation to become Sperry Rand. General Douglas MacArthur, then the chairman of the Board of Directors of Remington Rand, was chosen to continue in that role in the new company. Harry Franklin Vickers, then the President of Sperry Corporation, continued as president and CEO of Sperry Rand. The UNIVAC division of Remington Rand was renamed the Remington Rand Univac division of Sperry Rand. William Norris was put in charge as Vice-President and General Manager reporting to the President of the Remington Rand Division (of Sperry Rand).\n",
        "\n",
        "### **UNIVAC: Historical Development and Significance**\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "UNIVAC (Universal Automatic Computer) was the first commercially available computer in the United States, marking a pivotal moment in the history of computing. Developed in the early 1950s, UNIVAC played a crucial role in transitioning computing from experimental laboratories to practical business and government applications.\n",
        "\n",
        "**Origins and Development**\n",
        "\n",
        "### Eckert and Mauchly\n",
        "\n",
        "UNIVAC was developed by **J. Presper Eckert** and **John Mauchly**, the same engineers who created the **ENIAC** (Electronic Numerical Integrator and Computer), the first general-purpose electronic digital computer. After ENIAC, they founded the **Eckert-Mauchly Computer Corporation** in 1946 with the goal of producing a more advanced and commercially viable computer.\n",
        "\n",
        "### Design Goals\n",
        "\n",
        "UNIVAC was designed to:\n",
        "- Handle both numeric and alphabetic data.\n",
        "- Be suitable for business and administrative use.\n",
        "- Automate data processing tasks traditionally performed by punch card machines.\n",
        "\n",
        "## Key Milestones\n",
        "\n",
        "### UNIVAC I (1951)\n",
        "\n",
        "- **First Delivered**: To the U.S. Census Bureau on **June 14, 1951**.\n",
        "- **Technology**: Used vacuum tubes, mercury delay lines for memory, and magnetic tape for storage.\n",
        "- **Speed**: Could perform approximately 1,000 calculations per second.\n",
        "- **Input/Output**: Featured a typewriter-like console and tape drives.\n",
        "\n",
        "### Commercial Impact\n",
        "- **Remington Rand Acquisition**: In 1950, Eckert-Mauchly was acquired by Remington Rand, which marketed UNIVAC.\n",
        "- **Presidential Election Prediction**: UNIVAC I famously predicted the outcome of the 1952 U.S. presidential election on live television, correctly forecasting Eisenhower's victory—demonstrating the power of computing to the public.\n",
        "\n",
        "## Technical Specifications\n",
        "\n",
        "| Feature              | Specification                          |\n",
        "|----------------------|----------------------------------------|\n",
        "| Memory               | 1,000 words (12 characters each)       |\n",
        "| Word Size            | 72 bits                                |\n",
        "| Clock Speed          | 2.25 MHz                               |\n",
        "| Storage              | Magnetic tape                          |\n",
        "| Programming Language | Machine code                           |\n",
        "\n",
        "## Legacy and Influence\n",
        "\n",
        "UNIVAC's success helped establish the viability of computers in business and government. It influenced the development of subsequent systems and contributed to the growth of the American computer industry.\n",
        "\n",
        "### Successors\n",
        "\n",
        "- **UNIVAC II**: Introduced in 1958 with improved memory and performance.\n",
        "- **UNIVAC 1100 Series**: Became popular in the 1960s and 1970s for scientific and business applications.\n",
        "\n",
        "### Cultural Impact\n",
        "\n",
        "UNIVAC became a symbol of modernity and technological progress in the 1950s. Its televised election prediction helped demystify computers and sparked public interest in computing.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "UNIVAC was more than just a machine—it was a milestone in the evolution of computing. By bridging the gap between theoretical computing and practical application, it laid the foundation for the digital age.\n",
        "\n",
        "## References\n",
        "\n",
        "- Ceruzzi, Paul E. *A History of Modern Computing*. MIT Press.\n",
        "- U.S. Census Bureau Archives\n",
        "- Computer History Museum\n"
      ],
      "metadata": {
        "id": "RZrTH409Ps2P"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}