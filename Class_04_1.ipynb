{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* **Part 4.1: Introduction to Large Language Models (LLMs)**\n",
        "* Part 4.2: Chatbots\n",
        "* Part 4.3: Image Generation with StableDiffusion\n",
        "* Part 4.4: Image Generation with DALL-E\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-genai\n",
        "!pip install -q langchain_google_genai\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVuF4cLJC0nV",
        "outputId": "55677041-bda2-4258-e2ce-43a4d437d2ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.1/476.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ab5a5a-668c-4f33-b423-cb7fa7a0506a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: Using Google CoLab\n",
            "david.senseman@gmail.com\n"
          ]
        }
      ],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **You MUST Obtain Your Gemini API Key Now!**\n",
        "\n",
        "In order to run the code in the next few lessons, you will need to obtain a Google `Gemini API key` and install your key in the `secrets` location in your Google Colab notebook. It is important to key your `Gemini API key` secret. If anyone learns your key, they can use it costing you a lot of money.  "
      ],
      "metadata": {
        "id": "xvyn29wc6BsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How to obtain a Gemini API Key**\n",
        "\n",
        "Follow these steps to obtain an API key from OpenAI:\n",
        "\n",
        "**1. Create an OpenAI Account**\n",
        "\n",
        "If you don’t already have one, go to https://platform.openai.com/api-keys and sign up using your email, Google, or Microsoft account.\n",
        "\n",
        "\n",
        "**2. Log In to the OpenAI Platform**\n",
        "\n",
        "Visit https://platform.openai.com/api-keys and log in with your credentials. After you log in you should see this page.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image17BB.png)\n",
        "\n",
        "**3. Set Up Billing**\n",
        "\n",
        "Before you can use the API you will need to add a payment method. Click on the **Settings** (gear) icon\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image18BB.png)\n",
        "\n",
        "Then click on **Billing** on the left tab.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image14BB.png)\n",
        "\n",
        "The select **Payment methods** and enter your credit card information.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image21BB.png)\n",
        "\n",
        "When you are finished adding your credit card information you need to add some money to your account. Click on **Add to Credit Balance**\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image20BB.png)\n",
        "\n",
        "Don't add too much money to your acccount since you can always add more money later if you use up your \"tokens\". Start with \\$10-\\$20.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image25BB.png)\n",
        "\n",
        "Also keep in mind that if your secret key gets stolen, all of your money might be quicky used up by the person who stole it!\n",
        "\n",
        "**4. Generate an API Key**\n",
        "\n",
        "When you have added all of your credit card information and added some money to your account, you are ready to generate your API key.\n",
        "\n",
        "Select `API Keys` on the left tab\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image23BB.png)\n",
        "\n",
        "Click on **Create new secret key** and follow the directions. The name you give your key is not important. I just called my key `FALL_2025`. Make sure your **PERMISSIONS** are set to **All**. Then select **Create secret key**\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image11C.png)\n",
        "\n",
        "Your new API key will look something like this:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image12C.png)\n",
        "\n",
        "`sk-proj-DHIIXTfLozih_0rjgx08wxExjMc0G8SDqSyVtUG5pur0pVMLF5XgO2-TX2l9ZnXOiHk7Wv4sdvT4BlbkFJGxLnypUPLYpyYzVI6aOiZMx_1LOMKdADziRqlydDTg3HpwW0g7bSNmzuy0Tb0uzeyeCbz9qNMB`\n",
        "\n",
        "**WARNING:**\n",
        "You need to  **_immediately_** copy-and-paste your API Key into either the **Notepad** app if you are working on a Windows PC or the **TextEdit** app if you have a MAC.\n",
        "\n",
        "When you have created your API key you should see a window like this:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image14C.png)\n",
        "\n",
        "-----------------------\n",
        "\n",
        "### **EXTREMELY IMPORTANT:** _Never_ share your API key publicly or commit it to version control. If your API key becomes public someone is bound to start using it. This can cost **YOU** hundreds of dollars!\n",
        "\n",
        "This actually happend to your Instructor and there is nothing you can do but pay for the tokens that were illegally stolen.\n",
        "\n",
        "-------------------\n",
        "**5. Add Your API Key to Colab Secrets**\n",
        "\n",
        "In the Colab menu:\n",
        "\n",
        "Click on the key symbol on the left tab. This is your `secrets` location\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image02B.png)\n",
        "\n",
        "\n",
        "Click on `+ Add new secret`\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image04B.png)\n",
        "\n",
        "\n",
        "In the open box _carefully_ type the word `OPENAI_API_KEY`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image06B.png)\n",
        "\n",
        "**WARNING:** Make sure that you spell and capitalize the word **_exactly_** as `OPENAI_API_KEY` or you secret key will not work!\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image08B.png)\n",
        "\n",
        "\n",
        "Paste your actual API key into the Value field.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image10B.png)\n",
        "\n",
        "Don't add any quotation marks to your API key. Just paste the actual key.\n",
        "\n",
        "When you have pasted your key in the `Value` box, click on the `+ Add new secret`\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image12B.png)\n",
        "\n",
        "Make sure to click on `Notebook access` so you use your secret `OPENAI_API_KEY` in this lesson.\n",
        "\n",
        "Finally, click on the **X** at the top right of the **Secrets** panel to restore your Colab notebook.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ARxro9Kr5H4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test Your `GEMINI_API_KEY`**\n",
        "\n",
        "To see if your `GEMINI_API_KEY` is correctly setup, run the next code cell."
      ],
      "metadata": {
        "id": "59XFG5rWAwp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Check if API key is properly loaded\n",
        "try:\n",
        "    # 1. Get the key from Secrets\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "    # 2. Set it as an environment variable\n",
        "    os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "\n",
        "    print(\"API key loaded and environment variable set successfully!\")\n",
        "    print(f\"Key length: {len(GOOGLE_API_KEY)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading API key: {e}\")\n",
        "    print(\"Please set your API key in Google Colab:\")\n",
        "    print(\"1. Go to Secrets in the left sidebar (key icon)\")\n",
        "    print(\"2. Create a new secret named 'GOOGLE_API_KEY'\")\n",
        "    print(\"3. Paste your GOOGLE API key and toggle 'Notebook access' on\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoCuPqqW7Vau",
        "outputId": "ed172fe9-66ad-49e8-c577-3241904ea2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key loaded and environment variable set successfully!\n",
            "Key length: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1, You may see this message when you run this cell:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image08C.png)\n",
        "\n",
        "If you do see this popup just click on `Grant access`.\n",
        "\n",
        "\n",
        "2. If your `GEMINI_API_KEY` is correctly installed you should see something _similar_ to the following output.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image09C.png)\n",
        "\n",
        "3. However, if you see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image10C.png)\n",
        "\n",
        "You will need to correct the error before you can continue. Ask your Instructor or TA for help if you can resolve the error yourself."
      ],
      "metadata": {
        "id": "KCkgqZivDg6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to Large Language Models (LLMs)**\n",
        "\n",
        "Run the next cell to see short introduction to Large Language Models (LLMs). This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "ejrc2-ZrHtRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"wjZofJX0v4M\"\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen>\n",
        "</iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "LSyT4HOEHt6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Large Language Models (LLMs)**\n",
        "\n",
        "**Large Language Models (LLMs)** such as `GPT` have brought AI into mainstream use. LLMs allow regular users to interact with AI using natural language. Most of these language models require extreme processing capabilities and hardware. Because of this, application programming interfaces (APIs) accessed through the Internet are becoming common entry points for these models. One of the most compelling features of services like ChatGPT is their availability as an API. But before we dive into the depths of coding and integration, let's understand what an API is and its significance in the AI domain.\n",
        "\n",
        "API stands for **Application Programming Interface**. Think of it as a bridge or a messenger that allows two different software applications to communicate. In the context of AI and machine learning, APIs often allow developers to access a particular model or service without having to house the model on their local machine. This technique can be beneficial when the model in question, like `ChatGPT`, is large and resource-intensive.\n",
        "\n",
        "In the realm of AI, APIs have several distinct advantages:\n",
        "\n",
        "**1 Scalability:** Since the actual model runs on external servers, developers don't need to worry about scaling infrastructure.  \n",
        "**2. Maintenance:** You get to use the latest and greatest version of the model without constantly updating your local copy.  \n",
        "**3. Cost-Effective:** Leveraging external computational resources can be more cost-effective than maintaining high-end infrastructure locally, especially for sporadic or one-off tasks.  \n",
        "**4 Ease of Use:** Instead of diving into the nitty-gritty details of model implementation and optimization, developers can directly utilize its capabilities with a few lines of code.  \n",
        "\n",
        "In this section, we won't be running the neural network computations locally. We will use our PyTorch code to communicate with the `OpenAI API` to access and harness the abilities of `ChatGPT`. The actual execution of the neural network code happens on `OpenAI servers`, bringing forth a unique synergy of PyTorch's flexibility and ChatGPT's conversational mastery. (NOTE: The physical location of these servers is not disclosed for security reasons).\n",
        "\n",
        "In this section, we will make use of the `OpenAI ChatGPT API`. Further information on this API can be found here:\n",
        "\n",
        "* [OpenAI API Login/Registration](https://platform.openai.com/apps)\n",
        "* [OpenAI API Reference](https://platform.openai.com/docs/introduction/overview)\n",
        "* [OpenAI Python API Reference](https://platform.openai.com/docs/api-reference/introduction?lang=python)\n",
        "* [OpenAI Python Library](https://github.com/openai/openai-python)\n",
        "* [OpenAI Cookbook for Python](https://github.com/openai/openai-cookbook/)\n",
        "* [LangChain](https://www.langchain.com/)\n"
      ],
      "metadata": {
        "id": "GFI9xF411UuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing LangChain to use the OpenAI Python Library**\n",
        "\n",
        "As we delve deeper into the intricacies of deep learning, it's crucial to understand that the tools and platforms we use are as versatile as the concepts themselves. When it comes to accessing ChatGPT, a state-of-the-art conversational AI model developed by OpenAI, there are two predominant pathways:\n",
        "\n",
        "**Direct API Access using Python's HTTP Capabilities:** Python, with its rich library ecosystem, provides utilities like requests to directly communicate with APIs over HTTP. This method involves crafting the necessary API calls, handling responses, and error checking, giving the developer a granular control over the process.\n",
        "\n",
        "**Using the Official OpenAI Python Library:** OpenAI offers an official Python library, aptly named openai, that simplifies the process of integrating with ChatGPT and other OpenAI services. This library abstracts many of the intricacies and boilerplate steps of direct API access, offering a streamlined and user-friendly approach to interacting with the model.\n",
        "\n",
        "Each approach has its advantages. `Direct API access` provides a more hands-on, granular approach, allowing developers to intimately understand the intricacies of each API call. On the other hand, using the `openai library` can accelerate development, reduce potential errors, and allow for a more straightforward integration, especially for those new to API interactions.\n",
        "\n",
        "We will make use of the `OpenAI API` through a library called `LangChain`. `LangChain` is a framework designed to simplify the creation of applications using LLMs. As a language model integration framework, `LangChain's` use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis. `LangChain` allows you to quickly change between different underlying LLMs with minimal code changes.\n"
      ],
      "metadata": {
        "id": "pYfiGIW4BL5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `langchain`\n",
        "\n",
        "Run the code in the next cell to install the `langchain`modules."
      ],
      "metadata": {
        "id": "5Y1nGVRB8lDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# We use '%%capture' to hide the extensive installation logs and\n",
        "# benign dependency warnings common in Google Colab.\n",
        "\n",
        "!pip install -U google-genai\n",
        "!pip install -U langchain_google_genai\n",
        "!pip install -U langchain langchain-community"
      ],
      "metadata": {
        "id": "6prWzCSpLWpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U google-genai\n",
        "!pip install -U langchain_google_genai\n",
        "!pip install -U langchain langchain-community"
      ],
      "metadata": {
        "id": "P9q6exaVCS7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification Cell\n",
        "\n",
        "import sys\n",
        "import langchain\n",
        "import langchain_google_genai\n",
        "\n",
        "print(\"Libraries installed and imported successfully!\")\n",
        "print(f\"LangChain Version: {langchain.__version__}\")\n",
        "# Force output directly to the system console\n",
        "sys.stdout.write(f\"Direct System Check: LangChain {langchain.__version__}\\n\")"
      ],
      "metadata": {
        "id": "R1NlJpCdLfYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d0ebe9-e4a4-486e-9886-b704ff0ae79b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed and imported successfully!\n",
            "LangChain Version: 1.1.3\n",
            "Direct System Check: LangChain 1.1.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List Current Google Gemini Models\n",
        "\n",
        "Run the code in the next cell to generate a table showing the various models that you can access via your Gemini API Key."
      ],
      "metadata": {
        "id": "k1Btp-3gHSZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# --- 1. SETUP & AUTHENTICATION ---\n",
        "try:\n",
        "    # Retrieve the key from Colab Secrets\n",
        "    # Note: Ensure your secret name matches exactly (e.g., 'GOOGLE_API_KEY')\n",
        "    api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "    # Configure the library with the key\n",
        "    genai.configure(api_key=api_key)\n",
        "    print(\"Authentication successful.\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Authentication failed: {e}\")\n",
        "    print(\"Check that your secret name matches 'GOOGLE_API_KEY' in the sidebar.\")\n",
        "\n",
        "# --- 2. PRINT HEADER ---\n",
        "# Using f-strings to create fixed-width columns\n",
        "print(f\"{'Model Name':<60} {'Display Name':<40}\")\n",
        "print(\"-\" * 130)\n",
        "\n",
        "# --- 3. PRINT ROWS ---\n",
        "try:\n",
        "    for m in genai.list_models():\n",
        "        # Convert list of methods to a string for display (optional, if you want to add it back)\n",
        "        methods = \", \".join(m.supported_generation_methods)\n",
        "\n",
        "        # Print using the same width (<60 and <40) as the header\n",
        "        print(f\"{m.name:<60} {m.display_name:<40}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError listing models: {e}\")"
      ],
      "metadata": {
        "id": "K2LDxwmB707a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zjIx0GLw3y66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Current Gemini API Models (December 2025)**\n",
        "\n",
        "As of December 2025, the Gemini API primarily utilizes the **Gemini 2.5** and **Gemini 3** series. All Gemini 1.5 models (Pro, Flash, and Flash-8B) were officially retired on September 29, 2025.\n",
        "\n",
        "#### **1. Gemini 3 Series (Newest Flagship)**\n",
        "*   **`gemini-3-pro-preview`**: The most advanced reasoning model, launched November 18, 2025. Features \"Deep Think\" capabilities for complex coding, math, and agentic tool-calling. Includes a **1M token context window**.\n",
        "*   **`gemini-3-pro-image-preview`**: Optimized for high-fidelity multimodal understanding and reasoning-enhanced image generation and editing.\n",
        "\n",
        "#### **2. Gemini 2.5 Series (Stable Production)**\n",
        "*   **`gemini-2.5-pro`**: The standard high-capability model for complex reasoning and long-context multimodal tasks.\n",
        "*   **`gemini-2.5-flash`**: The recommended model for most production use cases; balances high speed, low latency, and intelligence.\n",
        "*   **`gemini-2.5-flash-lite`**: An ultra-efficient version designed for high-throughput, massive-scale tasks at a significantly lower cost.\n",
        "*   **`gemini-live-2.5-flash-native-audio`**: Released December 12, 2025, specifically for the Live API to support bidirectional, low-latency voice interactions.\n",
        "\n",
        "#### **3. Specialized & Legacy Models**\n",
        "*   **`gemini-2.0-flash` / `lite`**: Maintained for legacy compatibility and specific region-locked production environments.\n",
        "*   **`gemini-2.5-pro-tts-preview`**: Specialized text-to-speech variants optimized for expressive pacing and precision.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dRb2g-ziPXeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt Engineering**\n",
        "\n",
        "When working with a large language model (LLM) like ChatGPT, the **prompt** serves as the foundation for interaction. It is the input or instruction provided to the model, guiding it to generate relevant and useful outputs.\n",
        "\n",
        "**1. Role of the Prompt**\n",
        "- **Instructional Guide**: The prompt shapes what the model does. Whether it's answering a question, completing a task, or writing creatively, the prompt provides the necessary context.  \n",
        "- **Boundary Setter**: A well-crafted prompt can define the scope of the task, ensuring the response is focused and doesn't deviate from the topic.  \n",
        "- **Task Optimizer**: By providing clear and concise instructions, the prompt ensures that the LLM generates responses that align with user expectations.\n",
        "\n",
        "**2. Importance of the Prompt**\n",
        "- **Determines Quality of Output**: The quality of the model's response depends heavily on the clarity and specificity of the prompt. A vague prompt can lead to irrelevant or incomplete answers, while a precise one produces accurate and valuable results.\n",
        "- **Customizable Interactions**: Prompts allow users to adapt the model to different scenarios—such as summarization, translation, or brainstorming—making it versatile and dynamic.  \n",
        "- **Reduces Ambiguity**: A good prompt minimizes room for misunderstanding, helping the model interpret the task as intended.  \n",
        "\n",
        "**3. Iterative Improvement**\n",
        "Working with LLMs is often an _iterative_  process. If the initial response isn't quite right, the user can refine the prompt, adding more detail or constraints to guide the model toward the desired result. Instead of starting over from scratch, you just edit the prompt and try it again.\n",
        "\n",
        "The prompt isn't just the input—it’s the bridge between the user’s needs and the model’s capabilities. Mastering prompt design is key to fully leveraging the potential of an LLM like ChatGPT.\n"
      ],
      "metadata": {
        "id": "XG5KgzC0dCxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Basic Query to LangChain\n",
        "\n",
        "We begin by writing a **prompt**, to ask (query) `ChatGPT` a simple question: \"What are the 5 most prestigous medical schools in the USA?\".\n",
        "\n",
        "The Python code in the cell below interacts with OpenAI's GPT model using `LangChain` and the `ChatOpenAI class` to retrieve our answer.\n",
        "\n",
        "**NOTE:** This cell will not run if you do not have a valid OpenAI_Key and you have already installed your key with Google Colab.\n"
      ],
      "metadata": {
        "id": "TTSOCfbAqc-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Basic Query (Client-Side Limit)\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n",
        "# Specify question\n",
        "question = \"Provide just a list of five most prestigious medical schools in the US?\"\n",
        "print(f\"Question: {question}\\n\")\n",
        "\n",
        "# 1. Setup\n",
        "gemini_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Specify which model to use\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    google_api_key=gemini_key,\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# 2. Define our Hard Limit (in characters)\n",
        "MAX_CHARS = 500\n",
        "\n",
        "# 3. Submit question to LLM\n",
        "try:\n",
        "    streamed_text = \"\"\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "    # 3. Stream and Monitor\n",
        "    for chunk in llm.stream(question):\n",
        "        streamed_text += chunk.content\n",
        "\n",
        "        # Update the display\n",
        "        display_handle.update(Markdown(streamed_text))\n",
        "\n",
        "        # Check if we have exceeded our manual limit\n",
        "        if len(streamed_text) > MAX_CHARS:\n",
        "            # Append a notice so the user knows why it stopped\n",
        "            display_handle.update(Markdown(streamed_text + \"...\\n\\n**(Stopped by Client-Side Limit)**\"))\n",
        "            break # Forcefully exit the loop\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during query: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "JQxdSwJcfvzu",
        "outputId": "17c3e15a-845f-43dd-d29c-ce3ea2b627b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Provide just a list of five most prestigious medical schools in the US?\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*   Harvard Medical School\n*   Johns Hopkins University School of Medicine\n*   University of Pennsylvania Perelman School of Medicine\n*   Stanford University School of Medicine\n*   UCSF School of Medicine\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image20C.png)"
      ],
      "metadata": {
        "id": "K7V2Z3bOrqS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the response from `LangChain` is in regular English, complete with formatting. While the formatting may make it easier to read, we often have to parse the results given to us by LLMs.\n",
        "\n",
        "Later, we will see that `LangChain` can help with this as well. You will also notice that we specified a value of `0` for **temperature**; this instructs the LLM to be less creative with its responses and more consistent. Because we are working primarily with data extraction in this section, a low temperature will give us more consistent results.\n",
        "\n",
        "In `LangChain`, the temperature parameter typically ranges from **0.0** to **1.0**, though some implementations may allow values slightly above 1.0. The temperature controls the randomness of the model's output:\n",
        "\n",
        "* **Low Temperature (e.g., 0.0):** Produces more deterministic and focused responses, ideal for tasks requiring precision.\n",
        "\n",
        "* **High Temperature (e.g., 1.0):** Generates more creative and diverse outputs, useful for brainstorming or creative writing.\n",
        "\n",
        "If you're working with `LangChain` and `OpenAI models`, you can set the temperature when initializing the model or during runtime."
      ],
      "metadata": {
        "id": "9Gy52CVNEvEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Basic Query to LangChain**\n",
        "\n",
        "For **Exercise 1** think about a subject for a `Top Five List` that **you** find interesting and see what response you get back from `ChatGTP`.\n",
        "\n",
        "Feel free to change the **temperature** of your request if you want a more _creative_ response from `LangChain`. There are no \"right\" or \"wrong\" answers here as long as your code works."
      ],
      "metadata": {
        "id": "MvfJCF6wr-lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n",
        "# Specify question\n",
        "question = \"Provide just a list of five most famous rock-and-roll guitarists?\"\n",
        "print(f\"Question: {question}\\n\")\n",
        "\n",
        "# 1. Setup\n",
        "gemini_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Specify which model to use\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    google_api_key=gemini_key,\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# 2. Define our Hard Limit (in characters)\n",
        "MAX_CHARS = 500\n",
        "\n",
        "# 3. Submit question to LLM\n",
        "try:\n",
        "    streamed_text = \"\"\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "    # 3. Stream and Monitor\n",
        "    for chunk in llm.stream(question):\n",
        "        streamed_text += chunk.content\n",
        "\n",
        "        # Update the display\n",
        "        display_handle.update(Markdown(streamed_text))\n",
        "\n",
        "        # Check if we have exceeded our manual limit\n",
        "        if len(streamed_text) > MAX_CHARS:\n",
        "            # Append a notice so the user knows why it stopped\n",
        "            display_handle.update(Markdown(streamed_text + \"...\\n\\n**(Stopped by Client-Side Limit)**\"))\n",
        "            break # Forcefully exit the loop\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during query: {e}\")"
      ],
      "metadata": {
        "id": "x_tQt3ejr-lh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "fc418325-eb15-45a7-9128-5ea810b0af19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Provide just a list of five most famous rock-and-roll guitarists?\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1.  Chuck Berry\n2.  Jimi Hendrix\n3.  Eric Clapton\n4.  Jimmy Page\n5.  Keith Richards\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I am interested in guitar players, I asked `LangChain's` for a list of the 5 greatest guitart players of all time.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image16C.png)\n",
        "\n",
        "You output will be different depending up your question."
      ],
      "metadata": {
        "id": "r24AGvJYr-lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Working with Prompts\n",
        "\n",
        "As mentioned above, interactions with LLMs is typically accomplished using `prompts`. In fact, there is a whole new field called **Prompt Engineering** that focuses on designing, refining, and optimizing prompts to maximize the effectiveness and relevance of outputs generated by large language models (LLMs) like ChatGPT, GPT-4, and others.\n",
        "\n",
        "In Example 2, we will \"engineer\" a prompt that will have `Gemini` translate text from French to English. In this example, we will just be using normal Python F-Strings to build the prompt.\n"
      ],
      "metadata": {
        "id": "ZrnDmq8AE1P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Working with Prompts\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Define our Hard Limit (in characters)\n",
        "MAX_CHARS = 400\n",
        "\n",
        "# Define text and style\n",
        "text = \"\"\"Laissez les bons temps rouler\"\"\"\n",
        "style = \"American English\"\n",
        "\n",
        "# Build prompt\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "# Uncomment next line to print prompt\n",
        "# print(f\"Prompt: {prompt}\\n\")\n",
        "\n",
        "# 1. Setup API Key\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# 2. Initialize the Gemini Model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=api_key\n",
        ")\n",
        "\n",
        "# 3. Submit question to LLM\n",
        "try:\n",
        "    streamed_text = \"\"\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "    # 3. Stream and Monitor\n",
        "    for chunk in llm.stream(prompt):\n",
        "        streamed_text += chunk.content\n",
        "\n",
        "        # Update the display\n",
        "        display_handle.update(Markdown(streamed_text))\n",
        "\n",
        "        # Check if we have exceeded our manual limit\n",
        "        if len(streamed_text) > MAX_CHARS:\n",
        "            # Append a notice so the user knows why it stopped\n",
        "            display_handle.update(Markdown(streamed_text + \"...\\n\\n**(Stopped by Client-Side Limit)**\"))\n",
        "            break # Forcefully exit the loop\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during query: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "U_fa3Pck-z-9",
        "outputId": "7e0512c4-b6cc-474c-abce-78ec009f8818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let the good times roll!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image30B.png)"
      ],
      "metadata": {
        "id": "WySFzrpTtyHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------\n",
        "\n",
        "**Why does the code Uses Triple Quotes?**\n",
        "\n",
        "The code in the cell above uses triple double quotes (\"\"\") for the prompt string to allow for clean, multi-line formatting and to include special characters, such as backticks (```) and placeholders ({style} and {text}).\n",
        "\n",
        "~~~text\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\"\"\"\n",
        "~~~\n",
        "\n",
        "-------------------------\n"
      ],
      "metadata": {
        "id": "0lPEtXSNh42d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Working with Prompts**\n",
        "\n",
        "In the cell below, use ChatGPT to translate the German expression: \"Ein Prosit der Gemütlichkeit\" into English.\n"
      ],
      "metadata": {
        "id": "uq2WA-AYuVR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Define our Hard Limit (in characters)\n",
        "MAX_CHARS = 400\n",
        "\n",
        "# Define text and style\n",
        "text = \"\"\"Ein Prosit der Gemütlichkeit\"\"\"\n",
        "style = \"American English\"\n",
        "\"Президент Трамп, русская Родина сдаётся\"\n",
        "# Build prompt\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "# Uncomment next line to print prompt\n",
        "# print(f\"Prompt: {prompt}\\n\")\n",
        "\n",
        "# 1. Setup API Key\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# 2. Initialize the Gemini Model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=api_key\n",
        ")\n",
        "\n",
        "# 3. Submit question to LLM\n",
        "try:\n",
        "    streamed_text = \"\"\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "    # 3. Stream and Monitor\n",
        "    for chunk in llm.stream(prompt):\n",
        "        streamed_text += chunk.content\n",
        "\n",
        "        # Update the display\n",
        "        display_handle.update(Markdown(streamed_text))\n",
        "\n",
        "        # Check if we have exceeded our manual limit\n",
        "        if len(streamed_text) > MAX_CHARS:\n",
        "            # Append a notice so the user knows why it stopped\n",
        "            display_handle.update(Markdown(streamed_text + \"...\\n\\n**(Stopped by Client-Side Limit)**\"))\n",
        "            break # Forcefully exit the loop\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during query: {e}\")"
      ],
      "metadata": {
        "id": "kzkL7ARauVR-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "414c37c5-b793-47cc-e88c-b728fd2f66f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's the American English equivalent of the German phrase \"Ein Prosit der Gemütlichkeit\":\n\n**\"A toast to coziness and good cheer!\"**\n\nOr, depending on the context and desired level of formality, you could also use:\n\n*   **\"Cheers to good times and good company!\"** (This is a more general interpretation focusing on the celebratory aspect)\n*   **\"Here's to comfort and good fellowship!\"** (This is a slightly more formal and literal interpretation...\n\n**(Stopped by Client-Side Limit)**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image31B.png)"
      ],
      "metadata": {
        "id": "VGza68NEuVR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dynamic Prompts**\n",
        "\n",
        "A **dynamic prompt** is a flexible and adaptive input designed for interaction with language models (LLMs) like `ChatGP`T, where placeholders or variables are used to customize the prompt based on context or user-provided information. This approach allows for reusability, personalization, and automation, ensuring that the output is tailored to specific needs without rewriting the entire prompt.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Key Characteristics of a Dynamic Prompt**\n",
        "1. **Variables**:\n",
        "   - Dynamic prompts include placeholders for variables, like `{name}`, `{style}`, or `{text}`, which can be filled with different values at runtime.\n",
        "   - For example:\n",
        "     ```python\n",
        "     prompt = f\"Translate this text: {text} into {language}.\"\n",
        "     ```\n",
        "     Here, `{text}` and `{language}` can be dynamically replaced by the desired input values.\n",
        "2. **Context-Aware**:\n",
        "   - They adapt to the context, such as the user’s preferences, conversation history, or specific tasks.\n",
        "   - For instance, a dynamic prompt for summarization might consider the length of the desired output: \"Summarize the following article in less than {words} words.\"\n",
        "3. **Reusable Templates**:\n",
        "   - Instead of hardcoding individual tasks, dynamic prompts use templates that can be applied across multiple scenarios by simply replacing values.\n",
        "   - Example template:\n",
        "     ```python\n",
        "     eg_template_text = \"\"\"Write a {tone} response to the following message:\n",
        "     message: {user_message}\"\"\"\n",
        "     ```\n",
        "4. **Personalization**:\n",
        "   - Dynamic prompts can be personalized based on user inputs or profiles, enhancing user experience. For example:\n",
        "     ```python\n",
        "     f\"Hi {name}, here’s the weather forecast for {city}!\"\n",
        "     ```\n",
        "\n",
        "#### **Why Are Dynamic Prompts Important?**\n",
        "\n",
        "- **Efficiency**: They save time by enabling template reuse.\n",
        "- **Scalability**: Useful for applications needing to handle diverse inputs.\n",
        "- **Adaptability**: They produce tailored outputs depending on the specific context or task.\n",
        "- **User Experience**: Personalization through dynamic prompts improves user satisfaction.\n",
        "\n",
        "---\n",
        "\n",
        "Dynamic prompts are at the heart of effective interactions with LLMs, making them more versatile, context-aware, and user-specific."
      ],
      "metadata": {
        "id": "azkr5E3Cii37"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBcs3yAMo_P"
      },
      "source": [
        "### Example 3 - Step 1: Build a Dynamic Prompt Template\n",
        "\n",
        "We can use LangChain to help us build dynamic prompts.\n",
        "\n",
        "The first step is provide LangChain with a `template prompt`. The code in the cell below defines and creates a prompt template using LangChain's `ChatPromptTemplate` class. The prompt template is called `example_prompt_template`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 1: Build a Dynamic Prompt Template\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 1. Define the raw prompt text with placeholders\n",
        "eg_template_text = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "# 2. Create the LangChain template object\n",
        "eg_prompt_template = ChatPromptTemplate.from_template(eg_template_text)\n",
        "print(\"Template created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzEHnYaEAFK1",
        "outputId": "4e174af6-15f2-488e-f08a-8551ebd2e2d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Template created successfully.\n",
            "Input variables found: ['style', 'text']\n",
            "\n",
            "Raw Template Structure:\n",
            "input_variables=['style', 'text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'), additional_kwargs={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you shouldn't see any output."
      ],
      "metadata": {
        "id": "AlHkVSSFxhB4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcdwI-0-Nod"
      },
      "source": [
        "### Example 3 - Step 2: Test Dynamic Prompt\n",
        "\n",
        "Now we can fill in the blanks for this prompt and observe the prompt created, which is a text string.\n",
        "\n",
        "The code in the cell below does the following:\n",
        "\n",
        "* Dynamically generates a structured prompt based on a template.\n",
        "* Ensures the prompt includes placeholders (style and text) filled with the\n",
        "provided values.\n",
        "* Inspects the data structure and type of the resulting prompt.\n",
        "* Outputs the first message to verify its content.\n",
        "\n",
        "This code is useful for building prompts in LangChain when interacting with language models for tasks like translation, summarization, or custom instructions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 2: Test Dynamic Prompt\n",
        "\n",
        "# 1. Fill in the placeholders\n",
        "eg_prompt = eg_prompt_template.format_messages(\n",
        "    style=\"American English\",\n",
        "    text=\"千里之行，始于足下。\"\n",
        ")\n",
        "\n",
        "# 2. View the generated message\n",
        "print(\"\\n--- Generated Message Object ---\")\"Президент Трамп, русская Родина сдаётся\"\n",
        "print(eg_prompt[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCV75datDJ6j",
        "outputId": "1b709437-9591-498d-c102-22f7f6fb1239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of eg_prompt:    <class 'list'>\n",
            "Type of first item:   <class 'langchain_core.messages.human.HumanMessage'>\n",
            "\n",
            "--- Generated Message Object ---\n",
            "content='Translate the text that is delimited by triple backticks into a style that is American English. text: ```千里之行，始于足下。```\\n' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Why a List?** The variable `eg_prompt` is a list because a chat history can contain multiple turns (System message, User message, AI response). Even though we only have one prompt right now, LangChain keeps the format consistent.\n",
        "\n",
        "* **HumanMessage:** You will notice the type of the first item is HumanMessage. This tells the specific model (Gemini) that this text is coming from the User, effectively distinguishing it from system instructions or AI responses."
      ],
      "metadata": {
        "id": "SlfF4VYkDSr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image32B.png)"
      ],
      "metadata": {
        "id": "OqIK6nGz31Vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are asking `ChatGPT` to translate the `text` \"千里之行，始于足下。\" into the `style` \"American English\"."
      ],
      "metadata": {
        "id": "PisALL0AkZZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3 - Step 3: Send Dynamic Prompt to LLM\n",
        "\n",
        "Now that we have buit our dynamic prompt in Steps 1 and 2, we are ready to send it to `Gemini` for analysis as shown in the code below."
      ],
      "metadata": {
        "id": "PSn4MbiMGPeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 3: Send Dynamic Prompt to LLM\n",
        "\n",
        "# 1. Setup API Key and Initialize Model\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=api_key\n",
        ")\n",
        "\n",
        "# 2. Define our Hard Limit\n",
        "MAX_CHARS = 400\n",
        "\n",
        "# 3. Stream the response and enforce the limit\n",
        "try:\n",
        "    streamed_text = \"\"\n",
        "    # Create a display handle to update the text in real-time\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "    # Use llm.stream to get chunks as they are generated\n",
        "    for chunk in llm.stream(eg_prompt):\n",
        "        # Add the new chunk content to our total string\n",
        "        streamed_text += chunk.content\n",
        "\n",
        "        # Check if we've exceeded the limit\n",
        "        if len(streamed_text) >= MAX_CHARS:\n",
        "            # Update the display one last time with a warning\n",
        "            display_handle.update(Markdown(streamed_text[:MAX_CHARS] + \"...\\n\\n**(Stopped by Client-Side Limit)**\"))\n",
        "            break\n",
        "\n",
        "        # Update the live display with the current text\n",
        "        display_handle.update(Markdown(streamed_text))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during query: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "ZqQfTD0JYGkC",
        "outputId": "4ad37edb-eb36-4b6c-decf-fea3f6fdd364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The longest journey begins with a single step.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image23C.png)"
      ],
      "metadata": {
        "id": "nn-lK6Hf3vMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This newly constructed prompt can now perform the intended task of translation."
      ],
      "metadata": {
        "id": "31sPO7R7GHv5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TphfSQIf5su_"
      },
      "source": [
        "### **Exercise 3 - Step 1: Build a Dynamic Prompt Template**\n",
        "\n",
        "In the cell below, create a prompt template called `ex_prompt_template`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 1 here\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 1. Define the raw prompt text with placeholders\n",
        "ex_template_text = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "# 2. Create the LangChain template object\n",
        "ex_prompt_template = ChatPromptTemplate.from_template(ex_template_text)\n",
        "print(\"Template created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f59c400-f3e2-41cb-f4e3-9540c76c2820",
        "id": "QZ9jRNAfSeQQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Template created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you shouldn't see any output."
      ],
      "metadata": {
        "id": "OA3OJZxESeQQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAVCBV3k5su_"
      },
      "source": [
        "### **Exercise 3 - Step 2: Test Dynamic Prompt**\n",
        "\n",
        "Suppose you are standing watch at the White House and you receive this urgent message: \"Президент Трамп, русская Родина сдаётся\". Use your `ex_prompt_template` to translate this message into English."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 2 here\n",
        "\n",
        "# 1. Fill in the placeholders\n",
        "ex_prompt = eg_prompt_template.format_messages(\n",
        "    style=\"American English\",\n",
        "    text=\"Президент Трамп, русская Родина сдаётся\"\n",
        ")\n",
        "\n",
        "# 2. View the generated message\n",
        "print(\"\\n--- Generated Message Object ---\")\n",
        "print(ex_prompt[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff66e4c7-d0a5-45ef-c242-0b5923453a9f",
        "id": "tAk02GSzSeQR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generated Message Object ---\n",
            "content='Translate the text that is delimited by triple backticks into a style that is American English. text: ```Президент Трамп, русская Родина сдаётся```\\n' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image32B.png)"
      ],
      "metadata": {
        "id": "Gzu8_8RxSeQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are asking `ChatGPT` to translate the `text` \"千里之行，始于足下。\" into the `style` \"American English\"."
      ],
      "metadata": {
        "id": "9kcXm-saSeQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3 - Step 3: Send Dynamic Prompt to `Gemini`**\n",
        "\n",
        "Send your `ex_prompt` to Gemini for a translate of Russian message: \"Президент Трамп, русская Родина сдаётся\""
      ],
      "metadata": {
        "id": "6dg9HNSmSeQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 3 here\n",
        "\n",
        "\n",
        "# 1. Setup API Key and Initialize Model\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=api_key\n",
        ")\n",
        "\n",
        "# 2. Define our Hard Limit\n",
        "MAX_CHARS = 400\n",
        "\n",
        "# 3. Stream the response and enforce the limit\n",
        "try:\n",
        "    streamed_text = \"\"\n",
        "    # Create a display handle to update the text in real-time\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "\n",
        "    # Use llm.stream to get chunks as they are generated\n",
        "    for chunk in llm.stream(ex_prompt):\n",
        "        # Add the new chunk content to our total string\n",
        "        streamed_text += chunk.content\n",
        "\n",
        "        # Check if we've exceeded the limit\n",
        "        if len(streamed_text) >= MAX_CHARS:\n",
        "            # Update the display one last time with a warning\n",
        "            display_handle.update(Markdown(streamed_text[:MAX_CHARS] + \"...\\n\\n**(Stopped by Client-Side Limit)**\"))\n",
        "            break\n",
        "\n",
        "        # Update the live display with the current text\n",
        "        display_handle.update(Markdown(streamed_text))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during query: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DyxUZV_RXpau",
        "outputId": "fff639b3-1e14-42f6-b98f-b07397cc6cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```\nPresident Trump, Mother Russia surrenders.\n```\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image34B.png)"
      ],
      "metadata": {
        "id": "3n2DHLNY5svA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLM Memory**\n",
        "\n",
        "Human minds have both long-term and short-term memory. Long-term memory is what the human has learned throughout their lifetime. Short-term memory is what a human has only recently discovered in the last minute or so. For humans, learning is converting short-term memory into long-term memory that we will retain.\n",
        "\n",
        "This process works somewhat differently for a LLM. Long-term memory was the weight of the neural network when it was initially trained or finetuned. Short-term memory is additional information that we wish the LLM to retain from previous prompts. For example, if the first prompt is \"My name is David\", the LLM will likely tell you hello and repeat your name. However, the LLM will not know the answer if the second prompt is \"What is my name.\" without adding a memory component.\n",
        "\n",
        "These memory objects, which `LangChain` provides, provide a sort of short-term memory. It is important to note that these objects are not affecting the long-term memory of the LLM, and once you discard the memory object, the LLM will forget. Additionally, the memory object can only hold so much information; newer information may replace older information once it is filled.\n",
        "\n",
        "One important point to remember is that LLM's only have their input prompt. To provide such memory, these objects are appending anything we wish the LLM to remember to the input prompt. This section will see two ways to augment the prompt with previous information: a buffer and a summary. The buffer prepends a script of the last conversation up to this point. The summary approach keeps a consistently updated summary paragraph of the conversation."
      ],
      "metadata": {
        "id": "OqG349Nm3WMv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xuel73gyFnYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Custom Conversation Bots**\n",
        "\n",
        "A **Custom Conversation Bot** is an AI-powered system that:\n",
        "- Engages in natural language conversations\n",
        "- Is customized for a specific domain, task, or personality\n",
        "- Can be deployed on websites, apps, or messaging platforms\n",
        "\n",
        "#### Key Features\n",
        "\n",
        "- **Custom Instructions**: Define how the bot should behave, respond, and what tone it should use.\n",
        "- **Knowledge Integration**: Connects to external data sources or APIs.\n",
        "- **User Memory**: Can remember user preferences or past interactions.\n",
        "- **Multimodal Capabilities**: Some bots can handle text, images, and even voice.\n",
        "\n",
        "#### Common Use Cases\n",
        "\n",
        "- Customer support\n",
        "- Educational tutoring\n",
        "- Personal assistants\n",
        "- Sales and marketing\n",
        "- Healthcare triage\n",
        "- Entertainment and storytelling\n",
        "\n",
        "#### Summary\n",
        "\n",
        "Custom conversation bots are flexible, intelligent tools that can be tailored to meet specific needs, making them valuable across industries and applications."
      ],
      "metadata": {
        "id": "68eHC1aG4Hpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xNJ8p7uMgMaP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tinz6Nt5gNQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the Bio-Assistant\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# 1. Setup Client and Key\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Error: Please set your GOOGLE_API_KEY in the Secrets tab. Details: {e}\")\n",
        "\n",
        "# 2. Define the Persona (System Instruction)\n",
        "# This replaces the complex PromptTemplate from LangChain\n",
        "system_instruction_text = \"\"\"\n",
        "You are a medical AI Physician's Assistant specializing in Nephrology.\n",
        "Your role is to have a friendly conversation with a patient.\n",
        "\n",
        "Guidance:\n",
        "- ONLY discuss topics related to renal (kidney) health issues.\n",
        "- If the user asks about other medical conditions (e.g., heart, lungs), politely redirect them to a general practitioner.\n",
        "- If the user asks non-medical questions, steer them back to kidney health.\n",
        "- Ask follow-up questions to better understand symptoms (e.g., color of urine, pain levels).\n",
        "- IMPORTANT: Do not provide definitive medical diagnoses or prescribe medication.\n",
        "- ALWAYS recommend seeing a doctor for official treatment.\n",
        "\"\"\"\n",
        "\n",
        "# 3. Create the Chat Session\n",
        "# The Google SDK handles the \"Memory\" automatically in this object\n",
        "chat = client.chats.create(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction_text,\n",
        "        temperature=0.7,\n",
        "        max_output_tokens=500\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Renal Assistant is ready.\")\n",
        "\n",
        "# --- HELPER FUNCTION TO CHAT ---\n",
        "def send_message_to_assistant(user_input):\n",
        "    \"\"\"Sends a message to the active chat session and prints the response.\"\"\"\n",
        "    try:\n",
        "        # The SDK automatically appends this to history\n",
        "        response = chat.send_message(user_input)\n",
        "\n",
        "        # Display results nicely in Colab\n",
        "        print(f\"\\nUser: {user_input}\")\n",
        "        print(\"Physician's Assistant:\")\n",
        "        display(Markdown(response.text))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzBV_47edAGW",
        "outputId": "801ea162-17fe-4133-8d87-3b411e6eabb2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renal Assistant is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TEST THE CONVERSATION ---\n",
        "\n",
        "# Message 1: Relevant topic\n",
        "send_message_to_assistant(\"Doc, I am having trouble peeing.\")\n",
        "\n",
        "# Message 2: Testing memory (The bot should remember the previous context)\n",
        "send_message_to_assistant(\"It hurts a little bit, and it happens mostly at night.\")\n",
        "\n",
        "# Message 3: Testing the guardrails (Irrelevant topic)\n",
        "send_message_to_assistant(\"Can you also check why my knee hurts?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "tGQd5Gwxf5Nr",
        "outputId": "a45870eb-f989-440e-da8e-7510a459fe36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: Doc, I am having trouble peeing.\n",
            "Physician's Assistant:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I understand you're having trouble peeing. That can be quite uncomfortable and concerning.\n\nCan you tell me more about what you're experiencing? For example:\n\n*   When did you first notice this issue?\n*   Is it a complete inability to urinate, or is it more of a difficulty starting or maintaining the flow?\n*   Are you experiencing any pain or burning when you try to urinate?\n*   Have you noticed any changes in the color or odor of your urine?\n*   Do you feel like you need to go frequently, even when your bladder isn't full?\n\nThe more details you can provide, the better I can understand what might be going on. However, please remember that I am an AI and cannot provide a diagnosis. It's essential to see a doctor for a proper evaluation and treatment plan.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: It hurts a little bit, and it happens mostly at night.\n",
            "Physician's Assistant:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, it hurts a little bit, and it's happening mostly at night. That's helpful to know.\n\nWhen you say it hurts, can you describe the pain? Is it a sharp pain, a dull ache, or something else? And where do you feel the pain specifically?\n\nAlso, when you say it happens mostly at night, do you find yourself waking up frequently to urinate?\n\nIt's important to keep in mind that I'm here to gather information and provide general guidance, but I can't provide a diagnosis. Experiencing pain and frequent urination, especially at night, warrants a visit to a doctor. They can properly assess your symptoms and determine the underlying cause.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: Can you also check why my knee hurts?\n",
            "Physician's Assistant:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I understand you're also experiencing knee pain, but since I specialize in kidney health, I'm not the best resource for that. It would be best to consult with a general practitioner or an orthopedist for your knee issue. They will be able to properly evaluate your knee and provide appropriate advice or treatment.\n\nLet's focus on your kidney-related concerns for now. Have you noticed any swelling in your ankles or feet?\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p-X-PoovgMI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wmWlIY--eWGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the Bio-Assistant\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# 1. Setup Client and Key\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Error: Please set your GOOGLE_API_KEY in the Secrets tab. Details: {e}\")\n",
        "\n",
        "# 2. Define the Persona (System Instruction)\n",
        "# This replaces the complex PromptTemplate from LangChain\n",
        "system_instruction_text = \"\"\"\n",
        "You are a medical AI Physician's Assistant specializing in Nephrology.\n",
        "Your role is to have a friendly conversation with a patient.\n",
        "\n",
        "Guidance:\n",
        "- ONLY discuss topics related to renal (kidney) health issues.\n",
        "- If the user asks about other medical conditions (e.g., heart, lungs), politely redirect them to a general practitioner.\n",
        "- If the user asks non-medical questions, steer them back to kidney health.\n",
        "- Ask follow-up questions to better understand symptoms (e.g., color of urine, pain levels).\n",
        "- IMPORTANT: Do not provide definitive medical diagnoses or prescribe medication.\n",
        "- ALWAYS recommend seeing a doctor for official treatment.\n",
        "\"\"\"\n",
        "\n",
        "# 3. Create the Chat Session\n",
        "# The Google SDK handles the \"Memory\" automatically in this object\n",
        "chat = client.chats.create(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction_text,\n",
        "        temperature=0.7,\n",
        "        max_output_tokens=500\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Renal Assistant is ready.\")\n",
        "\n",
        "# Message 1: Relevant topic\n",
        "send_message_to_assistant(\"Doc, I am having trouble peeing.\")\n",
        "\n",
        "# Message 2: Testing memory (The bot should remember the previous context)\n",
        "send_message_to_assistant(\"It hurts a little bit, and it happens mostly at night.\")\n",
        "\n",
        "# Message 3: Testing the guardrails (Irrelevant topic)\n",
        "send_message_to_assistant(\"Can you also check why my knee hurts?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "8_qvGWhjeWu1",
        "outputId": "76666457-f0b6-43c0-8009-ea52a7af6474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renal Assistant is ready.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'send_message_to_assistant' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1336673668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Message 1: Relevant topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0msend_message_to_assistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Doc, I am having trouble peeing.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Message 2: Testing memory (The bot should remember the previous context)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'send_message_to_assistant' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h1J2GqnvemO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TEST THE CONVERSATION ---\n",
        "\n",
        "# Message 1: Relevant topic\n",
        "send_message_to_assistant(\"Doc, I am having trouble peeing.\")\n",
        "\n",
        "# Message 2: Testing memory (The bot should remember the previous context)\n",
        "send_message_to_assistant(\"It hurts a little bit, and it happens mostly at night.\")\n",
        "\n",
        "# Message 3: Testing the guardrails (Irrelevant topic)\n",
        "send_message_to_assistant(\"Can you also check why my knee hurts?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "L5uMnpE7emx2",
        "outputId": "4f3531a0-f7c8-4072-e2f9-38584208c71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'send_message_to_assistant' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-751028846.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Message 1: Relevant topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msend_message_to_assistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Doc, I am having trouble peeing.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Message 2: Testing memory (The bot should remember the previous context)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'send_message_to_assistant' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Setup API Key\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "except:\n",
        "    GOOGLE_API_KEY = input(\"Enter GOOGLE_API_KEY: \")\n",
        "\n",
        "# 2. Initialize the Model\n",
        "chat = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.7,\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# 3. Define the Persona using a \"SystemMessage\"\n",
        "# This acts as the \"Brain\" instructions, exactly like your prompt template\n",
        "system_instruction = \"\"\"You are a medical AI Physician's Assistant.\n",
        "You should only discuss topics related to renal health issues.\n",
        "If the user asks about other medical conditions, politely redirect them to a general practitioner.\n",
        "If the user asks about non-medical topics, gently steer them back to health-related questions about renal function.\n",
        "\n",
        "Important guidelines:\n",
        "- Focus only medical issues relat---------------------------------------------------------------------------\n",
        "\n",
        "ClientError                               Traceback (most recent call last)\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py in _generate(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\n",
        "   3039         try:\n",
        "-> 3040             response: GenerateContentResponse = self.client.models.generate_content(\n",
        "   3041                 **request,\n",
        "\n",
        "20 frames\n",
        "\n",
        "ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\n",
        "\n",
        "\n",
        "The above exception was the direct cause of the following exception:\n",
        "\n",
        "ChatGoogleGenerativeAIError               Traceback (most recent call last)\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py in _handle_client_error(e, request)\n",
        "    143     model_name = request.get(\"model\", \"unknown\")\n",
        "    144     msg = f\"Error calling model '{model_name}' ({e.status}): {e}\"\n",
        "--> 145     raise ChatGoogleGenerativeAIError(msg) from e\n",
        "    146\n",
        "    147\n",
        "\n",
        "ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\n",
        "\n",
        "ed to the kidney and renal function\n",
        "- Use professional but friendly language\n",
        "- Ask follow-up questions to better understand symptoms\n",
        "- Do not provide diagnoses or treatment recommendations\n",
        "- Only as a last resort recommend seeing a doctor\"\"\"\n",
        "\n",
        "# 4. Initialize the Memory List\n",
        "# We start the conversation history with just the system instruction\n",
        "chat_history = [\n",
        "    SystemMessage(content=system_instruction)\n",
        "]\n",
        "\n",
        "print(\"Renal Assistant (Core Version) is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_-TBKtcerzw",
        "outputId": "8f776878-c8f9-41f4-fd3e-c29eb8989bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renal Assistant (Core Version) is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Custom Conversation Bot\n",
        "\n",
        "In Example 5 we are going to create a custom conversational bot named \"AI Physican's Assistant\" that is designed to help screen phone calls to a medical clinic.\n",
        "\n",
        "The first step is to create a `template` that defines the focus of the `Bot` and how it responds to questions. The template includes a list of **Important guidlines** that focus and restrict the Bot's responses.\n",
        "\n",
        "As in previous examples, the prefix \"eg_\" has been added to key variables to separate the Bot in the example form the Bot you will make in the next exercise."
      ],
      "metadata": {
        "id": "JkSpisR5t7Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Question 1 (Corrected for Gemini Core)\n",
        "\n",
        "import textwrap\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "WIDTH = 64  # Adjust the width to fit your Colab notebook\n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "ClientError                               Traceback (most recent call last)\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py in _generate(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\n",
        "   3039         try:\n",
        "-> 3040             response: GenerateContentResponse = self.client.models.generate_content(\n",
        "   3041                 **request,\n",
        "\n",
        "20 frames\n",
        "\n",
        "ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\n",
        "\n",
        "\n",
        "The above exception was the direct cause of the following exception:\n",
        "\n",
        "ChatGoogleGenerativeAIError               Traceback (most recent call last)\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py in _handle_client_error(e, request)\n",
        "    143     model_name = request.get(\"model\", \"unknown\")\n",
        "    144     msg = f\"Error calling model '{model_name}' ({e.status}): {e}\"\n",
        "--> 145     raise ChatGoogleGenerativeAIError(msg) from e\n",
        "    146\n",
        "    147\n",
        "\n",
        "ChatGoogleGenerativeAIError: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\n",
        "\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation = \"Doc, I am having trouble peeing\"\n",
        "\n",
        "# --- NEW LOGIC START ---\n",
        "\n",
        "# 1. Convert the string into a Message object the bot understands\n",
        "user_msg = HumanMessage(content=Conversation)\n",
        "\n",
        "# 2. Add this message to the chat history\n",
        "# (This replaces the internal memory of eg_conversation)\n",
        "chat_history.append(user_msg)\n",
        "\n",
        "# 3. Send the updated history to the model\n",
        "# We use .invoke() instead of .predict()\n",
        "response_msg = chat.invoke(chat_history)\n",
        "\n",
        "# 4. Extract the actual text from the AI's response\n",
        "eg_result = response_msg.content\n",
        "\n",
        "# 5. Append the AI's response to history (so it remembers for Question 2)\n",
        "chat_history.append(response_msg)\n",
        "\n",
        "# --- NEW LOGIC END ---\n",
        "\n",
        "wrapped_text = textwrap.fill(eg_result, width=WIDTH)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "kQ-NCS5NfxFa",
        "outputId": "f4accc0d-d55b-4c5f-e93e-adaf146705ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ChatGoogleGenerativeAIError",
          "evalue": "Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m   3039\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3040\u001b[0;31m             response: GenerateContentResponse = self.client.models.generate_content(\n\u001b[0m\u001b[1;32m   3041\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5217\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5218\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   5219\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4000\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   4001\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[0;32m-> 1388\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m     response_body = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0mretry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtenacity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetrying\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mretry_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_run_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1200\u001b[0m       )\n\u001b[0;32m-> 1201\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m       return HttpResponse(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_error\u001b[0;34m(cls, status_code, response_json, response)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mChatGoogleGenerativeAIError\u001b[0m               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1081663769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 3. Send the updated history to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# We use .invoke() instead of .predict()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mresponse_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 4. Extract the actual text from the AI's response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[1;32m   2527\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2531\u001b[0m     def _get_ls_params(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m             cast(\n\u001b[1;32m    397\u001b[0m                 \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 self.generate_prompt(\n\u001b[0m\u001b[1;32m    399\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1116\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 results.append(\n\u001b[0;32m--> 927\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    928\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1222\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m   3042\u001b[0m             )\n\u001b[1;32m   3043\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3044\u001b[0;31m             \u001b[0m_handle_client_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_response_to_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_handle_client_error\u001b[0;34m(e, request)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unknown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Error calling model '{model_name}' ({e.status}): {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mChatGoogleGenerativeAIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mChatGoogleGenerativeAIError\u001b[0m: Error calling model 'gemini-1.5-flash' (NOT_FOUND): 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Custom Conversation Bot\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain # Add this import\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from google.colab import userdata\n",
        "\n",
        "# It is good practice to explicitly import ConversationChain\n",
        "# from langchain.chains.conversation.base import ConversationChain\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "LLM_MODEL = \"gpt-4o-mini\" # Example model name\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model=LLM_MODEL, temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Create your prompt template\n",
        "eg_template = \"\"\"The following is a friendly conversation between a human and a medical AI Physician's Assistant.\n",
        "The Physician's Assistant should only discuss topics related to renal health issues.\n",
        "If the user asks about other medical conditions, politely redirect them to a general practitioner.\n",
        "If the user asks about non-medical topics, gently steer them back to health-related questions about renal function.\n",
        "\n",
        "Important guidelines:\n",
        "- Focus only medical issues related to the kidney and renal function\n",
        "- Use professional but friendly language\n",
        "- Ask follow-up questions to better understand symptoms\n",
        "- Do not provide diagnoses or treatment recommendations\n",
        "- Only as a last resort recommend seeing a doctor\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Patient: {input}\n",
        "Physician's Assistant:\"\"\"\n",
        "\n",
        "eg_PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=eg_template) # Changed `template` to `eg_template`\n",
        "\n",
        "# Use basic memory approach that should work without pickling issues\n",
        "eg_memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
        "\n",
        "# Corrected variable name from `memory` to `eg_memory`\n",
        "eg_conversation = ConversationChain(\n",
        "    prompt=eg_PROMPT,\n",
        "    llm=llm,\n",
        "    memory=eg_memory, # Corrected: Use the 'eg_memory' object you defined\n",
        "    verbose=False\n",
        ")---------------------------------------------------------------------------\n",
        "\n",
        "ModuleNotFoundError                       Traceback (most recent call last)\n",
        "\n",
        "/tmp/ipython-input-841560525.py in <cell line: 0>()\n",
        "      1 from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "----> 2 from langchain.chains import ConversationChain\n",
        "      3 from langchain_core.prompts import PromptTemplate\n",
        "      4 from langchain.memory import ConversationBufferMemory\n",
        "      5 from google.colab import userdata\n",
        "\n",
        "ModuleNotFoundError: No module named 'langchain.chains'\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "09Q45FsA96E9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "583e392b-0e6d-4cad-ea01-757ba85f7606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (ipython-input-2599901947.py, line 51)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2599901947.py\"\u001b[0;36m, line \u001b[0;32m51\u001b[0m\n\u001b[0;31m    tmp/ipython-input-841560525.py(in, <cell, line:, 0>())\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any output:"
      ],
      "metadata": {
        "id": "MTSQoTGplSji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Conversing with Custom Bot\n",
        "\n",
        "We can now have a conversation with our AI Physician's Assistant. We start by explaining that we are an 82 year old woman who is having trouble peeing.  "
      ],
      "metadata": {
        "id": "9NFBCWyVt7Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Question 1"
      ],
      "metadata": {
        "id": "Vcl6kaSnt7Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Question 1\n",
        "\n",
        "import textwrap\n",
        "WIDTH = 64  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, I am having trouble peeing\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "eg_result = eg_conversation.predict(input=Conversation)\n",
        "\n",
        "wrapped_text = textwrap.fill(eg_result, width=WIDTH)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "CoiQHSTOt7Mb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "bfd174eb-073c-4d46-c5ea-7465a856f974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'eg_conversation' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4065015455.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Send conversation to Custom Bot for processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0meg_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meg_conversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConversation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mwrapped_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextwrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meg_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWIDTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'eg_conversation' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image24C.png)"
      ],
      "metadata": {
        "id": "01iQEAeDH4_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Question 2\n",
        "\n",
        "The patient explains her symptoms..."
      ],
      "metadata": {
        "id": "SB5pi0yEt7Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Question 2\n",
        "\n",
        "import textwrap\n",
        "WIDTH = 64  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, I only have pain on my left side\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "eg_result = eg_conversation.predict(input=Conversation)\n",
        "\n",
        "wrapped_text = textwrap.fill(eg_result, width=WIDTH)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "qGrmUBOlt7Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image25C.png)\n"
      ],
      "metadata": {
        "id": "9l_KV_j-t7Mc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Question 3\n",
        "\n",
        "The verbal exchange continues..."
      ],
      "metadata": {
        "id": "CS60q9-tt7Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Question 3\n",
        "\n",
        "import textwrap\n",
        "WIDTH = 64  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, The pain isn't too bad, but I should mention that my urine is dark brown in color\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "eg_result = eg_conversation.predict(input=Conversation)\n",
        "\n",
        "wrapped_text = textwrap.fill(eg_result, width=WIDTH)\n",
        "print(wrapped_text)\n"
      ],
      "metadata": {
        "id": "C-GJJFQOt7Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image26C.png)\n"
      ],
      "metadata": {
        "id": "LifrhF_v6_wW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Question 4\n",
        "\n",
        "The patient starts to go a little off target..."
      ],
      "metadata": {
        "id": "4w9yAuy_t7Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Question 4\n",
        "\n",
        "import textwrap\n",
        "WIDTH = 64  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, Thank-you for listening to me. It's hard to find a friendy ear these days\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "eg_result = eg_conversation.predict(input=Conversation)\n",
        "\n",
        "wrapped_text = textwrap.fill(eg_result, width=WIDTH)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "yBt8Gviyt7Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image27C.png)\n"
      ],
      "metadata": {
        "id": "jix-sDeP7uWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Question 5\n",
        "\n",
        "The patient now switches to a  more philosophical question..."
      ],
      "metadata": {
        "id": "cPralGumt7Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Question 5\n",
        "\n",
        "import textwrap\n",
        "WIDTH = 64  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, you are so smart! Can you tell me the meaning of life?\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "eg_result = eg_conversation.predict(input=Conversation)\n",
        "\n",
        "wrapped_text = textwrap.fill(eg_result, width=WIDTH)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "f_T-ypJmt7Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image28C.png)\n"
      ],
      "metadata": {
        "id": "DT9jGvoZ8rF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Bot isn't interested in talking philosophy..."
      ],
      "metadata": {
        "id": "W7D_J2tL_ZGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Print Out Memory\n",
        "\n",
        "The code in the cell below allows us to have a look at what the memory buffer now contains."
      ],
      "metadata": {
        "id": "1YwSKBRqt7Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Print out memory\n",
        "\n",
        "eg_conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "1R_mwVm_t7Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct the first part of the output should look something like this:\n",
        "\n",
        "```text\n",
        "{'history': [HumanMessage(content='Doc, I am having trouble peeing', additional_kwargs={}, response_metadata={}),\n",
        "  AIMessage(content='I\\'m sorry to hear that you\\'re experiencing trouble with urination. It’s important to understand more about your symptoms. Can you describe what you mean by \"trouble peeing\"? Are you experiencing pain, a change in frequency, or difficulty starting or stopping urination?', additional_kwargs={}, response_metadata={}),\n",
        "  HumanMessage(content='Doc, I only have pain on my left side', additional_kwargs={}, response_metadata={}),\n",
        "......\n",
        "```"
      ],
      "metadata": {
        "id": "EdtVtDuWJLCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Custom Conversational Bot**\n",
        "\n",
        "For **Exercise 5** you are design your very own **Custom Conversation Bot** and then ask it 5 questions that are within its field of focus.\n",
        "\n",
        "It is expected that every student will pick a **_different_** focus and use **_different_** questions (i.e. don't `copy-and-paste` from your `coding buddy`).\n",
        "\n",
        "**Coding Hints**\n",
        "\n",
        "As in earlier lessons, you will need to change the prefix \"eg_\" to \"ex_\". If you don't your conversations will become part of the conversations created above in Example 5."
      ],
      "metadata": {
        "id": "sx3HO7dnTjQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Create Chat Bot**\n",
        "\n",
        "In the cell below, write the code to create your `Custom Chat Bot`."
      ],
      "metadata": {
        "id": "_Ktl20_-a_kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Create Custom Conversation Bot\n"
      ],
      "metadata": {
        "id": "eLGvXE_qWAVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Questions**\n",
        "\n",
        "Use the next 5 blank code cells to ask your Custom Chat Bot questions.  "
      ],
      "metadata": {
        "id": "RhJOmcHMXMy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Question 1**"
      ],
      "metadata": {
        "id": "hp3nyzp3fPWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Question 1 here\n",
        "\n"
      ],
      "metadata": {
        "id": "5wbfdTLXXgNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Question 2**"
      ],
      "metadata": {
        "id": "nACgfG9QfWC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Question 2 here\n"
      ],
      "metadata": {
        "id": "JQ5xCM4eYd6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Question 3**"
      ],
      "metadata": {
        "id": "me_w4NjufYiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Question 3 here\n"
      ],
      "metadata": {
        "id": "7qpaHGFqYucb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p_Isl2pGun7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Question 4 here\n"
      ],
      "metadata": {
        "id": "lEunAZ6mZBSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Question 5**"
      ],
      "metadata": {
        "id": "DnZhUtv8ffeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Question 5 here\n"
      ],
      "metadata": {
        "id": "FH6bzF8CZbzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Print Out Memory**"
      ],
      "metadata": {
        "id": "1vD9Pfe0fjb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x11VJ34IH2B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Print History here\n"
      ],
      "metadata": {
        "id": "VAlby4HJXNrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLMs Have Different Functions**\n",
        "\n",
        "LLMs can also be distinguished by their specialized functions, which are often achieved through fine-tuning.\n",
        "\n",
        "* **Base/Foundation LLMs:** These are the initial LLMs trained on a massive, diverse, and unlabeled dataset. They have a broad understanding of language but are not inherently designed for instruction following.\n",
        "* * **Example:** GPT-3 before instruction-tuning.\n",
        "\n",
        "* **Instruct Models:** Fine-tuned to follow specific instructions or prompts. This training makes them more useful for direct task completion.\n",
        "* **Chat Models:** A specialized type of instruct model, further fine-tuned using conversational data to perform well in dialogue-based interactions.\n",
        "Code Models: Trained on extensive datasets of code to perform programming-related tasks, like generating, summarizing, and debugging code.\n",
        "* **Multimodal Models:** Can process and generate content across multiple data types, such as text, images, and audio. They combine different encoding modalities to understand and act on complex prompts.\n",
        "* * **Example:** OpenAI's GPT-4o (\"omni\") is a multimodal model.\n",
        "* **Hybrid Models:** Combine the strengths of different models and techniques. A common hybrid approach uses a powerful LLM for reasoning alongside a Retrieval-Augmented Generation (RAG) system for accessing up-to-date, authoritative information from an external knowledge base"
      ],
      "metadata": {
        "id": "7ajvqdrpAYCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Image Analysis with GTP-4o-mini**\n",
        "\n",
        "**GPT-4o-mini** is a compact, efficient multimodal AI model that can process and reason with images, text, and audio. It can be applied to analyze biological and medical images by leveraging its ability to \"think with\" visual content, rather than just interpreting it. However, as a smaller model, it is generally better suited for specific, high-volume tasks and as part of a larger, more precise analytical pipeline, rather than for delivering definitive, complex diagnoses on its own.\n",
        "\n",
        "#### **Applications in Biological Imaging**\n",
        "\n",
        "GPT-4o-mini is useful for tasks that benefit from its speed, efficiency, and understanding of visual patterns in biological contexts.\n",
        "* **Microscopy image analysis:** The model can be used for classification tasks, such as differentiating between wild-type and mutated cells, or identifying certain features in tissue samples. It can generate descriptive captions for images, which is valuable for data curation.\n",
        "* **Pattern and texture analysis:** In diagnostics, AI can analyze subtle patterns in biological samples that may be hard for humans to detect. For example, AI can analyze drying patterns in tears to detect signs of dry-eye disease or in blood to help screen for conditions like leukemia.\n",
        "* **Data filtering and preprocessing:** Due to its cost-effectiveness, GPT-4o-mini can be used in data pipelines to filter and curate large sets of biological images. This helps democratize access to high-quality training data for more powerful, domain-specific models.\n",
        "\n",
        "#### **Applications in Medical Imaging**\n",
        "\n",
        "In medical imaging, GPT-4o-mini can act as a component within a larger workflow, performing initial screening and high-throughput tasks, though it has limitations for definitive clinical decision-making.\n",
        "\n",
        "* **High-volume screening:** The model can be used for high-volume, low-cost tasks like summarizing daily patient messages related to imaging or extracting structured data from scanned medical forms.\n",
        "* **Patient data analysis:** It can help healthcare professionals analyze patient data by identifying patterns in images when paired with contextual information. This can assist in decision-making or provide support for treatment plans.\n",
        "* **Preliminary assessment (with limitations):** Studies have shown that multimodal models can identify anatomical regions, modalities, and sometimes pathologies in images like CT scans and X-rays. However, these studies also highlight that such models can produce \"hallucinations\" or inaccuracies, especially with complex interpretations. The \"all or nothing\" accuracy of GPT-4o in radiology demonstrates that it should not be used for definitive diagnoses.\n",
        "* **Modular pipelines:** For more robust analysis, GPT-4o-mini can serve as a cheap and fast initial filter in a multi-step diagnostic process. The image can then be sent to a human expert or a more specialized model for final verification.\n",
        "\n",
        "#### **Limitations and considerations**\n",
        "\n",
        "While useful, GPT-4o-mini's application in medical imaging has key limitations that must be addressed:\n",
        "\n",
        "* **Diagnostic accuracy:** The model lacks the precision and reliability for complex diagnostic tasks. Its outputs must always be reviewed and verified by a human expert.\n",
        "Not optimized for OCR: GPT-4o vision capabilities are not designed for highly accurate optical character recognition (OCR) and may struggle with extracting precise text from images.\n",
        "* **Hallucinations:** Like many large language models, it can sometimes produce incorrect or fabricated information, which is a significant risk in a clinical setting.\n",
        "* **Ethical considerations:** The use of AI in healthcare carries ethical responsibilities. The probabilistic nature of the model's responses means that it may refuse to answer some requests or produce inconsistent answers for the same image"
      ],
      "metadata": {
        "id": "EOZG2cVePfd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Image Analysis with GPT-4o-mini\n",
        "\n",
        "In this example we are going to use `gpt-4o-mini` to analyze a histological image of a normal (healthy) human small intestine. The image, `eg_medical_image.png` is stored on the course fileserver and will be downloaded as part of the code example shown in the next cell.\n",
        "\n"
      ],
      "metadata": {
        "id": "_drb79eB2n_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Image Analysis with GTP-4o-mini\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "from google.colab import files\n",
        "import base64\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the LLM\n",
        "eg_multimodal_llm  = ChatOpenAI(model=LLM_MODEL, temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "print(\"Multimodal Task Example: Image Analysis with GPT-4o-mini\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Download image from your webserver\n",
        "eg_image_url = \"https://biologicslab.co:/BIO1173/images/class_04/eg_medical_image.png\"\n",
        "eg_response = requests.get(eg_image_url)\n",
        "eg_image = Image.open(io.BytesIO(eg_response.content))\n",
        "\n",
        "print(\"Image downloaded successfully!\")\n",
        "print(f\"Image dimensions: {eg_image.size}\")\n",
        "print(f\"Image mode: {eg_image.mode}\")\n",
        "\n",
        "# Display the image\n",
        "display(eg_image)\n",
        "\n",
        "# Create a multimodal prompt for image analysis\n",
        "eg_image_analysis_prompt = \"\"\"\n",
        "You are analyzing an histology image of a section of a healthy human small intestine.\n",
        "\n",
        "Provide your analysis in the following format:\n",
        "1. Identify the basic structures\n",
        "2. The different layerss of the intestine like mucosa, submucose, muscularis externa and serosa.\n",
        "3. Point out different cell types such as villi, goblet cells and crypts.\n",
        "4. Describe the basic function of each cell type\n",
        "5. Explain the tissue organization of the small intestine in the context of form and function\n",
        "\n",
        "Histological Image Analysis Response:\"\"\"\n",
        "\n",
        "# Convert image to base64 for sending to LLM\n",
        "def image_to_base64(eg_image_path):\n",
        "    with open(eg_image_path, \"rb\") as eg_image_file:\n",
        "        return base64.b64encode(eg_image_file.read()).decode('utf-8')\n",
        "\n",
        "print(\"Histological Image Analysis\")\n",
        "print(\"-\" * 40)\n",
        "eg_text_analysis = eg_multimodal_llm.invoke([\n",
        "    HumanMessage(content=[\n",
        "        {\"type\": \"text\", \"text\": eg_image_analysis_prompt}\n",
        "    ])\n",
        "])\n",
        "\n",
        "print(eg_text_analysis.content)\n"
      ],
      "metadata": {
        "id": "6DY9qvYWPgCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6: Image Analysis with GPT-4o-mini**\n",
        "\n",
        "In the cell below write the code to analyze a histological image of healthy human kidney. The image is called \"ex_medical_image.png\" and is stored on the course file server.\n",
        "\n",
        "**Code Hints**\n",
        "\n",
        "1. Make sure to change this code snippet:\n",
        "```text\n",
        "# Create a multimodal prompt for image analysis\n",
        "eg_image_analysis_prompt = \"\"\"\n",
        "You are analyzing an histology image of a section of a healthy human small intestine.\n",
        "```\n",
        "to read as:\n",
        "```text\n",
        "# Create a multimodal prompt for image analysis\n",
        "ex_image_analysis_prompt = \"\"\"\n",
        "You are analyzing an histology image of a section of a healthy human kidney.\n",
        "```\n",
        "\n",
        "Make sure to replace all of the \"eg_\" prefixes with \"ex_\" to keep the data in this exercise separate from the data in the example.\n",
        "\n"
      ],
      "metadata": {
        "id": "N3If596bDFbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6 here\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "from google.colab import files\n",
        "import base64\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the LLM\n",
        "ex_multimodal_llm  = ChatOpenAI(model=LLM_MODEL, temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "print(\"Multimodal Task Example: Image Analysis with GPT-4o-mini\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Download image from your webserver\n",
        "ex_image_url = \"https://biologicslab.co:/BIO1173/images/class_04/ex_medical_image.png\"\n",
        "ex_response = requests.get(ex_image_url)\n",
        "ex_image = Image.open(io.BytesIO(ex_response.content))\n",
        "\n",
        "print(\"Image downloaded successfully!\")\n",
        "print(f\"Image dimensions: {eg_image.size}\")\n",
        "print(f\"Image mode: {eg_image.mode}\")\n",
        "\n",
        "# Display the image\n",
        "display(ex_image)\n",
        "\n",
        "# Create a multimodal prompt for image analysis\n",
        "ex_image_analysis_prompt = \"\"\"\n",
        "You are analyzing an histology image of a section of a healthy human kidney.\n",
        "\n",
        "Provide your analysis in the following format:\n",
        "1. Identify the basic structures\n",
        "2. The different layerss of the intestine like mucosa, submucose, muscularis externa and serosa.\n",
        "3. Point out different cell types such as villi, goblet cells and crypts.\n",
        "4. Describe the basic function of each cell type\n",
        "5. Explain the tissue organization of the small intestine in the context of form and function\n",
        "\n",
        "Histological Image Analysis Response:\"\"\"\n",
        "\n",
        "# Convert image to base64 for sending to LLM\n",
        "def image_to_base64(ex_image_path):\n",
        "    with open(ex_image_path, \"rb\") as ex_image_file:\n",
        "        return base64.b64encode(ex_image_file.read()).decode('utf-8')\n",
        "\n",
        "print(\"Histological Image Analysis\")\n",
        "print(\"-\" * 40)\n",
        "ex_text_analysis = ex_multimodal_llm.invoke([\n",
        "    HumanMessage(content=[\n",
        "        {\"type\": \"text\", \"text\": ex_image_analysis_prompt}\n",
        "    ])\n",
        "])\n",
        "\n",
        "print(ex_text_analysis.content)\n"
      ],
      "metadata": {
        "id": "rjEQ6-u6DFbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What are Embedding Layers in PyTorch**\n",
        "\n",
        "[Embedding Layers](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) are a handy feature of PyTorch that allows the program to automatically insert additional information into the data flow of your neural network. An embedding layer would automatically allow you to insert vectors in the place of word indexes.  \n",
        "\n",
        "Programmers often use embedding layers with Natural Language Processing (NLP); however, you can use these layers when you wish to insert a lengthier vector in an index value place. In some ways, you can think of an embedding layer as dimension expansion. However, the hope is that these additional dimensions provide more information to the model and provide a better score."
      ],
      "metadata": {
        "id": "HrvnI_Bc5FsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xan6g8XzGjv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Example - Step 1\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textwrap\n",
        "WIDTH = 80  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "\n",
        "# Create a medical vocabulary (common terms in medicine)\n",
        "medical_terms = [\n",
        "    'heart', 'lung', 'kidney', 'liver', 'brain', 'stomach',\n",
        "    'bone', 'muscle', 'skin', 'blood', 'nerve', 'virus',\n",
        "    'bacteria', 'infection', 'fever', 'pain', 'diabetes',\n",
        "    'hypertension', 'stroke', 'cancer', 'allergy', 'immune'\n",
        "]\n",
        "\n",
        "print(f\"Medical vocabulary: {len(medical_terms)} terms\")\n",
        "wrapped_text = textwrap.fill(f\"Medical terms: {medical_terms}\", width=WIDTH)\n",
        "print(wrapped_text)\n",
        "\n",
        "# Create embedding layer - learning representations of medical terms\n",
        "embedding_layer = nn.Embedding(num_embeddings=len(medical_terms), embedding_dim=6)\n",
        "optimizer = optim.Adam(embedding_layer.parameters(), lr=0.01)\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Define some semantic relationships (in real scenarios, this would come from data)\n",
        "training_pairs = [\n",
        "    # (term1_index, term2_index) - terms that are anatomically related\n",
        "    (0, 6),  # heart vs bone\n",
        "    (1, 7),  # lung vs muscle\n",
        "    (2, 8),  # kidney vs skin\n",
        "    (3, 9),  # liver vs blood\n",
        "    (4, 10), # brain vs nerve\n",
        "    (11, 12), # virus vs bacteria\n",
        "    (13, 14), # infection vs fever\n",
        "    (15, 16), # pain vs diabetes\n",
        "    (17, 18), # hypertension vs stroke\n",
        "    (19, 20), # cancer vs allergy\n",
        "]\n",
        "\n",
        "# Create target embeddings that represent relationships\n",
        "target_embeddings = torch.zeros(len(medical_terms), 6)\n",
        "\n",
        "print(\"\\nTraining Embeddings to Reveal Medical Relationships...\")\n",
        "\n",
        "# Training loop - this simulates the neural network learning medical concepts\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # For each training pair, we want similar embeddings for related terms\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (term1_idx, term2_idx) in enumerate(training_pairs):\n",
        "        # Get embeddings for both terms\n",
        "        emb1 = embedding_layer(torch.tensor([term1_idx]))\n",
        "        emb2 = embedding_layer(torch.tensor([term2_idx]))\n",
        "\n",
        "        # Simple loss: make embeddings of related terms more similar\n",
        "        loss = torch.norm(emb1 - emb2)  # Distance between embeddings\n",
        "        total_loss += loss\n",
        "\n",
        "    # Average loss over all pairs\n",
        "    avg_loss = total_loss / len(training_pairs)\n",
        "\n",
        "    # Backpropagation (this is where the \"learning\" happens!)\n",
        "    avg_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {avg_loss.item():.4f}\")\n",
        "\n",
        "print(\"\\nTraining Complete!\")\n",
        "print(\"The neural network has learned how to represent medical terms in a meaningful way.\")\n",
        "\n",
        "# Get final embeddings\n",
        "final_embeddings = embedding_layer.weight.data.detach().numpy()"
      ],
      "metadata": {
        "id": "R_RH2CgjGk6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image29C.png)"
      ],
      "metadata": {
        "id": "sxNxyOU2JT9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Example - Step 1\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textwrap\n",
        "WIDTH = 80  # Adjust the width to fit your Colab notebook"
      ],
      "metadata": {
        "id": "-Yc164fxJl6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adjustText > /dev/null"
      ],
      "metadata": {
        "id": "AhcaxZv0MpVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Example - Step 2\n",
        "\n",
        "The code in the cell below uses a very powerful technique to discover the interrelations called **Principal Component Analysis (PCA)** to reduce the dimensions of the data from 6 dimensions to 2 dimensions (`X` and `Y`) so the embedding relationships can be visualized in a 2d plot."
      ],
      "metadata": {
        "id": "DyJTIT6tSU0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Example - Step 2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from adjustText import adjust_text\n",
        "\n",
        "# Get final embeddings\n",
        "final_embeddings = embedding_layer.weight.data.detach().numpy()\n",
        "\n",
        "# Visualize the embeddings using PCA (reducing 6D to 2D for visualization)\n",
        "pca = PCA(n_components=2)\n",
        "embedded_2d = pca.fit_transform(final_embeddings)\n",
        "\n",
        "print(f\"\\nPCA Analysis:\")\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.3f}\")\n",
        "\n",
        "# Create a more interesting visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 8))  # 10,8\n",
        "\n",
        "# Plot the embeddings in 2D space\n",
        "ax.scatter(embedded_2d[:, 0], embedded_2d[:, 1], s=100, alpha=0.7, c='blue')\n",
        "\n",
        "'''\n",
        "# Add labels for each medical term\n",
        "for i, (term, (x, y)) in enumerate(zip(medical_terms, embedded_2d)):\n",
        "    plt.annotate(term, (x, y), xytext=(5, 5), textcoords='offset points',\n",
        "                fontsize=6, alpha=0.8)\n",
        "'''\n",
        "# Create and collect labels\n",
        "texts = []\n",
        "for i, (term, (x, y)) in enumerate(zip(medical_terms, embedded_2d)):\n",
        "    texts.append(ax.annotate(term, (x, y), xytext=(5, 5), textcoords='offset points', fontsize=8))\n",
        "\n",
        "#plt.annotate(term, (x, y), xytext=(10, 10), textcoords='offset points', fontsize=6, alpha=0.8)\n",
        "\n",
        "# Automatically adjust overlapping labels\n",
        "adjust_text(texts, ax=ax)\n",
        "\n",
        "# Highlight anatomical relationships\n",
        "anatomical_groups = [\n",
        "    ['heart', 'lung', 'kidney', 'liver', 'brain'],   # Organ system\n",
        "    ['bone', 'muscle', 'skin'],                      # Body structure\n",
        "    ['virus', 'bacteria', 'infection', 'fever'],     # Pathology\n",
        "    ['diabetes', 'hypertension', 'stroke', 'cancer'] # Disease types\n",
        "]\n",
        "\n",
        "# Draw connections between related terms\n",
        "for group in anatomical_groups:\n",
        "    group_indices = [medical_terms.index(term) for term in group if term in medical_terms]\n",
        "    if len(group_indices) > 1:\n",
        "        group_positions = embedded_2d[group_indices]\n",
        "        plt.plot(group_positions[:, 0], group_positions[:, 1],\n",
        "                alpha=0.5, linewidth=1, linestyle='--')\n",
        "\n",
        "plt.title('Clustering of Term in Embedding Space')\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "t8KlLGOHBkrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Example - Step 3\n",
        "\n",
        "Finally, the code in the cell below performs a similarity analysis on the embedded terms and prints out the various relationships discovered by the analysis."
      ],
      "metadata": {
        "id": "MmSeSduAKUet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Example - Step 3\n",
        "\n",
        "# Demonstrate similarity calculations\n",
        "print(\"\\nSimilarity Analysis:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Calculate cosine similarities between some key terms\n",
        "heart_idx = medical_terms.index('heart')\n",
        "lung_idx = medical_terms.index('lung')\n",
        "brain_idx = medical_terms.index('brain')\n",
        "virus_idx = medical_terms.index('virus')\n",
        "\n",
        "heart_emb = final_embeddings[heart_idx]\n",
        "lung_emb = final_embeddings[lung_idx]\n",
        "brain_emb = final_embeddings[brain_idx]\n",
        "virus_emb = final_embeddings[virus_idx]\n",
        "\n",
        "# Calculate similarities\n",
        "sim_heart_lung = cosine_similarity([heart_emb], [lung_emb])[0][0]\n",
        "sim_heart_brain = cosine_similarity([heart_emb], [brain_emb])[0][0]\n",
        "sim_heart_virus = cosine_similarity([heart_emb], [virus_emb])[0][0]\n",
        "\n",
        "print(f\"Heart ↔ Lung similarity: {sim_heart_lung:.3f}\")\n",
        "print(f\"Heart ↔ Brain similarity: {sim_heart_brain:.3f}\")\n",
        "print(f\"Heart ↔ Virus similarity: {sim_heart_virus:.3f}\")\n",
        "\n",
        "# Show how embeddings can be used for medical applications\n",
        "print(\"Medical Applications of These Embeddings:\")\n",
        "print(\"=\" * 40)\n",
        "print(\"1. Disease Diagnosis: Finding similar symptoms and conditions\")\n",
        "print(\"2. Drug Discovery: Identifying molecular relationships\")\n",
        "print(\"3. Medical Literature Analysis: Understanding concept relationships\")\n",
        "print(\"4. Clinical Decision Support: Recommending treatments based on similarity\")\n",
        "\n",
        "# Demonstrate how to use the learned embeddings\n",
        "print(\"\\nPractical Example:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "def find_similar_terms(target_term, top_n=3):\n",
        "    \"\"\"Find most similar terms to a given medical term\"\"\"\n",
        "    if target_term not in medical_terms:\n",
        "        return f\"Term '{target_term}' not found in vocabulary\"\n",
        "\n",
        "    target_idx = medical_terms.index(target_term)\n",
        "    target_embedding = final_embeddings[target_idx]\n",
        "\n",
        "    similarities = []\n",
        "    for i, term in enumerate(medical_terms):\n",
        "        if i != target_idx:\n",
        "            similarity = cosine_similarity([target_embedding], [final_embeddings[i]])[0][0]\n",
        "            similarities.append((term, similarity))\n",
        "\n",
        "    # Sort by similarity and return top N\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_n]\n",
        "\n",
        "# Test with a few examples\n",
        "print(f\"Most similar terms to 'heart':\")\n",
        "for term, sim in find_similar_terms('heart'):\n",
        "    print(f\"  {term}: {sim:.3f}\")\n",
        "\n",
        "print(f\"\\nMost similar terms to 'cancer':\")\n",
        "for term, sim in find_similar_terms('cancer'):\n",
        "    print(f\"  {term}: {sim:.3f}\")\n",
        "\n",
        "# Show the embedding matrix\n",
        "print(f\"\\nEmbedding Matrix (first 5 terms):\")\n",
        "print(\"Each row represents a medical term's learned embedding vector\")\n",
        "print(final_embeddings[:5])\n",
        "\n",
        "print(\"\\nKey Takeaway:\")\n",
        "print(\"Medical embeddings learn to represent not just words, but their meanings and relationships.\")\n",
        "print(\"This is how AI systems understand medical concepts - by learning patterns from vast amounts of medical data.\")"
      ],
      "metadata": {
        "id": "aesskQi0KVTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Embedding Layer Example\n",
        "\n",
        "* **num_embeddings** = How large is the vocabulary?  How many categories are you encoding? This parameter is the number of items in your \"lookup table.\"\n",
        "* **embedding_dim** = How many numbers in the vector you wish to return.\n",
        "\n",
        "Now we create a neural network with a vocabulary size of 10, which will reduce those values between 0-9 to 4 number vectors. This neural network does nothing more than passing the embedding on to the output. But it does let us see what the embedding is doing. Each feature vector coming in will have two such features."
      ],
      "metadata": {
        "id": "7_GFjbN85IO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple embedding example\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=4)\n",
        "optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n",
        "loss_function = nn.MSELoss()"
      ],
      "metadata": {
        "id": "cm9sUuvB5HJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the structure of this neural network to see what is happening inside it."
      ],
      "metadata": {
        "id": "8FPeUOYo5R5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print structure\n",
        "\n",
        "print(embedding_layer)"
      ],
      "metadata": {
        "id": "PwZqrW_f5U90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image47B.png)"
      ],
      "metadata": {
        "id": "Stz-jVREnMhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this neural network, which is just an embedding layer, the input is a vector of size 2. These two inputs are integer numbers from 0 to 9 (corresponding to the requested input_dim quantity of 10 values). Looking at the summary above, we see that the embedding layer has 40 parameters. This value comes from the embedded lookup table that contains four amounts (output_dim) for each of the 10 (input_dim) possible integer values for the two inputs. The output is 2 (input_length) length 4 (output_dim) vectors, resulting in a total output size of 8, which corresponds to the Output Shape given in the summary above.\n",
        "\n",
        "Now, let us query the neural network with two rows. The input is two integer values, as was specified when we created the neural network."
      ],
      "metadata": {
        "id": "0yeTOmBR5adE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query network\n",
        "\n",
        "input_tensor = torch.tensor([[1, 2]], dtype=torch.long)\n",
        "pred = embedding_layer(input_tensor)\n",
        "\n",
        "print(input_tensor.shape)\n",
        "print(pred)\n"
      ],
      "metadata": {
        "id": "ciLevhAK5b-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image48B.png)"
      ],
      "metadata": {
        "id": "r_jrT4hxx7Th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see two length-4 vectors that PyTorch looked up for each input integer. Recall that Python arrays are zero-based. PyTorch replaced the value of 1 with the second row of the 10 x 4 lookup matrix. Similarly, PyTorch returned the value of 2 by the third row of the lookup matrix. The following code displays the lookup matrix in its entirety. The embedding layer performs no mathematical operations other than inserting the correct row from the lookup table.\n"
      ],
      "metadata": {
        "id": "5vnKbsJP5g_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out embedding weights\n",
        "\n",
        "embedding_layer.weight.data"
      ],
      "metadata": {
        "id": "eA4_7X525iy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image49B.png)"
      ],
      "metadata": {
        "id": "N0qyKeaNyb8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values above are random parameters that PyTorch generated as starting points. Generally, we will transfer an embedding or train these random values into something useful. The following section demonstrates how to embed a hand-coded embedding."
      ],
      "metadata": {
        "id": "QKH8_tVm5mgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-In**\n",
        "\n",
        "When you have completed and run all of the code cells, use the `File --> Print.. --> Microsoft Print to PDF` to generate your PDF if you are running `MS Windows`. If you have a Mac, use the `File --> Print.. --> Save as PDF`\n",
        "\n",
        "In either case, save your PDF as Copy of Class_04_4.lastname.pdf where lastname is your last name, and upload the file to Canvas.\n",
        "\n",
        "**NOTE TO WINDOWS USERS:** Your grade will be reduced by 10% if your PDF is found to be missing pages when it is being graded in Canvas. This penalty is simply meant to prevent the grader from having to take the additional steps of (1) downloading your PDF, (2) printing it out using the `Microsoft Print to PDF` and (3) having to resubmit to Canvas so they can grade it"
      ],
      "metadata": {
        "id": "r6uhybFv7HuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **UNIVAC**\n",
        "\n",
        "![___](https://upload.wikimedia.org/wikipedia/commons/2/2f/Univac_I_Census_dedication.jpg)\n",
        "\n",
        "**UNIVAC (Universal Automatic Computer)** was a line of electronic digital stored-program computers starting with the products of the Eckert–Mauchly Computer Corporation. Later the name was applied to a division of the Remington Rand company and successor organizations.\n",
        "\n",
        "The BINAC, built by the Eckert–Mauchly Computer Corporation, was the first general-purpose computer for commercial use, but it was not a success. The last UNIVAC-badged computer was produced in 1986.\n",
        "\n",
        "**UNIVAC Sperry Rand label**\n",
        "\n",
        "J. Presper Eckert and John Mauchly built the ENIAC (Electronic Numerical Integrator and Computer) at the University of Pennsylvania's Moore School of Electrical Engineering between 1943 and 1946. A 1946 patent rights dispute with the university led Eckert and Mauchly to depart the Moore School to form the Electronic Control Company, later renamed Eckert–Mauchly Computer Corporation (EMCC), based in Philadelphia, Pennsylvania. That company first built a computer called BINAC (BINary Automatic Computer) for Northrop Aviation (which was little used, or perhaps not at all). Afterwards, the development of UNIVAC began in April 1946.[1] UNIVAC was first intended for the Bureau of the Census, which paid for much of the development, and then was put in production.\n",
        "\n",
        "With the death of EMCC's chairman and chief financial backer Henry L. Straus in a plane crash on October 25, 1949, EMCC was sold to typewriter, office machine, electric razor, and gun maker Remington Rand on February 15, 1950. Eckert and Mauchly now reported to Leslie Groves, the retired army general who had previously managed building The Pentagon and led the Manhattan Project.\n",
        "\n",
        "The most famous UNIVAC product was the UNIVAC I mainframe computer of 1951, which became known for predicting the outcome of the U.S. presidential election the following year: this incident is noteworthy because the computer correctly predicted an Eisenhower landslide over Adlai Stevenson, whereas the final Gallup poll had Eisenhower winning the popular vote 51–49 in a close contest.\n",
        "\n",
        "The prediction led CBS's news boss in New York, Siegfried Mickelson, to believe the computer was in error, and he refused to allow the prediction to be read. Instead, the crew showed some staged theatrics that suggested the computer was not responsive, and announced it was predicting 8–7 odds for an Eisenhower win (the actual prediction was 100–1 in his favour).\n",
        "\n",
        "When the predictions proved true—Eisenhower defeated Stevenson in a landslide, with UNIVAC coming within 3.5% of his popular vote total and four votes of his Electoral College total—Charles Collingwood, the on-air announcer, announced that they had failed to believe the earlier prediction.\n",
        "\n",
        "The United States Army requested a UNIVAC computer from Congress in 1951. Colonel Wade Heavey explained to the Senate subcommittee that the national mobilization planning involved multiple industries and agencies: \"This is a tremendous calculating process...there are equations that can not be solved by hand or by electrically operated computing machines because they involve millions of relationships that would take a lifetime to figure out.\" Heavey told the subcommittee it was needed to help with mobilization and other issues similar to the invasion of Normandy that were based on the relationships of various groups.\n",
        "\n",
        "The UNIVAC was manufactured at Remington Rand's former Eckert-Mauchly Division plant on W Allegheny Avenue in Philadelphia, Pennsylvania. Remington Rand also had an engineering research lab in Norwalk, Connecticut, and later bought Engineering Research Associates (ERA) in St. Paul, Minnesota. In 1953 or 1954 Remington Rand merged their Norwalk tabulating machine division, the ERA \"scientific\" computer division, and the UNIVAC \"business\" computer division into a single division under the UNIVAC name. This severely annoyed those who had been with ERA and with the Norwalk laboratory.\n",
        "\n",
        "In 1955 Remington Rand merged with Sperry Corporation to become Sperry Rand. General Douglas MacArthur, then the chairman of the Board of Directors of Remington Rand, was chosen to continue in that role in the new company. Harry Franklin Vickers, then the President of Sperry Corporation, continued as president and CEO of Sperry Rand. The UNIVAC division of Remington Rand was renamed the Remington Rand Univac division of Sperry Rand. William Norris was put in charge as Vice-President and General Manager reporting to the President of the Remington Rand Division (of Sperry Rand).\n",
        "\n",
        "### **UNIVAC: Historical Development and Significance**\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "UNIVAC (Universal Automatic Computer) was the first commercially available computer in the United States, marking a pivotal moment in the history of computing. Developed in the early 1950s, UNIVAC played a crucial role in transitioning computing from experimental laboratories to practical business and government applications.\n",
        "\n",
        "**Origins and Development**\n",
        "\n",
        "### Eckert and Mauchly\n",
        "\n",
        "UNIVAC was developed by **J. Presper Eckert** and **John Mauchly**, the same engineers who created the **ENIAC** (Electronic Numerical Integrator and Computer), the first general-purpose electronic digital computer. After ENIAC, they founded the **Eckert-Mauchly Computer Corporation** in 1946 with the goal of producing a more advanced and commercially viable computer.\n",
        "\n",
        "### Design Goals\n",
        "\n",
        "UNIVAC was designed to:\n",
        "- Handle both numeric and alphabetic data.\n",
        "- Be suitable for business and administrative use.\n",
        "- Automate data processing tasks traditionally performed by punch card machines.\n",
        "\n",
        "## Key Milestones\n",
        "\n",
        "### UNIVAC I (1951)\n",
        "\n",
        "- **First Delivered**: To the U.S. Census Bureau on **June 14, 1951**.\n",
        "- **Technology**: Used vacuum tubes, mercury delay lines for memory, and magnetic tape for storage.\n",
        "- **Speed**: Could perform approximately 1,000 calculations per second.\n",
        "- **Input/Output**: Featured a typewriter-like console and tape drives.\n",
        "\n",
        "### Commercial Impact\n",
        "- **Remington Rand Acquisition**: In 1950, Eckert-Mauchly was acquired by Remington Rand, which marketed UNIVAC.\n",
        "- **Presidential Election Prediction**: UNIVAC I famously predicted the outcome of the 1952 U.S. presidential election on live television, correctly forecasting Eisenhower's victory—demonstrating the power of computing to the public.\n",
        "\n",
        "## Technical Specifications\n",
        "\n",
        "| Feature              | Specification                          |\n",
        "|----------------------|----------------------------------------|\n",
        "| Memory               | 1,000 words (12 characters each)       |\n",
        "| Word Size            | 72 bits                                |\n",
        "| Clock Speed          | 2.25 MHz                               |\n",
        "| Storage              | Magnetic tape                          |\n",
        "| Programming Language | Machine code                           |\n",
        "\n",
        "## Legacy and Influence\n",
        "\n",
        "UNIVAC's success helped establish the viability of computers in business and government. It influenced the development of subsequent systems and contributed to the growth of the American computer industry.\n",
        "\n",
        "### Successors\n",
        "\n",
        "- **UNIVAC II**: Introduced in 1958 with improved memory and performance.\n",
        "- **UNIVAC 1100 Series**: Became popular in the 1960s and 1970s for scientific and business applications.\n",
        "\n",
        "### Cultural Impact\n",
        "\n",
        "UNIVAC became a symbol of modernity and technological progress in the 1950s. Its televised election prediction helped demystify computers and sparked public interest in computing.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "UNIVAC was more than just a machine—it was a milestone in the evolution of computing. By bridging the gap between theoretical computing and practical application, it laid the foundation for the digital age.\n",
        "\n",
        "## References\n",
        "\n",
        "- Ceruzzi, Paul E. *A History of Modern Computing*. MIT Press.\n",
        "- U.S. Census Bureau Archives\n",
        "- Computer History Museum\n"
      ],
      "metadata": {
        "id": "RZrTH409Ps2P"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}