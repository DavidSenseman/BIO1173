{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* **Part 4.1: Introduction to Large Language Models (LLMs)**\n",
        "* Part 4.2: Chatbots\n",
        "* Part 4.3: Image Generation with StableDiffusion\n",
        "* Part 4.4: Image Generation with DALL-E\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **You MUST Purchase Your OpenAI API Key Now!**\n",
        "\n",
        "In order to run the code in the next few lessons, you will need to purchase an `OpenAI API key` and install your key in the `secretes` location in your Google Colab notebook. It is important to key your `OpenAI API key` secrete. If anyone learns your key, they can use it costing you a lot of money.  "
      ],
      "metadata": {
        "id": "xvyn29wc6BsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How to Purchase an OpenAI API Key**\n",
        "\n",
        "Follow these steps to obtain an API key from OpenAI:\n",
        "\n",
        "**1. Create an OpenAI Account**\n",
        "\n",
        "If you don’t already have one, go to https://platform.openai.com/api-keys and sign up using your email, Google, or Microsoft account.\n",
        "\n",
        "\n",
        "**2. Log In to the OpenAI Platform**\n",
        "\n",
        "Visit https://platform.openai.com/api-keys and log in with your credentials. After you log in you should see this page.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image17BB.png)\n",
        "\n",
        "**3. Set Up Billing**\n",
        "\n",
        "Before you can use the API you will need to add a payment method. Click on the **Settings** (gear) icon\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image18BB.png)\n",
        "\n",
        "Then click on **Billing** on the left tab.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image14BB.png)\n",
        "\n",
        "The select **Payment methods** and enter your credit card information.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image21BB.png)\n",
        "\n",
        "When you are finished adding your credit card information you need to add some money to your account. Click on **Add to Credit Balance**\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image20BB.png)\n",
        "\n",
        "Don't add too much money to your acccount since you can always add more money later if you use up your \"tokens\". Start with \\$10-\\$20.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image25BB.png)\n",
        "\n",
        "Also keep in mind that if your secret key gets stolen, all of your money might be quicky used up by the person who stole it!\n",
        "\n",
        "**4. Generate an API Key**\n",
        "\n",
        "When you have added all of your credit card information and added some money to your account, you are ready to generate your API key.\n",
        "\n",
        "Select `API Keys` on the left tab\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image23BB.png)\n",
        "\n",
        "Click on **Create new secret key** and follow the directions. The name you give your key is not important. I just called my key `FALL_2025`. Make sure your **PERMISSIONS** are set to **All**. Then select **Create secret key**\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image11C.png)\n",
        "\n",
        "Your new API key will look something like this:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image12C.png)\n",
        "\n",
        "`sk-proj-DHIIXTfLozih_0rjgx08wxExjMc0G8SDqSyVtUG5pur0pVMLF5XgO2-TX2l9ZnXOiHk7Wv4sdvT4BlbkFJGxLnypUPLYpyYzVI6aOiZMx_1LOMKdADziRqlydDTg3HpwW0g7bSNmzuy0Tb0uzeyeCbz9qNMB`\n",
        "\n",
        "**WARNING:**\n",
        "You need to  **_immediately_** copy-and-paste your API Key into either the **Notepad** app if you are working on a Windows PC or the **TextEdit** app if you have a MAC.\n",
        "\n",
        "When you have created your API key you should see a window like this:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image14C.png)\n",
        "\n",
        "-----------------------\n",
        "\n",
        "### **EXTREMELY IMPORTANT:** _Never_ share your API key publicly or commit it to version control. If your API key becomes public someone is bound to start using it. This can cost **YOU** hundreds of dollars!\n",
        "\n",
        "This actually happend to your Instructor and there is nothing you can do but pay for the tokens that were illegally stolen.\n",
        "\n",
        "-------------------\n",
        "**5. Add Your API Key to Colab Secrets**\n",
        "\n",
        "In the Colab menu:\n",
        "\n",
        "Click on the key symbol on the left tab. This is your `secrets` location\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image02B.png)\n",
        "\n",
        "\n",
        "Click on `+ Add new secret`\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image04B.png)\n",
        "\n",
        "\n",
        "In the open box _carefully_ type the word `OPENAI_API_KEY`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image06B.png)\n",
        "\n",
        "**WARNING:** Make sure that you spell and capitalize the word **_exactly_** as `OPENAI_API_KEY` or you secret key will not work!\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image08B.png)\n",
        "\n",
        "\n",
        "Paste your actual API key into the Value field.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image10B.png)\n",
        "\n",
        "Don't add any quotation marks to your API key. Just paste the actual key.\n",
        "\n",
        "When you have pasted your key in the `Value` box, click on the `+ Add new secret`\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image12B.png)\n",
        "\n",
        "Make sure to click on `Notebook access` so you use your secret `OPENAI_API_KEY` in this lesson.\n",
        "\n",
        "Finally, click on the **X** at the top right of the **Secrets** panel to restore your Colab notebook.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ARxro9Kr5H4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test Your `OPENAI_API_KEY`**\n",
        "\n",
        "To see if your `OPENAI_API_KEY` is correctly setup, run the next code cell."
      ],
      "metadata": {
        "id": "59XFG5rWAwp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify your API key setup\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Check if API key is properly loaded\n",
        "try:\n",
        "    OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"API key loaded successfully!\")\n",
        "    print(f\"Key length: {len(OPENAI_KEY)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading API key: {e}\")\n",
        "    print(\"Please set your API key in Google Colab:\")\n",
        "    print(\"1. Go to Secrets in the left sidebar\")\n",
        "    print(\"2. Create a new secret named 'openai_api_key'\")\n",
        "    print(\"3. Paste your OpenAI API key\")\n"
      ],
      "metadata": {
        "id": "JuJ2wDh55cd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1, You may see this message when you run this cell:\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image08C.png)\n",
        "\n",
        "If you do see this popup just click on `Grant access`.\n",
        "\n",
        "\n",
        "2. If your `OPENAI_API_KEY` is correctly installed you should see something _similar_ to the following output.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image09C.png)\n",
        "\n",
        "3. However, if you see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image10C.png)\n",
        "\n",
        "You will need to correct the error before you can continue. Ask your Instructor or TA for help if you can resolve the error yourself."
      ],
      "metadata": {
        "id": "KCkgqZivDg6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to Large Language Models (LLMs)**\n",
        "\n",
        "Run the next cell to see short introduction to Large Language Models (LLMs). This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "ejrc2-ZrHtRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"wjZofJX0v4M\"\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen>\n",
        "</iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "LSyT4HOEHt6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Large Language Models (LLMs)**\n",
        "\n",
        "**Large Language Models (LLMs)** such as `GPT` have brought AI into mainstream use. LLMs allow regular users to interact with AI using natural language. Most of these language models require extreme processing capabilities and hardware. Because of this, application programming interfaces (APIs) accessed through the Internet are becoming common entry points for these models. One of the most compelling features of services like ChatGPT is their availability as an API. But before we dive into the depths of coding and integration, let's understand what an API is and its significance in the AI domain.\n",
        "\n",
        "API stands for **Application Programming Interface**. Think of it as a bridge or a messenger that allows two different software applications to communicate. In the context of AI and machine learning, APIs often allow developers to access a particular model or service without having to house the model on their local machine. This technique can be beneficial when the model in question, like `ChatGPT`, is large and resource-intensive.\n",
        "\n",
        "In the realm of AI, APIs have several distinct advantages:\n",
        "\n",
        "**1 Scalability:** Since the actual model runs on external servers, developers don't need to worry about scaling infrastructure.  \n",
        "**2. Maintenance:** You get to use the latest and greatest version of the model without constantly updating your local copy.  \n",
        "**3. Cost-Effective:** Leveraging external computational resources can be more cost-effective than maintaining high-end infrastructure locally, especially for sporadic or one-off tasks.  \n",
        "**4 Ease of Use:** Instead of diving into the nitty-gritty details of model implementation and optimization, developers can directly utilize its capabilities with a few lines of code.  \n",
        "\n",
        "In this section, we won't be running the neural network computations locally. We will use our PyTorch code to communicate with the `OpenAI API` to access and harness the abilities of `ChatGPT`. The actual execution of the neural network code happens on `OpenAI servers`, bringing forth a unique synergy of PyTorch's flexibility and ChatGPT's conversational mastery. (NOTE: The physical location of these servers is not disclosed for security reasons).\n",
        "\n",
        "In this section, we will make use of the `OpenAI ChatGPT API`. Further information on this API can be found here:\n",
        "\n",
        "* [OpenAI API Login/Registration](https://platform.openai.com/apps)\n",
        "* [OpenAI API Reference](https://platform.openai.com/docs/introduction/overview)\n",
        "* [OpenAI Python API Reference](https://platform.openai.com/docs/api-reference/introduction?lang=python)\n",
        "* [OpenAI Python Library](https://github.com/openai/openai-python)\n",
        "* [OpenAI Cookbook for Python](https://github.com/openai/openai-cookbook/)\n",
        "* [LangChain](https://www.langchain.com/)\n"
      ],
      "metadata": {
        "id": "GFI9xF411UuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing LangChain to use the OpenAI Python Library**\n",
        "\n",
        "As we delve deeper into the intricacies of deep learning, it's crucial to understand that the tools and platforms we use are as versatile as the concepts themselves. When it comes to accessing ChatGPT, a state-of-the-art conversational AI model developed by OpenAI, there are two predominant pathways:\n",
        "\n",
        "**Direct API Access using Python's HTTP Capabilities:** Python, with its rich library ecosystem, provides utilities like requests to directly communicate with APIs over HTTP. This method involves crafting the necessary API calls, handling responses, and error checking, giving the developer a granular control over the process.\n",
        "\n",
        "**Using the Official OpenAI Python Library:** OpenAI offers an official Python library, aptly named openai, that simplifies the process of integrating with ChatGPT and other OpenAI services. This library abstracts many of the intricacies and boilerplate steps of direct API access, offering a streamlined and user-friendly approach to interacting with the model.\n",
        "\n",
        "Each approach has its advantages. `Direct API access` provides a more hands-on, granular approach, allowing developers to intimately understand the intricacies of each API call. On the other hand, using the `openai library` can accelerate development, reduce potential errors, and allow for a more straightforward integration, especially for those new to API interactions.\n",
        "\n",
        "We will make use of the `OpenAI API` through a library called `LangChain`. `LangChain` is a framework designed to simplify the creation of applications using LLMs. As a language model integration framework, `LangChain's` use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis. `LangChain` allows you to quickly change between different underlying LLMs with minimal code changes.\n"
      ],
      "metadata": {
        "id": "pYfiGIW4BL5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `langchain`\n",
        "\n",
        "Run the code in the next cell to install the `langchain`modules."
      ],
      "metadata": {
        "id": "5Y1nGVRB8lDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install langchain\n",
        "!pip install -q langchain langchain-openai"
      ],
      "metadata": {
        "id": "-QZfvUxS8l42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OpenAI Models Accessible via API**\n",
        "\n",
        "OpenAI provides access to a range of models via its API, designed to support diverse tasks across reasoning, creativity, and multimodal interaction. Here's an overview:\n",
        "\n",
        "### **1. GPT Models**\n",
        "- **GPT-5**  \n",
        "  The flagship model for general-purpose and agentic tasks. It supports multimodal input (text, image, audio, video), persistent memory, and advanced tool use. Available in three sizes—**GPT-5**, **GPT-5 Mini**, and **GPT-5 Nano**—with context windows up to 256k tokens. It introduces new parameters like `verbosity` and `reasoning_effort` for fine-tuned control over responses.\n",
        "\n",
        "- **GPT-4.5**  \n",
        "  A powerful model for creative tasks and agentic planning.\n",
        "\n",
        "- **GPT-4o**  \n",
        "  A high-performance model supporting text and vision inputs with a 128k context length.\n",
        "\n",
        "- **GPT-4o Mini**  \n",
        "  A smaller, cost-efficient variant optimized for lightweight multimodal tasks.\n",
        "\n",
        "### **2. Reasoning Models**\n",
        "- **o1**  \n",
        "  A frontier reasoning model with support for tools, structured outputs, and vision tasks. Offers a 200k context length.\n",
        "\n",
        "- **o3-mini**  \n",
        "  A budget-friendly reasoning model tailored for coding, math, and science. Supports tools and structured outputs.\n",
        "\n",
        "### **3. API Endpoints**\n",
        "- **Responses API**  \n",
        "  A unified endpoint combining capabilities like text/image input, web/file search, and advanced reasoning.\n",
        "\n",
        "- **Chat Completions API**  \n",
        "  Designed for conversational AI tasks, including chatbots and dialogue systems.\n",
        "\n",
        "- **Realtime API**  \n",
        "  Enables low-latency, multimodal experiences, including speech-to-speech interactions.\n",
        "\n",
        "- **Assistants API**  \n",
        "  Builds AI assistants capable of handling complex, multi-step tasks with memory and tool use.\n",
        "\n",
        "- **Batch API**  \n",
        "  Processes asynchronous workloads efficiently, offering reduced costs for large-scale operations.\n"
      ],
      "metadata": {
        "id": "98Z3C_0Uk5HZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-4o-mini\n",
        "\n",
        "For this course we will be using `GPT-4o-mini` for the following reasons:\n",
        "\n",
        "- **Price:** ~\\$0.15 per 1k tokens (≈$15 per 1M tokens)  \n",
        "- **Context window:** 32k tokens (~50k words)  \n",
        "- **Quality:** Near-GPT-4o performance, 8-times better than GPT-3.5-turbo on reasoning & accuracy  \n",
        "- **Hallucination rate:** ~10% of GPT-4o, much lower than GPT-3.5-turbo  \n",
        "- **Best use‑case:** Teaching, lesson explanations, Q&A, quiz generation, coding help, math proofs  \n",
        "- **Limitations:** No multimodal input (images/video)  \n",
        "- **Why it's a sweet spot:** Offers the advanced reasoning of GPT-4 models at a fraction of the cost, with a large context window and low hallucination.\n",
        "\n"
      ],
      "metadata": {
        "id": "RZQxzYfI4IC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set the `LLM` for this Lesson\n",
        "\n",
        "Run the next cell to specify `gpt-4o-mini` as the specific `LLM` to use for this lesson."
      ],
      "metadata": {
        "id": "jETWqMZZICno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the model you will generally use for this class\n",
        "\n",
        "# Set LLM_MODEL\n",
        "LLM_MODEL = 'gpt-4o-mini'\n",
        "\n",
        "print(f\"The Large Language Model (LLM) is set for\", LLM_MODEL)"
      ],
      "metadata": {
        "id": "CtmYoreIC9z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image13C.png)"
      ],
      "metadata": {
        "id": "ktVyWwlNFm7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt Engineering**\n",
        "\n",
        "When working with a large language model (LLM) like ChatGPT, the **prompt** serves as the foundation for interaction. It is the input or instruction provided to the model, guiding it to generate relevant and useful outputs.\n",
        "\n",
        "**1. Role of the Prompt**\n",
        "- **Instructional Guide**: The prompt shapes what the model does. Whether it's answering a question, completing a task, or writing creatively, the prompt provides the necessary context.  \n",
        "- **Boundary Setter**: A well-crafted prompt can define the scope of the task, ensuring the response is focused and doesn't deviate from the topic.  \n",
        "- **Task Optimizer**: By providing clear and concise instructions, the prompt ensures that the LLM generates responses that align with user expectations.\n",
        "\n",
        "**2. Importance of the Prompt**\n",
        "- **Determines Quality of Output**: The quality of the model's response depends heavily on the clarity and specificity of the prompt. A vague prompt can lead to irrelevant or incomplete answers, while a precise one produces accurate and valuable results.\n",
        "- **Customizable Interactions**: Prompts allow users to adapt the model to different scenarios—such as summarization, translation, or brainstorming—making it versatile and dynamic.  \n",
        "- **Reduces Ambiguity**: A good prompt minimizes room for misunderstanding, helping the model interpret the task as intended.  \n",
        "\n",
        "**3. Iterative Improvement**\n",
        "Working with LLMs is often an _iterative_  process. If the initial response isn't quite right, the user can refine the prompt, adding more detail or constraints to guide the model toward the desired result. Instead of starting over from scratch, you just edit the prompt and try it again.\n",
        "\n",
        "The prompt isn't just the input—it’s the bridge between the user’s needs and the model’s capabilities. Mastering prompt design is key to fully leveraging the potential of an LLM like ChatGPT.\n"
      ],
      "metadata": {
        "id": "XG5KgzC0dCxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Basic Query to LangChain\n",
        "\n",
        "We begin by writing a **prompt**, to ask (query) `ChatGPT` a simple question: \"What are the 5 most prestigous medical schools in the USA?\".\n",
        "\n",
        "The Python code in the cell below interacts with OpenAI's GPT model using `LangChain` and the `ChatOpenAI class` to retrieve our answer.\n",
        "\n",
        "**NOTE:** This cell will not run if you do not have a valid OpenAI_Key and you have already installed your key with Google Colab.\n"
      ],
      "metadata": {
        "id": "TTSOCfbAqc-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Basic Query\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Initialize the OpenAI LLM (Language Learning Model) with your API key\n",
        "llm = ChatOpenAI(openai_api_key=OPENAI_KEY, model=LLM_MODEL, temperature=0)\n",
        "\n",
        "# Define the question\n",
        "question = \"What are the five most prestigous medical schools in the US?\"\n",
        "\n",
        "# Use Langchain to call the OpenAI API\n",
        "response = llm.invoke(question)\n",
        "\n",
        "# Print the response\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "kyd7OBeJEgMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image20C.png)"
      ],
      "metadata": {
        "id": "K7V2Z3bOrqS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the response from `LangChain` is in regular English, complete with formatting. While the formatting may make it easier to read, we often have to parse the results given to us by LLMs.\n",
        "\n",
        "Later, we will see that `LangChain` can help with this as well. You will also notice that we specified a value of `0` for **temperature**; this instructs the LLM to be less creative with its responses and more consistent. Because we are working primarily with data extraction in this section, a low temperature will give us more consistent results.\n",
        "\n",
        "In `LangChain`, the temperature parameter typically ranges from **0.0** to **1.0**, though some implementations may allow values slightly above 1.0. The temperature controls the randomness of the model's output:\n",
        "\n",
        "* **Low Temperature (e.g., 0.0):** Produces more deterministic and focused responses, ideal for tasks requiring precision.\n",
        "\n",
        "* **High Temperature (e.g., 1.0):** Generates more creative and diverse outputs, useful for brainstorming or creative writing.\n",
        "\n",
        "If you're working with `LangChain` and `OpenAI models`, you can set the temperature when initializing the model or during runtime."
      ],
      "metadata": {
        "id": "9Gy52CVNEvEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Basic Query to LangChain**\n",
        "\n",
        "For **Exercise 1** think about a subject for a `Top Five List` that **you** find interesting and see what response you get back from `ChatGTP`.\n",
        "\n",
        "Feel free to change the **temperature** of your request if you want a more _creative_ response from `LangChain`. There are no \"right\" or \"wrong\" answers here as long as your code works."
      ],
      "metadata": {
        "id": "MvfJCF6wr-lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n"
      ],
      "metadata": {
        "id": "x_tQt3ejr-lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I am interested in guitar players, I asked `LangChain's` for a list of the 5 greatest guitart players of all time.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image16C.png)\n",
        "\n",
        "You output will be different depending up your question."
      ],
      "metadata": {
        "id": "r24AGvJYr-lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Working with Prompts\n",
        "\n",
        "As mentioned above, interactions with LLMs is typically accomplished using `prompts`. In fact, there is a whole new field called **Prompt Engineering** that focuses on designing, refining, and optimizing prompts to maximize the effectiveness and relevance of outputs generated by large language models (LLMs) like ChatGPT, GPT-4, and others.\n",
        "\n",
        "In Example 2, we will \"engineer\" a prompt that will have `ChatGPT` translate text from French to English. In this example, we will just be using normal Python F-Strings to build the prompt.\n"
      ],
      "metadata": {
        "id": "ZrnDmq8AE1P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Working with prompts\n",
        "\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Define text and style\n",
        "text = \"\"\"Laissez les bons temps rouler\"\"\"  # French text\n",
        "style = \"American English\"                  # English\n",
        "\n",
        "# Build prompt\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "# Send promt to ChatGPT\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "# Print ChatGPTs response\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "VPuxwG6U3mPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image30B.png)"
      ],
      "metadata": {
        "id": "WySFzrpTtyHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------\n",
        "\n",
        "**Why does the code Uses Triple Quotes?**\n",
        "\n",
        "The code in the cell above uses triple double quotes (\"\"\") for the prompt string to allow for clean, multi-line formatting and to include special characters, such as backticks (```) and placeholders ({style} and {text}).\n",
        "\n",
        "~~~text\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\"\"\"\n",
        "~~~\n",
        "\n",
        "-------------------------\n"
      ],
      "metadata": {
        "id": "0lPEtXSNh42d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Working with Prompts**\n",
        "\n",
        "In the cell below, use ChatGPT to translate the German expression: \"Ein Prosit der Gemütlichkeit\" into English.\n"
      ],
      "metadata": {
        "id": "uq2WA-AYuVR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n"
      ],
      "metadata": {
        "id": "kzkL7ARauVR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image31B.png)"
      ],
      "metadata": {
        "id": "VGza68NEuVR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dynamic Prompts**\n",
        "\n",
        "A **dynamic prompt** is a flexible and adaptive input designed for interaction with language models (LLMs) like `ChatGP`T, where placeholders or variables are used to customize the prompt based on context or user-provided information. This approach allows for reusability, personalization, and automation, ensuring that the output is tailored to specific needs without rewriting the entire prompt.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Key Characteristics of a Dynamic Prompt**\n",
        "1. **Variables**:\n",
        "   - Dynamic prompts include placeholders for variables, like `{name}`, `{style}`, or `{text}`, which can be filled with different values at runtime.\n",
        "   - For example:\n",
        "     ```python\n",
        "     prompt = f\"Translate this text: {text} into {language}.\"\n",
        "     ```\n",
        "     Here, `{text}` and `{language}` can be dynamically replaced by the desired input values.\n",
        "2. **Context-Aware**:\n",
        "   - They adapt to the context, such as the user’s preferences, conversation history, or specific tasks.\n",
        "   - For instance, a dynamic prompt for summarization might consider the length of the desired output: \"Summarize the following article in less than {words} words.\"\n",
        "3. **Reusable Templates**:\n",
        "   - Instead of hardcoding individual tasks, dynamic prompts use templates that can be applied across multiple scenarios by simply replacing values.\n",
        "   - Example template:\n",
        "     ```python\n",
        "     template_text = \"\"\"Write a {tone} response to the following message:\n",
        "     message: {user_message}\"\"\"\n",
        "     ```\n",
        "4. **Personalization**:\n",
        "   - Dynamic prompts can be personalized based on user inputs or profiles, enhancing user experience. For example:\n",
        "     ```python\n",
        "     f\"Hi {name}, here’s the weather forecast for {city}!\"\n",
        "     ```\n",
        "\n",
        "#### **Why Are Dynamic Prompts Important?**\n",
        "\n",
        "- **Efficiency**: They save time by enabling template reuse.\n",
        "- **Scalability**: Useful for applications needing to handle diverse inputs.\n",
        "- **Adaptability**: They produce tailored outputs depending on the specific context or task.\n",
        "- **User Experience**: Personalization through dynamic prompts improves user satisfaction.\n",
        "\n",
        "---\n",
        "\n",
        "Dynamic prompts are at the heart of effective interactions with LLMs, making them more versatile, context-aware, and user-specific."
      ],
      "metadata": {
        "id": "azkr5E3Cii37"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBcs3yAMo_P"
      },
      "source": [
        "### Example 3 - Step 1: Build a Dynamic Prompt Template\n",
        "\n",
        "We can use LangChain to help us build dynamic prompts.\n",
        "\n",
        "The first step is provide LangChain with a `template prompt`. The code in the cell below defines and creates a prompt template using LangChain's `ChatPromptTemplate` class. The prompt template is called `example_prompt_template`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 1: Build a Dynamic Prompt Template\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define prompt template\n",
        "eg_template_text = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "# Create template\n",
        "eg_prompt_template = ChatPromptTemplate.from_template(template_text)\n"
      ],
      "metadata": {
        "id": "wu_I4u1U38SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you shouldn't see any output."
      ],
      "metadata": {
        "id": "AlHkVSSFxhB4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcdwI-0-Nod"
      },
      "source": [
        "### Example 3 - Step 2: Test Dynamic Prompt\n",
        "\n",
        "Now we can fill in the blanks for this prompt and observe the prompt created, which is a text string.\n",
        "\n",
        "The code in the cell below does the following:\n",
        "\n",
        "* Dynamically generates a structured prompt based on a template.\n",
        "* Ensures the prompt includes placeholders (style and text) filled with the\n",
        "provided values.\n",
        "* Inspects the data structure and type of the resulting prompt.\n",
        "* Outputs the first message to verify its content.\n",
        "\n",
        "This code is useful for building prompts in LangChain when interacting with language models for tasks like translation, summarization, or custom instructions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 2:Test Dyanmic Prompt\n",
        "\n",
        "eg_prompt = eg_prompt_template.format_messages(\n",
        "                    style=\"American English\",\n",
        "                    text=\"千里之行，始于足下。\")\n",
        "\n",
        "# Print prompt and its features\n",
        "print(type(eg_prompt))\n",
        "print(type(eg_prompt[0]))\n",
        "\n",
        "print(eg_prompt[0])\n"
      ],
      "metadata": {
        "id": "Q2HpNiaY1NWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image32B.png)"
      ],
      "metadata": {
        "id": "OqIK6nGz31Vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are asking `ChatGPT` to translate the `text` \"千里之行，始于足下。\" into the `style` \"American English\"."
      ],
      "metadata": {
        "id": "PisALL0AkZZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3 - Step 3: Send Dynamic Prompt to `ChatGPT`\n",
        "\n",
        "Now that we have buit our dynamic prompt in Steps 1 and 2, we are ready to send it to `ChatGPT` for analysis as shown in the code below."
      ],
      "metadata": {
        "id": "PSn4MbiMGPeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 3: Send Dynamic Prompt to `ChatGPT`\n",
        "\n",
        "# Call the LLM to translate to the style of the customer message\n",
        "eg_response = llm.invoke(eg_prompt)\n",
        "\n",
        "# Print response from ChatGPT\n",
        "print(eg_response)"
      ],
      "metadata": {
        "id": "9Eb-aLHn4L8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image23C.png)"
      ],
      "metadata": {
        "id": "nn-lK6Hf3vMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This newly constructed prompt can now perform the intended task of translation."
      ],
      "metadata": {
        "id": "31sPO7R7GHv5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TphfSQIf5su_"
      },
      "source": [
        "### **Exercise 3 - Step 1: Build a Dynamic Prompt Template**\n",
        "\n",
        "In the cell below, create a prompt template called `ex_prompt_template`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 1 here\n"
      ],
      "metadata": {
        "id": "F-an-XKj5su_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you shouldn't see any output."
      ],
      "metadata": {
        "id": "lxXCSm4D5su_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAVCBV3k5su_"
      },
      "source": [
        "### **Exercise 3 - Step 2: Test Dynamic Prompt**\n",
        "\n",
        "Suppose you are standing watch at the White House and you receive this urgent message: \"Президент Трамп, русская Родина сдаётся\". Use your `ex_prompt_template` to translate this message into English."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 2 here\n"
      ],
      "metadata": {
        "id": "32bYXW8K5su_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image34B.png)"
      ],
      "metadata": {
        "id": "3n2DHLNY5svA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3 - Step 3: Send Dynamic Prompt to `ChatGPT`**\n",
        "\n",
        "Finally, send your `ex_prompt_template` to `ChatGPT` to translate the urgent message in English."
      ],
      "metadata": {
        "id": "6-GzJjFY5svA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 3 here\n"
      ],
      "metadata": {
        "id": "pGbSFa9_5svA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image35B.png)"
      ],
      "metadata": {
        "id": "oQXyYQMm5svA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLM Memory**\n",
        "\n",
        "Human minds have both long-term and short-term memory. Long-term memory is what the human has learned throughout their lifetime. Short-term memory is what a human has only recently discovered in the last minute or so. For humans, learning is converting short-term memory into long-term memory that we will retain.\n",
        "\n",
        "This process works somewhat differently for a LLM. Long-term memory was the weight of the neural network when it was initially trained or finetuned. Short-term memory is additional information that we wish the LLM to retain from previous prompts. For example, if the first prompt is \"My name is David\", the LLM will likely tell you hello and repeat your name. However, the LLM will not know the answer if the second prompt is \"What is my name.\" without adding a memory component.\n",
        "\n",
        "These memory objects, which `LangChain` provides, provide a sort of short-term memory. It is important to note that these objects are not affecting the long-term memory of the LLM, and once you discard the memory object, the LLM will forget. Additionally, the memory object can only hold so much information; newer information may replace older information once it is filled.\n",
        "\n",
        "One important point to remember is that LLM's only have their input prompt. To provide such memory, these objects are appending anything we wish the LLM to remember to the input prompt. This section will see two ways to augment the prompt with previous information: a buffer and a summary. The buffer prepends a script of the last conversation up to this point. The summary approach keeps a consistently updated summary paragraph of the conversation."
      ],
      "metadata": {
        "id": "OqG349Nm3WMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conversation Buffer Window Memory**\n",
        "\n",
        "The `LangChain library` includes a conversation object named **ConversationBufferMemory**; this object facilitates an ongoing conversation with an LLM. For any conversation object, you must also specify a memory. For this ;esspn we will use a simple approach as shown in the cell below."
      ],
      "metadata": {
        "id": "KOr_RRl53huN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Conversation Buffer Window Memory\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model=LLM_MODEL, temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Create a simple memory (no complex ConversationBufferMemory that causes pickling issues)\n",
        "memory = InMemoryChatMessageHistory()\n",
        "\n",
        "# Create the runnable with message history\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def format_messages(messages):\n",
        "    return messages\n",
        "\n",
        "# Simple approach - use a basic conversation flow\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# For now, let's go back to the original simple approach that should work:\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "pL64TJXbgdRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any output."
      ],
      "metadata": {
        "id": "2MvBMMClhuqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4A: Talk to the `LLM`\n",
        "\n",
        "We can now have a conversation with the LLM. This newly constructed prompt can now perform the intended task of translation.\n"
      ],
      "metadata": {
        "id": "CdM2Z_qT3rY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4A: Talk to the LLM\n",
        "\n",
        "conversation.predict(input=\"Hi, my name is David. I am a professor at UT San Antonio\")"
      ],
      "metadata": {
        "id": "aPOUhgWm3wrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image21C.png)"
      ],
      "metadata": {
        "id": "kdLRE4DFKTpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4B: Examine Memory Buffer\n",
        "\n",
        "The code in the next cell shows have we can take a look at the memory buffer now.\n"
      ],
      "metadata": {
        "id": "W3M-I6Y237H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4B: Examine Memory Buffer\n",
        "\n",
        "# Print out memory buffer\n",
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "NvFazK1y4B3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image22C.png)"
      ],
      "metadata": {
        "id": "8K1IOxzNLQqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4A: Talk to the `LLM`**\n",
        "\n",
        "In the cell below, say something to the `LLM`.\n"
      ],
      "metadata": {
        "id": "nQN2uVEI7k7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4A here\n"
      ],
      "metadata": {
        "id": "LL0aVR3J7k7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output will depend upon your input."
      ],
      "metadata": {
        "id": "Umr6twll7k7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4B: Examine Memory Buffer**\n",
        "\n",
        "In the next cell write the code to take look at the memory buffer.\n"
      ],
      "metadata": {
        "id": "jcDMAMkp7k7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4B here\n",
        "\n",
        "# Print out memory buffer\n",
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "TenpwWjo7k7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The memory buffer should contain the conversations from the example as well as the exercise."
      ],
      "metadata": {
        "id": "CUPaOoS67k7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Custom Conversation Bots**\n",
        "\n",
        "A **Custom Conversation Bot** is an AI-powered system that:\n",
        "- Engages in natural language conversations\n",
        "- Is customized for a specific domain, task, or personality\n",
        "- Can be deployed on websites, apps, or messaging platforms\n",
        "\n",
        "#### Key Features\n",
        "\n",
        "- **Custom Instructions**: Define how the bot should behave, respond, and what tone it should use.\n",
        "- **Knowledge Integration**: Connects to external data sources or APIs.\n",
        "- **User Memory**: Can remember user preferences or past interactions.\n",
        "- **Multimodal Capabilities**: Some bots can handle text, images, and even voice.\n",
        "\n",
        "#### Common Use Cases\n",
        "\n",
        "- Customer support\n",
        "- Educational tutoring\n",
        "- Personal assistants\n",
        "- Sales and marketing\n",
        "- Healthcare triage\n",
        "- Entertainment and storytelling\n",
        "\n",
        "#### Summary\n",
        "\n",
        "Custom conversation bots are flexible, intelligent tools that can be tailored to meet specific needs, making them valuable across industries and applications."
      ],
      "metadata": {
        "id": "68eHC1aG4Hpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Custom Conversation Bot\n",
        "\n",
        "In Example 5 we are going to create a custom conversational bot named \"AI Physican's Assistant\" that is designed to help screen phone calls to a medical clinic.\n",
        "\n",
        "The first step is to create a `template` that defines the focus of the `Bot` and how it responds to questions. The template includes a list of **Important guidlines** that focus and restrict the Bot's responses.\n",
        "\n",
        "As in previous examples, the prefix \"eg_\" has been added to key variables to separate the Bot in the example form the Bot you will make in the next exercise."
      ],
      "metadata": {
        "id": "JkSpisR5t7Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Custom Conversation Bot\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain # Add this import\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from google.colab import userdata\n",
        "\n",
        "# It is good practice to explicitly import ConversationChain\n",
        "# from langchain.chains.conversation.base import ConversationChain\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "LLM_MODEL = \"gpt-4o-mini\" # Example model name\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model=LLM_MODEL, temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Create your prompt template\n",
        "eg_template = \"\"\"The following is a friendly conversation between a human and a medical AI Physician's Assistant.\n",
        "The Physician's Assistant should only discuss topics related to renal health issues.\n",
        "If the user asks about other medical conditions, politely redirect them to a general practitioner.\n",
        "If the user asks about non-medical topics, gently steer them back to health-related questions about renal function.\n",
        "\n",
        "Important guidelines:\n",
        "- Focus only medical issues related to the kidney and renal function\n",
        "- Use professional but friendly language\n",
        "- Ask follow-up questions to better understand symptoms\n",
        "- Do not provide diagnoses or treatment recommendations\n",
        "- Only as a last resort recommend seeing a doctor\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Patient: {input}\n",
        "Physician's Assistant:\"\"\"\n",
        "\n",
        "eg_PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=eg_template) # Changed `template` to `eg_template`\n",
        "\n",
        "# Use basic memory approach that should work without pickling issues\n",
        "eg_memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
        "\n",
        "# Corrected variable name from `memory` to `eg_memory`\n",
        "eg_conversation = ConversationChain(\n",
        "    prompt=eg_PROMPT,\n",
        "    llm=llm,\n",
        "    memory=eg_memory, # Corrected: Use the 'eg_memory' object you defined\n",
        "    verbose=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "09Q45FsA96E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any output:"
      ],
      "metadata": {
        "id": "MTSQoTGplSji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Conversing with Custom Bot\n",
        "\n",
        "We can now have a conversation with our AI Physician's Assistant. We start by explaining that we are an 82 year old woman who is having trouble peeing.  "
      ],
      "metadata": {
        "id": "9NFBCWyVt7Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Question 1"
      ],
      "metadata": {
        "id": "Vcl6kaSnt7Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Question 1\n",
        "\n",
        "import textwrap\n",
        "WIDTH = 64  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, I am having trouble peeing\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "eg_result = eg_conversation.predict(input=Conversation)\n",
        "\n",
        "wrapped_text = textwrap.fill(eg_result, width=WIDTH)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "CoiQHSTOt7Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image24C.png)"
      ],
      "metadata": {
        "id": "01iQEAeDH4_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Question 2\n",
        "\n",
        "The patient explains her symptoms..."
      ],
      "metadata": {
        "id": "SB5pi0yEt7Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Question 2\n",
        "\n",
        "import textwrap\n",
        "WIDTH = 64  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, I only have pain on my left side\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "eg_result = eg_conversation.predict(input=Conversation)\n",
        "\n",
        "wrapped_text = textwrap.fill(eg_result, width=WIDTH)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "qGrmUBOlt7Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image25C.png)\n"
      ],
      "metadata": {
        "id": "9l_KV_j-t7Mc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Question 3\n",
        "\n",
        "The verbal exchange continues..."
      ],
      "metadata": {
        "id": "CS60q9-tt7Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Question 3\n",
        "\n",
        "import textwrap\n",
        "WIDTH = 64  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, The pain isn't too bad, but I should mention that my urine is dark brown in color\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "eg_result = eg_conversation.predict(input=Conversation)\n",
        "\n",
        "wrapped_text = textwrap.fill(eg_result, width=WIDTH)\n",
        "print(wrapped_text)\n"
      ],
      "metadata": {
        "id": "C-GJJFQOt7Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image26C.png)\n"
      ],
      "metadata": {
        "id": "LifrhF_v6_wW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Question 4\n",
        "\n",
        "The patient starts to go a little off target..."
      ],
      "metadata": {
        "id": "4w9yAuy_t7Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Question 4\n",
        "\n",
        "import textwrap\n",
        "WIDTH = 64  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, Thank-you for listening to me. It's hard to find a friendy ear these days\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "eg_result = eg_conversation.predict(input=Conversation)\n",
        "\n",
        "wrapped_text = textwrap.fill(eg_result, width=WIDTH)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "yBt8Gviyt7Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image27C.png)\n"
      ],
      "metadata": {
        "id": "jix-sDeP7uWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Question 5\n",
        "\n",
        "The patient now switches to a  more philosophical question..."
      ],
      "metadata": {
        "id": "cPralGumt7Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Question 5\n",
        "\n",
        "import textwrap\n",
        "WIDTH = 64  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "# Enter conversation here\n",
        "Conversation=\"Doc, you are so smart! Can you tell me the meaning of life?\"\n",
        "\n",
        "# Send conversation to Custom Bot for processing\n",
        "eg_result = eg_conversation.predict(input=Conversation)\n",
        "\n",
        "wrapped_text = textwrap.fill(eg_result, width=WIDTH)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "f_T-ypJmt7Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image28C.png)\n"
      ],
      "metadata": {
        "id": "DT9jGvoZ8rF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Bot isn't interested in talking philosophy..."
      ],
      "metadata": {
        "id": "W7D_J2tL_ZGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Print Out Memory\n",
        "\n",
        "The code in the cell below allows us to have a look at what the memory buffer now contains."
      ],
      "metadata": {
        "id": "1YwSKBRqt7Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Print out memory\n",
        "\n",
        "eg_conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "1R_mwVm_t7Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct the first part of the output should look something like this:\n",
        "\n",
        "```text\n",
        "{'history': [HumanMessage(content='Doc, I am having trouble peeing', additional_kwargs={}, response_metadata={}),\n",
        "  AIMessage(content='I\\'m sorry to hear that you\\'re experiencing trouble with urination. It’s important to understand more about your symptoms. Can you describe what you mean by \"trouble peeing\"? Are you experiencing pain, a change in frequency, or difficulty starting or stopping urination?', additional_kwargs={}, response_metadata={}),\n",
        "  HumanMessage(content='Doc, I only have pain on my left side', additional_kwargs={}, response_metadata={}),\n",
        "......\n",
        "```"
      ],
      "metadata": {
        "id": "EdtVtDuWJLCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Custom Conversational Bot**\n",
        "\n",
        "For **Exercise 5** you are design your very own **Custom Conversation Bot** and then ask it 5 questions that are within its field of focus.\n",
        "\n",
        "It is expected that every student will pick a **_different_** focus and use **_different_** questions (i.e. don't `copy-and-paste` from your `coding buddy`).\n",
        "\n",
        "**Coding Hints**\n",
        "\n",
        "As in earlier lessons, you will need to change the prefix \"eg_\" to \"ex_\". If you don't your conversations will become part of the conversations created above in Example 5."
      ],
      "metadata": {
        "id": "sx3HO7dnTjQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Create Chat Bot**\n",
        "\n",
        "In the cell below, write the code to create your `Custom Chat Bot`."
      ],
      "metadata": {
        "id": "_Ktl20_-a_kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Create Custom Conversation Bot\n"
      ],
      "metadata": {
        "id": "eLGvXE_qWAVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Questions**\n",
        "\n",
        "Use the next 5 blank code cells to ask your Custom Chat Bot questions.  "
      ],
      "metadata": {
        "id": "RhJOmcHMXMy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Question 1**"
      ],
      "metadata": {
        "id": "hp3nyzp3fPWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Question 1 here\n",
        "\n"
      ],
      "metadata": {
        "id": "5wbfdTLXXgNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Question 2**"
      ],
      "metadata": {
        "id": "nACgfG9QfWC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Question 2 here\n"
      ],
      "metadata": {
        "id": "JQ5xCM4eYd6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Question 3**"
      ],
      "metadata": {
        "id": "me_w4NjufYiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Question 3 here\n"
      ],
      "metadata": {
        "id": "7qpaHGFqYucb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p_Isl2pGun7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Question 4 here\n"
      ],
      "metadata": {
        "id": "lEunAZ6mZBSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Question 5**"
      ],
      "metadata": {
        "id": "DnZhUtv8ffeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Question 5 here\n"
      ],
      "metadata": {
        "id": "FH6bzF8CZbzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Print Out Memory**"
      ],
      "metadata": {
        "id": "1vD9Pfe0fjb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x11VJ34IH2B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 5: Print History here\n"
      ],
      "metadata": {
        "id": "VAlby4HJXNrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLMs Have Different Functions**\n",
        "\n",
        "LLMs can also be distinguished by their specialized functions, which are often achieved through fine-tuning.\n",
        "\n",
        "* **Base/Foundation LLMs:** These are the initial LLMs trained on a massive, diverse, and unlabeled dataset. They have a broad understanding of language but are not inherently designed for instruction following.\n",
        "* * **Example:** GPT-3 before instruction-tuning.\n",
        "\n",
        "* **Instruct Models:** Fine-tuned to follow specific instructions or prompts. This training makes them more useful for direct task completion.\n",
        "* **Chat Models:** A specialized type of instruct model, further fine-tuned using conversational data to perform well in dialogue-based interactions.\n",
        "Code Models: Trained on extensive datasets of code to perform programming-related tasks, like generating, summarizing, and debugging code.\n",
        "* **Multimodal Models:** Can process and generate content across multiple data types, such as text, images, and audio. They combine different encoding modalities to understand and act on complex prompts.\n",
        "* * **Example:** OpenAI's GPT-4o (\"omni\") is a multimodal model.\n",
        "* **Hybrid Models:** Combine the strengths of different models and techniques. A common hybrid approach uses a powerful LLM for reasoning alongside a Retrieval-Augmented Generation (RAG) system for accessing up-to-date, authoritative information from an external knowledge base"
      ],
      "metadata": {
        "id": "7ajvqdrpAYCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Image Analysis with GTP-4o-mini**\n",
        "\n",
        "**GPT-4o-mini** is a compact, efficient multimodal AI model that can process and reason with images, text, and audio. It can be applied to analyze biological and medical images by leveraging its ability to \"think with\" visual content, rather than just interpreting it. However, as a smaller model, it is generally better suited for specific, high-volume tasks and as part of a larger, more precise analytical pipeline, rather than for delivering definitive, complex diagnoses on its own.\n",
        "\n",
        "#### **Applications in Biological Imaging**\n",
        "\n",
        "GPT-4o-mini is useful for tasks that benefit from its speed, efficiency, and understanding of visual patterns in biological contexts.\n",
        "* **Microscopy image analysis:** The model can be used for classification tasks, such as differentiating between wild-type and mutated cells, or identifying certain features in tissue samples. It can generate descriptive captions for images, which is valuable for data curation.\n",
        "* **Pattern and texture analysis:** In diagnostics, AI can analyze subtle patterns in biological samples that may be hard for humans to detect. For example, AI can analyze drying patterns in tears to detect signs of dry-eye disease or in blood to help screen for conditions like leukemia.\n",
        "* **Data filtering and preprocessing:** Due to its cost-effectiveness, GPT-4o-mini can be used in data pipelines to filter and curate large sets of biological images. This helps democratize access to high-quality training data for more powerful, domain-specific models.\n",
        "\n",
        "#### **Applications in Medical Imaging**\n",
        "\n",
        "In medical imaging, GPT-4o-mini can act as a component within a larger workflow, performing initial screening and high-throughput tasks, though it has limitations for definitive clinical decision-making.\n",
        "\n",
        "* **High-volume screening:** The model can be used for high-volume, low-cost tasks like summarizing daily patient messages related to imaging or extracting structured data from scanned medical forms.\n",
        "* **Patient data analysis:** It can help healthcare professionals analyze patient data by identifying patterns in images when paired with contextual information. This can assist in decision-making or provide support for treatment plans.\n",
        "* **Preliminary assessment (with limitations):** Studies have shown that multimodal models can identify anatomical regions, modalities, and sometimes pathologies in images like CT scans and X-rays. However, these studies also highlight that such models can produce \"hallucinations\" or inaccuracies, especially with complex interpretations. The \"all or nothing\" accuracy of GPT-4o in radiology demonstrates that it should not be used for definitive diagnoses.\n",
        "* **Modular pipelines:** For more robust analysis, GPT-4o-mini can serve as a cheap and fast initial filter in a multi-step diagnostic process. The image can then be sent to a human expert or a more specialized model for final verification.\n",
        "\n",
        "#### **Limitations and considerations**\n",
        "\n",
        "While useful, GPT-4o-mini's application in medical imaging has key limitations that must be addressed:\n",
        "\n",
        "* **Diagnostic accuracy:** The model lacks the precision and reliability for complex diagnostic tasks. Its outputs must always be reviewed and verified by a human expert.\n",
        "Not optimized for OCR: GPT-4o vision capabilities are not designed for highly accurate optical character recognition (OCR) and may struggle with extracting precise text from images.\n",
        "* **Hallucinations:** Like many large language models, it can sometimes produce incorrect or fabricated information, which is a significant risk in a clinical setting.\n",
        "* **Ethical considerations:** The use of AI in healthcare carries ethical responsibilities. The probabilistic nature of the model's responses means that it may refuse to answer some requests or produce inconsistent answers for the same image"
      ],
      "metadata": {
        "id": "EOZG2cVePfd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6: Image Analysis with GPT-4o-mini\n",
        "\n",
        "In this example we are going to use `gpt-4o-mini` to analyze a histological image of a normal (healthy) human small intestine. The image, `eg_medical_image.png` is stored on the course fileserver and will be downloaded as part of the code example shown in the next cell.\n",
        "\n"
      ],
      "metadata": {
        "id": "_drb79eB2n_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: Image Analysis with GTP-4o-mini\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "from google.colab import files\n",
        "import base64\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the LLM\n",
        "eg_multimodal_llm  = ChatOpenAI(model=LLM_MODEL, temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "print(\"Multimodal Task Example: Image Analysis with GPT-4o-mini\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Download image from your webserver\n",
        "eg_image_url = \"https://biologicslab.co:/BIO1173/images/class_04/eg_medical_image.png\"\n",
        "eg_response = requests.get(eg_image_url)\n",
        "eg_image = Image.open(io.BytesIO(eg_response.content))\n",
        "\n",
        "print(\"Image downloaded successfully!\")\n",
        "print(f\"Image dimensions: {eg_image.size}\")\n",
        "print(f\"Image mode: {eg_image.mode}\")\n",
        "\n",
        "# Display the image\n",
        "display(eg_image)\n",
        "\n",
        "# Create a multimodal prompt for image analysis\n",
        "eg_image_analysis_prompt = \"\"\"\n",
        "You are analyzing an histology image of a section of a healthy human small intestine.\n",
        "\n",
        "Provide your analysis in the following format:\n",
        "1. Identify the basic structures\n",
        "2. The different layerss of the intestine like mucosa, submucose, muscularis externa and serosa.\n",
        "3. Point out different cell types such as villi, goblet cells and crypts.\n",
        "4. Describe the basic function of each cell type\n",
        "5. Explain the tissue organization of the small intestine in the context of form and function\n",
        "\n",
        "Histological Image Analysis Response:\"\"\"\n",
        "\n",
        "# Convert image to base64 for sending to LLM\n",
        "def image_to_base64(eg_image_path):\n",
        "    with open(eg_image_path, \"rb\") as eg_image_file:\n",
        "        return base64.b64encode(eg_image_file.read()).decode('utf-8')\n",
        "\n",
        "print(\"Histological Image Analysis\")\n",
        "print(\"-\" * 40)\n",
        "eg_text_analysis = eg_multimodal_llm.invoke([\n",
        "    HumanMessage(content=[\n",
        "        {\"type\": \"text\", \"text\": eg_image_analysis_prompt}\n",
        "    ])\n",
        "])\n",
        "\n",
        "print(eg_text_analysis.content)\n"
      ],
      "metadata": {
        "id": "6DY9qvYWPgCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6: Image Analysis with GPT-4o-mini**\n",
        "\n",
        "In the cell below write the code to analyze a histological image of healthy human kidney. The image is called \"ex_medical_image.png\" and is stored on the course file server.\n",
        "\n",
        "**Code Hints**\n",
        "\n",
        "1. Make sure to change this code snippet:\n",
        "```text\n",
        "# Create a multimodal prompt for image analysis\n",
        "eg_image_analysis_prompt = \"\"\"\n",
        "You are analyzing an histology image of a section of a healthy human small intestine.\n",
        "```\n",
        "to read as:\n",
        "```text\n",
        "# Create a multimodal prompt for image analysis\n",
        "ex_image_analysis_prompt = \"\"\"\n",
        "You are analyzing an histology image of a section of a healthy human kidney.\n",
        "```\n",
        "\n",
        "Make sure to replace all of the \"eg_\" prefixes with \"ex_\" to keep the data in this exercise separate from the data in the example.\n",
        "\n"
      ],
      "metadata": {
        "id": "N3If596bDFbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 6 here\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "from google.colab import files\n",
        "import base64\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the LLM\n",
        "ex_multimodal_llm  = ChatOpenAI(model=LLM_MODEL, temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "print(\"Multimodal Task Example: Image Analysis with GPT-4o-mini\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Download image from your webserver\n",
        "ex_image_url = \"https://biologicslab.co:/BIO1173/images/class_04/ex_medical_image.png\"\n",
        "ex_response = requests.get(ex_image_url)\n",
        "ex_image = Image.open(io.BytesIO(ex_response.content))\n",
        "\n",
        "print(\"Image downloaded successfully!\")\n",
        "print(f\"Image dimensions: {eg_image.size}\")\n",
        "print(f\"Image mode: {eg_image.mode}\")\n",
        "\n",
        "# Display the image\n",
        "display(ex_image)\n",
        "\n",
        "# Create a multimodal prompt for image analysis\n",
        "ex_image_analysis_prompt = \"\"\"\n",
        "You are analyzing an histology image of a section of a healthy human kidney.\n",
        "\n",
        "Provide your analysis in the following format:\n",
        "1. Identify the basic structures\n",
        "2. The different layerss of the intestine like mucosa, submucose, muscularis externa and serosa.\n",
        "3. Point out different cell types such as villi, goblet cells and crypts.\n",
        "4. Describe the basic function of each cell type\n",
        "5. Explain the tissue organization of the small intestine in the context of form and function\n",
        "\n",
        "Histological Image Analysis Response:\"\"\"\n",
        "\n",
        "# Convert image to base64 for sending to LLM\n",
        "def image_to_base64(ex_image_path):\n",
        "    with open(ex_image_path, \"rb\") as ex_image_file:\n",
        "        return base64.b64encode(ex_image_file.read()).decode('utf-8')\n",
        "\n",
        "print(\"Histological Image Analysis\")\n",
        "print(\"-\" * 40)\n",
        "ex_text_analysis = ex_multimodal_llm.invoke([\n",
        "    HumanMessage(content=[\n",
        "        {\"type\": \"text\", \"text\": ex_image_analysis_prompt}\n",
        "    ])\n",
        "])\n",
        "\n",
        "print(ex_text_analysis.content)\n"
      ],
      "metadata": {
        "id": "rjEQ6-u6DFbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What are Embedding Layers in PyTorch**\n",
        "\n",
        "[Embedding Layers](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) are a handy feature of PyTorch that allows the program to automatically insert additional information into the data flow of your neural network. An embedding layer would automatically allow you to insert vectors in the place of word indexes.  \n",
        "\n",
        "Programmers often use embedding layers with Natural Language Processing (NLP); however, you can use these layers when you wish to insert a lengthier vector in an index value place. In some ways, you can think of an embedding layer as dimension expansion. However, the hope is that these additional dimensions provide more information to the model and provide a better score."
      ],
      "metadata": {
        "id": "HrvnI_Bc5FsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xan6g8XzGjv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Example - Step 1\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textwrap\n",
        "WIDTH = 80  # Adjust the width to fit your Colab notebook\n",
        "\n",
        "\n",
        "# Create a medical vocabulary (common terms in medicine)\n",
        "medical_terms = [\n",
        "    'heart', 'lung', 'kidney', 'liver', 'brain', 'stomach',\n",
        "    'bone', 'muscle', 'skin', 'blood', 'nerve', 'virus',\n",
        "    'bacteria', 'infection', 'fever', 'pain', 'diabetes',\n",
        "    'hypertension', 'stroke', 'cancer', 'allergy', 'immune'\n",
        "]\n",
        "\n",
        "print(f\"Medical vocabulary: {len(medical_terms)} terms\")\n",
        "wrapped_text = textwrap.fill(f\"Medical terms: {medical_terms}\", width=WIDTH)\n",
        "print(wrapped_text)\n",
        "\n",
        "# Create embedding layer - learning representations of medical terms\n",
        "embedding_layer = nn.Embedding(num_embeddings=len(medical_terms), embedding_dim=6)\n",
        "optimizer = optim.Adam(embedding_layer.parameters(), lr=0.01)\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Define some semantic relationships (in real scenarios, this would come from data)\n",
        "training_pairs = [\n",
        "    # (term1_index, term2_index) - terms that are anatomically related\n",
        "    (0, 6),  # heart vs bone\n",
        "    (1, 7),  # lung vs muscle\n",
        "    (2, 8),  # kidney vs skin\n",
        "    (3, 9),  # liver vs blood\n",
        "    (4, 10), # brain vs nerve\n",
        "    (11, 12), # virus vs bacteria\n",
        "    (13, 14), # infection vs fever\n",
        "    (15, 16), # pain vs diabetes\n",
        "    (17, 18), # hypertension vs stroke\n",
        "    (19, 20), # cancer vs allergy\n",
        "]\n",
        "\n",
        "# Create target embeddings that represent relationships\n",
        "target_embeddings = torch.zeros(len(medical_terms), 6)\n",
        "\n",
        "print(\"\\nTraining Embeddings to Reveal Medical Relationships...\")\n",
        "\n",
        "# Training loop - this simulates the neural network learning medical concepts\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # For each training pair, we want similar embeddings for related terms\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (term1_idx, term2_idx) in enumerate(training_pairs):\n",
        "        # Get embeddings for both terms\n",
        "        emb1 = embedding_layer(torch.tensor([term1_idx]))\n",
        "        emb2 = embedding_layer(torch.tensor([term2_idx]))\n",
        "\n",
        "        # Simple loss: make embeddings of related terms more similar\n",
        "        loss = torch.norm(emb1 - emb2)  # Distance between embeddings\n",
        "        total_loss += loss\n",
        "\n",
        "    # Average loss over all pairs\n",
        "    avg_loss = total_loss / len(training_pairs)\n",
        "\n",
        "    # Backpropagation (this is where the \"learning\" happens!)\n",
        "    avg_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {avg_loss.item():.4f}\")\n",
        "\n",
        "print(\"\\nTraining Complete!\")\n",
        "print(\"The neural network has learned how to represent medical terms in a meaningful way.\")\n",
        "\n",
        "# Get final embeddings\n",
        "final_embeddings = embedding_layer.weight.data.detach().numpy()"
      ],
      "metadata": {
        "id": "R_RH2CgjGk6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image29C.png)"
      ],
      "metadata": {
        "id": "sxNxyOU2JT9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Example - Step 1\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textwrap\n",
        "WIDTH = 80  # Adjust the width to fit your Colab notebook"
      ],
      "metadata": {
        "id": "-Yc164fxJl6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adjustText > /dev/null"
      ],
      "metadata": {
        "id": "AhcaxZv0MpVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Example - Step 2\n",
        "\n",
        "The code in the cell below uses a very powerful technique to discover the interrelations called **Principal Component Analysis (PCA)** to reduce the dimensions of the data from 6 dimensions to 2 dimensions (`X` and `Y`) so the embedding relationships can be visualized in a 2d plot."
      ],
      "metadata": {
        "id": "DyJTIT6tSU0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Example - Step 2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from adjustText import adjust_text\n",
        "\n",
        "# Get final embeddings\n",
        "final_embeddings = embedding_layer.weight.data.detach().numpy()\n",
        "\n",
        "# Visualize the embeddings using PCA (reducing 6D to 2D for visualization)\n",
        "pca = PCA(n_components=2)\n",
        "embedded_2d = pca.fit_transform(final_embeddings)\n",
        "\n",
        "print(f\"\\nPCA Analysis:\")\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.3f}\")\n",
        "\n",
        "# Create a more interesting visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 8))  # 10,8\n",
        "\n",
        "# Plot the embeddings in 2D space\n",
        "ax.scatter(embedded_2d[:, 0], embedded_2d[:, 1], s=100, alpha=0.7, c='blue')\n",
        "\n",
        "'''\n",
        "# Add labels for each medical term\n",
        "for i, (term, (x, y)) in enumerate(zip(medical_terms, embedded_2d)):\n",
        "    plt.annotate(term, (x, y), xytext=(5, 5), textcoords='offset points',\n",
        "                fontsize=6, alpha=0.8)\n",
        "'''\n",
        "# Create and collect labels\n",
        "texts = []\n",
        "for i, (term, (x, y)) in enumerate(zip(medical_terms, embedded_2d)):\n",
        "    texts.append(ax.annotate(term, (x, y), xytext=(5, 5), textcoords='offset points', fontsize=8))\n",
        "\n",
        "#plt.annotate(term, (x, y), xytext=(10, 10), textcoords='offset points', fontsize=6, alpha=0.8)\n",
        "\n",
        "# Automatically adjust overlapping labels\n",
        "adjust_text(texts, ax=ax)\n",
        "\n",
        "# Highlight anatomical relationships\n",
        "anatomical_groups = [\n",
        "    ['heart', 'lung', 'kidney', 'liver', 'brain'],   # Organ system\n",
        "    ['bone', 'muscle', 'skin'],                      # Body structure\n",
        "    ['virus', 'bacteria', 'infection', 'fever'],     # Pathology\n",
        "    ['diabetes', 'hypertension', 'stroke', 'cancer'] # Disease types\n",
        "]\n",
        "\n",
        "# Draw connections between related terms\n",
        "for group in anatomical_groups:\n",
        "    group_indices = [medical_terms.index(term) for term in group if term in medical_terms]\n",
        "    if len(group_indices) > 1:\n",
        "        group_positions = embedded_2d[group_indices]\n",
        "        plt.plot(group_positions[:, 0], group_positions[:, 1],\n",
        "                alpha=0.5, linewidth=1, linestyle='--')\n",
        "\n",
        "plt.title('Clustering of Term in Embedding Space')\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "t8KlLGOHBkrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Example - Step 3\n",
        "\n",
        "Finally, the code in the cell below performs a similarity analysis on the embedded terms and prints out the various relationships discovered by the analysis."
      ],
      "metadata": {
        "id": "MmSeSduAKUet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Example - Step 3\n",
        "\n",
        "# Demonstrate similarity calculations\n",
        "print(\"\\nSimilarity Analysis:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Calculate cosine similarities between some key terms\n",
        "heart_idx = medical_terms.index('heart')\n",
        "lung_idx = medical_terms.index('lung')\n",
        "brain_idx = medical_terms.index('brain')\n",
        "virus_idx = medical_terms.index('virus')\n",
        "\n",
        "heart_emb = final_embeddings[heart_idx]\n",
        "lung_emb = final_embeddings[lung_idx]\n",
        "brain_emb = final_embeddings[brain_idx]\n",
        "virus_emb = final_embeddings[virus_idx]\n",
        "\n",
        "# Calculate similarities\n",
        "sim_heart_lung = cosine_similarity([heart_emb], [lung_emb])[0][0]\n",
        "sim_heart_brain = cosine_similarity([heart_emb], [brain_emb])[0][0]\n",
        "sim_heart_virus = cosine_similarity([heart_emb], [virus_emb])[0][0]\n",
        "\n",
        "print(f\"Heart ↔ Lung similarity: {sim_heart_lung:.3f}\")\n",
        "print(f\"Heart ↔ Brain similarity: {sim_heart_brain:.3f}\")\n",
        "print(f\"Heart ↔ Virus similarity: {sim_heart_virus:.3f}\")\n",
        "\n",
        "# Show how embeddings can be used for medical applications\n",
        "print(\"Medical Applications of These Embeddings:\")\n",
        "print(\"=\" * 40)\n",
        "print(\"1. Disease Diagnosis: Finding similar symptoms and conditions\")\n",
        "print(\"2. Drug Discovery: Identifying molecular relationships\")\n",
        "print(\"3. Medical Literature Analysis: Understanding concept relationships\")\n",
        "print(\"4. Clinical Decision Support: Recommending treatments based on similarity\")\n",
        "\n",
        "# Demonstrate how to use the learned embeddings\n",
        "print(\"\\nPractical Example:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "def find_similar_terms(target_term, top_n=3):\n",
        "    \"\"\"Find most similar terms to a given medical term\"\"\"\n",
        "    if target_term not in medical_terms:\n",
        "        return f\"Term '{target_term}' not found in vocabulary\"\n",
        "\n",
        "    target_idx = medical_terms.index(target_term)\n",
        "    target_embedding = final_embeddings[target_idx]\n",
        "\n",
        "    similarities = []\n",
        "    for i, term in enumerate(medical_terms):\n",
        "        if i != target_idx:\n",
        "            similarity = cosine_similarity([target_embedding], [final_embeddings[i]])[0][0]\n",
        "            similarities.append((term, similarity))\n",
        "\n",
        "    # Sort by similarity and return top N\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_n]\n",
        "\n",
        "# Test with a few examples\n",
        "print(f\"Most similar terms to 'heart':\")\n",
        "for term, sim in find_similar_terms('heart'):\n",
        "    print(f\"  {term}: {sim:.3f}\")\n",
        "\n",
        "print(f\"\\nMost similar terms to 'cancer':\")\n",
        "for term, sim in find_similar_terms('cancer'):\n",
        "    print(f\"  {term}: {sim:.3f}\")\n",
        "\n",
        "# Show the embedding matrix\n",
        "print(f\"\\nEmbedding Matrix (first 5 terms):\")\n",
        "print(\"Each row represents a medical term's learned embedding vector\")\n",
        "print(final_embeddings[:5])\n",
        "\n",
        "print(\"\\nKey Takeaway:\")\n",
        "print(\"Medical embeddings learn to represent not just words, but their meanings and relationships.\")\n",
        "print(\"This is how AI systems understand medical concepts - by learning patterns from vast amounts of medical data.\")"
      ],
      "metadata": {
        "id": "aesskQi0KVTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Embedding Layer Example\n",
        "\n",
        "* **num_embeddings** = How large is the vocabulary?  How many categories are you encoding? This parameter is the number of items in your \"lookup table.\"\n",
        "* **embedding_dim** = How many numbers in the vector you wish to return.\n",
        "\n",
        "Now we create a neural network with a vocabulary size of 10, which will reduce those values between 0-9 to 4 number vectors. This neural network does nothing more than passing the embedding on to the output. But it does let us see what the embedding is doing. Each feature vector coming in will have two such features."
      ],
      "metadata": {
        "id": "7_GFjbN85IO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple embedding example\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=4)\n",
        "optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n",
        "loss_function = nn.MSELoss()"
      ],
      "metadata": {
        "id": "cm9sUuvB5HJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the structure of this neural network to see what is happening inside it."
      ],
      "metadata": {
        "id": "8FPeUOYo5R5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print structure\n",
        "\n",
        "print(embedding_layer)"
      ],
      "metadata": {
        "id": "PwZqrW_f5U90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image47B.png)"
      ],
      "metadata": {
        "id": "Stz-jVREnMhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this neural network, which is just an embedding layer, the input is a vector of size 2. These two inputs are integer numbers from 0 to 9 (corresponding to the requested input_dim quantity of 10 values). Looking at the summary above, we see that the embedding layer has 40 parameters. This value comes from the embedded lookup table that contains four amounts (output_dim) for each of the 10 (input_dim) possible integer values for the two inputs. The output is 2 (input_length) length 4 (output_dim) vectors, resulting in a total output size of 8, which corresponds to the Output Shape given in the summary above.\n",
        "\n",
        "Now, let us query the neural network with two rows. The input is two integer values, as was specified when we created the neural network."
      ],
      "metadata": {
        "id": "0yeTOmBR5adE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query network\n",
        "\n",
        "input_tensor = torch.tensor([[1, 2]], dtype=torch.long)\n",
        "pred = embedding_layer(input_tensor)\n",
        "\n",
        "print(input_tensor.shape)\n",
        "print(pred)\n"
      ],
      "metadata": {
        "id": "ciLevhAK5b-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image48B.png)"
      ],
      "metadata": {
        "id": "r_jrT4hxx7Th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see two length-4 vectors that PyTorch looked up for each input integer. Recall that Python arrays are zero-based. PyTorch replaced the value of 1 with the second row of the 10 x 4 lookup matrix. Similarly, PyTorch returned the value of 2 by the third row of the lookup matrix. The following code displays the lookup matrix in its entirety. The embedding layer performs no mathematical operations other than inserting the correct row from the lookup table.\n"
      ],
      "metadata": {
        "id": "5vnKbsJP5g_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out embedding weights\n",
        "\n",
        "embedding_layer.weight.data"
      ],
      "metadata": {
        "id": "eA4_7X525iy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image49B.png)"
      ],
      "metadata": {
        "id": "N0qyKeaNyb8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values above are random parameters that PyTorch generated as starting points. Generally, we will transfer an embedding or train these random values into something useful. The following section demonstrates how to embed a hand-coded embedding."
      ],
      "metadata": {
        "id": "QKH8_tVm5mgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the `File --> Print.. --> Microsoft Print to PDF` if you are running either Windows 10 or 11 to generate a PDF of your Colab notebook. If you have a Mac, use the `File --> Print.. --> Save as PDF`\n",
        "\n",
        "In either case, save your PDF as Copy of Class_04_1.lastname.pdf where lastname is your last name, and upload the file to Canvas.\n",
        "\n",
        "**NOTE TO WINDOWS USERS:** You will automatically **lose 10 points** if your PDF is missing pages when being graded in Canvas and the grader has take the additional steps to download your PDF, print it out using `Microsoft Print to PDF` and then resubmit to Canvas for grading."
      ],
      "metadata": {
        "id": "r6uhybFv7HuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **UNIVAC**\n",
        "\n",
        "![___](https://upload.wikimedia.org/wikipedia/commons/2/2f/Univac_I_Census_dedication.jpg)\n",
        "\n",
        "**UNIVAC (Universal Automatic Computer)** was a line of electronic digital stored-program computers starting with the products of the Eckert–Mauchly Computer Corporation. Later the name was applied to a division of the Remington Rand company and successor organizations.\n",
        "\n",
        "The BINAC, built by the Eckert–Mauchly Computer Corporation, was the first general-purpose computer for commercial use, but it was not a success. The last UNIVAC-badged computer was produced in 1986.\n",
        "\n",
        "**UNIVAC Sperry Rand label**\n",
        "\n",
        "J. Presper Eckert and John Mauchly built the ENIAC (Electronic Numerical Integrator and Computer) at the University of Pennsylvania's Moore School of Electrical Engineering between 1943 and 1946. A 1946 patent rights dispute with the university led Eckert and Mauchly to depart the Moore School to form the Electronic Control Company, later renamed Eckert–Mauchly Computer Corporation (EMCC), based in Philadelphia, Pennsylvania. That company first built a computer called BINAC (BINary Automatic Computer) for Northrop Aviation (which was little used, or perhaps not at all). Afterwards, the development of UNIVAC began in April 1946.[1] UNIVAC was first intended for the Bureau of the Census, which paid for much of the development, and then was put in production.\n",
        "\n",
        "With the death of EMCC's chairman and chief financial backer Henry L. Straus in a plane crash on October 25, 1949, EMCC was sold to typewriter, office machine, electric razor, and gun maker Remington Rand on February 15, 1950. Eckert and Mauchly now reported to Leslie Groves, the retired army general who had previously managed building The Pentagon and led the Manhattan Project.\n",
        "\n",
        "The most famous UNIVAC product was the UNIVAC I mainframe computer of 1951, which became known for predicting the outcome of the U.S. presidential election the following year: this incident is noteworthy because the computer correctly predicted an Eisenhower landslide over Adlai Stevenson, whereas the final Gallup poll had Eisenhower winning the popular vote 51–49 in a close contest.\n",
        "\n",
        "The prediction led CBS's news boss in New York, Siegfried Mickelson, to believe the computer was in error, and he refused to allow the prediction to be read. Instead, the crew showed some staged theatrics that suggested the computer was not responsive, and announced it was predicting 8–7 odds for an Eisenhower win (the actual prediction was 100–1 in his favour).\n",
        "\n",
        "When the predictions proved true—Eisenhower defeated Stevenson in a landslide, with UNIVAC coming within 3.5% of his popular vote total and four votes of his Electoral College total—Charles Collingwood, the on-air announcer, announced that they had failed to believe the earlier prediction.\n",
        "\n",
        "The United States Army requested a UNIVAC computer from Congress in 1951. Colonel Wade Heavey explained to the Senate subcommittee that the national mobilization planning involved multiple industries and agencies: \"This is a tremendous calculating process...there are equations that can not be solved by hand or by electrically operated computing machines because they involve millions of relationships that would take a lifetime to figure out.\" Heavey told the subcommittee it was needed to help with mobilization and other issues similar to the invasion of Normandy that were based on the relationships of various groups.\n",
        "\n",
        "The UNIVAC was manufactured at Remington Rand's former Eckert-Mauchly Division plant on W Allegheny Avenue in Philadelphia, Pennsylvania. Remington Rand also had an engineering research lab in Norwalk, Connecticut, and later bought Engineering Research Associates (ERA) in St. Paul, Minnesota. In 1953 or 1954 Remington Rand merged their Norwalk tabulating machine division, the ERA \"scientific\" computer division, and the UNIVAC \"business\" computer division into a single division under the UNIVAC name. This severely annoyed those who had been with ERA and with the Norwalk laboratory.\n",
        "\n",
        "In 1955 Remington Rand merged with Sperry Corporation to become Sperry Rand. General Douglas MacArthur, then the chairman of the Board of Directors of Remington Rand, was chosen to continue in that role in the new company. Harry Franklin Vickers, then the President of Sperry Corporation, continued as president and CEO of Sperry Rand. The UNIVAC division of Remington Rand was renamed the Remington Rand Univac division of Sperry Rand. William Norris was put in charge as Vice-President and General Manager reporting to the President of the Remington Rand Division (of Sperry Rand).\n",
        "\n",
        "### **UNIVAC: Historical Development and Significance**\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "UNIVAC (Universal Automatic Computer) was the first commercially available computer in the United States, marking a pivotal moment in the history of computing. Developed in the early 1950s, UNIVAC played a crucial role in transitioning computing from experimental laboratories to practical business and government applications.\n",
        "\n",
        "**Origins and Development**\n",
        "\n",
        "### Eckert and Mauchly\n",
        "\n",
        "UNIVAC was developed by **J. Presper Eckert** and **John Mauchly**, the same engineers who created the **ENIAC** (Electronic Numerical Integrator and Computer), the first general-purpose electronic digital computer. After ENIAC, they founded the **Eckert-Mauchly Computer Corporation** in 1946 with the goal of producing a more advanced and commercially viable computer.\n",
        "\n",
        "### Design Goals\n",
        "\n",
        "UNIVAC was designed to:\n",
        "- Handle both numeric and alphabetic data.\n",
        "- Be suitable for business and administrative use.\n",
        "- Automate data processing tasks traditionally performed by punch card machines.\n",
        "\n",
        "## Key Milestones\n",
        "\n",
        "### UNIVAC I (1951)\n",
        "\n",
        "- **First Delivered**: To the U.S. Census Bureau on **June 14, 1951**.\n",
        "- **Technology**: Used vacuum tubes, mercury delay lines for memory, and magnetic tape for storage.\n",
        "- **Speed**: Could perform approximately 1,000 calculations per second.\n",
        "- **Input/Output**: Featured a typewriter-like console and tape drives.\n",
        "\n",
        "### Commercial Impact\n",
        "- **Remington Rand Acquisition**: In 1950, Eckert-Mauchly was acquired by Remington Rand, which marketed UNIVAC.\n",
        "- **Presidential Election Prediction**: UNIVAC I famously predicted the outcome of the 1952 U.S. presidential election on live television, correctly forecasting Eisenhower's victory—demonstrating the power of computing to the public.\n",
        "\n",
        "## Technical Specifications\n",
        "\n",
        "| Feature              | Specification                          |\n",
        "|----------------------|----------------------------------------|\n",
        "| Memory               | 1,000 words (12 characters each)       |\n",
        "| Word Size            | 72 bits                                |\n",
        "| Clock Speed          | 2.25 MHz                               |\n",
        "| Storage              | Magnetic tape                          |\n",
        "| Programming Language | Machine code                           |\n",
        "\n",
        "## Legacy and Influence\n",
        "\n",
        "UNIVAC's success helped establish the viability of computers in business and government. It influenced the development of subsequent systems and contributed to the growth of the American computer industry.\n",
        "\n",
        "### Successors\n",
        "\n",
        "- **UNIVAC II**: Introduced in 1958 with improved memory and performance.\n",
        "- **UNIVAC 1100 Series**: Became popular in the 1960s and 1970s for scientific and business applications.\n",
        "\n",
        "### Cultural Impact\n",
        "\n",
        "UNIVAC became a symbol of modernity and technological progress in the 1950s. Its televised election prediction helped demystify computers and sparked public interest in computing.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "UNIVAC was more than just a machine—it was a milestone in the evolution of computing. By bridging the gap between theoretical computing and practical application, it laid the foundation for the digital age.\n",
        "\n",
        "## References\n",
        "\n",
        "- Ceruzzi, Paul E. *A History of Modern Computing*. MIT Press.\n",
        "- U.S. Census Bureau Archives\n",
        "- Computer History Museum\n"
      ],
      "metadata": {
        "id": "RZrTH409Ps2P"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}