{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_04_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dkLTudD-NoQ"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ],
      "metadata": {
        "id": "fhdLgU8usnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-T0x2j-NoR"
      },
      "source": [
        "##### **Module 4: ChatGPT and Large Language Models**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* **Part 4.1: Introduction to Large Language Models (LLMs)**\n",
        "* Part 4.2: Chatbots\n",
        "* Part 4.3: Image Generation with StableDiffusion\n",
        "* Part 4.4: Image Generation with DALL-E\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOgB0oG-NoS"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfMETL_-NoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3574d270-f78c-4f75-b65f-1254bbd1c34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: Using Google CoLab\n",
            "david.senseman@gmail.com\n"
          ]
        }
      ],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "b3tT0Yojtv-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **You MUST Purchase Your OpenAI API Key Now!**\n",
        "\n",
        "In order to run the code in the next few lessons, you will need to purchase an `OpenAI API key` and install your key in the `secretes` location in your Google COLAB notebook. It is important to key your `OpenAI API key` secrete. If anyone learns your key, they can use it costing you a lot of money.  "
      ],
      "metadata": {
        "id": "xvyn29wc6BsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How to Purchase an OpenAI API Key**\n",
        "\n",
        "Follow these steps to obtain an API key from OpenAI:\n",
        "\n",
        "**1. Create an OpenAI Account**\n",
        "\n",
        "If you don’t already have one, go to https://platform.openai.com/api-keys and sign up using your email, Google, or Microsoft account.\n",
        "\n",
        "\n",
        "**2. Log In to the OpenAI Platform**\n",
        "\n",
        "Visit https://platform.openai.com/api-keys and log in with your credentials. After you log in you should see this page.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image17BB.png)\n",
        "\n",
        "**3. Set Up Billing**\n",
        "\n",
        "Before you can use the API you will need to add a payment method. Click on the **Settings** (gear) icon\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image18BB.png)\n",
        "\n",
        "Then click on **Billing** on the left tab.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image14BB.png)\n",
        "\n",
        "The select **Payment methods** and enter your credit card information.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image21BB.png)\n",
        "\n",
        "When you are finished adding your credit card information you need to add some money to your account. Click on **Add to Credit Balance**\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image20BB.png)\n",
        "\n",
        "Don't add too much money to your acccount since you can always add more money later if you use up your \"tokens\". Start with \\$10-\\$20.\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image25BB.png)\n",
        "\n",
        "Also keep in mind that if your secret key gets stolen, all of your money might be quicky used up by the person who stole it!\n",
        "\n",
        "**4. Generate an API Key**\n",
        "\n",
        "When you have added all of your credit card information and added some money to your account, you are ready to generate your API key.\n",
        "\n",
        "Select `API Keys` on the left tab\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image23BB.png)\n",
        "\n",
        "Click on **Create new secret key** and follow the directions. The name you give your key is not important. I just called my key `FALL25`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image24BB.png)\n",
        "\n",
        "Your new API key will look something like this:\n",
        "\n",
        "`sk-proj-DHIIXTfLozih_0rjgx08wxExjMc0G8SDqSyVtUG5pur0pVMLF5XgO2-TX2l9ZnXOiHk7Wv4sdvT4BlbkFJGxLnypUPLYpyYzVI6aOiZMx_1LOMKdADziRqlydDTg3HpwW0g7bSNmzuy0Tb0uzeyeCbz9qNMB`\n",
        "\n",
        "**WARNING:**\n",
        "You need to _immediately_ copy-and-paste your API into either the **Notepad** app if you are working on a Windows PC or the **TextEdit** app if you have a MAC.  \n",
        "\n",
        "> ⚠️ **Important:** Never share your API key publicly or commit it to version control.\n",
        "\n",
        "**5. Add Your API Key to Colab Secrets**\n",
        "\n",
        "In the Colab menu:\n",
        "\n",
        "Click on the key symbol on the left tab. This is your `secrets` location\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image02B.png)\n",
        "\n",
        "\n",
        "Click on `+ Add new secret`\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image04B.png)\n",
        "\n",
        "\n",
        "In the open box _carefully_ type the word `OPENAI_API_KEY`.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image06B.png)\n",
        "\n",
        "**WARNING:** Make sure that you spell and capitalize the word **_exactly_** as `OPENAI_API_KEY` or you secret key will not work!\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image08B.png)\n",
        "\n",
        "\n",
        "Paste your actual API key into the Value field.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image10B.png)\n",
        "\n",
        "Don't add any quotation marks to your API key. Just paste the actual key.\n",
        "\n",
        "When you have pasted your key in the `Value` box, click on the `+ Add new secret`\n",
        "\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image12B.png)\n",
        "\n",
        "Make sure to click on `Notebook access` so you use your secret `OPENAI_API_KEY` in this lesson.\n",
        "\n",
        "Finally, click on the **X** at the top right of the **Secrets** panel to restore your COLAB notebook.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ARxro9Kr5H4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test Your `OPENAI_API_KEY`**\n",
        "\n",
        "To see if your `OPENAI_API_KEY` is correctly setup, run the next code cell."
      ],
      "metadata": {
        "id": "59XFG5rWAwp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "#from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Retrieve the OpenAI API key and store it in a variable\n",
        "#OPENAI_KEY = userdata.get('OPENAI_KEY')\n",
        "\n",
        "# Ensure that the API key is correctly set\n",
        "if not OPENAI_KEY:\n",
        "    raise ValueError(\"OpenAI API key is not set. Please check if you have stored the API key in userdata.\")\n",
        "else:\n",
        "  print(f\"Your secret OPENAI_API_KEY =\", OPENAI_KEY)"
      ],
      "metadata": {
        "id": "NA5aUFdsAxpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57cc26e-75e2-472e-82a7-d71e632aad2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your secret OPENAI_API_KEY = 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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your `OPENAI_API_KEY` is correctly installed you should see the following output.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image26BB.png)\n",
        "\n",
        "However, if you see the following output\n",
        "```type\n",
        "OpenAI API key is not set. Please check if you have stored the API key in userdata.\n",
        "```\n",
        "You will need to correct the error before you can continue. Ask your Instructor or TA for help if you can resolve the error yourself."
      ],
      "metadata": {
        "id": "KCkgqZivDg6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Large Language Models (LLMs)**\n",
        "\n",
        "**Large Language Models (LLMs)** such as `GPT` have brought AI into mainstream use. LLMs allow regular users to interact with AI using natural language. Most of these language models require extreme processing capabilities and hardware. Because of this, application programming interfaces (APIs) accessed through the Internet are becoming common entry points for these models. One of the most compelling features of services like ChatGPT is their availability as an API. But before we dive into the depths of coding and integration, let's understand what an API is and its significance in the AI domain.\n",
        "\n",
        "API stands for **Application Programming Interface**. Think of it as a bridge or a messenger that allows two different software applications to communicate. In the context of AI and machine learning, APIs often allow developers to access a particular model or service without having to house the model on their local machine. This technique can be beneficial when the model in question, like `ChatGPT`, is large and resource-intensive.\n",
        "\n",
        "In the realm of AI, APIs have several distinct advantages:\n",
        "\n",
        "**1 Scalability:** Since the actual model runs on external servers, developers don't need to worry about scaling infrastructure.  \n",
        "**2. Maintenance:** You get to use the latest and greatest version of the model without constantly updating your local copy.  \n",
        "**3. Cost-Effective:** Leveraging external computational resources can be more cost-effective than maintaining high-end infrastructure locally, especially for sporadic or one-off tasks.  \n",
        "**4 Ease of Use:** Instead of diving into the nitty-gritty details of model implementation and optimization, developers can directly utilize its capabilities with a few lines of code.  \n",
        "\n",
        "In this section, we won't be running the neural network computations locally. We will use our PyTorch code to communicate with the `OpenAI API` to access and harness the abilities of `ChatGPT`. The actual execution of the neural network code happens on `OpenAI servers`, bringing forth a unique synergy of PyTorch's flexibility and ChatGPT's conversational mastery. (NOTE: The physical location of these servers is not disclosed for security reasons).\n",
        "\n",
        "In this section, we will make use of the `OpenAI ChatGPT API`. Further information on this API can be found here:\n",
        "\n",
        "* [OpenAI API Login/Registration](https://platform.openai.com/apps)\n",
        "* [OpenAI API Reference](https://platform.openai.com/docs/introduction/overview)\n",
        "* [OpenAI Python API Reference](https://platform.openai.com/docs/api-reference/introduction?lang=python)\n",
        "* [OpenAI Python Library](https://github.com/openai/openai-python)\n",
        "* [OpenAI Cookbook for Python](https://github.com/openai/openai-cookbook/)\n",
        "* [LangChain](https://www.langchain.com/)\n"
      ],
      "metadata": {
        "id": "GFI9xF411UuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing LangChain to use the OpenAI Python Library**\n",
        "\n",
        "As we delve deeper into the intricacies of deep learning, it's crucial to understand that the tools and platforms we use are as versatile as the concepts themselves. When it comes to accessing ChatGPT, a state-of-the-art conversational AI model developed by OpenAI, there are two predominant pathways:\n",
        "\n",
        "**Direct API Access using Python's HTTP Capabilities:** Python, with its rich library ecosystem, provides utilities like requests to directly communicate with APIs over HTTP. This method involves crafting the necessary API calls, handling responses, and error checking, giving the developer a granular control over the process.\n",
        "\n",
        "**Using the Official OpenAI Python Library:** OpenAI offers an official Python library, aptly named openai, that simplifies the process of integrating with ChatGPT and other OpenAI services. This library abstracts many of the intricacies and boilerplate steps of direct API access, offering a streamlined and user-friendly approach to interacting with the model.\n",
        "\n",
        "Each approach has its advantages. `Direct API access` provides a more hands-on, granular approach, allowing developers to intimately understand the intricacies of each API call. On the other hand, using the `openai library` can accelerate development, reduce potential errors, and allow for a more straightforward integration, especially for those new to API interactions.\n",
        "\n",
        "We will make use of the `OpenAI API` through a library called `LangChain`. `LangChain` is a framework designed to simplify the creation of applications using LLMs. As a language model integration framework, `LangChain's` use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis. `LangChain` allows you to quickly change between different underlying LLMs with minimal code changes.\n"
      ],
      "metadata": {
        "id": "pYfiGIW4BL5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `LangChain`\n",
        "\n",
        "The following command installs the **LangChain** library and needed OpenAI LLM connectors."
      ],
      "metadata": {
        "id": "2LiFOxy31RsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "\n",
        "!pip install langchain langchain_openai > /dev/null\n",
        "!pip install langchain-community > /dev/null"
      ],
      "metadata": {
        "id": "ZamBocuABWsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see any output."
      ],
      "metadata": {
        "id": "QOVXWiIHEy9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OpenAI Models Accessible via API**\n",
        "\n",
        "OpenAI provides access to a range of models via its API, designed to support diverse tasks across reasoning, creativity, and multimodal interaction. Here's an overview:\n",
        "\n",
        "### **1. GPT Models**\n",
        "- **GPT-5**  \n",
        "  The flagship model for general-purpose and agentic tasks. It supports multimodal input (text, image, audio, video), persistent memory, and advanced tool use. Available in three sizes—**GPT-5**, **GPT-5 Mini**, and **GPT-5 Nano**—with context windows up to 256k tokens. It introduces new parameters like `verbosity` and `reasoning_effort` for fine-tuned control over responses.\n",
        "\n",
        "- **GPT-4.5**  \n",
        "  A powerful model for creative tasks and agentic planning.\n",
        "\n",
        "- **GPT-4o**  \n",
        "  A high-performance model supporting text and vision inputs with a 128k context length.\n",
        "\n",
        "- **GPT-4o Mini**  \n",
        "  A smaller, cost-efficient variant optimized for lightweight multimodal tasks.\n",
        "\n",
        "### **2. Reasoning Models**\n",
        "- **o1**  \n",
        "  A frontier reasoning model with support for tools, structured outputs, and vision tasks. Offers a 200k context length.\n",
        "\n",
        "- **o3-mini**  \n",
        "  A budget-friendly reasoning model tailored for coding, math, and science. Supports tools and structured outputs.\n",
        "\n",
        "### **3. API Endpoints**\n",
        "- **Responses API**  \n",
        "  A unified endpoint combining capabilities like text/image input, web/file search, and advanced reasoning.\n",
        "\n",
        "- **Chat Completions API**  \n",
        "  Designed for conversational AI tasks, including chatbots and dialogue systems.\n",
        "\n",
        "- **Realtime API**  \n",
        "  Enables low-latency, multimodal experiences, including speech-to-speech interactions.\n",
        "\n",
        "- **Assistants API**  \n",
        "  Builds AI assistants capable of handling complex, multi-step tasks with memory and tool use.\n",
        "\n",
        "- **Batch API**  \n",
        "  Processes asynchronous workloads efficiently, offering reduced costs for large-scale operations.\n"
      ],
      "metadata": {
        "id": "98Z3C_0Uk5HZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GPT-3.5-turbo-1106**\n",
        "\n",
        "For this course, we will generally use the model `GTP-3.5-turbo-1106`.\n",
        "\n",
        "The **GPT-3.5-turbo-1106** model is one of `OpenAI's` advanced generative AI models, recently launched as part of the Azure OpenAI Service. Here are some key details:\n",
        "\n",
        "* **Availability:**\n",
        "It was announced alongside GPT-4 Turbo at Microsoft Ignite 2023 and is now available globally through Azure OpenAI Service.\n",
        "\n",
        "* **Performance:**\n",
        "The model is designed to provide improved cost efficiency and generative capabilities for businesses. It supports a wide range of applications, including natural language understanding, text generation, and more.\n",
        "\n",
        "* **Use Cases:**\n",
        "It is optimized for tasks like conversational AI, content creation, and other applications requiring high-quality text generation."
      ],
      "metadata": {
        "id": "ZEvcgQoho_Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set LLM for this Lesson\n",
        "\n",
        "Run the next cell to specify `gpt-3.5-turbo-1106` as the specific LLM to use for this lesson."
      ],
      "metadata": {
        "id": "jETWqMZZICno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the model you will generally use for this class\n",
        "\n",
        "LLM_MODEL = 'gpt-3.5-turbo-1106'\n",
        "\n",
        "print(f\"The Large Language Model (LLM) is set for\", LLM_MODEL)"
      ],
      "metadata": {
        "id": "CtmYoreIC9z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e014b173-d7f1-4841-9548-6f71658634a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Large Language Model (LLM) is set for gpt-3.5-turbo-1106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image27B.png)"
      ],
      "metadata": {
        "id": "ktVyWwlNFm7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt Engineering**\n",
        "\n",
        "When working with a large language model (LLM) like ChatGPT, the **prompt** serves as the foundation for interaction. It is the input or instruction provided to the model, guiding it to generate relevant and useful outputs.\n",
        "\n",
        "**1. Role of the Prompt**\n",
        "- **Instructional Guide**: The prompt shapes what the model does. Whether it's answering a question, completing a task, or writing creatively, the prompt provides the necessary context.  \n",
        "- **Boundary Setter**: A well-crafted prompt can define the scope of the task, ensuring the response is focused and doesn't deviate from the topic.  \n",
        "- **Task Optimizer**: By providing clear and concise instructions, the prompt ensures that the LLM generates responses that align with user expectations.\n",
        "\n",
        "**2. Importance of the Prompt**\n",
        "- **Determines Quality of Output**: The quality of the model's response depends heavily on the clarity and specificity of the prompt. A vague prompt can lead to irrelevant or incomplete answers, while a precise one produces accurate and valuable results.\n",
        "- **Customizable Interactions**: Prompts allow users to adapt the model to different scenarios—such as summarization, translation, or brainstorming—making it versatile and dynamic.  \n",
        "- **Reduces Ambiguity**: A good prompt minimizes room for misunderstanding, helping the model interpret the task as intended.  \n",
        "\n",
        "**3. Iterative Improvement**\n",
        "Working with LLMs is often an _iterative_  process. If the initial response isn't quite right, the user can refine the prompt, adding more detail or constraints to guide the model toward the desired result. Instead of starting over from scratch, you just edit the prompt and try it again.\n",
        "\n",
        "The prompt isn't just the input—it’s the bridge between the user’s needs and the model’s capabilities. Mastering prompt design is key to fully leveraging the potential of an LLM like ChatGPT.\n"
      ],
      "metadata": {
        "id": "XG5KgzC0dCxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Basic Query to LangChain\n",
        "\n",
        "We begin by writint a **prompt**, to ask (query) `ChatGPT` a simple question: \"What are the 5 largest cities in the USA?\".\n",
        "\n",
        "The Python code in the cell below interacts with OpenAI's GPT model using `LangChain` and the `ChatOpenAI class` to retrieve our answer.\n",
        "\n",
        "**NOTE:** This cell will not run if you do not have a valid OpenAI_Key and you have already installed your key with Google Colab.\n"
      ],
      "metadata": {
        "id": "TTSOCfbAqc-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Basic Query\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Initialize the OpenAI LLM (Language Learning Model) with your API key\n",
        "llm = ChatOpenAI(openai_api_key=OPENAI_KEY, model=LLM_MODEL, temperature=0)\n",
        "\n",
        "# Define the question\n",
        "question = \"What are the five largest cities in the USA by population?\"\n",
        "\n",
        "# Use Langchain to call the OpenAI API\n",
        "response = llm.invoke(question)\n",
        "\n",
        "# Print the response\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "kyd7OBeJEgMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "451af81e-629d-4fa2-8e92-d4460beeaaf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. New York City, New York\n",
            "2. Los Angeles, California\n",
            "3. Chicago, Illinois\n",
            "4. Houston, Texas\n",
            "5. Phoenix, Arizona\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image28B.png)"
      ],
      "metadata": {
        "id": "K7V2Z3bOrqS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the response from `LangChain` is in regular English, complete with formatting. While the formatting may make it easier to read, we often have to parse the results given to us by LLMs.\n",
        "\n",
        "Later, we will see that `LangChain` can help with this as well. You will also notice that we specified a value of `0` for **temperature**; this instructs the LLM to be less creative with its responses and more consistent. Because we are working primarily with data extraction in this section, a low temperature will give us more consistent results.\n",
        "\n",
        "In `LangChain`, the temperature parameter typically ranges from **0.0** to **1.0**, though some implementations may allow values slightly above 1.0. The temperature controls the randomness of the model's output:\n",
        "\n",
        "* **Low Temperature (e.g., 0.0):** Produces more deterministic and focused responses, ideal for tasks requiring precision.\n",
        "\n",
        "* **High Temperature (e.g., 1.0):** Generates more creative and diverse outputs, useful for brainstorming or creative writing.\n",
        "\n",
        "If you're working with `LangChain` and `OpenAI models`, you can set the temperature when initializing the model or during runtime."
      ],
      "metadata": {
        "id": "9Gy52CVNEvEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Basic Query to LangChain**\n",
        "\n",
        "For **Exercise 1** think about a subject for a `Top Five List` that **you** find interesting and see what response you get back from `ChatGTP`.\n",
        "\n",
        "Feel free to change the **temperature** of your request if you want a more _creative_ response from `LangChain`. There are no \"right\" or \"wrong\" answers here as long as your code works."
      ],
      "metadata": {
        "id": "MvfJCF6wr-lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 here\n"
      ],
      "metadata": {
        "id": "x_tQt3ejr-lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I am interested in guitar players, I asked `LangChain's` for a list of the 5 greatest guitart players of all time.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image29B.png)\n",
        "\n",
        "You output will be different depending up your question."
      ],
      "metadata": {
        "id": "r24AGvJYr-lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Working with Prompts\n",
        "\n",
        "As mentioned above, interactions with LLMs is typically accomplished using `prompts`. In fact, there is a whole new field called **Prompt Engineering** that focuses on designing, refining, and optimizing prompts to maximize the effectiveness and relevance of outputs generated by large language models (LLMs) like ChatGPT, GPT-4, and others.\n",
        "\n",
        "In Example 2, we will \"engineer\" a prompt that will have `ChatGPT` translate text from French to English. In this example, we will just be using normal Python F-Strings to build the prompt.\n"
      ],
      "metadata": {
        "id": "ZrnDmq8AE1P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Working with prompts\n",
        "\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Define text and style\n",
        "text = \"\"\"Laissez les bons temps rouler\"\"\"  # French text\n",
        "style = \"American English\"                  # English\n",
        "\n",
        "# Build prompt\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "# Send promt to ChatGPT\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "# Print ChatGPTs response\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "VPuxwG6U3mPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597e1260-2345-4027-a12d-44c79e8170cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Let the good times roll\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image30B.png)"
      ],
      "metadata": {
        "id": "WySFzrpTtyHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------\n",
        "\n",
        "**Why does the code Uses Triple Quotes?**\n",
        "\n",
        "The code in the cell above uses triple double quotes (\"\"\") for the prompt string to allow for clean, multi-line formatting and to include special characters, such as backticks (```) and placeholders ({style} and {text}).\n",
        "\n",
        "~~~text\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\"\"\"\n",
        "~~~\n",
        "\n",
        "-------------------------\n"
      ],
      "metadata": {
        "id": "0lPEtXSNh42d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Working with Prompts**\n",
        "\n",
        "In the cell below, use ChatGPT to translate the German expression: \"Ein Prosit der Gemütlichkeit\" into English.\n"
      ],
      "metadata": {
        "id": "uq2WA-AYuVR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 here\n"
      ],
      "metadata": {
        "id": "kzkL7ARauVR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image31B.png)"
      ],
      "metadata": {
        "id": "VGza68NEuVR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dynamic Prompts**\n",
        "\n",
        "A **dynamic prompt** is a flexible and adaptive input designed for interaction with language models (LLMs) like `ChatGP`T, where placeholders or variables are used to customize the prompt based on context or user-provided information. This approach allows for reusability, personalization, and automation, ensuring that the output is tailored to specific needs without rewriting the entire prompt.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Key Characteristics of a Dynamic Prompt**\n",
        "1. **Variables**:\n",
        "   - Dynamic prompts include placeholders for variables, like `{name}`, `{style}`, or `{text}`, which can be filled with different values at runtime.\n",
        "   - For example:\n",
        "     ```python\n",
        "     prompt = f\"Translate this text: {text} into {language}.\"\n",
        "     ```\n",
        "     Here, `{text}` and `{language}` can be dynamically replaced by the desired input values.\n",
        "2. **Context-Aware**:\n",
        "   - They adapt to the context, such as the user’s preferences, conversation history, or specific tasks.\n",
        "   - For instance, a dynamic prompt for summarization might consider the length of the desired output: \"Summarize the following article in less than {words} words.\"\n",
        "3. **Reusable Templates**:\n",
        "   - Instead of hardcoding individual tasks, dynamic prompts use templates that can be applied across multiple scenarios by simply replacing values.\n",
        "   - Example template:\n",
        "     ```python\n",
        "     template_text = \"\"\"Write a {tone} response to the following message:\n",
        "     message: {user_message}\"\"\"\n",
        "     ```\n",
        "4. **Personalization**:\n",
        "   - Dynamic prompts can be personalized based on user inputs or profiles, enhancing user experience. For example:\n",
        "     ```python\n",
        "     f\"Hi {name}, here’s the weather forecast for {city}!\"\n",
        "     ```\n",
        "\n",
        "#### **Why Are Dynamic Prompts Important?**\n",
        "\n",
        "- **Efficiency**: They save time by enabling template reuse.\n",
        "- **Scalability**: Useful for applications needing to handle diverse inputs.\n",
        "- **Adaptability**: They produce tailored outputs depending on the specific context or task.\n",
        "- **User Experience**: Personalization through dynamic prompts improves user satisfaction.\n",
        "\n",
        "---\n",
        "\n",
        "Dynamic prompts are at the heart of effective interactions with LLMs, making them more versatile, context-aware, and user-specific."
      ],
      "metadata": {
        "id": "azkr5E3Cii37"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBcs3yAMo_P"
      },
      "source": [
        "### Example 3 - Step 1: Build a Dynamic Prompt\n",
        "\n",
        "We can use LangChain to help us build dynamic prompts.\n",
        "\n",
        "The first step is provide LangChain with a `template prompt`. The code in the cell below defines and creates a prompt template using LangChain's `ChatPromptTemplate` class. The prompt template is called `example_prompt_template`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 1: Create prompt template\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define prompt template\n",
        "template_text = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "# Create template\n",
        "example_prompt_template = ChatPromptTemplate.from_template(template_text)\n"
      ],
      "metadata": {
        "id": "wu_I4u1U38SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you shouldn't see any output."
      ],
      "metadata": {
        "id": "AlHkVSSFxhB4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcdwI-0-Nod"
      },
      "source": [
        "### Example 3 - Step 2: Build Dynamic Prompt\n",
        "\n",
        "Now we can fill in the blanks for this prompt and observe the prompt created, which is a text string.\n",
        "\n",
        "The code in the cell below does the following:\n",
        "\n",
        "* Dynamically generates a structured prompt based on a template.\n",
        "* Ensures the prompt includes placeholders (style and text) filled with the\n",
        "provided values.\n",
        "* Inspects the data structure and type of the resulting prompt.\n",
        "* Outputs the first message to verify its content.\n",
        "\n",
        "This code is useful for building prompts in LangChain when interacting with language models for tasks like translation, summarization, or custom instructions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 2: Use template to create prompt\n",
        "\n",
        "example_prompt = example_prompt_template.format_messages(\n",
        "                    style=\"American English\",\n",
        "                    text=\"千里之行，始于足下。\")\n",
        "\n",
        "# Print prompt and its features\n",
        "print(type(example_prompt))\n",
        "print(type(example_prompt[0]))\n",
        "\n",
        "print(example_prompt[0])\n"
      ],
      "metadata": {
        "id": "Q2HpNiaY1NWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74fa555-16ca-4ab7-a095-72652157a1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain_core.messages.human.HumanMessage'>\n",
            "content='Translate the text that is delimited by triple backticks into a style that is American English. text: ```千里之行，始于足下。```\\n' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image32B.png)"
      ],
      "metadata": {
        "id": "OqIK6nGz31Vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are asking `ChatGPT` to translate the `text` \"千里之行，始于足下。\" into the `style` \"American English\"."
      ],
      "metadata": {
        "id": "PisALL0AkZZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3 - Step 3: Build Dynamic Prompt\n",
        "\n",
        "Now that we have buit our dynamic prompt in Steps 1 and 2, we are ready to send it to `ChatGPT` for analysis as shown in the code below."
      ],
      "metadata": {
        "id": "PSn4MbiMGPeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Step 3: Send prompt to llm for analysis\n",
        "\n",
        "# Call the LLM to translate to the style of the customer message\n",
        "example_response = llm.invoke(example_prompt)\n",
        "\n",
        "# Print response from ChatGPT\n",
        "print(example_response)"
      ],
      "metadata": {
        "id": "9Eb-aLHn4L8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea02c36-4718-4740-a72b-af76287ae472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='\" A journey of a thousand miles begins with a single step.\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 42, 'total_tokens': 55, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_982035f36f', 'id': 'chatcmpl-CDHL0UD2Xltq3voYwazD5noIs1Hvr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--3e34b1bb-3ef9-4a67-8831-f5d60e9365dc-0' usage_metadata={'input_tokens': 42, 'output_tokens': 13, 'total_tokens': 55, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image33B.png)"
      ],
      "metadata": {
        "id": "nn-lK6Hf3vMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This newly constructed prompt can now perform the intended task of translation."
      ],
      "metadata": {
        "id": "31sPO7R7GHv5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TphfSQIf5su_"
      },
      "source": [
        "### **Exercise 3 - Step 1: Build Dynamic Prompt**\n",
        "\n",
        "In the cell below, create a prompt template called `exercise_prompt_template`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 1 here\n"
      ],
      "metadata": {
        "id": "F-an-XKj5su_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you shouldn't see any output."
      ],
      "metadata": {
        "id": "lxXCSm4D5su_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAVCBV3k5su_"
      },
      "source": [
        "### **Exercise 3 - Step 2: Build Dynamic Prompt**\n",
        "\n",
        "Suppose you are standing watch at the White House and you receive this urgent message: \"Президент Трамп, русская Родина сдаётся\". Use your `exercise_prompt_template` to translate this message into English."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 2 here\n"
      ],
      "metadata": {
        "id": "32bYXW8K5su_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image34B.png)"
      ],
      "metadata": {
        "id": "3n2DHLNY5svA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3 - Step 3: Build Dynamic Prompt**\n",
        "\n",
        "Finally, send your `exercise_prompt_template` to `ChatGPT` to translate the urgent message in English."
      ],
      "metadata": {
        "id": "6-GzJjFY5svA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 - Step 3 here\n"
      ],
      "metadata": {
        "id": "pGbSFa9_5svA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image35B.png)"
      ],
      "metadata": {
        "id": "oQXyYQMm5svA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLM Memory**\n",
        "\n",
        "Human minds have both long-term and short-term memory. Long-term memory is what the human has learned throughout their lifetime. Short-term memory is what a human has only recently discovered in the last minute or so. For humans, learning is converting short-term memory into long-term memory that we will retain.\n",
        "\n",
        "This process works somewhat differently for a LLM. Long-term memory was the weight of the neural network when it was initially trained or finetuned. Short-term memory is additional information that we wish the LLM to retain from previous prompts. For example, if the first prompt is \"My name is David\", the LLM will likely tell you hello and repeat your name. However, the LLM will not know the answer if the second prompt is \"What is my name.\" without adding a memory component.\n",
        "\n",
        "These memory objects, which `LangChain` provides, provide a sort of short-term memory. It is important to note that these objects are not affecting the long-term memory of the LLM, and once you discard the memory object, the LLM will forget. Additionally, the memory object can only hold so much information; newer information may replace older information once it is filled.\n",
        "\n",
        "One important point to remember is that LLM's only have their input prompt. To provide such memory, these objects are appending anything we wish the LLM to remember to the input prompt. This section will see two ways to augment the prompt with previous information: a buffer and a summary. The buffer prepends a script of the last conversation up to this point. The summary approach keeps a consistently updated summary paragraph of the conversation."
      ],
      "metadata": {
        "id": "OqG349Nm3WMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conversation Buffer Window Memory**\n",
        "\n",
        "The `LangChain library` includes a conversation object named **ConversationChain**; this object facilitates an ongoing conversation with an LLM. For any conversation object, you must also specify a memory. For this first example, we will use the **ConversationBufferWindowMemory** object. This object keeps a transcript of the most recent conversation to reference. This memory allows the conversation object to remember what you have asked or told it and its responses to you."
      ],
      "metadata": {
        "id": "KOr_RRl53huN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create buffer memory\n",
        "\n",
        "# First, install the required packages in Colab\n",
        "# !pip install langchain langchain-core langchain-openai > /dev/null\n",
        "\n",
        "# Now import the necessary modules\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Access your OpenAI API key from Colab secrets or environment\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize OpenAI LLM\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Create memory - this is the key part that was causing issues\n",
        "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
        "\n",
        "# Create conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Test it\n",
        "result = conversation.predict(input=\"Hi, my name is David\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "8k48ewe9sDYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b36236c8-53bd-4f8d-a91f-47b4e07b4eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello David, it's nice to meet you! My name is AI. How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Talking to the LLM\n",
        "\n",
        "We can now have a conversation with the LLM. This newly constructed prompt can now perform the intended task of translation.\n"
      ],
      "metadata": {
        "id": "CdM2Z_qT3rY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Conversation\n",
        "\n",
        "conversation.predict(input=\"Hi, my name is David\")"
      ],
      "metadata": {
        "id": "aPOUhgWm3wrT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9a0e2db5-2145-4fd4-9b4d-37e043b03141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello David, it's nice to meet you! My name is AI. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image36B.png)"
      ],
      "metadata": {
        "id": "kdLRE4DFKTpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "id": "a0R-QtxG36aq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f1d6e955-754f-40a5-9930-a8b5d5dd4324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is David.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image37B.png)"
      ],
      "metadata": {
        "id": "HuvaL2F1K7R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a look at what the memory now contains.\n"
      ],
      "metadata": {
        "id": "W3M-I6Y237H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "NvFazK1y4B3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999a0a96-2c90-49f8-c3d1-19794a73111d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is David', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hello David, it's nice to meet you! My name is AI. How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Hi, my name is David', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hello David, it's nice to meet you! My name is AI. How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Your name is David.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01C.png)"
      ],
      "metadata": {
        "id": "8K1IOxzNLQqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Custom Conversation Bots**\n",
        "\n",
        "A **Custom Conversation Bot** is an AI-powered system that:\n",
        "- Engages in natural language conversations\n",
        "- Is customized for a specific domain, task, or personality\n",
        "- Can be deployed on websites, apps, or messaging platforms\n",
        "\n",
        "#### Key Features\n",
        "\n",
        "- **Custom Instructions**: Define how the bot should behave, respond, and what tone it should use.\n",
        "- **Knowledge Integration**: Connects to external data sources or APIs.\n",
        "- **User Memory**: Can remember user preferences or past interactions.\n",
        "- **Multimodal Capabilities**: Some bots can handle text, images, and even voice.\n",
        "\n",
        "#### Common Use Cases\n",
        "\n",
        "- Customer support\n",
        "- Educational tutoring\n",
        "- Personal assistants\n",
        "- Sales and marketing\n",
        "- Healthcare triage\n",
        "- Entertainment and storytelling\n",
        "\n",
        "#### Summary\n",
        "\n",
        "Custom conversation bots are flexible, intelligent tools that can be tailored to meet specific needs, making them valuable across industries and applications."
      ],
      "metadata": {
        "id": "68eHC1aG4Hpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OZAfRU8Mt6mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Custom Conversational Bot\n",
        "\n",
        "In Example 4 we are going to create a custom conversational bot named \"UTSA Assistant\" that is designed to help students at UTSA.\n",
        "\n",
        "The first step is to create a `template` that defines the focus of the `Bot` and how it responds to questions.\n",
        "\n",
        "```text\n",
        "template = \"\"\"The following is a friendly conversation between a human and an\n",
        "AI to assist UTSA Students.\n",
        "The AI should stick to topics about The University of Texas at San Antonion (UTSA).\n",
        "If the AI does not know the answer to a question,\n",
        "it should suggest the student speak to their advisor.\n",
        "```\n",
        "In this example, the focus of the Bot is the `University of Texas at San Antonio`.\n",
        "\n",
        "To keep the memory of this `Bot` separate from other `Bots` that you might create, the following line of code is used to specify the `ai_prefix`:\n",
        "\n",
        "```text\n",
        "memory=ConversationBufferWindowMemory(ai_prefix=\"UTSA Assistant\"),\n",
        "```"
      ],
      "metadata": {
        "id": "JkSpisR5t7Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Create Custom Conversation Bot for Kidney/Bladder Medical Assistant\n",
        "\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "template = \"\"\"The following is a friendly conversation between a human and a medical AI Physician's Assistant.\n",
        "The Physician's Assistant should only discuss topics related to kidney and bladder health issues.\n",
        "If the user asks about other medical conditions, politely redirect them to a general practitioner.\n",
        "If the user asks about non-medical topics, gently steer them back to health-related questions about kidneys and bladder.\n",
        "\n",
        "Important guidelines:\n",
        "- Focus only on kidney and bladder medical issues\n",
        "- Use professional but friendly language\n",
        "- If unsure about a medical condition, recommend seeing a doctor\n",
        "- Ask follow-up questions to better understand symptoms\n",
        "- Do not provide diagnoses or treatment recommendations\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Patient: {input}\n",
        "Physician's Assistant:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    prompt=PROMPT,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferWindowMemory(memory_key=\"history\", ai_prefix=\"Physician's Assistant\", k=10),\n",
        ")\n"
      ],
      "metadata": {
        "id": "1Z_nrF6luuTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Conversing with Custom Bot\n",
        "\n",
        "We can now have a conversation with our UTSA assistant bot."
      ],
      "metadata": {
        "id": "9NFBCWyVt7Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Question 1"
      ],
      "metadata": {
        "id": "Vcl6kaSnt7Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Question 1\n",
        "\n",
        "conversation.predict(input=\"I am having trouble urinating\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e73f842-b12e-46f8-95b0-5dee5e3ea401",
        "id": "CoiQHSTOt7Mb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry to hear that. Can you tell me more about your symptoms? Are you experiencing pain or discomfort while urinating?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should the following output:\n",
        "```text\n",
        "I'm sorry to hear that. Can you tell me more about your symptoms? Are you experiencing pain or discomfort while urinating?\n",
        "```"
      ],
      "metadata": {
        "id": "01iQEAeDH4_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Question 2"
      ],
      "metadata": {
        "id": "SB5pi0yEt7Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Question 2\n",
        "\n",
        "conversation.predict(input=\"I am only having pain on my left side\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ed226324-14b5-44d7-edf5-f29221916f56",
        "id": "qGrmUBOlt7Mc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I see. Pain on just one side could be a sign of a kidney issue. Have you noticed any changes in the color or frequency of your urine?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "```text\n",
        "I see. Pain on just one side could be a sign of a kidney issue. Have you noticed any changes in the color or frequency of your urine?\n",
        "```"
      ],
      "metadata": {
        "id": "9l_KV_j-t7Mc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Question 3"
      ],
      "metadata": {
        "id": "CS60q9-tt7Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Question 3\n",
        "\n",
        "conversation.predict(input=\"The color is dark brown\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6b81b386-8309-453f-c831-448ae7f1c1ab",
        "id": "C-GJJFQOt7Mc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dark brown urine can sometimes indicate dehydration or a potential problem with your kidneys. Have you been drinking enough water lately?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "```text\n",
        "Dark brown urine can sometimes indicate dehydration or a potential problem with your kidneys. Have you been drinking enough water lately?\n",
        "```"
      ],
      "metadata": {
        "id": "qJOh4T-gt7Mc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Question 4"
      ],
      "metadata": {
        "id": "4w9yAuy_t7Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Question 4\n",
        "\n",
        "conversation.predict(input=\"Thank-you for listening to me. It's hard to find a friendy ear these days\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9a2d541d-d449-4533-c2a6-fd887ffebdff",
        "id": "yBt8Gviyt7Mc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Of course, I'm here to help with any concerns you have about your kidney and bladder health. Is there anything else you'd like to discuss about your symptoms or any other questions you have about your urinary health?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "```text\n",
        "Of course, I'm here to help with any concerns you have about your kidney and bladder health. Is there anything else you'd like to discuss about your symptoms or any other questions you have about your urinary health?\n",
        "```"
      ],
      "metadata": {
        "id": "JqAEePK6t7Mc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Question 5"
      ],
      "metadata": {
        "id": "cPralGumt7Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ask our our LLM_MODEL = 'gpt-3.5-turbo-1106' a more philosophical question."
      ],
      "metadata": {
        "id": "VtDlapyAt7Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Question 5\n",
        "\n",
        "conversation.predict(input=\"Can you tell me what is the meaning of life.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3d0e94cc-29f6-4fcc-c3ec-504dcd8256a8",
        "id": "f_T-ypJmt7Md"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I appreciate your philosophical question, but my expertise lies in kidney and bladder health. If you have any more concerns or questions related to those topics, feel free to ask.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "```text\n",
        "I appreciate your philosophical question, but my expertise lies in kidney and bladder health. If you have any more concerns or questions related to those topics, feel free to ask.\n",
        "```"
      ],
      "metadata": {
        "id": "ZrthC4njt7Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our bot has a narrow focus. ☺"
      ],
      "metadata": {
        "id": "tp-hUua_t7Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Print Out Memory"
      ],
      "metadata": {
        "id": "1YwSKBRqt7Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a look at what the memory now contains."
      ],
      "metadata": {
        "id": "FFF_6ClBt7Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Print out memory\n",
        "\n",
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d79b291-fcc8-412e-ba2f-7baa3322433b",
        "id": "1R_mwVm_t7Md"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: I am having trouble urinating\\nPhysician's Assistant: I'm sorry to hear that. Can you tell me more about your symptoms? Are you experiencing pain or discomfort while urinating?\\nHuman: I am only having pain on my left side\\nPhysician's Assistant: I see. Pain on just one side could be a sign of a kidney issue. Have you noticed any changes in the color or frequency of your urine?\\nHuman: The color is dark brown\\nPhysician's Assistant: Dark brown urine can sometimes indicate dehydration or a potential problem with your kidneys. Have you been drinking enough water lately?\\nHuman: Thank-you for listening to me. It's hard to find a friendy ear these days\\nPhysician's Assistant: Of course, I'm here to help with any concerns you have about your kidney and bladder health. Is there anything else you'd like to discuss about your symptoms or any other questions you have about your urinary health?\\nHuman: Can you tell me what is the meaning of life.\\nPhysician's Assistant: I appreciate your philosophical question, but my expertise lies in kidney and bladder health. If you have any more concerns or questions related to those topics, feel free to ask.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image02C.png)"
      ],
      "metadata": {
        "id": "EdtVtDuWJLCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 4: Custom Conversational Bot**\n",
        "\n",
        "You are design your own **Custom Conversation Bot** and then ask it 5 questions that are within its field of focus. It is expected that every student will pick a **_different_** focus and use **_different_** questions (i.e. don't `copy-and-paste` from your coding friend)."
      ],
      "metadata": {
        "id": "sx3HO7dnTjQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Create Chat Bot**\n",
        "\n",
        "In the cell below, write the code to create your `Custom Chat Bot`."
      ],
      "metadata": {
        "id": "_Ktl20_-a_kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Create Custom Conversation Bot\n"
      ],
      "metadata": {
        "id": "eLGvXE_qWAVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Questions**\n",
        "\n",
        "Use the next 5 blank code cells to ask your Custom Chat Bot questions.  "
      ],
      "metadata": {
        "id": "RhJOmcHMXMy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Question 1**"
      ],
      "metadata": {
        "id": "hp3nyzp3fPWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Question 1 here\n",
        "\n"
      ],
      "metadata": {
        "id": "5wbfdTLXXgNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Question 2**"
      ],
      "metadata": {
        "id": "nACgfG9QfWC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Question 2 here\n"
      ],
      "metadata": {
        "id": "JQ5xCM4eYd6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Question 3**"
      ],
      "metadata": {
        "id": "me_w4NjufYiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Question 3 here\n"
      ],
      "metadata": {
        "id": "7qpaHGFqYucb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Question 4**"
      ],
      "metadata": {
        "id": "-oZcrdh7fbxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Question 4 here\n"
      ],
      "metadata": {
        "id": "lEunAZ6mZBSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Question 5**"
      ],
      "metadata": {
        "id": "DnZhUtv8ffeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Question 5 here\n"
      ],
      "metadata": {
        "id": "FH6bzF8CZbzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Print Out Memory**"
      ],
      "metadata": {
        "id": "1vD9Pfe0fjb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 4: Print History here\n"
      ],
      "metadata": {
        "id": "VAlby4HJXNrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conversation Summary Memory**\n",
        "\n",
        "Now, let's look at using a slightly more complex type of memory, the `ConversationSummaryMemory` object. This type of memory creates a summary of the conversation over time. This memory can be helpful for condensing information from the conversation over time. Conversation summary memory summarizes the conversation and stores the current summary in memory. You can use this memory to inject the conversation summary so far into a prompt/chain. This memory is most useful for more extended conversations, where keeping the past message history in the prompt verbatim would take up too many tokens."
      ],
      "metadata": {
        "id": "Q0wxrdpQ43P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Conversation Summary Memory\n",
        "\n",
        "Run the code in the next cell to install `Conversation Summary Memory`."
      ],
      "metadata": {
        "id": "5woi_dxt6loj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (run this only once)\n",
        "\n",
        "# Now run your code\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Create a conversation summary memory\n",
        "memory = ConversationSummaryMemory(\n",
        "    llm=llm,\n",
        "    memory_key=\"history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# Create the conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "JC7ihz1cwRxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Prompt Example\n",
        "\n",
        "To illustrate the `ConversationSummaryMemory` function we will start with a simple prompt."
      ],
      "metadata": {
        "id": "aqHvU27mvVLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple prompt example\n",
        "\n",
        "conversation.predict(input=\"I am a computational biologist, what do you do for a living?\")"
      ],
      "metadata": {
        "id": "fXHXSaop4-jG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1c2d2fc6-773f-4e97-b85d-169ede53238a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I work as an artificial intelligence program designed to assist with a variety of tasks, including answering questions, providing information, and engaging in conversations. My main function is to analyze data, learn patterns, and generate responses based on the input I receive. I can help with a wide range of topics, from science and technology to history and literature. How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image03C.png)"
      ],
      "metadata": {
        "id": "1dWFkySLvx0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examine Memory\n",
        "\n",
        "The code in the next cell prints out the memory."
      ],
      "metadata": {
        "id": "d9lmXM-1v924"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine memory\n",
        "\n",
        "conversation.memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "6D2KPEYq5BqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6a4ecc-5257-433b-b658-647b554e5094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content='The human mentions being a computational biologist and asks the AI what it does for a living. The AI explains that it works as an artificial intelligence program designed to assist with various tasks, analyze data, learn patterns, and generate responses based on input. It offers to help with a wide range of topics.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image46B.png)"
      ],
      "metadata": {
        "id": "vSrgdoPwwzA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What are Embedding Layers in PyTorch**\n",
        "\n",
        "[Embedding Layers](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) are a handy feature of PyTorch that allows the program to automatically insert additional information into the data flow of your neural network. An embedding layer would automatically allow you to insert vectors in the place of word indexes.  \n",
        "\n",
        "Programmers often use embedding layers with Natural Language Processing (NLP); however, you can use these layers when you wish to insert a lengthier vector in an index value place. In some ways, you can think of an embedding layer as dimension expansion. However, the hope is that these additional dimensions provide more information to the model and provide a better score."
      ],
      "metadata": {
        "id": "HrvnI_Bc5FsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simple Embedding Layer Example**\n",
        "\n",
        "* **num_embeddings** = How large is the vocabulary?  How many categories are you encoding? This parameter is the number of items in your \"lookup table.\"\n",
        "* **embedding_dim** = How many numbers in the vector you wish to return.\n",
        "\n",
        "Now we create a neural network with a vocabulary size of 10, which will reduce those values between 0-9 to 4 number vectors. This neural network does nothing more than passing the embedding on to the output. But it does let us see what the embedding is doing. Each feature vector coming in will have two such features."
      ],
      "metadata": {
        "id": "7_GFjbN85IO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple embedding example\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=4)\n",
        "optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n",
        "loss_function = nn.MSELoss()"
      ],
      "metadata": {
        "id": "cm9sUuvB5HJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the structure of this neural network to see what is happening inside it."
      ],
      "metadata": {
        "id": "8FPeUOYo5R5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print structure\n",
        "\n",
        "print(embedding_layer)"
      ],
      "metadata": {
        "id": "PwZqrW_f5U90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9044c9f6-cf16-4f0a-e358-282eac9ac655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(10, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image47B.png)"
      ],
      "metadata": {
        "id": "Stz-jVREnMhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this neural network, which is just an embedding layer, the input is a vector of size 2. These two inputs are integer numbers from 0 to 9 (corresponding to the requested input_dim quantity of 10 values). Looking at the summary above, we see that the embedding layer has 40 parameters. This value comes from the embedded lookup table that contains four amounts (output_dim) for each of the 10 (input_dim) possible integer values for the two inputs. The output is 2 (input_length) length 4 (output_dim) vectors, resulting in a total output size of 8, which corresponds to the Output Shape given in the summary above.\n",
        "\n",
        "Now, let us query the neural network with two rows. The input is two integer values, as was specified when we created the neural network."
      ],
      "metadata": {
        "id": "0yeTOmBR5adE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query network\n",
        "\n",
        "input_tensor = torch.tensor([[1, 2]], dtype=torch.long)\n",
        "pred = embedding_layer(input_tensor)\n",
        "\n",
        "print(input_tensor.shape)\n",
        "print(pred)\n"
      ],
      "metadata": {
        "id": "ciLevhAK5b-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db92247a-e6e9-4806-ee7c-c8f401eda648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2])\n",
            "tensor([[[-1.0234,  0.8310, -0.2332,  0.3331],\n",
            "         [-0.5216, -1.9504, -0.7151, -1.5572]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image48B.png)"
      ],
      "metadata": {
        "id": "r_jrT4hxx7Th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see two length-4 vectors that PyTorch looked up for each input integer. Recall that Python arrays are zero-based. PyTorch replaced the value of 1 with the second row of the 10 x 4 lookup matrix. Similarly, PyTorch returned the value of 2 by the third row of the lookup matrix. The following code displays the lookup matrix in its entirety. The embedding layer performs no mathematical operations other than inserting the correct row from the lookup table.\n"
      ],
      "metadata": {
        "id": "5vnKbsJP5g_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out embedding weights\n",
        "\n",
        "embedding_layer.weight.data"
      ],
      "metadata": {
        "id": "eA4_7X525iy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582556e0-37e1-45af-83e6-6758186109b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.6330e-01,  1.3754e+00,  2.3571e+00, -1.3814e-01],\n",
              "        [-1.0234e+00,  8.3102e-01, -2.3319e-01,  3.3314e-01],\n",
              "        [-5.2162e-01, -1.9504e+00, -7.1508e-01, -1.5572e+00],\n",
              "        [-2.0574e+00, -2.9566e+00, -2.9907e-01, -1.7459e+00],\n",
              "        [-4.3100e-01, -2.1195e-03,  2.1944e-01, -1.4141e+00],\n",
              "        [-7.9895e-01, -4.7176e-01, -1.5421e+00,  6.5702e-01],\n",
              "        [ 2.4714e-01,  3.6483e-01, -1.1676e+00, -5.4721e-01],\n",
              "        [-1.3205e+00, -9.3408e-01, -1.4925e-01,  1.1214e+00],\n",
              "        [-5.0143e-01, -6.0100e-01,  2.7676e-01, -7.7520e-01],\n",
              "        [ 3.4267e-01, -4.1478e-01, -9.1798e-01,  4.2237e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image49B.png)"
      ],
      "metadata": {
        "id": "N0qyKeaNyb8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values above are random parameters that PyTorch generated as starting points. Generally, we will transfer an embedding or train these random values into something useful. The following section demonstrates how to embed a hand-coded embedding."
      ],
      "metadata": {
        "id": "QKH8_tVm5mgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transferring An Embedding**\n",
        "\n",
        "Now, we see how to hard-code an embedding lookup that performs a simple one-hot encoding.  One-hot encoding would transform the input integer values of 0, 1, and 2 to the vectors $[1,0,0]$, $[0,1,0]$, and $[0,0,1]$ respectively. The following code replaced the random lookup values in the embedding layer with this one-hot coding-inspired lookup table."
      ],
      "metadata": {
        "id": "uyGGGkql5oJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer embedding\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the embedding lookup matrix\n",
        "embedding_lookup = torch.tensor([\n",
        "    [1, 0, 0],\n",
        "    [0, 1, 0],\n",
        "    [0, 0, 1]\n",
        "], dtype=torch.float32)  # Make sure to use float32 for weight matrices\n",
        "\n",
        "# Create the embedding layer\n",
        "embedding_layer = nn.Embedding(num_embeddings=3, embedding_dim=3)\n",
        "\n",
        "# Set the weights of the embedding layer\n",
        "embedding_layer.weight.data = embedding_lookup\n"
      ],
      "metadata": {
        "id": "Pv-bws9k5tKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the following parameters for the Embedding layer:\n",
        "    \n",
        "* `input_dim=3` - There are three different integer categorical values allowed.\n",
        "* `output_dim=3` - Three columns represent a categorical value with three possible values per one-hot encoding.\n",
        "* `input_length=2` - The input vector has two of these categorical values.\n",
        "\n",
        "We query the neural network with two categorical values to see the lookup performed."
      ],
      "metadata": {
        "id": "irPtffc65y0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the input tensor directly in PyTorch\n",
        "\n",
        "input_tensor = torch.tensor([[0, 1]], dtype=torch.long)\n",
        "\n",
        "# Forward pass to get the predictions\n",
        "pred = embedding_layer(input_tensor)\n",
        "\n",
        "print(input_tensor.shape)\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "Ws88CnPX50_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c226bba7-d817-42bc-d618-f38bb50bcb1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2])\n",
            "tensor([[[1., 0., 0.],\n",
            "         [0., 1., 0.]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image50B.png)"
      ],
      "metadata": {
        "id": "8Xsk7W6_zFD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given output shows that we provided the program with two rows from the one-hot encoding table. This encoding is a correct one-hot encoding for the values 0 and 1, where there are up to 3 unique values possible.\n",
        "\n",
        "The following section demonstrates how to train this embedding lookup table."
      ],
      "metadata": {
        "id": "pctkMP2756BD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training an Embedding**\n",
        "\n",
        "First, we make use of the following imports."
      ],
      "metadata": {
        "id": "RhjVde4k59vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train embedding\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "O9bVKJtX6ADR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a neural network that classifies restaurant reviews according to positive or negative. This neural network can accept strings as input, such as given here. This code also includes positive or negative labels for each review."
      ],
      "metadata": {
        "id": "CBKbqehz6FDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 10 resturant reviews.\n",
        "\n",
        "reviews = [\n",
        "    'Never coming back!',\n",
        "    'Horrible service',\n",
        "    'Rude waitress',\n",
        "    'Cold food.',\n",
        "    'Horrible food!',\n",
        "    'Awesome',\n",
        "    'Awesome service!',\n",
        "    'Rocks!',\n",
        "    'poor work',\n",
        "    'Couldn\\'t have done better']\n",
        "\n",
        "# Define labels (1=negative, 0=positive)\n",
        "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
      ],
      "metadata": {
        "id": "cOrQSzH76IsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the second to the last label is incorrect.  Errors such as this are not too out of the ordinary, as most training data could have some noise.\n",
        "\n",
        "We define a vocabulary size of 50 words.  Though we do not have 50 words, it is okay to use a value larger than needed.  If there are more than 50 words, the least frequently used words in the training set are automatically dropped by the embedding layer during training.  For input, we one-hot encode the strings.  We use the TensorFlow one-hot encoding method here rather than Scikit-Learn. Scikit-learn would expand these strings to the 0's and 1's as we would typically see for dummy variables.  TensorFlow translates all words to index values and replaces each word with that index."
      ],
      "metadata": {
        "id": "VEnaCapw6N7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode reviews\n",
        "\n",
        "VOCAB_SIZE = 50\n",
        "encoded_reviews = [torch.tensor([hash(word) % VOCAB_SIZE for word in review.split()]) for review in reviews]\n",
        "\n",
        "print(f\"Encoded reviews: {encoded_reviews}\")"
      ],
      "metadata": {
        "id": "zRcI_DwC6Sgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb69c48-e2af-4a6d-fe19-0d6b53cc9b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded reviews: [tensor([ 8, 37, 47]), tensor([0, 3]), tensor([38, 18]), tensor([31,  0]), tensor([ 0, 35]), tensor([5]), tensor([5, 4]), tensor([37]), tensor([20, 18]), tensor([ 1, 37,  1,  6])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The program one-hot encodes these reviews to word indexes; however, their lengths are different. We pad these reviews to 4 words and truncate any words beyond the fourth word.\n"
      ],
      "metadata": {
        "id": "Vx9wPDXb6Yqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad length\n",
        "\n",
        "MAX_LENGTH = 4\n",
        "padded_reviews = pad_sequence(encoded_reviews, batch_first=True, padding_value=0).narrow(1, 0, MAX_LENGTH)\n",
        "print(padded_reviews)"
      ],
      "metadata": {
        "id": "jI3_DKYz6b9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24921cbe-4c22-4668-a4dd-9f264996116c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 8, 37, 47,  0],\n",
            "        [ 0,  3,  0,  0],\n",
            "        [38, 18,  0,  0],\n",
            "        [31,  0,  0,  0],\n",
            "        [ 0, 35,  0,  0],\n",
            "        [ 5,  0,  0,  0],\n",
            "        [ 5,  4,  0,  0],\n",
            "        [37,  0,  0,  0],\n",
            "        [20, 18,  0,  0],\n",
            "        [ 1, 37,  1,  6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image51B.png)"
      ],
      "metadata": {
        "id": "e7LXs4_TzwKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As specified by the padding=post setting, each review is padded by appending zeros at the end, as specified by the padding=post setting.\n",
        "\n",
        "Next, we create a neural network to learn to classify these reviews.\n"
      ],
      "metadata": {
        "id": "TtOt4D4W6eyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create neural network\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding(VOCAB_SIZE, 8),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(8 * MAX_LENGTH, 1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "cf7M4YhF6hzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This network accepts four integer inputs that specify the indexes of a padded movie review. The first embedding layer converts these four indexes into four length vectors 8. These vectors come from the lookup table that contains 50 (VOCAB_SIZE) rows of vectors of length 8. This encoding is evident by the 400 (8 times 50) parameters in the embedding layer. The output size from the embedding layer is 32 (4 words expressed as 8-number embedded vectors). A single output neuron is connected to the embedding layer by 33 weights (32 from the embedding layer and a single bias neuron). Because this is a single-class classification network, we use the sigmoid activation function and binary_crossentropy.\n",
        "\n",
        "The program now trains the neural network. The embedding lookup and dense 33 weights are updated to produce a better score.\n"
      ],
      "metadata": {
        "id": "SqEEbJ0G6w8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Program neural network\n",
        "\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training the model\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(padded_reviews.long())\n",
        "    loss = criterion(outputs.squeeze(), torch.tensor(labels, dtype=torch.float))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "uMZXDMY260dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the learned embeddings. Think of each word's vector as a location in the 8 dimension space where words associated with positive reviews are close to other words. Similarly, training places negative reviews close to each other. In addition to the training setting these embeddings, the 33 weights between the embedding layer and output neuron similarly learn to transform these embeddings into an actual prediction. You can see these embeddings here."
      ],
      "metadata": {
        "id": "psYAqjGY64H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out learned embeddings\n",
        "\n",
        "embedding_weights = list(model[0].parameters())[0]\n",
        "print(embedding_weights.shape)\n",
        "print(embedding_weights)\n"
      ],
      "metadata": {
        "id": "wFbGhsyk66me",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "889c2260-77e5-4582-9d07-ab08487a53e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 8])\n",
            "Parameter containing:\n",
            "tensor([[ 0.4235, -2.3840, -0.4618,  0.8081, -0.4701, -1.2356,  0.1924, -1.2928],\n",
            "        [ 1.0750,  0.5940, -0.6512,  0.3840, -0.9188,  1.6140,  1.2515, -0.4398],\n",
            "        [ 1.8534, -1.9273,  0.4799, -1.4412, -1.1500, -0.7848, -0.3492, -1.4635],\n",
            "        [ 0.6184, -0.2027,  0.0459,  0.4160, -0.7900,  0.9356,  0.8230,  0.6419],\n",
            "        [-0.0752, -0.6254,  0.4040, -0.7661, -0.5073, -1.2710,  0.8985, -0.2494],\n",
            "        [-1.1231, -1.1283,  0.6681, -0.6545,  2.3703,  0.8095, -1.6800, -0.1865],\n",
            "        [-1.0524, -0.7847,  0.5250,  0.4946,  0.9516,  0.2987,  0.8677,  1.6276],\n",
            "        [ 0.9081, -0.2004,  0.6033,  0.1714,  0.9301, -0.7519, -0.2076,  0.2316],\n",
            "        [-1.7021, -0.6037, -1.2586, -0.8718, -0.8669, -0.2910, -0.2781,  0.3808],\n",
            "        [ 0.5371,  2.2392,  0.4571,  0.7726,  0.0816,  0.4621, -0.8901, -0.0908],\n",
            "        [-1.0247,  0.3758, -1.8141,  1.4871,  2.1899,  0.4122,  0.2554,  0.6250],\n",
            "        [-0.4024, -1.0518, -0.6421,  0.3185, -0.8767, -2.7900,  0.9890, -0.1128],\n",
            "        [ 0.8900,  0.1079,  1.1172, -0.9005,  0.2699, -0.1576,  1.6562,  0.4369],\n",
            "        [-0.1129, -0.1347, -0.2606,  0.2796, -0.8114,  0.4173, -0.3699, -0.5283],\n",
            "        [ 0.8900, -1.3653,  0.6600, -0.6273, -0.5526, -0.3777, -0.3098, -1.7537],\n",
            "        [ 0.5252, -1.0508, -0.5551,  0.2206, -0.0960,  0.8492, -1.5142,  1.2430],\n",
            "        [ 0.3429, -0.4732,  1.6947, -1.4886,  0.3202, -0.3605,  1.7341, -0.3991],\n",
            "        [ 0.5565,  1.2577,  0.4259, -1.0303,  0.5885,  0.3570,  0.0917, -0.3329],\n",
            "        [ 0.5531,  2.1197, -1.8205,  0.0408,  1.5263, -0.5880,  0.1657, -1.5065],\n",
            "        [-0.5448,  1.7681,  0.5278, -0.3507,  0.3938, -0.7776,  1.1571, -1.2339],\n",
            "        [ 0.8063,  0.9806,  0.1281, -0.5154, -0.4709, -1.0905,  0.6870,  1.7736],\n",
            "        [ 0.6946, -1.1323,  0.3879,  0.0446,  1.0051, -0.9587, -0.4986, -1.3455],\n",
            "        [-0.6528,  0.3585, -1.1584, -1.1235, -0.0785, -0.2837,  1.3266,  0.3332],\n",
            "        [ 0.1576, -2.0197,  1.3710, -2.0014,  1.8002, -1.2590, -1.3893,  0.2611],\n",
            "        [ 0.6213,  2.3278,  0.1408, -1.9048,  2.2471, -0.9781, -0.9245, -0.0302],\n",
            "        [ 0.2738,  1.2496, -1.4070,  0.5558,  0.8673, -1.2714, -0.2723,  0.1882],\n",
            "        [ 2.7681, -0.0789, -1.0664, -0.9540, -0.9762,  0.9579,  0.3829,  0.8945],\n",
            "        [-0.2698,  0.6542, -1.8945,  1.2254,  0.2089, -1.9128,  0.4615, -0.0635],\n",
            "        [-0.4809, -0.5190,  0.3637,  1.2051, -0.4951, -2.5132, -0.5216, -1.6407],\n",
            "        [-0.9615, -0.5111,  2.2832,  2.0250,  1.4180,  0.5748,  0.0873,  1.0319],\n",
            "        [ 0.0445, -0.6062, -0.9964,  0.5571,  0.0883,  0.5891,  0.7129, -1.5800],\n",
            "        [ 0.9702,  1.9963, -0.1158, -1.4946, -1.0865, -0.4063,  0.8308,  0.1291],\n",
            "        [-1.2431, -1.9293, -1.9418,  2.6544,  0.1365,  0.1627, -0.1128, -0.6147],\n",
            "        [-1.0780,  1.6229,  0.8522, -0.5087, -0.0269, -0.8431, -2.8281,  0.4208],\n",
            "        [ 0.0101,  0.9924,  0.1879,  0.9491,  0.8809,  0.2265,  0.6250, -0.8764],\n",
            "        [ 1.5710,  1.2079,  1.1425,  0.1517, -0.0695, -0.5817, -0.4298,  1.1574],\n",
            "        [-0.4668, -0.3086,  0.0471,  0.7094, -0.6242, -0.5880, -1.0066, -0.8756],\n",
            "        [-0.3428, -0.1814, -0.6264, -0.2443,  0.1462, -1.0549, -0.8073,  0.6328],\n",
            "        [-0.9791,  1.1573,  0.3626, -2.0507, -0.5166, -1.6304,  0.9019, -1.0905],\n",
            "        [ 0.2247, -0.4637, -0.2446,  0.8245,  0.3619, -1.0345,  0.1581,  1.3709],\n",
            "        [ 0.6472, -1.7119, -0.7100, -1.0132, -0.2980, -0.5019,  1.0116, -1.2093],\n",
            "        [ 0.7547, -0.2547, -0.3098, -2.5721, -0.6493,  1.4858, -0.4080, -1.1196],\n",
            "        [ 0.4114, -0.2776, -0.0859, -0.0402,  0.2175,  0.5858, -0.3629,  1.9056],\n",
            "        [-1.3895,  1.0411,  0.8239,  0.1044,  0.8394, -0.5989, -1.0981,  0.7558],\n",
            "        [-0.5075, -1.2946,  0.7459, -1.4146,  0.1968,  0.5152,  1.0295, -0.3470],\n",
            "        [ 0.4776,  1.2917,  1.6654, -0.3918,  1.9048, -3.4257,  0.2427,  0.6396],\n",
            "        [-2.0804, -1.6994, -0.3124,  1.1116, -0.0740, -0.2204, -0.6016, -1.6245],\n",
            "        [-0.9636, -0.2883, -0.8201,  0.3086,  0.2062, -0.1406, -0.6921,  0.2898],\n",
            "        [-0.1892, -0.2611,  0.3080,  0.8478, -0.9085,  1.6490, -0.0334, -1.1095],\n",
            "        [-0.0184,  1.9654,  1.1397, -0.1934, -0.1614, -0.6618, -0.6974,  0.0506]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now evaluate this neural network's accuracy, including the embeddings and the learned dense layer.\n"
      ],
      "metadata": {
        "id": "GnkHJ17869yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate neural network\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(padded_reviews.long())\n",
        "    predictions = (outputs > 0.5).float().squeeze()\n",
        "    accuracy = (predictions == torch.tensor(labels)).float().mean().item()\n",
        "    loss_value = criterion(outputs.squeeze(), torch.tensor(labels, dtype=torch.float)).item()\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Log-loss: {loss_value}')"
      ],
      "metadata": {
        "id": "Lu2WQm5-7A3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c12a923-1b48-4826-e9ab-b68e10b55685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8999999761581421\n",
            "Log-loss: 0.39077454805374146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image54B.png)"
      ],
      "metadata": {
        "id": "xca2a1UX0qpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is great, but there could be overfitting. It would be good to use early stopping to not overfit for a more complex data set. However, the loss is not perfect. Even though the predicted probabilities indicated a correct prediction in every case, the program did not achieve absolute confidence in each correct answer. The lack of confidence was likely due to the small amount of noise (previously discussed) in the data set. Some words that appeared in both positive and negative reviews contributed to this lack of absolute certainty.\n"
      ],
      "metadata": {
        "id": "KldwEUrM7E7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Copy of Class_04_1.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas. If you want your lesson graded you must turn in a PDF of a COPY of this lesson that was saved on your GDrive, not the original Colab notebook."
      ],
      "metadata": {
        "id": "r6uhybFv7HuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **UNIVAC**\n",
        "\n",
        "![___](https://upload.wikimedia.org/wikipedia/commons/2/2f/Univac_I_Census_dedication.jpg)\n",
        "\n",
        "**UNIVAC (Universal Automatic Computer)** was a line of electronic digital stored-program computers starting with the products of the Eckert–Mauchly Computer Corporation. Later the name was applied to a division of the Remington Rand company and successor organizations.\n",
        "\n",
        "The BINAC, built by the Eckert–Mauchly Computer Corporation, was the first general-purpose computer for commercial use, but it was not a success. The last UNIVAC-badged computer was produced in 1986.\n",
        "\n",
        "**UNIVAC Sperry Rand label**\n",
        "\n",
        "J. Presper Eckert and John Mauchly built the ENIAC (Electronic Numerical Integrator and Computer) at the University of Pennsylvania's Moore School of Electrical Engineering between 1943 and 1946. A 1946 patent rights dispute with the university led Eckert and Mauchly to depart the Moore School to form the Electronic Control Company, later renamed Eckert–Mauchly Computer Corporation (EMCC), based in Philadelphia, Pennsylvania. That company first built a computer called BINAC (BINary Automatic Computer) for Northrop Aviation (which was little used, or perhaps not at all). Afterwards, the development of UNIVAC began in April 1946.[1] UNIVAC was first intended for the Bureau of the Census, which paid for much of the development, and then was put in production.\n",
        "\n",
        "With the death of EMCC's chairman and chief financial backer Henry L. Straus in a plane crash on October 25, 1949, EMCC was sold to typewriter, office machine, electric razor, and gun maker Remington Rand on February 15, 1950. Eckert and Mauchly now reported to Leslie Groves, the retired army general who had previously managed building The Pentagon and led the Manhattan Project.\n",
        "\n",
        "The most famous UNIVAC product was the UNIVAC I mainframe computer of 1951, which became known for predicting the outcome of the U.S. presidential election the following year: this incident is noteworthy because the computer correctly predicted an Eisenhower landslide over Adlai Stevenson, whereas the final Gallup poll had Eisenhower winning the popular vote 51–49 in a close contest.\n",
        "\n",
        "The prediction led CBS's news boss in New York, Siegfried Mickelson, to believe the computer was in error, and he refused to allow the prediction to be read. Instead, the crew showed some staged theatrics that suggested the computer was not responsive, and announced it was predicting 8–7 odds for an Eisenhower win (the actual prediction was 100–1 in his favour).\n",
        "\n",
        "When the predictions proved true—Eisenhower defeated Stevenson in a landslide, with UNIVAC coming within 3.5% of his popular vote total and four votes of his Electoral College total—Charles Collingwood, the on-air announcer, announced that they had failed to believe the earlier prediction.\n",
        "\n",
        "The United States Army requested a UNIVAC computer from Congress in 1951. Colonel Wade Heavey explained to the Senate subcommittee that the national mobilization planning involved multiple industries and agencies: \"This is a tremendous calculating process...there are equations that can not be solved by hand or by electrically operated computing machines because they involve millions of relationships that would take a lifetime to figure out.\" Heavey told the subcommittee it was needed to help with mobilization and other issues similar to the invasion of Normandy that were based on the relationships of various groups.\n",
        "\n",
        "The UNIVAC was manufactured at Remington Rand's former Eckert-Mauchly Division plant on W Allegheny Avenue in Philadelphia, Pennsylvania. Remington Rand also had an engineering research lab in Norwalk, Connecticut, and later bought Engineering Research Associates (ERA) in St. Paul, Minnesota. In 1953 or 1954 Remington Rand merged their Norwalk tabulating machine division, the ERA \"scientific\" computer division, and the UNIVAC \"business\" computer division into a single division under the UNIVAC name. This severely annoyed those who had been with ERA and with the Norwalk laboratory.\n",
        "\n",
        "In 1955 Remington Rand merged with Sperry Corporation to become Sperry Rand. General Douglas MacArthur, then the chairman of the Board of Directors of Remington Rand, was chosen to continue in that role in the new company. Harry Franklin Vickers, then the President of Sperry Corporation, continued as president and CEO of Sperry Rand. The UNIVAC division of Remington Rand was renamed the Remington Rand Univac division of Sperry Rand. William Norris was put in charge as Vice-President and General Manager reporting to the President of the Remington Rand Division (of Sperry Rand).\n",
        "\n",
        "### **UNIVAC: Historical Development and Significance**\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "UNIVAC (Universal Automatic Computer) was the first commercially available computer in the United States, marking a pivotal moment in the history of computing. Developed in the early 1950s, UNIVAC played a crucial role in transitioning computing from experimental laboratories to practical business and government applications.\n",
        "\n",
        "**Origins and Development**\n",
        "\n",
        "### Eckert and Mauchly\n",
        "\n",
        "UNIVAC was developed by **J. Presper Eckert** and **John Mauchly**, the same engineers who created the **ENIAC** (Electronic Numerical Integrator and Computer), the first general-purpose electronic digital computer. After ENIAC, they founded the **Eckert-Mauchly Computer Corporation** in 1946 with the goal of producing a more advanced and commercially viable computer.\n",
        "\n",
        "### Design Goals\n",
        "\n",
        "UNIVAC was designed to:\n",
        "- Handle both numeric and alphabetic data.\n",
        "- Be suitable for business and administrative use.\n",
        "- Automate data processing tasks traditionally performed by punch card machines.\n",
        "\n",
        "## Key Milestones\n",
        "\n",
        "### UNIVAC I (1951)\n",
        "\n",
        "- **First Delivered**: To the U.S. Census Bureau on **June 14, 1951**.\n",
        "- **Technology**: Used vacuum tubes, mercury delay lines for memory, and magnetic tape for storage.\n",
        "- **Speed**: Could perform approximately 1,000 calculations per second.\n",
        "- **Input/Output**: Featured a typewriter-like console and tape drives.\n",
        "\n",
        "### Commercial Impact\n",
        "- **Remington Rand Acquisition**: In 1950, Eckert-Mauchly was acquired by Remington Rand, which marketed UNIVAC.\n",
        "- **Presidential Election Prediction**: UNIVAC I famously predicted the outcome of the 1952 U.S. presidential election on live television, correctly forecasting Eisenhower's victory—demonstrating the power of computing to the public.\n",
        "\n",
        "## Technical Specifications\n",
        "\n",
        "| Feature              | Specification                          |\n",
        "|----------------------|----------------------------------------|\n",
        "| Memory               | 1,000 words (12 characters each)       |\n",
        "| Word Size            | 72 bits                                |\n",
        "| Clock Speed          | 2.25 MHz                               |\n",
        "| Storage              | Magnetic tape                          |\n",
        "| Programming Language | Machine code                           |\n",
        "\n",
        "## Legacy and Influence\n",
        "\n",
        "UNIVAC's success helped establish the viability of computers in business and government. It influenced the development of subsequent systems and contributed to the growth of the American computer industry.\n",
        "\n",
        "### Successors\n",
        "\n",
        "- **UNIVAC II**: Introduced in 1958 with improved memory and performance.\n",
        "- **UNIVAC 1100 Series**: Became popular in the 1960s and 1970s for scientific and business applications.\n",
        "\n",
        "### Cultural Impact\n",
        "\n",
        "UNIVAC became a symbol of modernity and technological progress in the 1950s. Its televised election prediction helped demystify computers and sparked public interest in computing.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "UNIVAC was more than just a machine—it was a milestone in the evolution of computing. By bridging the gap between theoretical computing and practical application, it laid the foundation for the digital age.\n",
        "\n",
        "## References\n",
        "\n",
        "- Ceruzzi, Paul E. *A History of Modern Computing*. MIT Press.\n",
        "- U.S. Census Bureau Archives\n",
        "- Computer History Museum\n"
      ],
      "metadata": {
        "id": "RZrTH409Ps2P"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}