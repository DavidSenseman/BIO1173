{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_02_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbURuTh6tO6I"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV0tFW_9tO6J"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re89HmzytO6J"
      },
      "source": [
        "**Module 2: Machine Learning**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 2 Material\n",
        "\n",
        "* **Part 2.1: Pandas DataFrame Operations**\n",
        "* Part 2.2: Categorical Values\n",
        "* Part 2.3: Grouping, Sorting and Shuffling on Pandas  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcLEqX98tO6J"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to ```/content/drive``` and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM"
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your GMAIL address is visible in the output above."
      ],
      "metadata": {
        "id": "o2ysZ5SepcHE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhlJqA5otO6K"
      },
      "source": [
        "## Datasets for Class_02_1\n",
        "\n",
        "An important objective of this course is to introduce you to a number of different **_datasets_** that are similar to kinds of data that you might encounter as pursue your career as a biologist or as a clinical investigator.\n",
        "\n",
        "In this class we will be introduced to two new datasets, the **_Apple Quality_** dataset, for the Examples, and the **_Obesity Prediction_** dataset, for the **Exercises**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "Jf9jpkHmtO6K"
      },
      "source": [
        "### **Apple Quality Dataset**\n",
        "\n",
        "[Apple Quality Data Set](https://www.kaggle.com/datasets/nelgiriyewithana/apple-quality)\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/apples.jpg)\n",
        "\n",
        "A student trained in biology would have a good foundation for understanding the biological processes involved in the growth and development of agricultural produce. If they were specifically interested in the quality of agricultural produce, from a biological perspective, they might need to assess factors such as nutrient content, pesticide residues, presence of pathogens or contaminants, and overall health and viability of the produce. This could involve conducting laboratory tests, analyzing data, and interpreting results to determine the quality and safety of the agricultural products.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "The Apple Quality dataset contains information about various attributes of a large sample of apples (_n_=4000), providing insights into their characteristics. The dataset includes details such as fruit ID, size, weight, sweetness, crunchiness, juiciness, ripeness, acidity, and quality.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "* **A_id:** Unique identifier for each fruit\n",
        "* **Size:** Size of the fruit\n",
        "* **Weight:** Weight of the fruit\n",
        "* **Sweetness:** Degree of sweetness of the fruit\n",
        "* **Crunchiness:** Texture indicating the crunchiness of the fruit\n",
        "* **Juiciness:** Level of juiciness of the fruit\n",
        "* **Ripeness:** Stage of ripeness of the fruit\n",
        "* **Acidity:** Acidity level of the fruit\n",
        "* **Quality:** Overall quality of the fruit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adN12zeLtO6K"
      },
      "source": [
        "### Example 1: Read Datafile and Create DataFrame\n",
        "\n",
        "The cell below use the Pandas `read_csv()` function to read the CSV file `apple_quality.csv` located on the course HTTPS server using the following code chunk:\n",
        "\n",
        "```python\n",
        "aqDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/apple_quality.csv\",\n",
        "    na_values=['NA','?'])\n",
        "```\n",
        "The function`read_csv()` is frequently used for reading CSV files. In example, the `read_csv()` function takes 2 arguments. The first argument,\n",
        "\n",
        "```python\n",
        "\"https://biologicslab.co/BIO1173/data/apple_quality.csv\",\n",
        "\n",
        "```\n",
        "is a string that provides the filepath to, and filename of, the datafile.\n",
        "\n",
        "The second argument:\n",
        "\n",
        "```python\n",
        "na_values=['NA','?']`\n",
        "```\n",
        "converts any missing data points in the file (`?`) into the value, `NaN` which stands for Not-a-Number.As the file is read, Pandas creates a Pandas DataFrame called `aqDF` to hold the information.\n",
        "\n",
        "After reading the datafile into a DataFrame, it is always a good idea to use the `display()` function a print out a specified number of rows and columns of our new DataFrame to make sure the data was read correctly.\n",
        "\n",
        "In the cell below, the `pd.set_option()` function is used to specify the number of rows and the number of columns.  Many datasets in this course will be a large number of rows and columns which can not be printed out easily to your notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOBFQL0mtO6K"
      },
      "outputs": [],
      "source": [
        "# Example 1: Read the datafile and create a Pandas DataFrame\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read datafile and create DataFrame\n",
        "aqDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/apple_quality.csv\",\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "# Set the display for 12 rows and 6 columns\n",
        "pd.set_option('display.max_rows', 12)\n",
        "pd.set_option('display.max_columns', 6)\n",
        "\n",
        "# Display the DataFrame\n",
        "display(aqDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyXlQw49tO6K"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image04a.png)\n",
        "\n",
        "From the output, we can see that `aqDF` has 4000 rows and 9 columns. Only 6 of the 9 columns are printed out. The missing columns are represented by the `...`.\n",
        "\n",
        "In a Pandas DataFrame, each row is an _observation_, in this case a single piece of fruit (apple). The 9 columns record the various **_features_**, or factors, that were measured for each apple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "K8RVtFNvtO6L"
      },
      "source": [
        "### **Obesity Prediction Dataset**\n",
        "\n",
        "[Obesity Prediction Dataset](https://www.kaggle.com/datasets/mrsimple07/obesity-prediction)\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/obesity.jpg)\n",
        "\n",
        "**_Obesity_** is a medical condition characterized by excessive accumulation of body fat to the extent that it can have negative effects on health. It is typically defined by a body mass index (BMI) of 30 or above. Obesity increases the risk of various health problems, including heart disease, type 2 diabetes, certain types of cancer, and other chronic conditions.\n",
        "\n",
        "Studying obesity is important for several reasons:\n",
        "* **Public health impact:** Obesity is a significant public health concern worldwide, with rates of obesity on the rise in many countries. Understanding the causes, consequences, and potential interventions for obesity is crucial for developing effective public health strategies to prevent and manage this condition.\n",
        "* **Health consequences:** Obesity is associated with a wide range of health problems, including cardiovascular disease, diabetes, hypertension, and certain types of cancer. Studying obesity can help researchers and healthcare professionals better understand the mechanisms by which obesity contributes to these conditions and develop targeted interventions to improve health outcomes.\n",
        "* **Socioeconomic impact:** Obesity can have negative socioeconomic consequences, such as increased healthcare costs, reduced productivity, and decreased quality of life. By studying obesity, researchers can identify strategies to prevent and treat obesity that can help alleviate these economic burdens.\n",
        "* **Psychological and social impact:** Obesity can also have psychological and social\n",
        "  \n",
        "**Features of The Obesity Prediction Dataset**\n",
        "\n",
        "The Obesity Prediction dataset provides comprehensive information on individuals' demographic characteristics, physical attributes, and lifestyle habits, aiming to facilitate the analysis and prediction of obesity prevalence. It includes key variables such as age, gender, height, weight, body mass index (BMI), physical activity level, and obesity category.\n",
        "\n",
        "* **Age:** The age of the individual, expressed in years.\n",
        "* **Gender:** The gender of the individual, categorized as male or female.\n",
        "* **Height:** The height of the individual, typically measured in centimeters or inches.\n",
        "* **Weight:** The weight of the individual, typically measured in kilograms or pounds.\n",
        "* **BMI:** A calculated metric derived from the individual's weight and height\n",
        "* **PhysicalActivityLevel:** This variable quantifies the individual's level of physical activity\n",
        "* **ObesityCategory:** Categorization of individuals based on their BMI into different obesity categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2vDsUn9tO6L"
      },
      "source": [
        "### **Exercise 1: Read Datafile and Create DataFrame**\n",
        "\n",
        "In the cell below, use the Pandas `read_csv()` function to read the obesity dataset and create a DataFrame called `opDF` to hold the information. You can use this code chunk:\n",
        "~~~text\n",
        "# Read the datafile\n",
        "opDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/obesity_prediction.csv\",\n",
        "    na_values=['NA','?'])\n",
        "~~~\n",
        "\n",
        "Set the`display()` function to print out 8 rows and 7 columns of `opDF` before you display your new DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ktNXJQhytO6L"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xD4TdL5tO6L"
      },
      "source": [
        "If your code is correct you should see the following table.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image06a.png)\n",
        "\n",
        "As you can see, the `opDF` DataFrame contains information from 1000 patients (1 row/patient) with 7 measurements (features) for each patient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NliYcGDWtO6L"
      },
      "source": [
        "# **Pandas DataFrame Operations**\n",
        "\n",
        "In this lesson we continue our investigation of the software package, Pandas, looking at some of the DataFrame operations that you will be using to prepare data so it can be used to train a neural network.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMqmUb8rtO6L"
      },
      "source": [
        "### **Dealing with Outliers**\n",
        "\n",
        "**_Outliers_** are values that are unusually high or low. We typically consider outliers to be a value that is several standard deviations from the mean. Sometimes outliers are simply errors; this is a result of [observation error](https://en.wikipedia.org/wiki/Observational_error).\n",
        "\n",
        "Outliers that are really large or small values that may be difficult to address. The following **_function_** can remove such values. The code in the function defined below, will drop any _row_ of data that contains a selected value that is a specified number of standard deviations above, or below, the mean. In otherwords, if a certain patient has one unusually large or small clinical observation, _all_ the data from that patient will be removed.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj9e8dD2tO6L"
      },
      "outputs": [],
      "source": [
        "# FUNCTION TO REMOVE OUTLIERS\n",
        "\n",
        "# Remove all rows where the specified column is +/- sd standard deviations\n",
        "def remove_outliers(df, name, sd):\n",
        "    drop_rows = df.index[(np.abs(df[name] - df[name].mean())\n",
        "                          >= (sd * df[name].std()))]\n",
        "    df.drop(drop_rows, axis=0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnqInwjGtO6L"
      },
      "source": [
        "--------------------------------------\n",
        "\n",
        "### **FUNCTIONS**\n",
        "\n",
        "A Python **_function_** is a reusable block of code that performs a specific task. It is defined using the `def` keyword followed by the _function name_ and parentheses. Functions can take input values called arguments or parameters, which are defined inside the parentheses. These parameters can be used within the function's code block to perform computations or operations.\n",
        "\n",
        "Functions in Python are organized, modular, and help in code reusability. They promote code readability and maintainability by dividing complex logic into smaller, manageable units. They also allow programmers to encapsulate functionality and use it repeatedly throughout the program, improving efficiency and reducing duplication.\n",
        "\n",
        "Functions can have a return statement that provides the computed result or value back to the caller. This returned value can be stored in a variable or used directly. Functions can also be called within other functions, allowing for nested function calls. Overall, Python functions are a fundamental building block in programming that enables code organization, reusability, and abstraction.\n",
        "\n",
        "-----------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A09_VOv8tO6L"
      },
      "source": [
        "### Example 2: Remove Rows with Outliers\n",
        "\n",
        "The cell below uses the `remove_outliers()` function, created above, to remove outliers in the Apple Quality dataset. In order to use this function, you need to decide which feature (i.e. `column`) that you want to test for outliers. In this example, we will focus is on the `Acidity` measurement.\n",
        "\n",
        "Before we can apply the function to the DataFrame, we must create a vector to take care of any records in which the acidity value is missing (i.e. NaN).\n",
        "\n",
        "A **_vector_** usually consists of numeric values, in this case the statistical [median](https://en.wikipedia.org/wiki/Median) of the acidity level in all of the apples in the sample. This is done using `median()` as follows:\n",
        "~~~text\n",
        "med = aqDF['Acidity'].median()\n",
        "~~~\n",
        "\n",
        "Notice that we are using square bracket indexing to select **only** the column labeled `'Acidity'`. We then use the median acidity value to \"fill in\" any blank spaces with the following code line:\n",
        "~~~text\n",
        "aqDF['Acidity'] = aqDF['Acidity'].fillna(med)\n",
        "~~~\n",
        "\n",
        "Why chose the median value to fill in the blanks? By using replacing any missing value by the column's median value, simply keeps the column's median value the same.\n",
        "\n",
        "Having filled in any missing acidity values, we can now use our function to remove outliers:\n",
        "~~~text\n",
        "remove_outliers(aqDF,'Acidity',2)\n",
        "~~~\n",
        "The number `2` specifies the number of [standard deviations](https://en.wikipedia.org/wiki/Standard_deviation) for our upper and lower boundary. The function will remove any apple (i.e. row) from dataframe in which the acidity value that is greater than `2` standard deviations (sd) above, or below, the mean.\n",
        "\n",
        "Finally, to see if the function worked, we print out the number of rows in the DataFrame before, and then after, applying the function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sWOnUiwtO6L"
      },
      "outputs": [],
      "source": [
        "# Example 2: Use remove_outlier() function with Apple Quality dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# create vector\n",
        "med = aqDF['Acidity'].median()   # calculate the median value for acidity\n",
        "aqDF['Acidity'] = aqDF['Acidity'].fillna(med)  # replace an NaN with median\n",
        "\n",
        "# Print number of rows before\n",
        "print(\"Length before Acidity outliers dropped: {}\".format(len(aqDF)))\n",
        "\n",
        "# Apply funtion\n",
        "remove_outliers(aqDF,'Acidity',2)\n",
        "\n",
        "# Print the number of rows after\n",
        "print(\"Length after Acidity outliers dropped: {}\".format(len(aqDF)))\n",
        "\n",
        "# Set display\n",
        "pd.set_option('display.max_rows', 5)\n",
        "pd.set_option('display.max_columns', 8)\n",
        "\n",
        "# Display the new DataFrame\n",
        "display(aqDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5RiZ6JFtO6L"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image05a.png)\n",
        "\n",
        "Notice that after applying the `remove_outliers()` function, there are now `184` fewer rows (apples) in the `aqDF` DataFrame (i.e. 4000 - 3816).\n",
        "\n",
        "Were these acidity measurements faulty which is why they were so different?\n",
        "\n",
        "Probably not.\n",
        "\n",
        "If you have had statistics, this is almost exactly what you would expect if acidity values were **_normally distributed_**.\n",
        "\n",
        "According to the [**_68%-95%-99.7% Rule_**](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule), 95% of a normally distributed variable (in this case `Acidity`) should be within 2 standard deviations of the mean. If you multiple 4000 apples X 0.95, you get 3,800. In other words, you should expect about 200 of the 4,000 apples would have an acidity that was either 2 standard deviations too high, or too low. This is almost exactly what we got.\n",
        "\n",
        "In \"real life\" you almost never remove any data from sample _unless_ you have very strong reasons to suspect that the data was somehow faulty. And even then, you are obliged to report what was done, and why, to anyone reading your analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3Dj2OE_tO6L"
      },
      "source": [
        "### **Exercise 2:  Remove Rows with outliers**\n",
        "\n",
        "In the cell below, write the Python code to remove outliers from the `opDF` DataFrame using the `remove_outliers()` function created above. Focus on the characteristic called `BMI` that stands for Body Mass Index. Only discard patients (rows) that have a `BMI` that is more than `3` standard deviations (sd) from the mean. Make sure to fill in any missing `BMI` values with the median value for the `BMI` column.\n",
        "\n",
        "When you are done, print out `7` columns and `5` rows of the `opDF` DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlplC20WtO6L"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 2 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYEbLa0jtO6L"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image01b.png)\n",
        "\n",
        "After applying the `remove_outliers()` function with `sd=3`, you should have removed only 4 patients, leaving 996 patients (rows) in the `opDF` dataframe.\n",
        "\n",
        "Again, using the [**_68%-95%-99.7% Rule_**](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule), 99.7% of a normally distributed variable (in this case `BMI`) should be within 3 standard deviations of the mean. If you multiple 1000 patients X 0.997, you get 997. Again, the number of rows that were removed (4) is almost exactly what would be predicted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY90LC9atO6L"
      },
      "source": [
        "--------------------------------------\n",
        "\n",
        "## **Concatenating Rows and Columns**\n",
        "Python can concatenate rows and columns together to form new data frames. In Pandas, **_concatenation_** refers to the process of combining and merging two or more DataFrames or series along a particular axis to create a new data structure. It allows us to stack or join DataFrames/series vertically or horizontally.\n",
        "\n",
        "Pandas provides the `concat()` function to perform concatenation. By default, concatenation is done vertically along `axis 0`, resulting in a new DataFrame/series with rows appended. However, you can specify `axis=1` to concatenate horizontally, merging columns. The program does this by concatenating two columns together.\n",
        "\n",
        "In the example below concatenation will be done along `axis 1` so the columns end up side-by-side (horizontally).\n",
        "\n",
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC5O2-BatO6M"
      },
      "source": [
        "### Example 3: Concatenate Pandas columns\n",
        "\n",
        "The cell below shows the Python code for creating a new DataFrame called `catDF` by extracting the columns `Sweetness` and `Quality` from the `aqDF` DataFrame. The code then uses the Pandas `pd.concat()` function to combine these two columns together, side-by-side, into a new DataFrame called `catDF`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Ew49rOy7tO6M"
      },
      "outputs": [],
      "source": [
        "# Example 3: Concatenate Pandas columns\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a new DataFrame from Sweetness and Quality\n",
        "\n",
        "# Extract specific columns from aqDF\n",
        "col_sweet = aqDF['Sweetness']\n",
        "col_qual = aqDF['Quality']\n",
        "\n",
        "# Use Pandas concat() function to add them side-by-side\n",
        "catDF = pd.concat([col_sweet, col_qual], axis=1)\n",
        "\n",
        "# Set the display for 8 rows and all columns\n",
        "pd.set_option('display.max_rows', 8)\n",
        "pd.set_option('display.max_columns', 0)\n",
        "\n",
        "# Display new DataFrame\n",
        "display(catDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGx7QmiPtO6M"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image02a.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYOxFi1atO6M"
      },
      "source": [
        "### **Exercise 3: Concatenate Pandas columns**\n",
        "\n",
        "In the cell below write the Python code to extract the columns `ObesityCategory` and `BMI` from the `opDF` DataFrame. Use the Pandas `concat()` function to combine these two columns vertically to create a new DataFrame called `resDF` (for _result_ dataframe). Use the `display()` function to print out all of the columns and 8 rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5uqVEZJtO6M"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XBOKtzqtO6M"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image07a.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmNP1GKvtO6M"
      },
      "source": [
        "### Example 4: Concatenate Pandas Rows\n",
        "\n",
        "The **concat** function can also concatenate rows together.  The code in the cell below concatenates the first three rows and the last thress rows of the Apple Quality dataset.\n",
        "\n",
        "The code uses square bracket indexing to specify which rows to include in a new DataFrame called `catDF`. The code `[aqDF[0:3]` specifies all of the rows, from the first row, index `0`, up to but **not** including index `3` which is the 4th row.\n",
        "\n",
        "The code `aqDF[-3: ]` specifies _all_ of the rows starting at the 3rd row from the end, up to and including the last row. (Remember, leaving out an index means \"use everything\".)\n",
        "\n",
        "The code `axis=0` specifies the operation should be done on the _rows_ not the columns. If you want your operation to work on the columns instead, you would specify `axis=1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ir6bgIPBtO6M"
      },
      "outputs": [],
      "source": [
        "# Example 4: Concatenate Pandas Rows\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Use Pandas concat() function\n",
        "catDF = pd.concat([aqDF[0:3],aqDF[-3: ]], axis=0)\n",
        "\n",
        "# Set the display for all rows and 6 columns\n",
        "pd.set_option('display.max_rows', 0)\n",
        "pd.set_option('display.max_columns', 6)\n",
        "\n",
        "# Display new dataframe\n",
        "display(catDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WvoM4CxtO6M"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image03a.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTrSwMNZtO6M"
      },
      "source": [
        "### **Exercise 4: Concatenate Pandas rows**\n",
        "\n",
        "In the cell below, use the Pandas `concat()` function to concatenate the first two and last two rows of the Obesity Prevention dataset to create a new DataFrame called `resDF`. Display _all_ of the columns, and _all_ of the rows, in `resDF`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1dr3fwmtO6M"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 4 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia950WnptO6M"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image08a.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkbVOdWjtO6M"
      },
      "source": [
        "## Training and Validation\n",
        "\n",
        "We must evaluate a machine learning model based on its ability to predict values that it has never seen before. Because of this, we often divide the training data into a validation and training set. The machine learning model will learn from the training data but ultimately be evaluated based on the validation data.\n",
        "\n",
        "* **Training Data** - **In Sample Data** - The data that the neural network used to train.\n",
        "* **Validation Data** - **Out of Sample Data** - The data that the machine learning model is evaluated upon after it is fit to the training data.\n",
        "\n",
        "There are two effective means of dealing with training and validation data:\n",
        "\n",
        "* **Training/Validation Split** - The program splits the data according to some ratio between a training and validation (hold-out) set. Typical rates are 80% training and 20% validation.\n",
        "* **K-Fold Cross Validation** - The program splits the data into several folds and models. Because the program creates the same number of models as folds, the program can generate out-of-sample predictions for the entire dataset.\n",
        "\n",
        "The code below splits the data into a training and validation set. The training set uses 80% of the data, and the validation set uses 20%. Figure 2.TRN-VAL shows how we train a model on 80% of the data and then validated against the remaining 20%.\n",
        "\n",
        "**Training and Validation**\n",
        "![Training and Validation](https://biologicslab.co/BIO1173/images/class_1_train_val.png \"Training and Validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmPonsf3tO6P"
      },
      "source": [
        "### Example 5: Divide Data into Training and Validation Sets\n",
        "\n",
        "The code in the cell below begins by shuffling the `aqDF` dataset using Pandas `reindex()` method combined with the Numpy `random.permutation()` function. Once the data has been shuffled, the code splits the data in the Apple Quality dataset into a _Training_ DataFrame called `aq_TrainDf` and a _Validation_ DataFrame called `aq_validationDF`.\n",
        "\n",
        "To do this, the code first creates a _boolean mask_ called `mask` (see the next cell for a discussion of boolean masks) using this line of code:\n",
        "~~~text\n",
        "mask = np.random.rand(len(aqDF)) < 0.8\n",
        "~~~\n",
        "The code creates a `mask` that is a Numpy array containing the values either `True` or `False`. The number of `True` and `False` values is exactly the same as the number data points in the complete dataset. Approximately 80% of the values in the `mask` are `True` and the remaining are the value `False`.  \n",
        "\n",
        "To figure out the exact number of `True` values in the `mask`, we take advantage of the fact that a `True` is equal to the number `1` while a `False` is equal to the number `0`. Therefore, to compute the number of `True` values, we can simply compute the `sum` for the `mask` as shown by this line of code:\n",
        "~~~text\n",
        "print(f\"The number of True values in the mask: {sum(mask)}\")\n",
        "~~~\n",
        "\n",
        "To create the validation DataFrame, the following line of code is used:\n",
        "~~~text\n",
        "apValidationDF = pd.DataFrame(aqDF[~mask])\n",
        "~~~\n",
        "\n",
        "The character `~` before the `mask` is called a _tilde_. The ~ (tilde) operator in Python is used as a unary operator to perform the bitwise inversion operation on integers. It flips the bits of an integer, changing 0s to 1s and 1s to 0s. In other words, `[~mask]` means to flip every `True` to a `False` and every `False` to a `True`. Since the `mask` is inverted, it now \"selects\" the roughly 20% of the complete dataset that was **_not_** placed in the Training Dataframe, to be included in the Validation Dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZrxFNeotO6P"
      },
      "outputs": [],
      "source": [
        "# Example 5: Split aqDF into a training DF and a validation DF\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Make sure the training DF contains 80% of the data\n",
        "# And the validation DF contains the remaining 20%\n",
        "\n",
        "# Usually a good idea to shuffle\n",
        "aqDF = aqDF.reindex(np.random.permutation(aqDF.index))\n",
        "\n",
        "# Create a boolean mask that is 80% the length of the entire dataset\n",
        "mask = np.random.rand(len(aqDF)) < 0.80\n",
        "\n",
        "# Apply the mask to the whole data to produce the training DF\n",
        "aqTrainDF = pd.DataFrame(aqDF[mask])\n",
        "\n",
        "# Using the inverse of the mask to generate the validation DF\n",
        "aqValidationDF = pd.DataFrame(aqDF[~mask])\n",
        "\n",
        "# Print out the lengths of training and validation DF's\n",
        "print(f\"The original Apple DF: {len(aqDF)}\")\n",
        "print(f\"The number of True values in the mask: {sum(mask)}\")\n",
        "print(f\"Apple Training DF: {len(aqTrainDF)}\")\n",
        "print(f\"Apple Validation DF: {len(aqValidationDF)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN5j0gUatO6P"
      },
      "source": [
        "If your code is correct you should see something **_similar_** to the following output:\n",
        "~~~text\n",
        "The original Apple DF: 3816\n",
        "The number of True values in the mask: 3050\n",
        "Apple Training DF: 3050\n",
        "Apple Validation DF: 766\n",
        "~~~\n",
        "Since the generation the boolean mask is a _random process_, the number of records in the `Training DF` and the `Validation DF` will vary slightly each time the code is run. However, the number `True` values in the `mask` will _always_ be the same the length of `Training DF` and the sum of `Training DF` and the `Validation DF` will _always_ be equal to the same length as the original `Apple DF`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUIeM9YetO6P"
      },
      "source": [
        "------------------------------------------------------------\n",
        "## **MASKS**\n",
        "\n",
        "A mask in Pandas refers to a boolean series or a boolean array that is used to filter or select specific rows from a DataFrame based on certain conditions. It acts as a filter to determine which rows should be included or excluded in subsequent operations.\n",
        "\n",
        "The mask is a result of applying a logical condition to a dataframe, resulting in a series or an array where each element holds a boolean value indicating whether the condition is `True` or `False` for that particular row. The `mask` can be used to extract only the rows that satisfy the given condition.\n",
        "\n",
        "For example, consider a dataframe named `DF` with columns `A` and `B`. To create a mask using a condition (e.g., select rows where column `A` is greater than `5`), you can use the following syntax:\n",
        "~~~text\n",
        "mask = DF['A'] > 5\n",
        "~~~\n",
        "This will generate a _boolean series_ in mask, where each element represents if the corresponding row in column `A` satisfies the condition (`True`) or not (`False`).\n",
        "\n",
        "To apply this mask and obtain the filtered dataframe, `filteredDF`, containing only the rows where column `A` is greater than `5`, you can use the following syntax:\n",
        "~~~text\n",
        "filteredDF = DF[mask]\n",
        "~~~\n",
        "The resulting `filteredDF` will contain only the rows where the condition in the mask is `True`.\n",
        "\n",
        "Masks are particularly useful for conditional filtering operations and allow for easy extraction of subsets of data based on specific criteria.\n",
        "\n",
        "-----------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9CDcljEtO6P"
      },
      "source": [
        "### **Exercise 5: Divide Data into Training and Validation Sets**\n",
        "\n",
        "In the cell below write the Python code to split the Obesity Prediction data into a training set called `obTrainDF` with 80% of the data and a validation set called `obValidationDF` with the remaining 20%.\n",
        "\n",
        "Make sure to shuffle your data before you split it. Finally, print out the length of original dataset (i.e. `opDF`), the number of `True` values in the `mask`, as well as the lengths of the training and validation sets.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeKII6sptO6P"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 5 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuwtH_UOtO6P"
      },
      "source": [
        "If your code is correct you should see something similar, but not necessary identical to the following output:\n",
        "~~~text\n",
        "The original Obesity Predicti0n DF: 996\n",
        "The number of True values in the mask: 798\n",
        "Obesity Training DF: 798\n",
        "Validation DF: 198\n",
        "~~~\n",
        "Again, the reason for the variability in the output is due to the random process used to generate the boolean `mask`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czEm-9EItO6P"
      },
      "source": [
        "## **Converting a DataFrame to a Vector (Numpy Array)**\n",
        "\n",
        "There are several reasons why you might want to convert a Pandas DataFrame into a type of **_vector_** known as a Numpy (numeric) array.\n",
        "\n",
        "For example:\n",
        "\n",
        "* **Mathematical Operations:** If you need to perform mathematical operations on the data, converting the DataFrame to a numerical array can be advantageous. Numpy arrays allow for efficient numerical computations and provide a wide range of mathematical functions.\n",
        "\n",
        "* **Integration with Machine Learning Libraries:** Many machine learning libraries, such as Scikit-learn, expect input in the form of numerical arrays. Converting your DataFrame to a numerical array allows you to seamlessly integrate and use these libraries for tasks such as classification, regression, or clustering.\n",
        "\n",
        "* **Memory Efficiency:** Numpy arrays are more memory-efficient compared to Pandas DataFrames. If you have a large dataset and memory consumption is a concern, converting the DataFrame to a numerical array can help reduce memory usage.\n",
        "\n",
        "* **Compatibility with Statistical Packages:** Statistical packages like `SciPy` or `Statsmodels` often work more efficiently with numerical arrays. Converting your DataFrame to a numerical array can enhance compatibility and facilitate statistical analysis or hypothesis testing.\n",
        "\n",
        "* **Neural networks:** Neural networks do not directly operate on Python DataFrames.  A neural network requires a numeric vector.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXFLQZzWtO6P"
      },
      "source": [
        "### Example 6: Convert a DataFrame into a Vector\n",
        "\n",
        "For this example, we are using the complete Apple Quality dataset by re-reading the CSV datafile.\n",
        "\n",
        "The Python code in the cell below then uses the Pandas `df.values()` method to convert all the values in a DataFrame into a vector called `aqX`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F0cJAp-tO6Q"
      },
      "outputs": [],
      "source": [
        "# Example 6: Convert dataframe into a matrix\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the datafile\n",
        "aqDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/apple_quality.csv\",\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "# Use the Pandas .values method\n",
        "aqX = aqDF.values\n",
        "\n",
        "# Print first 4 values\n",
        "aqX[0:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQN8ANrVtO6Q"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "~~~text\n",
        "array([[0, -3.970048523, -2.512336381, 5.346329613, -1.012008712,\n",
        "        1.844900361, 0.329839797, -0.491590483, 'good'],\n",
        "       [1, -1.195217191, -2.839256528, 3.664058758, 1.588232309,\n",
        "        0.853285795, 0.867530082, -0.722809367, 'good'],\n",
        "       [2, -0.292023862, -1.351281995, -1.738429162, -0.342615928,\n",
        "        2.838635512, -0.038033328, 2.621636473, 'bad'],\n",
        "       [3, -0.657195773, -2.271626609, 1.324873847, -0.097874716,\n",
        "        3.637970491, -3.413761338, 0.790723217, 'good']], dtype=object)\n",
        "~~~\n",
        "\n",
        "By inspection of our vector `aqX`, we can see that the first 8 items for each observation (i.e. apple) are numeric, but the last one is a string (word), either the word `good` or `bad`.  The Pandas `pd.method` simply creates a Numpy array from a Pandas DataFrame. If we want all the values in our vector to be numeric (which we will), it will be up to us to convert any strings to numbers, before we generate our vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR8WgkW1tO6Q"
      },
      "source": [
        "### **Exercise 6: Convert a DataFrame into a Vector**\n",
        "\n",
        "In the cell below re-read the Obesity Prediction dataset from the course HTTPS server to re-create your original `opDF` DataFrame.\n",
        "\n",
        "Then use the Pandas `df.values()` method to convert values in `opDF` DataFrame into a vector called `opX`. Print out the first 4 values in  `opX`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_siaRzyxtO6Q"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 6 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_14D9dNtO6Q"
      },
      "source": [
        "If your code is correct you should see something similar to following output:\n",
        "~~~text\n",
        "array([[56, 'Male', 173.5752624383722, 71.98205082003972,\n",
        "        23.89178262396797, 4, 'Normal weight'],\n",
        "       [69, 'Male', 164.1273058223382, 89.95925553264384,\n",
        "        33.39520945079775, 2, 'Obese'],\n",
        "       [46, 'Female', 168.0722021276139, 72.93062926527617,\n",
        "        25.81773745564312, 4, 'Overweight'],\n",
        "       [32, 'Male', 168.4596328403327, 84.8869124724179,\n",
        "        29.912246975758787, 3, 'Overweight']], dtype=object)\n",
        "~~~\n",
        "\n",
        "As above, your vector `opX` will contain a mixture of numeric and string values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFG_SaCytO6Q"
      },
      "source": [
        "## Example 7: Convert a Subset of Columns to a Feature Vector\n",
        "\n",
        "One operation that you will be asked to due repeatedly in the course will be to create a feature vector containing numeric values from specific columns in a DataFrame. As you will see, there are several ways that you can handle this task.\n",
        "\n",
        "To make the code easier to understand, the process has been broken down in 3 steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK_8Og3DtO6Q"
      },
      "source": [
        "### Example 7-Step 1: Create List of Column Names\n",
        "\n",
        "In the first step, use Python's `list` function in combination with the `df.columns` method to create a list of all of the column names in the DataFrame. The name of the list is `aqX_columns`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMjNTlpRtO6Q"
      },
      "outputs": [],
      "source": [
        "# Example 7-Step 1: Create List of Column Names\n",
        "\n",
        "# Create list\n",
        "aqX_columns=list(aqDF.columns)\n",
        "\n",
        "# Print list\n",
        "aqX_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3K1jhMjtO6Q"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "~~~text\n",
        "['A_id',\n",
        " 'Size',\n",
        " 'Weight',\n",
        " 'Sweetness',\n",
        " 'Crunchiness',\n",
        " 'Juiciness',\n",
        " 'Ripeness',\n",
        " 'Acidity',\n",
        " 'Quality']\n",
        "~~~\n",
        "This list contains _all_ of the columns in our DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_UllPLptO6Q"
      },
      "source": [
        "### Example 7-Step 2: Remove Unwanted Column Names\n",
        "\n",
        "Invariably, there will be one, or more columns that you will **not** want to include in a particular feature vector. The code in the cell below shows how to use the `remove()` method to remove a specific column name from the list. Note that you can only remove one item at a time, so if you need to remove more than one column, each column has to be removed separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS6SLwfGtO6Q"
      },
      "outputs": [],
      "source": [
        "# Example 7-Step 2: Remove Unwanted Column Names\n",
        "\n",
        "# Remove each column name separately\n",
        "aqX_columns.remove('A_id')\n",
        "aqX_columns.remove('Quality')\n",
        "\n",
        "# Print list\n",
        "aqX_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X86nR7TUtO6Q"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "~~~text\n",
        "['Size',\n",
        " 'Weight',\n",
        " 'Sweetness',\n",
        " 'Crunchiness',\n",
        " 'Juiciness',\n",
        " 'Ripeness',\n",
        " 'Acidity']\n",
        "~~~\n",
        "\n",
        "Now our list contains only the column names we want to include in creating our feature vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK3Zwh2_tO6Q"
      },
      "source": [
        "### Example 7-Step 3: Generate Feature Vector\n",
        "\n",
        "Finally, we just use our list column names with the `values` method to generate our feature vector called `aqX`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xymJoIGZtO6Q"
      },
      "outputs": [],
      "source": [
        "# Example 7-Step 3: Generate Vector\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Generate X feature vector\n",
        "aqX = aqDF[aqX_columns].values\n",
        "\n",
        "# Print first 4 values\n",
        "aqX[0:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfXvMC5htO6Q"
      },
      "source": [
        "You should see the following output:\n",
        "~~~text\n",
        "array([[-3.97004852, -2.51233638,  5.34632961, -1.01200871,  1.84490036,\n",
        "         0.3298398 , -0.49159048],\n",
        "       [-1.19521719, -2.83925653,  3.66405876,  1.58823231,  0.8532858 ,\n",
        "         0.86753008, -0.72280937],\n",
        "       [-0.29202386, -1.35128199, -1.73842916, -0.34261593,  2.83863551,\n",
        "        -0.03803333,  2.62163647],\n",
        "       [-0.65719577, -2.27162661,  1.32487385, -0.09787472,  3.63797049,\n",
        "        -3.41376134,  0.79072322]])\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFpWWkIwtO6Q"
      },
      "source": [
        "## **Exercise 7: Convert a Subset of Columns to a Feature Vector**\n",
        "\n",
        "In the next 3 code cells, you are to repeat the steps shown in Example 7 to create a feature vector from your `opDF` DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TfuzbL9tO6Q"
      },
      "source": [
        "### **Exercise 7-Step 1: Create List of Column Names**\n",
        "\n",
        "Create a list containing all of the column names in `opDF` and print them out. Call your list `opX_columns`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp1kEwrbtO6Q"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 7-Step 1 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sjz2-iltO6Q"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "~~~text\n",
        "['Age',\n",
        " 'Gender',\n",
        " 'Height',\n",
        " 'Weight',\n",
        " 'BMI',\n",
        " 'PhysicalActivityLevel',\n",
        " 'ObesityCategory']\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oians46htO6Q"
      },
      "source": [
        "### **Exericse 7-Step 2: Remove Unwanted Column Names**\n",
        "\n",
        "In the cell below, remove the columns names `BMI`, `PhysicalActivityLevel` and `ObesityCategory` from you list. Print out your revised `opX_columns`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZccDyqqtO6Q"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 7-Step 2 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEBuIQ5NtO6R"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "~~~text\n",
        "['Age', 'Gender', 'Height', 'Weight']\n",
        "~~~\n",
        "\n",
        "Don't panic if you see the following error message:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image09a.png)\n",
        "\n",
        "It probably means that you re-ran the above code twice. The first time you ran it, the code removed the column names. Since the columns the second time you ran it, Python could find the column names to remove.  \n",
        "\n",
        "Just go back and re-run **Exercise 7-Step** to recreate your `opX_columns` and then re-run Step 2 again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXIVbRjrtO6R"
      },
      "source": [
        "### **Exercise 7-Step 3: Generate Feature Vector**\n",
        "\n",
        "Use your column list `opX_columns` to generate a feature vector called `opX`. Print out the first four values in `opX`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8Zq1Ab2tO6R"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 7-Step 3 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld__AukItO6R"
      },
      "source": [
        "If your code is correct you should see the following output:\n",
        "~~~text\n",
        "array([[56, 'Male', 173.5752624383722, 71.98205082003972],\n",
        "       [69, 'Male', 164.1273058223382, 89.95925553264384],\n",
        "       [46, 'Female', 168.0722021276139, 72.93062926527617],\n",
        "       [32, 'Male', 168.4596328403327, 84.8869124724179]], dtype=object)\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB-jYeNUtO6R"
      },
      "source": [
        "## Saving a DataFrame to CSV\n",
        "\n",
        "You might want to convert a Pandas DataFrame into a CSV file in various situations, including:\n",
        "\n",
        "* **Data Storage:** CSV (Comma-Separated Values) is a commonly used file format for storing tabular data. If you need to store your DataFrame as a standalone file, converting it to a CSV format allows for easy sharing, portability, and compatibility with other software.\n",
        "\n",
        "* **Data Exchange:** CSV is widely recognized and supported by numerous applications and programming languages. When you want to exchange data with other systems, convert your DataFrame to a CSV file to ensure seamless interoperability and enable the recipients to access and process the data without requiring Pandas or a specific library.\n",
        "\n",
        "* **Data Analysis:** Some statistical or data analysis software prefer CSV files as input. By converting your DataFrame to a CSV file, you can leverage these external tools or libraries for advanced analysis, visualization, or modeling.\n",
        "\n",
        "* **Database Import:** Many databases and data storage systems accept CSV files as a means of data insertion. By converting your DataFrame to a CSV file, you can easily import the data into a database, making it more manageable, searchable, and suitable for long-term storage.\n",
        "\n",
        "* **Data Backup:** Creating a CSV file from your DataFrame acts as a backup option, ensuring that your data remains accessible even if something happens to the original DataFrame or its environment. This can serve as a contingency plan or for version control purposes.\n",
        "\n",
        "Converting a Pandas DataFrame into a CSV file allows you to save, exchange, analyze, and backup your data efficiently, ensuring flexibility, compatibility, and ease of use across various applications and systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eQwgX4-tO6R"
      },
      "source": [
        "## Example 8: Read/Write a DataFrame to a local CSV file\n",
        "\n",
        "In Example 8, we first write our `opDF` DataFrame to a file, and then we read it back."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRJPeMrptO6R"
      },
      "source": [
        "### Example 8A: Write DataFrame to CSV file\n",
        "\n",
        "The code in the cell below saves (writes) the DataFrame `aqDF` to a CVS file called `AppleQualityCSV.csv`. Here we write the file to our current directory by specifying the path as a dot:\n",
        "~~~text\n",
        "# Specify the path\n",
        "path = \".\"\n",
        "~~~\n",
        "If we wanted to, we could easily write our CSV file to a different location on our computer/laptop by writing out the pathname.\n",
        "\n",
        "The code then joins the pathname to the filename using an \"operating system\" (`os.`) command:\n",
        "~~~text\n",
        "# Specify the file path and filename\n",
        "filename_write = os.path.join(path, \"AppleQualityCSV.csv\")\n",
        "~~~\n",
        "\n",
        "As an experiment, we will shuffle the data in our DataFrame before we write it to our local disk drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KZhLUBLtO6R"
      },
      "outputs": [],
      "source": [
        "# Example 8A: Write DataFrame to CSV file\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Specify the path\n",
        "path = \".\"\n",
        "\n",
        "# Specify the file path and filename\n",
        "filename_write = os.path.join(path, \"AppleQualityCSV.csv\")\n",
        "\n",
        "# Shuffle the data before saving\n",
        "aqDF = aqDF.reindex(np.random.permutation(aqDF.index))\n",
        "\n",
        "# Specify index = false to not write row numbers\n",
        "aqDF.to_csv(filename_write, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpBnPJG4tO6R"
      },
      "source": [
        "You should now see the _new_ file `AppleQualityCSV.csv` in your file browser panel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTk-B5mGtO6R"
      },
      "source": [
        "### Example 8B: Read DataFrame from a CSV file\n",
        "\n",
        "The code in the next cell reads the new file `AppleQualityCSV.csv` back into a new DataFrame called simply `df`. This is done to illustrate what data looks like when it is saved to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "N4z3P546tO6R"
      },
      "outputs": [],
      "source": [
        "# Example 8B: Read the new CSV file\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Use read_csv() function to read data and create dataframe\n",
        "df = pd.read_csv(\"./AppleQualityCSV.csv\", na_values=['NA','?'])\n",
        "\n",
        "\n",
        "# Set the display for 12 rows and 6 columns\n",
        "pd.set_option('display.max_rows', 12)\n",
        "pd.set_option('display.max_columns', 6)\n",
        "\n",
        "# Display 6 columns and 12 rows of the dataframe\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV-wWU2FtO6R"
      },
      "source": [
        "If your code is correct you should see the following table:\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image10a.png)\n",
        "\n",
        "Notice that even though we had shuffled the data before writing it the CSV file in Example 8A, the index numbers at the left have been restored. The data is still scrambled (compared to the original CSV datafile) but Pandas writes the dataframe using sequential index numbers.\n",
        "\n",
        "As you will see shortly, this is _not_ what will happen below when the data is written and then read back into memory using Pickel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC72ibBUtO6R"
      },
      "source": [
        "## **Exercise 8: Read/Write a DataFrame to a local CSV file**\n",
        "\n",
        "### **Exercise 8A: Write DataFrame to CSV file**\n",
        "\n",
        "In the cell below, save the DataFrame `opDF` to a CVS file called `ObesityPredictionCSV.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6ZZ6m1rtO6R"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 8A here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YRuwyLktO6R"
      },
      "source": [
        "You should now see the file `ObesityPredictionCSV.csv` in your file browser panel. There is no need to read your new CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuuYpS8KtO6R"
      },
      "source": [
        "## Saving a DataFrame to Pickle\n",
        "\n",
        "A variety of software programs can use text files stored as CSV. However, they take longer to generate and can sometimes lose small amounts of precision in the conversion. Generally, you will output to CSV because it is very compatible, even outside of Python.\n",
        "\n",
        "Another file format is [Pickle](https://docs.python.org/3/library/pickle.html). The code below stores the DataFrame to the Pickle file format. Pickle stores data in the **_exact binary representation_** used by Python. The benefit is that there is no loss of data going to CSV format. The disadvantage is that generally, only Python programs can read Pickle files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGOhRYPetO6R"
      },
      "source": [
        "### Example 9: Saving a DataFrame to Pickle\n",
        "\n",
        "The code in the cell below saves the DataFrame `apOrigDF` to a Pickel file called `AppleQualityPickel.pkl`. Before you can either read or save a Pickel file, you need to import the `pickel` module.\n",
        "\n",
        "The line of code:\n",
        "~~~text\n",
        "with open(filename_write,\"wb\") as fp:\n",
        "~~~\n",
        "opens a _file pointer_ `fp`. The argument `\"wb\"` tells Python that you are going to Write Binary data to the file you just opened.\n",
        "\n",
        "After writing to the file, the command `fp.close()` is used to _close the file_. Closing the file is essential to release system resources and ensure that any changes or data in the buffer are properly written to the disk. It is recommended to close the file as soon as you have finished working with it, to maintain good programming practices and avoid potential issues with file handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUEd2tIgtO6R"
      },
      "outputs": [],
      "source": [
        "# Example 9: Save a DataFrame to pickle\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Specify the path\n",
        "path = \".\"\n",
        "\n",
        "# Specify the file path and filename\n",
        "filename_write = os.path.join(path, \"AppleQualityPickel.pkl\")\n",
        "\n",
        "# Shuffle the data before saving\n",
        "aqDF = aqDF.reindex(np.random.permutation(aqDF.index))\n",
        "\n",
        "# Write out the dataframe to a pickel file\n",
        "with open(filename_write,\"wb\") as fp:\n",
        "    pickle.dump(aqDF, fp)\n",
        "\n",
        "# Close the file after writing to it\n",
        "fp.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxHhJClVtO6R"
      },
      "source": [
        "You should now see the file `AppleQualityPickel.pkl` in your file browser panel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhPLEaMjtO6R"
      },
      "source": [
        "### **Exercise 9: Saving a DataFrame to Pickle**\n",
        "\n",
        "In the cell below, save your DataFrame `opDF` to a Pickel file called `ObesityPredictionPickel.pkl`. Make sure to shuffle the data before writing to the Pickel file and to close the file after you are done writing to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3Ik494ctO6R"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 9 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykJy7Kp9tO6R"
      },
      "source": [
        "You should now see the file `ObesityPredictionPickel.pkl` in your file browser panel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71jIuwBntO6R"
      },
      "source": [
        "## Loading a Pickel File into Memory\n",
        "\n",
        "You might want to load a Pickle file into memory in the following scenarios:\n",
        "\n",
        "* **Preserving Data Structure:** When you have a complex data structure, such as a nested dictionary, list of dictionaries, or custom objects, Pickle allows you to serialize and deserialize the data, preserving its original structure. Loading a Pickle file back into memory ensures that you can restore the data structure exactly as it was when it was saved.\n",
        "\n",
        "* **Efficient Storage and Retrieval:** Pickle provides a convenient way to store large amounts of data in a compact binary format. If you have large datasets or complex objects that you need to save and retrieve efficiently, using Pickle files can offer significant advantages over other formats like CSV or JSON.\n",
        "\n",
        "* **Python Object Serialization:** Pickle is a native Python module, specifically designed for serializing Python objects. If you have Python-specific objects or data structures that you want to save and restore without losing any information or functionality, loading a Pickle file allows you to recreate the objects exactly as they were when they were pickled.\n",
        "\n",
        "* **Data Persistence:** Pickle can be used for persistent storage of data, allowing you to save and load data between program executions. By loading a Pickle file into memory, you can access and utilize the previously saved data, eliminating the need to recreate or recalculate it each time the program runs.\n",
        "\n",
        "* **Easy Interoperability:** Pickle files can be shared and used across different Python environments and versions, ensuring compatibility and easy transfer of data between systems. Loading a Pickle file is a quick and efficient way to import saved data in a format that can be readily understood and processed by any environment that supports Pickle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZLmSRBxtO6S"
      },
      "source": [
        "### Example 10: Load a Pickel file into memory\n",
        "\n",
        "Loading the pickle file back into memory is accomplished by the following lines of code.  Here we are using Pickel's `load()` function to read the file and create a new DataFrame called `aqPickelDF`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTgGJ9WmtO6S"
      },
      "outputs": [],
      "source": [
        "# Example 10: Load a pickel file into memory\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Specify the path\n",
        "path = \".\"\n",
        "\n",
        "# Specify the file path and filename\n",
        "filename_read = os.path.join(path, \"AppleQualityPickel.pkl\")\n",
        "\n",
        "# Open up a file pointer fp\n",
        "with open(filename_write,\"rb\") as fp:\n",
        "    aqPickelDF = pickle.load(fp)\n",
        "\n",
        "# Close the file after reading it\n",
        "fp.close()\n",
        "\n",
        "# Set the display for 8 rows and 7 columns\n",
        "pd.set_option('display.max_rows', 8)\n",
        "pd.set_option('display.max_columns', 7)\n",
        "\n",
        "# Display the new dataframe\n",
        "display(aqPickelDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iADw1OBtO6S"
      },
      "source": [
        "If your code if correct you should see something similar to the following table:\n",
        "\n",
        "![_](https://biologicslab.co/BIO1173/images/class_02/class_02_1_image11a.png)\n",
        "\n",
        "Notice that the index numbers at the left side are still jumbled from the previous shuffle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpFj-jrrtO6S"
      },
      "source": [
        "-----------------------------------\n",
        "\n",
        "## **Pickel Module**\n",
        "\n",
        "Python's **_pickle module_** provides functionality for serializing and deserializing Python objects. The `pickle.load()` method is used to deserialize and load a serialized object from a Pickle file or a file-like object.\n",
        "\n",
        "Here are key points about the `pickle.load()` method:\n",
        "\n",
        "* **Purpose:** The `pickle.load()` method is used to deserialize and load a Pickle object that was previously serialized with pickle.dump() or pickle.dumps().\n",
        "\n",
        "* **Usage:** To use `pickle.load()`, you first need to open a file in binary mode using the `open()` function and pass the file object as an argument to `pickle.load()`. Alternatively, you can use a file-like object that supports the necessary read operations.\n",
        "\n",
        "* **Deserialization:** `pickle.load()` reads the serialized object from the file or file-like object and reconstructs the original Python object in memory. It restores the object's state, including its attributes, methods, and other details.\n",
        "\n",
        "* **Unpickling Security:** It's crucial to note that loading Pickle data can have security implications. Untrusted or malicious Pickle files can execute arbitrary code when loaded using `pickle.load()`. Only load Pickle data from trusted sources to prevent security risks.\n",
        "\n",
        "* **Handling Different Pickle Formats:** The `pickle.load()` method can handle different Pickle format versions. It detects the Pickle format automatically and loads the object accordingly.\n",
        "\n",
        "* **Closing the File:** After loading the Pickle object successfully, remember to close the file using the close() method of the file object or by utilizing a with statement to ensure proper resource management.\n",
        "\n",
        "Using the `pickle.load()` method provides a convenient way to deserialize and load serialized Python objects. However, be cautious and load Pickle data only from trusted sources to avoid potential security vulnerabilities.\n",
        "\n",
        "------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi-3HxDHtO6S"
      },
      "source": [
        "### **Exercise 10: Load a Pickel file into memory**\n",
        "\n",
        "In the cell below, load the pickle file `ObesityPredictionPickel.pkl` back into memory  using Pickel's `load()` function, creating a DataFrame called `opPickelDF`. Display 7 columns and 8 rows of `opPickelDF`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD0aXfddtO6S"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 10 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaHfBZEstO6S"
      },
      "source": [
        "If your code is correct you should see something similar the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_02_1_Pickel1.png).\n",
        "\n",
        "Notice that the index numbers at the left side are still jumbled from the previous shuffle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U869rmztO6S"
      },
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_02_1.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Poly-A Tail**\n",
        "\n",
        "![___](https://upload.wikimedia.org/wikipedia/commons/d/d3/Glen_Beck_and_Betty_Snyder_program_the_ENIAC_in_building_328_at_the_Ballistic_Research_Laboratory.jpg)\n",
        "\n",
        "**ENIAC** (/ˈɛniæk/; Electronic Numerical Integrator and Computer) was the first programmable, electronic, general-purpose digital computer, completed in 1945. Other computers had some of these features, but ENIAC was the first to have them all. It was Turing-complete and able to solve \"a large class of numerical problems\" through reprogramming.\n",
        "\n",
        "ENIAC was designed by John Mauchly and J. Presper Eckert to calculate artillery firing tables for the United States Army's Ballistic Research Laboratory (which later became a part of the Army Research Laboratory). However, its first program was a study of the feasibility of the thermonuclear weapon.\n",
        "\n",
        "ENIAC was completed in 1945 and first put to work for practical purposes on December 10, 1945.\n",
        "\n",
        "ENIAC was formally dedicated at the University of Pennsylvania on February 15, 1946, having cost $487,000 (equivalent to $6,900,000 in 2023), and called a \"Giant Brain\" by the press. It had a speed on the order of one thousand times faster than that of electro-mechanical machines.\n",
        "\n",
        "ENIAC was formally accepted by the U.S. Army Ordnance Corps in July 1946. It was transferred to Aberdeen Proving Ground in Aberdeen, Maryland in 1947, where it was in continuous operation until 1955.\n",
        "\n",
        "The 1948 Manchester Baby was the first machine to contain all the elements essential to a modern electronic digital computer, as it could be reprogrammed electronically to hold stored programs instead of requiring setting of switches to program as ENIAC did.\n",
        "\n",
        "**Development and design**\n",
        "\n",
        "ENIAC's design and construction was financed by the United States Army, Ordnance Corps, Research and Development Command, led by Major General Gladeon M.Barnes. The total cost was about $487,000, equivalent to $6,900,000 in 2023.[14] The conception of ENIAC began in June 1941, when Friden calculators and differential analyzers were used by the United States Army Ordnance Department to compute firing tables for artillery, which was done by graduate students under John Mauchly's supervision. Mauchly began to wonder if electronics could be applied to mathematics for faster calculations. He partnered up with research associate J. Presper Eckert, as Mauchly wasn't an electronics expert, to draft an electronic computer that could work at an excellent pace. Later in August 1942, Mauchly proposed an all-electronic calculating machine that could help the U.S. Army calculate complex ballistics tables. The U.S. Army Ordnance accepted their plan, giving the University of Pennsylvania a six-months research contract for $61,700. The construction contract was signed on June 5, 1943; work on the computer began in secret at the University of Pennsylvania's Moore School of Electrical Engineering the following month, under the code name \"Project PX\", with John Grist Brainerd as principal investigator. Herman H. Goldstine persuaded the Army to fund the project, which put him in charge to oversee it for them. Assembly for the computer began in June 1944. Later in September of that year, Eckert and Mauchly completed their conception on the computer. Construction for the computer was complete in May 1945, and testing for it began at the Moore School. Later in November of that year, the duo, along with John Brainerd and Herman Goldstine, issued the first confidential published report on the computer, which talks about how it worked and the methods by which it was programmed.\n",
        "\n",
        "ENIAC was designed by Ursinus College physics professor John Mauchly and J. Presper Eckert of the University of Pennsylvania, U.S.[21] The team of design engineers assisting the development included Robert F. Shaw (function tables), Jeffrey Chuan Chu (divider/square-rooter), Thomas Kite Sharpless (master programmer), Frank Mural (master programmer), Arthur Burks (multiplier), Harry Huskey (reader/printer) and Jack Davis (accumulators).[22] Significant development work was undertaken by the female mathematicians who handled the bulk of the ENIAC programming: Jean Jennings, Marlyn Wescoff, Ruth Lichterman, Betty Snyder, Frances Bilas, and Kay McNulty.[23] In 1946, the researchers resigned from the University of Pennsylvania and formed the Eckert–Mauchly Computer Corporation.\n",
        "\n",
        "ENIAC was a large, modular computer, composed of individual panels to perform different functions. Twenty of these modules were accumulators that could not only add and subtract, but hold a ten-digit decimal number in memory. Numbers were passed between these units across several general-purpose buses (or trays, as they were called). In order to achieve its high speed, the panels had to send and receive numbers, compute, save the answer and trigger the next operation, all without any moving parts. Key to its versatility was the ability to branch; it could trigger different operations, depending on the sign of a computed result."
      ],
      "metadata": {
        "id": "W8UHHedC0Fc3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cav99caj02hI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}