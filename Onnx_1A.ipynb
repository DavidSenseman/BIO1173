{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBXSg9tMOT4uVqrlVP8/0V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Onnx_1A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NssSnfzn2vAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWfA2_Wj2nCD",
        "outputId": "de56aef0-00bf-421e-b112-b880e8c54247"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: Using Google CoLab\n",
            "david.senseman@gmail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "4qFfrF6qut9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXF3OJKbxdq-",
        "outputId": "39d118e4-d882-48ee-aad3-3a36b399e23f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install Required Packages"
      ],
      "metadata": {
        "id": "TFO8nSwv2mrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall the existing version\n",
        "!pip uninstall -y tf2onnx\n",
        "\n",
        "# Reinstall from GitHub\n",
        "!pip install git+https://github.com/onnx/tensorflow-onnx\n",
        "\n",
        "# Verify version\n",
        "import tf2onnx\n",
        "print(tf2onnx.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNGDbN2_uuhe",
        "outputId": "7d2a61c7-2a15-4011-d254-aac5e72806b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tf2onnx 1.16.1\n",
            "Uninstalling tf2onnx-1.16.1:\n",
            "  Successfully uninstalled tf2onnx-1.16.1\n",
            "Collecting git+https://github.com/onnx/tensorflow-onnx\n",
            "  Cloning https://github.com/onnx/tensorflow-onnx to /tmp/pip-req-build-kd34ecwg\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/onnx/tensorflow-onnx /tmp/pip-req-build-kd34ecwg\n",
            "  Resolved https://github.com/onnx/tensorflow-onnx to commit c34ac1d751427cf5d98023a21cce4c82b0cf96a1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from tf2onnx==1.16.1) (1.26.4)\n",
            "Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from tf2onnx==1.16.1) (1.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from tf2onnx==1.16.1) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from tf2onnx==1.16.1) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.12/dist-packages (from tf2onnx==1.16.1) (25.2.10)\n",
            "Requirement already satisfied: protobuf~=3.20 in /usr/local/lib/python3.12/dist-packages (from tf2onnx==1.16.1) (3.20.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->tf2onnx==1.16.1) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->tf2onnx==1.16.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->tf2onnx==1.16.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->tf2onnx==1.16.1) (2025.8.3)\n",
            "Building wheels for collected packages: tf2onnx\n",
            "\u001b[33m  DEPRECATION: Building 'tf2onnx' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'tf2onnx'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for tf2onnx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf2onnx: filename=tf2onnx-1.16.1-py3-none-any.whl size=456684 sha256=2145e79c257069876759caadf204ddf3ad04983afd51d5dc61995d2f92d81f30\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-33hhfetd/wheels/99/79/78/b2fe17d2abb23d9d75a4d8b998d4d53a8e62dd3b59769fe2b4\n",
            "Successfully built tf2onnx\n",
            "Installing collected packages: tf2onnx\n",
            "Successfully installed tf2onnx-1.16.1\n",
            "1.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Copy Model from Google Drive"
      ],
      "metadata": {
        "id": "MImolJLV2mdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the path to your model in Google Drive\n",
        "gdrive_model_path = '/content/drive/MyDrive/ResNet50_model_244.keras'\n",
        "\n",
        "# Copy the model to current working directory\n",
        "local_model_path = '/content/ResNet50_model_244.keras'\n",
        "shutil.copy(gdrive_model_path, local_model_path)\n",
        "\n",
        "print(f\"Model copied from Google Drive to: {local_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Amm1euHvUyqR",
        "outputId": "3af74200-df44-4f0c-be70-b7c50df4fea1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model copied from Google Drive to: /content/ResNet50_model_244.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Load the Keras Model\n"
      ],
      "metadata": {
        "id": "q2rubDFQUvqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(\"Loading Keras model...\")\n",
        "model = keras.models.load_model(local_model_path)\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Display model information\n",
        "print(f\"Model summary:\")\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "VoLJTkmzVLX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Get Model Input Shape and Create Test Data"
      ],
      "metadata": {
        "id": "8WEWcJNHUwQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model input shape for testing\n",
        "input_shape = model.input_shape\n",
        "print(f\"\\nInput shape: {input_shape}\")\n",
        "\n",
        "# Create sample input data for testing\n",
        "import numpy as np\n",
        "\n",
        "# Generate random test data matching your model's expected input\n",
        "if len(input_shape) == 4:  # Image input\n",
        "    batch_size = 1\n",
        "    height, width, channels = input_shape[1], input_shape[2], input_shape[3]\n",
        "    test_input = np.random.random((batch_size, height, width, channels)).astype(np.float32)\n",
        "else:\n",
        "    # For other input types, adjust accordingly\n",
        "    test_input = np.random.random(input_shape).astype(np.float32)\n",
        "\n",
        "print(f\"Test input shape: {test_input.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbOa8Y3aVdw2",
        "outputId": "b789e218-6d0f-4591-db8c-a8f7b9eca64d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input shape: (None, 244, 244, 3)\n",
            "Test input shape: (1, 244, 244, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Convert Keras Model to ONNX Format"
      ],
      "metadata": {
        "id": "eO16F9FfVi4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tf2onnx for conversion\n",
        "import tf2onnx\n",
        "\n",
        "print(\"Converting Keras model to ONNX format...\")\n",
        "\n",
        "try:\n",
        "    # Create the input specification for conversion\n",
        "    spec = (tf.TensorSpec(input_shape, tf.float32, name=\"input\"),)\n",
        "\n",
        "    # Convert the model to ONNX\n",
        "    output_path = \"/content/ResNet50_model_244.onnx\"\n",
        "\n",
        "    onnx_graph = tf2onnx.convert.from_keras(\n",
        "        model,\n",
        "        input_signature=spec,\n",
        "        output_path=output_path,\n",
        "        opset=13  # Use ONNX opset version 13\n",
        "        #enable_onnx_checker=False  # Skip ONNX checker to avoid potential issues\n",
        "    )\n",
        "\n",
        "    print(f\"Model converted successfully!\")\n",
        "    print(f\"ONNX model saved to: {output_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Conversion failed with error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrSnvoynVnje",
        "outputId": "2ef8cf70-a0a7-4b31-afa4-549abbb460eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting Keras model to ONNX format...\n",
            "Model converted successfully!\n",
            "ONNX model saved to: /content/ResNet50_model_244.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lVcvMpCBVh6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kmqxGk1tUwzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BlYinlpZUxqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify ONNX model\n",
        "try:\n",
        "    import onnx\n",
        "\n",
        "    # Load and check the ONNX model\n",
        "    onnx_model_loaded = onnx.load(output_path)\n",
        "    onnx.checker.check_model(onnx_model_loaded)\n",
        "    print(\"ONNX model validation successful!\")\n",
        "\n",
        "    # Display ONNX model information\n",
        "    print(\"\\nONNX Model Information:\")\n",
        "    print(f\"Model name: {onnx_model_loaded.graph.name}\")\n",
        "    print(f\"Number of inputs: {len(onnx_model_loaded.graph.input)}\")\n",
        "    print(f\"Number of outputs: {len(onnx_model_loaded.graph.output)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ONNX model validation failed: {e}\")\n",
        "\n",
        "# Run inference using ONNX Runtime\n",
        "import onnxruntime as ort\n",
        "\n",
        "print(\"\\nSetting up ONNX Runtime session...\")\n",
        "try:\n",
        "    session_options = ort.SessionOptions()\n",
        "    session_options.log_verbosity_level = 0  # Reduce logging verbosity\n",
        "\n",
        "    # Create ONNX Runtime session\n",
        "    ort_session = ort.InferenceSession(output_path, session_options)\n",
        "\n",
        "    # Get input and output names\n",
        "    input_name = ort_session.get_inputs()[0].name\n",
        "    output_name = ort_session.get_outputs()[0].name\n",
        "\n",
        "    print(f\"Input name: {input_name}\")\n",
        "    print(f\"Output name: {output_name}\")\n",
        "\n",
        "    # Run inference using ONNX Runtime\n",
        "    print(\"\\nRunning inference with ONNX Runtime...\")\n",
        "    ort_outputs = ort_session.run([output_name], {input_name: test_input})\n",
        "\n",
        "    print(\"ONNX Runtime inference completed successfully!\")\n",
        "    print(f\"Output shape: {ort_outputs[0].shape}\")\n",
        "    print(f\"First few output values: {ort_outputs[0][0][:5]}\")\n",
        "\n",
        "    # Compare with original Keras model\n",
        "    print(\"\\nComparing results between Keras and ONNX Runtime:\")\n",
        "    keras_output = model.predict(test_input)\n",
        "\n",
        "    # Calculate difference\n",
        "    difference = np.abs(keras_output - ort_outputs[0])\n",
        "    max_diff = np.max(difference)\n",
        "    mean_diff = np.mean(difference)\n",
        "\n",
        "    print(f\"Max absolute difference: {max_diff}\")\n",
        "    print(f\"Mean absolute difference: {mean_diff}\")\n",
        "\n",
        "    # If difference is small, the conversion was successful\n",
        "    if max_diff < 1e-4:\n",
        "        print(\"Conversion and inference successful! Models produce nearly identical results.\")\n",
        "    else:\n",
        "        print(\"Small differences detected, but conversion likely successful.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ONNX Runtime inference failed: {e}\")\n",
        "    print(\"This might be due to model complexity or ONNX version incompatibilities.\")\n",
        "\n",
        "# Display inference time comparison\n",
        "try:\n",
        "    import time\n",
        "\n",
        "    # Time Keras inference\n",
        "    start_time = time.time()\n",
        "    keras_pred = model.predict(test_input)\n",
        "    keras_time = time.time() - start_time\n",
        "\n",
        "    # Time ONNX Runtime inference\n",
        "    start_time = time.time()\n",
        "    ort_pred = ort_session.run([output_name], {input_name: test_input})\n",
        "    ort_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nInference Time Comparison:\")\n",
        "    print(f\"Keras model: {keras_time:.6f} seconds\")\n",
        "    print(f\"ONNX Runtime: {ort_time:.6f} seconds\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during timing comparison: {e}\")\n",
        "\n",
        "print(\"\\nDemo completed successfully!\")"
      ],
      "metadata": {
        "id": "eIbA8PIiDFOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938f060b-2baf-45a4-a51b-aff7937ef739"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model validation successful!\n",
            "\n",
            "ONNX Model Information:\n",
            "Model name: tf2onnx\n",
            "Number of inputs: 1\n",
            "Number of outputs: 1\n",
            "\n",
            "Setting up ONNX Runtime session...\n",
            "Input name: input\n",
            "Output name: dense_1\n",
            "\n",
            "Running inference with ONNX Runtime...\n",
            "ONNX Runtime inference completed successfully!\n",
            "Output shape: (1, 5)\n",
            "First few output values: [0.8828796  0.04820953 0.06018713 0.00569142 0.00303225]\n",
            "\n",
            "Comparing results between Keras and ONNX Runtime:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
            "Max absolute difference: 1.1920928955078125e-07\n",
            "Mean absolute difference: 5.774199962615967e-08\n",
            "Conversion and inference successful! Models produce nearly identical results.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step\n",
            "\n",
            "Inference Time Comparison:\n",
            "Keras model: 0.384204 seconds\n",
            "ONNX Runtime: 0.174383 seconds\n",
            "\n",
            "Demo completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DRzCSUIHbyo",
        "outputId": "696e4b65-5803-416d-a084-6d677ccff7e5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  ResNet50_model_244.keras  ResNet50_model_244.onnx  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K0dd7cxx_2wr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}