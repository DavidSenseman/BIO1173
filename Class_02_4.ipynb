{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_02_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- BIO1173_CLASS_02_4 -->"
      ],
      "metadata": {
        "id": "EW_fcsKub0_N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYZVwSpdbE3Y"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExN-OzpYbE3Y"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4imk1kbE3Y"
      },
      "source": [
        "##### **Module 2: Neural Networks with PyTorch**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 2 Material\n",
        "\n",
        "* Part 2.1: Introduction to Neural Networks with PyTorch\n",
        "* Part 2.2: Encoding Feature Vectors\n",
        "* Part 2.3: Controlling Overfitting\n",
        "* **Part 2.4: Saving and Loading a PyTorch Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_-lPkxLbE3Z"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "```text\n",
        "Mounted at /content/drive\n",
        "Note: Using Google CoLab\n",
        "studentbio1173@gmail.com\n",
        "```\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "xG3_sXTDfyjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Custom Functions\n",
        "\n",
        "Run the cell below to create one (or more) function(s) needed for this lesson."
      ],
      "metadata": {
        "id": "Mu5xJAWl_9vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to print out elasped time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "tnwowmLuABDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving and Loading a PyTorch Neural Network**\n",
        "\n",
        "Complex neural networks will take a _long_ time to train.  It is helpful to be able to save a trained neural network so that you can reload it and using it again--a reloaded neural network will **not** require retraining.  \n",
        "\n",
        "PyTorch provides the following formats for saving neural networks:\n",
        "\n",
        "* **State Dict** - Stores the model's state dictionary (weights and biases) in a format that can be easily loaded back into the same model architecture.\n",
        "* **Full Model** - Stores the complete neural network including both the model architecture and weights in a single file.\n",
        "\n",
        "Usually, you will want to save using the state dict approach as it's more flexible and memory efficient. The state dict only saves the parameters (weights and biases) while keeping the model class definition separate, allowing you to load the weights into different model architectures if needed.\n",
        "\n",
        "A primary goal of this lesson is to convince you that a PyTorch model that you \"regenerate\" from a file that was saved to your Google Drive, is exactly the same (i.e. same accuracy) as the original PyTorch model you created and trained."
      ],
      "metadata": {
        "id": "tggftLlR0pd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1A: Build and Train a Classification Neural Network\n",
        "\n",
        "The code in `Example 1A` builds, compiles and trains a neural network called `or_model` that can classify the Quality of an orange based on its physical and chemical characteristics.\n",
        "\n",
        "The code in the cell below reads the Orange Quality dataset from the course HTTP server and creates a DataFrame called `or_df` (i.e. \"orange\" DataFrame) using this code snippet:\n",
        "\n",
        "```Python\n",
        "or_df = pd.read_csv(\n",
        "\"https://biologicslab.co/BIO1173/data/orange_quality.csv\",\n",
        "    na_values=['NA', '?'])\n",
        "```\n",
        "\n",
        "In order to create a feature vector, the 3 non-numeric columns in the dataset: `Color`, `Variety` and `Blemished` must be pre-processed as follows:\n",
        "1. **Mapping strings to integers:** This is used to take care of the column `Color` which contains several string values. Here is the code snippet that does the mapping:\n",
        "\n",
        "```Python\n",
        "# Map str to int\n",
        "or_mapping = {'Orange':0,'Deep Orange':1,'Light Orange':2,'Orange-Red':3,'Yellow-Orange':4} or_df['Color'] = or_df['Color'].map(or_mapping)\n",
        "```\n",
        "\n",
        "2. **Normalization:** The following code chunk identifies which columns in `or_df` are numeric and then applies Z-normalization to the numeric values.\n",
        "    \n",
        "```Python\n",
        "# Standardise all numeric column with z‑score\n",
        "or_numeric_cols = or_df.select_dtypes(include=['int64', 'float64']).columns or_df[or_numeric_cols] = or_df[or_numeric_cols].apply(zscore) numeric_cols = or_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "```\n",
        "\n",
        "3. **Exclude Columns:** To take care the string values in the columns `Variety` and `Blemished`, we will simply exclude them from the list of columns to be used for creating the X feature vector `or_X`:\n",
        "\n",
        "```Python\n",
        "# Generate X-values\n",
        "or_X = or_df[['Size (cm)', 'Weight (g)', 'Brix (Sweetness)', 'pH (Acidity)','Softness (1-5)', 'HarvestTime (days)', 'Ripeness (1-5)','Color']].values or_X = np.asarray(or_X).astype('float32')\n",
        "```\n",
        "\n",
        "Since we are building a classification neural network, we will need to one-hot encode the column `Quality (1-5)` which contains the `Y-values` using this code snippet:\n",
        "\n",
        "```Python\n",
        "# Generate y-values (one-hot encoding)\n",
        "or_dummies = pd.get_dummies(or_df['Quality (1-5)'], dtype=int)\n",
        "or_Y = or_dummies.values\n",
        "or_Y = np.asarray(or_Y).astype('float32')\n",
        "```\n",
        "\n",
        "It should be noted that this column is already numeric, so we are **not** using one-hot encoding to replace string values with integer. Rather, one-hot encoding the `Y-values` is necessary to give them the **correct format** for the neural network.\n"
      ],
      "metadata": {
        "id": "_ZKRKpc-1prJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 1A: Build and Train a Classification Neural Network\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0️⃣  Imports\n",
        "# ------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣  Parameters\n",
        "# ------------------------------------------------------------\n",
        "EPOCHS        = 100\n",
        "PATIENCE      = 10\n",
        "VERBOSE       = 2\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣  Load data\n",
        "# ------------------------------------------------------------\n",
        "or_df = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/orange_quality.csv\",\n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "#  3️⃣ Preprocessing\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Map str to int\n",
        "or_mapping = {'Orange':0,'Deep Orange':1,\n",
        "           'Light Orange':2,'Orange-Red':3,\n",
        "           'Yellow-Orange':4}\n",
        "or_df['Color'] = or_df['Color'].map(or_mapping)\n",
        "\n",
        "# Standardise all numeric column with z‑score\n",
        "or_numeric_cols = or_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "or_df[or_numeric_cols] = or_df[or_numeric_cols].apply(zscore)\n",
        "\n",
        "\n",
        "# Generate X-values\n",
        "or_X = or_df[['Size (cm)', 'Weight (g)', 'Brix (Sweetness)', 'pH (Acidity)',\n",
        "       'Softness (1-5)', 'HarvestTime (days)', 'Ripeness (1-5)',\n",
        "        'Color']].values\n",
        "or_X = np.asarray(or_X).astype('float32')\n",
        "\n",
        "# Generate Y-values\n",
        "or_dummies = pd.get_dummies(or_df['Quality (1-5)'], dtype=int) # Classification\n",
        "or_Y = or_dummies.values\n",
        "or_Y = np.asarray(or_Y).astype('float32')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣  Prepare data for PyTorch\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "or_X_tensor = torch.FloatTensor(or_X)\n",
        "or_y_tensor = torch.LongTensor(np.argmax(or_Y, axis=1))  # Convert one-hot to class indices\n",
        "\n",
        "# Split into train and validation sets\n",
        "or_X_train, or_X_val, or_y_train, or_y_val = train_test_split(\n",
        "    or_X_tensor, or_y_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "or_train_dataset = TensorDataset(or_X_train, or_y_train)\n",
        "or_val_dataset = TensorDataset(or_X_val, or_y_val)\n",
        "or_train_loader = DataLoader(or_train_dataset, batch_size=32, shuffle=True)\n",
        "or_val_loader = DataLoader(or_val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣   Define model\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "class OrangeQualityModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(OrangeQualityModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)  # No softmax here - it's included in CrossEntropyLoss\n",
        "        return x\n",
        "\n",
        "# Create model instance\n",
        "or_input_dim = or_X_train.shape[1]\n",
        "or_num_classes = or_Y.shape[1]\n",
        "or_model = OrangeQualityModel(or_input_dim, or_num_classes)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6️⃣  Define optimizer and loss function\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(or_model.parameters(), lr=0.001)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7️⃣  Training loop with early stopping and history tracking\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Initialize history tracking lists\n",
        "or_train_losses = []\n",
        "or_val_losses = []\n",
        "or_train_accuracies = []\n",
        "or_val_accuracies = []\n",
        "\n",
        "# Early stopping variables\n",
        "best_val_acc = 0.0\n",
        "patience_counter = 0\n",
        "\n",
        "print(f\"------Training Starting for {EPOCHS} epochs --------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Training phase\n",
        "    or_model.train()\n",
        "    or_train_loss = 0.0\n",
        "    or_train_correct = 0\n",
        "    or_train_total = 0\n",
        "\n",
        "    for inputs, labels in or_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = or_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        or_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        or_train_total += labels.size(0)\n",
        "        or_train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Validation phase\n",
        "    or_model.eval()\n",
        "    or_val_loss = 0.0\n",
        "    or_val_correct = 0\n",
        "    or_val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in or_val_loader:\n",
        "            outputs = or_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            or_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            or_val_total += labels.size(0)\n",
        "            or_val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate accuracies\n",
        "    or_train_acc = 100. * or_train_correct / or_train_total\n",
        "    or_val_acc = 100. * or_val_correct / or_val_total\n",
        "\n",
        "    # Store history for plotting\n",
        "    or_train_losses.append(or_train_loss/len(or_train_loader))\n",
        "    or_val_losses.append(or_val_loss/len(or_val_loader))\n",
        "    or_train_accuracies.append(or_train_acc)\n",
        "    or_val_accuracies.append(or_val_acc)\n",
        "\n",
        "    if VERBOSE > 0:\n",
        "        print(f'Epoch [{epoch+1}/{EPOCHS}]')\n",
        "        print(f'Train Loss: {or_train_loss/len(or_train_loader):.4f}, '\n",
        "              f'Train Acc: {or_train_acc:.2f}%')\n",
        "        print(f'Val Loss: {or_val_loss/len(or_val_loader):.4f}, '\n",
        "              f'Val Acc: {or_val_acc:.2f}%')\n",
        "        print('-' * 50)\n",
        "\n",
        "\n",
        "    # Early stopping logic\n",
        "    if or_val_acc > best_val_acc:\n",
        "        best_val_acc = or_val_acc\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(or_model.state_dict(), \"or_best_classification_model.pth\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 8️⃣  Inspect training - Now with proper history tracking\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "print(f\"\\nTraining finished.\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))\n",
        "\n",
        "# Show the best validation accuracy and loss (same as original)\n",
        "best_val_acc_history = max(or_val_accuracies) if or_val_accuracies else 0.0\n",
        "best_val_loss_history = min(or_val_losses) if or_val_losses else float('inf')\n",
        "\n",
        "print(f\"Best validation accuracy: {best_val_acc_history:.4f}\")\n",
        "print(f\"Best validation loss: {best_val_loss_history:.4f}\")\n",
        "\n",
        "# Load the best model for final evaluation if needed\n",
        "or_model.load_state_dict(torch.load(\"or_best_classification_model.pth\"))"
      ],
      "metadata": {
        "id": "YqmjC8MJ2PVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "```text\n",
        "------Training Starting for 100 epochs --------------\n",
        "Epoch [1/100]\n",
        "Train Loss: 2.1255, Train Acc: 14.58%\n",
        "Val Loss: 1.9728, Val Acc: 38.78%\n",
        "--------------------------------------------------\n",
        "Epoch [2/100]\n",
        "Train Loss: 1.9516, Train Acc: 27.60%\n",
        "Val Loss: 1.8733, Val Acc: 40.82%\n",
        "--------------------------------------------------\n",
        "Epoch [3/100]\n",
        "Train Loss: 1.8267, Train Acc: 33.33%\n",
        "Val Loss: 1.7741, Val Acc: 36.73%\n",
        "--------------------------------------------------\n",
        "Epoch [4/100]\n",
        "Train Loss: 1.7224, Train Acc: 42.19%\n",
        "Val Loss: 1.6806, Val Acc: 40.82%\n",
        "--------------------------------------------------\n",
        "Epoch [5/100]\n",
        "Train Loss: 1.6149, Train Acc: 44.27%\n",
        "Val Loss: 1.6088, Val Acc: 40.82%\n",
        "--------------------------------------------------\n",
        "Epoch [6/100]\n",
        "Train Loss: 1.5469, Train Acc: 48.44%\n",
        "Val Loss: 1.5587, Val Acc: 40.82%\n",
        "--------------------------------------------------\n",
        "Epoch [7/100]\n",
        "Train Loss: 1.4805, Train Acc: 52.08%\n",
        "Val Loss: 1.5248, Val Acc: 40.82%\n",
        "--------------------------------------------------\n",
        "Epoch [8/100]\n",
        "Train Loss: 1.4443, Train Acc: 47.92%\n",
        "Val Loss: 1.5042, Val Acc: 40.82%\n",
        "--------------------------------------------------\n",
        "Epoch [9/100]\n",
        "Train Loss: 1.4370, Train Acc: 53.12%\n",
        "Val Loss: 1.4864, Val Acc: 44.90%\n",
        "--------------------------------------------------\n",
        "Epoch [10/100]\n",
        "Train Loss: 1.4064, Train Acc: 45.83%\n",
        "Val Loss: 1.4740, Val Acc: 44.90%\n",
        "--------------------------------------------------\n",
        "Epoch [11/100]\n",
        "Train Loss: 1.4126, Train Acc: 56.77%\n",
        "Val Loss: 1.4640, Val Acc: 44.90%\n",
        "--------------------------------------------------\n",
        "Epoch [12/100]\n",
        "Train Loss: 1.4329, Train Acc: 50.52%\n",
        "Val Loss: 1.4566, Val Acc: 42.86%\n",
        "--------------------------------------------------\n",
        "Epoch [13/100]\n",
        "Train Loss: 1.3214, Train Acc: 60.42%\n",
        "Val Loss: 1.4451, Val Acc: 42.86%\n",
        "--------------------------------------------------\n",
        "Epoch [14/100]\n",
        "Train Loss: 1.3171, Train Acc: 57.29%\n",
        "Val Loss: 1.4330, Val Acc: 42.86%\n",
        "--------------------------------------------------\n",
        "Epoch [15/100]\n",
        "Train Loss: 1.3055, Train Acc: 55.73%\n",
        "Val Loss: 1.4267, Val Acc: 44.90%\n",
        "--------------------------------------------------\n",
        "Epoch [16/100]\n",
        "Train Loss: 1.2657, Train Acc: 57.81%\n",
        "Val Loss: 1.4167, Val Acc: 44.90%\n",
        "--------------------------------------------------\n",
        "Epoch [17/100]\n",
        "Train Loss: 1.2969, Train Acc: 54.17%\n",
        "Val Loss: 1.4063, Val Acc: 44.90%\n",
        "--------------------------------------------------\n",
        "Epoch [18/100]\n",
        "Train Loss: 1.2549, Train Acc: 62.50%\n",
        "Val Loss: 1.4012, Val Acc: 44.90%\n",
        "--------------------------------------------------\n",
        "Epoch [19/100]\n",
        "Train Loss: 1.2340, Train Acc: 53.65%\n",
        "Val Loss: 1.3899, Val Acc: 42.86%\n",
        "--------------------------------------------------\n",
        "Early stopping at epoch 19\n",
        "\n",
        "Training finished.\n",
        "Best validation accuracy: 44.8980\n",
        "Elapsed time: 0:00:00.36\n",
        "Best validation accuracy: 44.8980\n",
        "Best validation loss: 1.3899\n",
        "<All keys matched successfully>\n",
        "```\n",
        "\n",
        "The `or_model` neural network trained very quickly (< 1 min) but the best validation accuracy (`val accuracy`) is only about 40-45%. It should also be noted that in this particular run, `Early Stopping` terminated training before the 20th epoch."
      ],
      "metadata": {
        "id": "O9l9OU8us5cQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1B: Plot Training History\n",
        "\n",
        "The code in the cell below generates two side-by-side plots, an **Accuracy Curve** and a **Loss Curve**. These curves provide a visual way to follow what happened during training of `or_model`."
      ],
      "metadata": {
        "id": "XSEK-qjtdMJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 1B: Plot Training History\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "\n",
        "# Main title above both subplots\n",
        "plt.suptitle('Training Visualization: Orange Quality Dataset',\n",
        "             fontsize=16, fontweight='medium')\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(or_train_accuracies, label='train')\n",
        "plt.plot(or_val_accuracies, label='val')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(or_train_losses, label='train')\n",
        "plt.plot(or_val_losses, label='val')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lfuH9EJ8-zLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_4_image01A.png)\n"
      ],
      "metadata": {
        "id": "JwOjf0rDzWqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1A: Build and Train a Classification Neural Network**\n",
        "\n",
        "In the cell below build and train a new classification neural network called `ap_model`.\n",
        "\n",
        "Start by reading the dataset and creating a DataFrame called ap_df (\"apple\" DataFrame) using this code chunk:\n",
        "\n",
        "```Python\n",
        "\n",
        "ap_df = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/apple_quality.csv\",\n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "```\n",
        "\n",
        "The goal of your neural network model ap_model will be to classify the apples in the Apple Quality dataset using the values in the following columns: 'Size', 'Weight', 'Sweetness', 'Crunchiness', 'Juiciness', 'Acidity' and 'Ripeness'. Since all of these columns are numeric, there is no need pre-process any of these columns. Moreover, the numerical values all have a similar magnitude so you don't need to standardize any column to their z-scores. Nor do you need to split the data into Training/Validation sets or suffle the data. When you generate your X-values, you should called them ap_X.\n",
        "\n",
        "Since you are building a classification neural network, you will need to One-Hot Encode the column containing the Y-values, Quality. This column is non-numeric, so by One-Hot Encoding it, you will accomplish two things: (1) replace string values with integers and (2) give the Y-values the correct format for the neural network. When you generate your Y-values, you should called them ap_Y.\n",
        "\n",
        "Again, because you want your ap_model to act as a classifier, use the softmax activation function in the output layer. You should also compile your model using categorical_crossentropy as the loss function.\n",
        "\n",
        "Finally, train (fit) your model on your X-values (ap_X) and your Y-values (ap_Y) for 100 epochs."
      ],
      "metadata": {
        "id": "6SCv81q2_y4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Build and Train a Classification Neural Network**\n",
        "\n",
        "In the cell below build and train a new classification neural network called `ap_model`.\n",
        "\n",
        "Start by `copy-and-paste` Example 1 into the cell below.\n",
        "\n",
        "Read the **`Apple Quality dataset`** and creat a DataFrame called `ap_df` (\"apple\" DataFrame) using this code chunk:\n",
        "\n",
        "```Python\n",
        "ap_df = pd.read_csv(\n",
        "\"https://biologicslab.co/BIO1173/data/apple_quality.csv\",\n",
        "na_values=['NA', '?'])\n",
        "```\n",
        "\n",
        "The goal of your training your neural network model `ap_model` will be to learn how to predict the classification apples using the values in the following columns: 'Size', 'Weight', 'Sweetness', 'Crunchiness', 'Juiciness', 'Acidity' and 'Ripeness'. Since all of these columns are numeric you should standardize their values by converting them to their z-scores. You can simply re-use the code in `Example 1` making sure to change the prefix `or_` to `ap_`.\n",
        "\n",
        "Again, since all of these columns are numeric, there is no need map any strings to integers as was done in `Example 1`. The easiest and _safest_ way to accomplish this is to simply comment-out the unecessary code in `Example 1` as follows:\n",
        "\n",
        "\n",
        "```Python\n",
        "# Map str to int\n",
        "#mapping = {'Orange':0,'Deep Orange':1,\n",
        "#           'Light Orange':2,'Orange-Red':3,\n",
        "#           'Yellow-Orange':4}\n",
        "#or_df['Color'] = or_df['Color'].map(mapping)\n",
        "```\n",
        "\n",
        "Commenting-out code instead of simply deleting it has the distinct advantage that if you make a mistake, is relatively easy to correct it by simply adding (or removing) the **`#`** before the line.\n",
        "\n",
        "\n",
        "To generate your X feature vector change this code chunk\n",
        "```Python\n",
        "# Generate X-values\n",
        "or_X = or_df[['Size (cm)', 'Weight (g)', 'Brix (Sweetness)', 'pH (Acidity)',\n",
        "'Softness (1-5)', 'HarvestTime (days)', 'Ripeness (1-5)', 'Color']].values or_X = np.asarray(or_X).astype('float32')\n",
        "```\n",
        "to read as\n",
        "```Python\n",
        "# Generate X‑values\n",
        "ap_X = ap_df[['Size', 'Weight', 'Sweetness', 'Crunchiness', 'Juiciness', 'Acidity', 'Ripeness']].values ap_X = np.asarray(ap_X).astype('float32')\n",
        "```\n",
        "\n",
        "This will make sure on the correct columns are used to generate your `X` feature vector.\n",
        "\n",
        "Since you are building a classification neural network, you will need to one-Hot encode the column called `Quality` containing your `Y-values`. You can simple re-use the code in `Example 1`, making sure to use the correct name for the target column (i.e `Quality`).\n",
        "\n",
        "Finally, train (fit) your model on your X-values (`ap_X`) and your Y-values (`ap_Y`) for 100 epochs with verbose set to `0`.  "
      ],
      "metadata": {
        "id": "9Mrt8H0R81h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 1: Build and Train a Classification Neural Networ\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9mfNvH90lmOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "```text\n",
        "------Training Starting for 100 epochs --------------\n",
        "Epoch [1/100]\n",
        "Train Loss: 0.4976, Train Acc: 75.47%\n",
        "Val Loss: 0.3623, Val Acc: 85.50%\n",
        "--------------------------------------------------\n",
        "Epoch [2/100]\n",
        "Train Loss: 0.3888, Train Acc: 81.88%\n",
        "Val Loss: 0.3093, Val Acc: 88.62%\n",
        "--------------------------------------------------\n",
        "Epoch [3/100]\n",
        "Train Loss: 0.3749, Train Acc: 82.47%\n",
        "Val Loss: 0.2911, Val Acc: 90.12%\n",
        "--------------------------------------------------\n",
        "Epoch [4/100]\n",
        "Train Loss: 0.3471, Train Acc: 83.31%\n",
        "Val Loss: 0.2793, Val Acc: 89.62%\n",
        "--------------------------------------------------\n",
        "Epoch [5/100]\n",
        "Train Loss: 0.3534, Train Acc: 84.53%\n",
        "Val Loss: 0.2800, Val Acc: 89.00%\n",
        "--------------------------------------------------\n",
        "Epoch [6/100]\n",
        "Train Loss: 0.3357, Train Acc: 84.50%\n",
        "Val Loss: 0.2761, Val Acc: 89.25%\n",
        "--------------------------------------------------\n",
        "Epoch [7/100]\n",
        "Train Loss: 0.3400, Train Acc: 85.06%\n",
        "Val Loss: 0.2663, Val Acc: 90.12%\n",
        "--------------------------------------------------\n",
        "Epoch [8/100]\n",
        "Train Loss: 0.3251, Train Acc: 85.56%\n",
        "Val Loss: 0.2615, Val Acc: 90.50%\n",
        "--------------------------------------------------\n",
        "Epoch [9/100]\n",
        "Train Loss: 0.3256, Train Acc: 85.53%\n",
        "Val Loss: 0.2626, Val Acc: 90.25%\n",
        "--------------------------------------------------\n",
        "Epoch [10/100]\n",
        "Train Loss: 0.3147, Train Acc: 85.81%\n",
        "Val Loss: 0.2525, Val Acc: 91.12%\n",
        "--------------------------------------------------\n",
        "Epoch [11/100]\n",
        "Train Loss: 0.2941, Train Acc: 86.97%\n",
        "Val Loss: 0.2464, Val Acc: 92.00%\n",
        "--------------------------------------------------\n",
        "Epoch [12/100]\n",
        "Train Loss: 0.3063, Train Acc: 86.34%\n",
        "Val Loss: 0.2439, Val Acc: 91.00%\n",
        "--------------------------------------------------\n",
        "Epoch [13/100]\n",
        "Train Loss: 0.3115, Train Acc: 86.09%\n",
        "Val Loss: 0.2444, Val Acc: 92.25%\n",
        "--------------------------------------------------\n",
        "Epoch [14/100]\n",
        "Train Loss: 0.2973, Train Acc: 86.75%\n",
        "Val Loss: 0.2419, Val Acc: 92.12%\n",
        "--------------------------------------------------\n",
        "Epoch [15/100]\n",
        "Train Loss: 0.2954, Train Acc: 87.31%\n",
        "Val Loss: 0.2331, Val Acc: 92.75%\n",
        "--------------------------------------------------\n",
        "Epoch [16/100]\n",
        "Train Loss: 0.2942, Train Acc: 86.88%\n",
        "Val Loss: 0.2315, Val Acc: 92.75%\n",
        "--------------------------------------------------\n",
        "Epoch [17/100]\n",
        "Train Loss: 0.2823, Train Acc: 88.28%\n",
        "Val Loss: 0.2268, Val Acc: 93.25%\n",
        "--------------------------------------------------\n",
        "Epoch [18/100]\n",
        "Train Loss: 0.2843, Train Acc: 88.06%\n",
        "Val Loss: 0.2193, Val Acc: 93.00%\n",
        "--------------------------------------------------\n",
        "Epoch [19/100]\n",
        "Train Loss: 0.2696, Train Acc: 88.59%\n",
        "Val Loss: 0.2216, Val Acc: 93.25%\n",
        "--------------------------------------------------\n",
        "Epoch [20/100]\n",
        "Train Loss: 0.2755, Train Acc: 88.34%\n",
        "Val Loss: 0.2146, Val Acc: 93.12%\n",
        "--------------------------------------------------\n",
        "Epoch [21/100]\n",
        "Train Loss: 0.2658, Train Acc: 88.50%\n",
        "Val Loss: 0.2118, Val Acc: 94.00%\n",
        "--------------------------------------------------\n",
        "Epoch [22/100]\n",
        "Train Loss: 0.2671, Train Acc: 88.72%\n",
        "Val Loss: 0.2122, Val Acc: 93.25%\n",
        "--------------------------------------------------\n",
        "Epoch [23/100]\n",
        "Train Loss: 0.2738, Train Acc: 87.94%\n",
        "Val Loss: 0.2113, Val Acc: 93.50%\n",
        "--------------------------------------------------\n",
        "Epoch [24/100]\n",
        "Train Loss: 0.2749, Train Acc: 88.38%\n",
        "Val Loss: 0.2046, Val Acc: 93.38%\n",
        "--------------------------------------------------\n",
        "Epoch [25/100]\n",
        "Train Loss: 0.2595, Train Acc: 89.22%\n",
        "Val Loss: 0.2019, Val Acc: 93.38%\n",
        "--------------------------------------------------\n",
        "Epoch [26/100]\n",
        "Train Loss: 0.2596, Train Acc: 89.00%\n",
        "Val Loss: 0.1988, Val Acc: 93.88%\n",
        "--------------------------------------------------\n",
        "Epoch [27/100]\n",
        "Train Loss: 0.2731, Train Acc: 88.53%\n",
        "Val Loss: 0.1997, Val Acc: 93.88%\n",
        "--------------------------------------------------\n",
        "Epoch [28/100]\n",
        "Train Loss: 0.2723, Train Acc: 88.31%\n",
        "Val Loss: 0.2030, Val Acc: 92.38%\n",
        "--------------------------------------------------\n",
        "Epoch [29/100]\n",
        "Train Loss: 0.2572, Train Acc: 88.84%\n",
        "Val Loss: 0.1995, Val Acc: 93.75%\n",
        "--------------------------------------------------\n",
        "Epoch [30/100]\n",
        "Train Loss: 0.2538, Train Acc: 89.41%\n",
        "Val Loss: 0.1945, Val Acc: 93.38%\n",
        "--------------------------------------------------\n",
        "Epoch [31/100]\n",
        "Train Loss: 0.2596, Train Acc: 89.25%\n",
        "Val Loss: 0.1955, Val Acc: 93.12%\n",
        "--------------------------------------------------\n",
        "Early stopping at epoch 31\n",
        "\n",
        "Training finished.\n",
        "Best validation accuracy: 94.0000\n",
        "Elapsed time: 0:00:09.60\n",
        "Best validation accuracy: 94.0000\n",
        "Best validation loss: 0.1945\n",
        "<All keys matched successfully>\n",
        "```\n",
        "\n",
        "```text\n",
        "Early stopping at epoch 19\n",
        "\n",
        "Training finished.\n",
        "Best validation accuracy: 44.8980\n",
        "Elapsed time: 0:00:00.36\n",
        "Best validation accuracy: 44.8980\n",
        "Best validation loss: 1.3899\n",
        "```\n",
        "Your `ap_model` neural network appears to have done a much better job with a best validation accuracy (`val accuracy`) over 90%. It should also be noted that in this particular run, `Early Stopping` terminated training at the 32rd epoch."
      ],
      "metadata": {
        "id": "PH3hBsCL09U1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1B: Visualize Training**\n",
        "\n",
        "In the cell below write the code to generate two side-by-side plots, an **Accuracy Curve** and a **Loss Curve** for your `ap_model`."
      ],
      "metadata": {
        "id": "dNKiSkZweeL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 1B: Visualize Training\n",
        "\n"
      ],
      "metadata": {
        "id": "AHpTicaKeeL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "![__](https://biologicslab.co:/BIO1173/images/class_02/class_02_4_image02A.png)\n"
      ],
      "metadata": {
        "id": "2s69Gsj92jc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------\n",
        "## **Time and Cost of Training Large Language Models (LLMs)**\n",
        "\n",
        "Large Language Models (LLMs) require a lot of time and money to train. Here is some of the available data as of September 2025.\n",
        "\n",
        "#### **Largest LLMs (as of September 2025)**\n",
        "\n",
        "| Model | Approx. size (parameters) | Rough training duration | Rough training cost | Source |\n",
        "|-------|---------------------------|-------------------------|---------------------|--------|\n",
        "| **PaLM 2‑G** (Google) | **540 B** | ~4 months on ≈ 5 000–10 000 GPUs (≈ 2 million GPU‑hours) | **$200 M – $250 M** | Google AI blog (2024), “PaLM 2: Language Models for the Web” |\n",
        "| **GPT‑4** (OpenAI) | **175 B** (largest released variant) | ~3 months on ≈ 10 000–15 000 GPUs (≈ 1.3 million GPU‑hours) | **$30 M – $50 M** | OpenAI press release (2023), estimates from *Bloomberg* and *The Verge* |\n",
        "| **Claude 3** (Anthropic) | **200 B** | ~3 months on ≈ 8 000 GPUs (≈ 1 million GPU‑hours) | **$30 M – $60 M** | Anthropic blog (2024) |\n",
        "| **LLaMA‑2‑70B** (Meta) | **70 B** | ~1 month on ≈ 1 500 GPUs (≈ 0.2 million GPU‑hours) | **$5 M – $10 M** | Meta AI research paper (2023) |\n",
        "\n",
        "\n",
        "####**Quick take-aways**\n",
        "\n",
        "1. **Largest publicly‑known LLM (as of 2025):**  \n",
        "   *PaLM 2‑G* – **540 billion** parameters, the only model known to exceed the 175‑B‑parameter range of GPT‑4.\n",
        "\n",
        "2. **Training time:**  \n",
        "   Even the smallest “state-of-the-art” models require **weeks to months** on **thousands of GPUs**.  \n",
        "   * PaLM 2-G ≈ **2 million GPU-hours** → ~4 months on a 5 000‑GPU cluster.\n",
        "\n",
        "3. **Training cost:**  \n",
        "   Costs run in the **tens to hundreds of millions** of dollars.  \n",
        "   * 175B-parameter models: **\\$30-50M**  \n",
        "   * 540B-parameter models: **\\$200-250M**\n",
        "\n",
        "> **Bottom line:**  \n",
        "> The field is rapidly moving toward ever larger models, but the practical ceiling is still in the *hundreds of billions* of parameters.  Training such a model is a **multi-month, multi‑million‑GPU‑hour operation** that costs **\\$30-250 million**, depending on size and hardware budget.\n",
        "\n",
        "---------------------\n"
      ],
      "metadata": {
        "id": "og2a5Hz67Mrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Determine the Model's RMSE and Accuracy\n",
        "\n",
        "The overall objective of this assignment is to convince you that you can save a trained neural network to a file, and then later, recreate the neural network from the file, without changing the model's accuracy.\n",
        "\n",
        "**Why is this important?**\n",
        "\n",
        "As you already know, it can take significant time and processing power to train even relatively small neural networks that we created so far in this course. Neural networks that are used commercially (think \"Siri\" or \"Alexa\" or ChatGPT) are many times larger and require enormous resources as well as weeks (or months) to train. Obviously, if you had to train a neural network every time you wanted to use it, it won't be very practical and there would be little interest in \"AI\". However, once the neural network has been trained, you can save it to a file, and then re-use it over and over again, without any loss in the neural network's ability to solve problems (i.e. loss in accuracy).\n",
        "\n",
        "The code in the cell below calculates the ability of the `or_model` neural network to predict an orange's quality (Y-value) based on its physical and chemical characteristics (X-values). Two measures of predictive ability are computed, the Root Mean Square Error (RMSE) and Accuracy. The code stores the RMSE value in the variable `or_Score` and the Accuracy value in the variable `or_Correct` and then prints out these values.\n"
      ],
      "metadata": {
        "id": "IS7YYOiMEHMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Determine the Model's RMSE and Accuracy\n",
        "\n",
        "The code in the cell below calculates 4 **accuracy metrics** about the `or_model` before we save it to disk."
      ],
      "metadata": {
        "id": "r1__6oDmqs45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 2: Determine the model's accuracy before saving to disk\n",
        "\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Print title\n",
        "print(\"Measurements of Orange NN model `or_model`\")\n",
        "print(\"--------------------------------------------\")\n",
        "\n",
        "# Measure RMSE error (using cross-entropy loss as proxy since we're doing classification)\n",
        "# For classification, we typically don't use RMSE directly, but here's how you'd calculate it:\n",
        "with torch.no_grad():\n",
        "    or_Pred = or_model(or_X_tensor).numpy()  # Get predictions from model\n",
        "    or_Score = np.sqrt(metrics.mean_squared_error(or_Pred, or_Y))\n",
        "    print(f\"Before save score (RMSE): {or_Score}\")\n",
        "\n",
        "# Measure the accuracy\n",
        "or_Predict_classes = np.argmax(or_Pred,axis=1)\n",
        "or_Expected_classes = np.argmax(or_Y,axis=1)\n",
        "or_Correct = accuracy_score(or_Expected_classes,or_Predict_classes)\n",
        "print(f\"Before save Accuracy: {or_Correct}\")"
      ],
      "metadata": {
        "id": "C6yo7dvpKhMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something _similar_ to the following output:\n",
        "```text\n",
        "Measurements of Orange NN model `or_model`\n",
        "--------------------------------------------\n",
        "Before save score (RMSE): 0.5760088673842864\n",
        "Before save Accuracy: 0.5518672199170125\n",
        "```\n",
        "\n",
        "We are going to use these scores to see if the model we save and then redeploy has the same accuracy or does the model's accuracy suffer?"
      ],
      "metadata": {
        "id": "pk0iFIzo_ZKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Determine the Model's Accuracy**\n",
        "\n",
        "In the cell below, write the code to calculate the same **accuracy metrics** for your `ap_model` shown in `Example 2` before we save it to disk."
      ],
      "metadata": {
        "id": "wP7Yge269bWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 2: Determine the Model's Accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "fTWlNJzA9fti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see something _similar_ to the following output:\n",
        "```text\n",
        "Measurements of Apple NN model `ap_model`\n",
        "--------------------------------------------\n",
        "Before save score (RMSE): 2.110792009039741\n",
        "Before save Accuracy: 0.94675\n",
        "```\n",
        "According to the output shown above, your `ap_model` is better than 95% accurate when it comes to predicting apple quality. Apparently, it's a little easier to predict an apple's `Quality` with a classification neural network than to predict orange quality."
      ],
      "metadata": {
        "id": "Om3ueI6p9idK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **PyTorch**, the primary difference lies in what is stored and how you must load it.\n",
        "### **PyTorch state_dict**\n",
        "* **What it is:** A standard Python dictionary that maps each layer name to its specific parameter tensors (weights and biases) and buffers (like BatchNorm running means).\n",
        "* **Storage:** It stores only the parameters, not the model's architecture or source code.\n",
        "* **Loading:** To use it, you must first initialize the model in your code (e.g., model = MyModel()) and then call model.load_state_dict(torch.load(PATH)).\n",
        "Best For: Production and sharing models. It is the recommended approach because it is flexible and doesn't break if you move your files around or refactor your project.\n",
        "\n",
        "### **Full Model (torch.save(model, PATH))**\n",
        "* **What it is:** A serialized version of the entire model object using Python’s pickle module.\n",
        "* **Storage:** It saves the architecture, parameters, and the file path to the class definition.\n",
        "* **Loading:** You can load it directly with model = torch.load(PATH) without defining the class first in your script."
      ],
      "metadata": {
        "id": "AMLdm38jneNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Save the Model\n",
        "\n",
        "The code in the cell below saves the trained neural network `or_model` as a file in two different file formats: **`PyTorch state dict`** and **`full model`**.\n",
        "\n",
        "Each file is saved in the current working directory (save_path = \".\"). The filename of the state dict file is `or_model_pytorch.pth` while the filename of the full model file is `or_full_model_pytorch.pth`."
      ],
      "metadata": {
        "id": "7NVtEsrt-xd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 3: Save the model\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Save path is the current directory\n",
        "save_path = \".\"\n",
        "\n",
        "# Save the PyTorch model state dict (weights and architecture)\n",
        "torch.save(or_model.state_dict(), os.path.join(save_path, \"or_model_pytorch.pth\"))\n",
        "\n",
        "# Save the entire model (architecture + weights)\n",
        "torch.save(or_model, os.path.join(save_path, \"or_full_model_pytorch.pth\"))\n",
        "\n",
        "# Print out the files in current directory\n",
        "files = os.listdir()\n",
        "print(files)\n"
      ],
      "metadata": {
        "id": "hdwIvJ5GD6rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see something _similar_ to the following output:\n",
        "\n",
        "```text\n",
        "['.config', 'or_full_model_pytorch.pth', 'ap_best_classification_model.pth', 'or_model_pytorch.pth', 'drive', 'or_best_classification_model.pth', 'sample_data']\n",
        "```\n",
        "After running the code cell above, there should now be two new files in your current directory, `or_model_pytorch.pth` and `or_full_model_pytorch.pth`."
      ],
      "metadata": {
        "id": "uKVJKqIo_fhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Save the Model**\n",
        "\n",
        "In the code cell below save your _trained_ neural network `ap_model` as a JSON file with the filename, `ap_model.json`, and as a native Keras file with the filenmane `ap_model.keras`. Save both files to your current working directory (`save_path = \".\"`)."
      ],
      "metadata": {
        "id": "OcoPhtQi_kiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 3: Save the Model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TbjbyLCGClTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should now see the two more files with your neural network, `ap_model_pytorch.pth` and `ap_full_model_pytorch.pth`."
      ],
      "metadata": {
        "id": "rl_-kxWhbuhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Create New Model from Saved Model\n",
        "\n",
        "Once a trained model has been saved, it is a simple matter to read the file to make an exact copy of the model. In PyTorch, we need to recreate the model architecture first and then load the saved weights into it.\n",
        "\n",
        "In Example 4, we have recreated the neural network architecture and loaded the saved weights into a new model instance called `or2_model` to differentiate it from the one that was built previously."
      ],
      "metadata": {
        "id": "p3EMkjl9HFkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example 4: Create new model from saved model\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "\n",
        "print(\"=== Model Loading Verification ===\")\n",
        "\n",
        "# Print details of model\n",
        "print(f\"Training data shapes:\")\n",
        "print(f\"X shape: {or_X.shape}\")  # Should be (samples, 8)\n",
        "print(f\"Y shape: {or_Y.shape}\")  # This is 8\n",
        "\n",
        "# Check dimensions\n",
        "num_classes = or_Y.shape[1]  # This is 8 - so we need to create a model with 8 output classes\n",
        "input_dim = or_X.shape[1]   # This is 8 - 8 input features\n",
        "\n",
        "print(f\"Input features: {input_dim}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Recreate the EXACT same model class structure as used in training\n",
        "class OrangeQualityModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(OrangeQualityModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create model with EXACT same dimensions as training\n",
        "or2_model = OrangeQualityModel(input_dim, num_classes)  # input_dim=8, num_classes=8\n",
        "\n",
        "try:\n",
        "    print(\"Loading weights from saved model...\")\n",
        "    or2_model.load_state_dict(torch.load(\"or_model_pytorch.pth\"))\n",
        "    print(\"✓ Orange Quality model loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading model: {e}\")"
      ],
      "metadata": {
        "id": "F_cMX2v1_qIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output:\n",
        "```text\n",
        "=== Model Loading Verification ===\n",
        "Training data shapes:\n",
        "X shape: (241, 8)\n",
        "Y shape: (241, 8)\n",
        "Input features: 8\n",
        "Number of classes: 8\n",
        "Loading weights from saved model...\n",
        "✓ Orange Quality model loaded successfully!\n",
        "```"
      ],
      "metadata": {
        "id": "-1pWvfA5cVcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 4: Create New Model from Saved Model**\n",
        "\n",
        "In the cell below create a new neural network called `ap2_model` from the file `ap_model_pytorch.pth` in your current directory. Print out a summary of your new `ap2_model`.\n",
        "\n",
        "**Code Hints:**\n",
        "\n",
        "Change the class name `OrangeQualityModel` to read `AppleQualityModel`."
      ],
      "metadata": {
        "id": "PhDXUkydD64K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 4: Create New Model from Saved Model\n",
        "\n"
      ],
      "metadata": {
        "id": "igrzYQ6lECiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct you should see the following output:\n",
        "\n",
        "```text\n",
        "=== Model Loading Verification ===\n",
        "Training data shapes:\n",
        "X shape: (4000, 7)\n",
        "Y shape: (4000, 2)\n",
        "Input features: 7\n",
        "Number of classes: 2\n",
        "Loading weights from saved model...\n",
        "✓ Apple Quality model loaded successfully!\n",
        "```"
      ],
      "metadata": {
        "id": "Wq-QSomoEMuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Compare the Predictive Accuracy of the Old and New Models\n",
        "\n",
        "The code in the cell below computes the RMSE error and the Accuracy of our new model `or2_model` and compares these values with the original `or_model`. We are trying to address the question whether re-loaded model has the same accuracy as the original model?"
      ],
      "metadata": {
        "id": "9CuvngfiEUgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Compare the Predictive Accuracy of the Old and New Models\n",
        "\n",
        "# Set both models to evaluation mode for consistent predictions\n",
        "or_model.eval()  # Make sure original is in eval mode\n",
        "or2_model.eval()  # Make sure new model is also in eval mode\n",
        "\n",
        "print(\"\\n=== MODEL VERIFICATION ===\")\n",
        "with torch.no_grad():\n",
        "    # Get predictions from both models (both should be in eval mode now)\n",
        "    or_pred = or_model(or_X_tensor).numpy()\n",
        "    or2_pred = or2_model(or_X_tensor).numpy()\n",
        "\n",
        "    print(f\"Original model prediction shape: {or_pred.shape}\")\n",
        "    print(f\"New model prediction shape: {or2_pred.shape}\")\n",
        "\n",
        "    # Show first few predictions for comparison\n",
        "    orig_preds = np.argmax(or_pred, axis=1)\n",
        "    new_preds = np.argmax(or2_pred, axis=1)\n",
        "\n",
        "    print(f\"\\nOriginal model (first 5): {orig_preds[:5]}\")\n",
        "    print(f\"New model      (first 5): {new_preds[:5]}\")\n",
        "\n",
        "    # Check exact match\n",
        "    are_equal = np.array_equal(orig_preds, new_preds)\n",
        "    print(f\"\\nPredictions identical: {are_equal}\")\n",
        "\n",
        "    if not are_equal:\n",
        "        diff_count = np.sum(orig_preds != new_preds)\n",
        "        print(f\"Number of different predictions: {diff_count}\")\n",
        "\n",
        "        # Show some examples where they differ\n",
        "        diff_indices = np.where(orig_preds != new_preds)[0]\n",
        "        if len(diff_indices) > 0:\n",
        "            print(\"First few differing indices:\", diff_indices[:5])\n",
        "            for i in range(min(5, len(diff_indices))):\n",
        "                idx = diff_indices[i]\n",
        "                print(f\"Index {idx}: Original={orig_preds[idx]}, New={new_preds[idx]}\")\n",
        "\n",
        "print(\"\\n=== Model Architecture Check ===\")\n",
        "print(\"Original model:\")\n",
        "print(or_model)\n",
        "print(\"\\nNew loaded model:\")\n",
        "print(or2_model)\n",
        "\n",
        "# Let's also check the weights are identical\n",
        "print(\"\\n=== Weight Comparison ===\")\n",
        "weight_diffs = []\n",
        "for (name1, param1), (name2, param2) in zip(or_model.named_parameters(), or2_model.named_parameters()):\n",
        "    diff = torch.sum(torch.abs(param1 - param2)).item()\n",
        "    weight_diffs.append(diff)\n",
        "    print(f\"{name1}: difference = {diff:.2e}\")\n",
        "\n",
        "total_weight_diff = sum(weight_diffs)\n",
        "print(f\"\\nTotal weight difference: {total_weight_diff:.2e}\")\n"
      ],
      "metadata": {
        "id": "B7bD9n_fHVt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct the output you should see something _similar_ to the following output:`\n",
        "\n",
        "```text\n",
        "=== MODEL VERIFICATION ===\n",
        "Original model prediction shape: (241, 8)\n",
        "New model prediction shape: (241, 8)\n",
        "\n",
        "Original model (first 5): [7 6 7 3 6]\n",
        "New model      (first 5): [7 6 7 3 6]\n",
        "\n",
        "Predictions identical: True\n",
        "\n",
        "=== Model Architecture Check ===\n",
        "Original model:\n",
        "OrangeQualityModel(\n",
        "  (fc1): Linear(in_features=8, out_features=128, bias=True)\n",
        "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (dropout1): Dropout(p=0.3, inplace=False)\n",
        "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
        "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (dropout2): Dropout(p=0.3, inplace=False)\n",
        "  (fc3): Linear(in_features=64, out_features=8, bias=True)\n",
        ")\n",
        "\n",
        "New loaded model:\n",
        "OrangeQualityModel(\n",
        "  (fc1): Linear(in_features=8, out_features=128, bias=True)\n",
        "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (dropout1): Dropout(p=0.3, inplace=False)\n",
        "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
        "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (dropout2): Dropout(p=0.3, inplace=False)\n",
        "  (fc3): Linear(in_features=64, out_features=8, bias=True)\n",
        ")\n",
        "\n",
        "=== Weight Comparison ===\n",
        "fc1.weight: difference = 0.00e+00\n",
        "fc1.bias: difference = 0.00e+00\n",
        "bn1.weight: difference = 0.00e+00\n",
        "bn1.bias: difference = 0.00e+00\n",
        "fc2.weight: difference = 0.00e+00\n",
        "fc2.bias: difference = 0.00e+00\n",
        "bn2.weight: difference = 0.00e+00\n",
        "bn2.bias: difference = 0.00e+00\n",
        "fc3.weight: difference = 0.00e+00\n",
        "fc3.bias: difference = 0.00e+00\n",
        "\n",
        "Total weight difference: 0.00e+00\n",
        "```\n",
        "\n",
        "As you can see, there is **_no difference_** in the accuracy of the saved model compared to the original one.\n",
        "\n",
        ">> ### **_Train Once_...Use Anywhere!**\n",
        "\n",
        "Big generative models like `ChatGTP` can take days or even months to train, but once they are trained and saved, they can process new data very fast at very little cost."
      ],
      "metadata": {
        "id": "7hyVZ21cIkgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5: Compare the Predictive Accuracy of the Old and New Models**\n",
        "\n",
        "In the cell below write the code to compute the RMSE and Accuracy values for your `ap2Model` and print out these values along with the values for your original `ap_model`."
      ],
      "metadata": {
        "id": "axjJIaLgEiPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exercise 5: Compare the Predictive Accuracy of the Old and New Models\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oUcDuK89EnKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct, you should see something _similar_ to the following output:\n",
        "```text\n",
        "=== MODEL VERIFICATION ===\n",
        "Original model prediction shape: (4000, 2)\n",
        "New model prediction shape: (4000, 2)\n",
        "\n",
        "Original model (first 5): [1 1 0 1 1]\n",
        "New model      (first 5): [1 1 0 1 1]\n",
        "\n",
        "Predictions identical: True\n",
        "\n",
        "=== Model Architecture Check ===\n",
        "Original model:\n",
        "AppleQualityModel(\n",
        "  (fc1): Linear(in_features=7, out_features=128, bias=True)\n",
        "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (dropout1): Dropout(p=0.3, inplace=False)\n",
        "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
        "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (dropout2): Dropout(p=0.3, inplace=False)\n",
        "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
        ")\n",
        "\n",
        "New loaded model:\n",
        "AppleQualityModel(\n",
        "  (fc1): Linear(in_features=7, out_features=128, bias=True)\n",
        "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (dropout1): Dropout(p=0.3, inplace=False)\n",
        "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
        "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (dropout2): Dropout(p=0.3, inplace=False)\n",
        "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
        ")\n",
        "\n",
        "=== Weight Comparison ===\n",
        "fc1.weight: difference = 0.00e+00\n",
        "fc1.bias: difference = 0.00e+00\n",
        "bn1.weight: difference = 0.00e+00\n",
        "bn1.bias: difference = 0.00e+00\n",
        "fc2.weight: difference = 0.00e+00\n",
        "fc2.bias: difference = 0.00e+00\n",
        "bn2.weight: difference = 0.00e+00\n",
        "bn2.bias: difference = 0.00e+00\n",
        "fc3.weight: difference = 0.00e+00\n",
        "fc3.bias: difference = 0.00e+00\n",
        "\n",
        "Total weight difference: 0.00e+00\n",
        "```"
      ],
      "metadata": {
        "id": "Jab1kYFwLrK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summary: Pre-trained Model Persistence and Its Importance for Large Language Models**\n",
        "\n",
        "The ability to save and reload trained PyTorch neural networks without any loss of accuracy represents a fundamental breakthrough in machine learning workflow efficiency. When we train a complex neural network, especially one with millions or billions of parameters like modern large language models (LLMs), the training process can take weeks or months using powerful computational resources. However, once training is complete, we can save the model's weights and architecture to disk, creating a portable file that captures all the learned knowledge.\n",
        "\n",
        "This capability becomes critically important in the era of extremely large LLMs because these models require enormous computational resources for training - often costing thousands of dollars in cloud computing time and consuming massive amounts of energy. By saving trained models, practitioners can:\n",
        "1. Avoid retraining identical or similar architectures from scratch\n",
        "2. Deploy models to production environments immediately\n",
        "3. Share pre-trained models with others in the research community\n",
        "4. Fine-tune existing models on new datasets without starting from random weights\n",
        "\n",
        "For LLMs specifically, this persistence allows researchers and developers to build upon existing knowledge rather than reinventing the wheel each time. A model trained on general text can be saved, then fine-tuned for specific applications like medical diagnosis, legal document analysis, or creative writing - all while maintaining the original model's accuracy and performance characteristics. This approach democratizes access to sophisticated AI capabilities by making powerful pre-trained models accessible to organizations that cannot afford expensive training infrastructure, ultimately accelerating innovation in artificial intelligence.\n"
      ],
      "metadata": {
        "id": "hBy1pZLiK22t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Electronic Submission**\n",
        "\n",
        "When you run the code in the cell below, it will grade your Colab notebook and tell you your pending grade as it currently stands. You will be given the choice to either submit your notebook for final grading or the option to continue your work on one (or more) **Exercises**. You no longer have the option to upload a PDF of your Colab notebook to Canvas for grading. Grant Access to your Colab Secrets if you are asked to do so.\n",
        "\n",
        "**NOTE:** Your grade on this Colab notebook will be based solely on the code in your **Exercises**. Failure to run one (or more) Examples will not affect your grade but failure to run one (or more) **Exercises** will significantly affect your score."
      ],
      "metadata": {
        "id": "Ih2Zy3JWdaxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Electronic Submission\n",
        "\n",
        "import urllib.request\n",
        "import ssl\n",
        "import time\n",
        "\n",
        "url = \"https://biologicslab.co/BIO1173/backend_code/validate.py?v=\" + str(time.time())\n",
        "\n",
        "ctx = ssl.create_default_context()\n",
        "ctx.check_hostname = False\n",
        "ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "req = urllib.request.Request(\n",
        "    url,\n",
        "    headers={\n",
        "        \"Cache-Control\": \"no-cache, no-store, must-revalidate\",\n",
        "        \"Pragma\": \"no-cache\",\n",
        "        \"Expires\": \"0\"\n",
        "    }\n",
        ")\n",
        "\n",
        "with urllib.request.urlopen(req, context=ctx) as r:\n",
        "    exec(r.read().decode(\"utf-8\"))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "nHgiiCyldieK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}