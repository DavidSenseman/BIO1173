{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhLMNzA0fBXZZniTnjZKOt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_05_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYZVwSpdbE3Y"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExN-OzpYbE3Y"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4imk1kbE3Y"
      },
      "source": [
        "##### **Module 5: Natural Language Processing**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 4 Material\n",
        "\n",
        "* **Part 5.1: Introduction to Hugging Face**\n",
        "* Part 5.2: Hugging Face Tokenizers\n",
        "* Part 5.3: Hugging Face Datasets\n",
        "* Part 5.4: Training Hugging Face models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_-lPkxLbE3Z"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    Colab = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    Colab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following output except your GMAIL address should appear on the last line.\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_04/class_04_1_image01B.png)\n",
        "\n",
        "If your GMAIL address does not appear your lesson will **not** be graded."
      ],
      "metadata": {
        "id": "xG3_sXTDfyjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **YouTube Introduction to Hugging Face**\n",
        "\n",
        "Run the next cell to see short introduction to Hugging Face. This is a suggested, but optional, part of the lesson."
      ],
      "metadata": {
        "id": "QDQzWK1HR7lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "video_id = \"GLO5FZzfrS0\"\n",
        "HTML(f\"\"\"\n",
        "<iframe width=\"560\" height=\"315\"\n",
        "  src=\"https://www.youtube.com/embed/{video_id}\"\n",
        "  title=\"YouTube video player\"\n",
        "  frameborder=\"0\"\n",
        "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
        "  allowfullscreen>\n",
        "</iframe>\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "3jhrUBRCR8Iv",
        "outputId": "217e2f0f-086e-4501-de6b-05ea0b679507"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"560\" height=\"315\"\n",
              "  src=\"https://www.youtube.com/embed/GLO5FZzfrS0\"\n",
              "  title=\"YouTube video player\"\n",
              "  frameborder=\"0\"\n",
              "  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
              "  allowfullscreen>\n",
              "</iframe>\n"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Hugging Face**\n",
        "\n",
        "**Hugging Face** is a company renowned for its pioneering work in the realm of Natural Language Processing (NLP). Established in 2016, Hugging Face has become synonymous with making cutting-edge NLP technologies more accessible to developers, researchers, and organizations. The company's commitment to open-source development has fostered a vibrant community, which plays a pivotal role in the rapid advancements of the field.\n",
        "\n",
        "Central to Hugging Face's acclaim is the Transformers library. This Python library provides an extensive collection of pre-trained models that span a wide range of NLP tasks, from text classification and tokenization to language modeling and translation. Some of the most groundbreaking models like BERT, GPT-2, T5, and RoBERTa can be effortlessly accessed and deployed through this library.\n",
        "\n",
        "A notable feature of the Transformers library is its user-friendly API. With just a few lines of code, one can:\n",
        "\n",
        "* Load a pre-trained model.\n",
        "* Tokenize input text.\n",
        "* Obtain model predictions or embeddings.\n",
        "* Fine-tune the model on a specific task.\n",
        "\n",
        "This ease of use, combined with comprehensive documentation and tutorials, makes it an invaluable tool for both NLP newcomers and seasoned professionals.\n",
        "\n",
        "Another significant contribution from Hugging Face is the Model Hub, a platform where researchers and developers can share, discover, and use NLP models. The hub promotes collaboration, ensuring that state-of-the-art models are easily available to the wider community. It's not uncommon to see models from recent research papers promptly available on the hub, ready for real-world applications.\n",
        "\n",
        "Tokenization, the process of converting text into tokens, is a fundamental step in NLP. Recognizing its importance, Hugging Face introduced the Tokenizers library, which provides a fast and efficient way to tokenize massive datasets without compromising on accuracy. Its compatibility with the Transformers library ensures seamless integration between tokenization and modeling.\n",
        "\n",
        "Democratization of NLP: By making advanced models and tools accessible, Hugging Face has democratized NLP, enabling even small teams or individual developers to harness the power of state-of-the-art models.\n",
        "\n",
        "Rapid Prototyping: The ease with which one can deploy models means that ideas can be tested and iterated upon swiftly, accelerating the pace of NLP advancements.\n",
        "\n",
        "Community and Collaboration: The open-source ethos of Hugging Face has cultivated a community that collaborates, contributes, and ensures that the field remains vibrant and progressive.\n",
        "\n",
        "In the ever-evolving landscape of NLP, Hugging Face stands out as a beacon of innovation, accessibility, and collaboration. Whether you are a researcher pushing the boundaries of what's possible, a developer aiming to integrate NLP into an application, or an enthusiast eager to learn, Hugging Face provides the tools and resources to realize those ambitions. As we delve deeper into this book, we will frequently use the Hugging Face platform to illustrate concepts, implement solutions, and explore the vast possibilities of NLP.\n",
        "\n"
      ],
      "metadata": {
        "id": "wVWCWSJurbIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using Python with Hugging Face**\n",
        "\n",
        "Transformers have become a mainstay of natural language processing. This module will examine the [Hugging Face](https://huggingface.co/) Python library for natural language processing, bringing together pretrained transformers, data sets, tokenizers, and other elements. Through the Hugging Face API, you can quickly begin using sentiment analysis, entity recognition, language translation, summarization, and text generation.\n",
        "\n",
        "Colab does not install Hugging face by default. Whether installing Hugging Face directly into a local computer or utilizing it through Colab, the following commands will install the library.\n",
        "\n",
        "Normally, when you use `pip` to install a package, a lot of information is printed out to show what subpackages were installed. We have prevented this by\n",
        "adding the text `> /dev/` after the `!pip install <package>`. Technically, this code `redirects` the standard output (in this case your Colab notebook) to a device called `null`. `/dev/null` is a special file that discards all data written to it. It is often used as a \"black hole\" for unwanted output or error messages, such as the output of commands that are meant to be silent or hidden.\n"
      ],
      "metadata": {
        "id": "5A4aEEHas1hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Hugging Face Libraries\n",
        "\n",
        "Run the next code cell to install the Hugging Face libraries needed for this lesson."
      ],
      "metadata": {
        "id": "P4vC0d23ws-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Hugging Face libraries\n",
        "\n",
        "!pip install transformers > /dev/null\n",
        "!pip install transformers[sentencepiece] > /dev/null"
      ],
      "metadata": {
        "id": "9nChFrxHrYsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should not see an output."
      ],
      "metadata": {
        "id": "10TTpU1dE-ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have Hugging Face installed, the following sections will demonstrate how to apply Hugging Face to a variety of everyday tasks. After this introduction, the remainder of this module will take a deeper look at several specific NLP tasks applied to Hugging Face."
      ],
      "metadata": {
        "id": "qBvhLL_JtBT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Sentiment Analysis**\n",
        "\n",
        "**Sentiment analysis** is a subfield of natural language processing (NLP) that focuses on identifying and categorizing the emotional tone or sentiment of a piece of text, such as a review, a social media post, or an article. The task of sentiment analysis involves analyzing the words and phrases in a given text to determine whether they are positive, negative, or neutral in nature, and assigning a numerical score or label to the overall sentiment of the text.\n",
        "\n",
        "Sentiment analysis can be applied to various domains such as social media monitoring, customer feedback analysis, product review analysis, and advertising campaign evaluation. It can help organizations to identify trends and patterns in customer opinion, detect fake reviews, and optimize their marketing strategies by understanding the emotions of their target audience.\n",
        "\n",
        "### What is sentiment analysis?\n",
        "\n",
        "Sentiment analysis is a subfield of natural language processing (NLP) that focuses on identifying and categorizing the emotional tone or sentiment of a piece of text, such as a review, a social media post, or an article. The task of sentiment analysis involves analyzing the words and phrases in a given text to determine whether they are positive, negative, or neutral in nature, and assigning a numerical score or label to the overall sentiment of the text.\n",
        "\n",
        "Sentiment analysis can be applied to various domains such as social media monitoring, customer feedback analysis, product review analysis, and advertising campaign evaluation. It can help organizations to identify trends and patterns in customer opinion, detect fake reviews, and optimize their marketing strategies by understanding the emotions of their target audience.\n",
        "\n",
        "The process of sentiment analysis typically involves several steps, including:\n",
        "\n",
        "1. Text Preprocessing: This step involves cleaning and normalizing the text data to prepare it for analysis. This may include removing stop words, punctuation, and converting all text to lowercase.\n",
        "2. Tokenization: This step involves breaking down the text into smaller units called tokens, which can be individual words or phrases.\n",
        "3. Sentiment Detection: This step involves analyzing the sentiment of each token in the text using techniques such as machine learning, deep learning, or rule-based approaches.\n",
        "4. Aggregation: This step involves combining the sentiment scores of all tokens to obtain the overall sentiment score of the text.\n",
        "\n",
        "Sentiment analysis can be performed using various techniques such as:\n",
        "\n",
        "1. **Rule-Based Approach:** This approach involves defining a set of rules to classify words or phrases as positive, negative, or neutral based on their meanings and context.\n",
        "2. **Machine Learning:** This approach involves training a machine learning model on a dataset of labeled text to learn the patterns and relationships between words and sentiment.\n",
        "3. **Deep Learning:** This approach involves using deep neural networks to analyze the complex patterns in text data and learn the relationships between words, phrases, and sentiment.\n",
        "\n",
        "Sentiment analysis uses natural language processing, text analysis, computational linguistics, and biometrics to identify the tone of written text. Passages of written text can be into simple binary states of positive or negative tone. More advanced sentiment analysis might classify text into additional categories: sadness, joy, love, anger, fear, or surprise.\n"
      ],
      "metadata": {
        "id": "HYqyqMEWtFct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1 - Step 1: Load Text for Sentiment Analyis\n",
        "\n",
        "To demonstrate sentiment analysis, we begin by loading Shakespeare's 18th sonnet:\n",
        "\n",
        "> Shall I compare thee to a summer's day?  \n",
        "> Thou art more lovely and more temperate:  \n",
        "> Rough winds do shake the darling buds of May,  \n",
        "> And summer's lease hath all too short a date:  \n",
        "> Sometime too hot the eye of heaven shines,  \n",
        "> And often is his gold complexion dimm'd;  \n",
        "> And every fair from fair sometimes declines,  \n",
        "> By chance or nature's changing course untrimm'd;  \n",
        "> But thy eternal summer shall not fade  \n",
        "> Nor lose possession of that fair thou owest;  \n",
        "> Nor shall Death brag thou wander'st in his shade,  \n",
        "> When in eternal lines to time thou growest.\n",
        "\n",
        "What do you think Shakespear's sentiment was when he wrote this sonnet? In particular, is Shakespear being **positive** or **negative**?\n",
        "\n",
        "Let's find out using **sentiment analysis**."
      ],
      "metadata": {
        "id": "Mm-Lhvv5Igr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Step 1 we read the text file `sonnet_18.txt. from the course file server using the `urlopen()` command. The text is stored in a variable called `EG_text_1` (\"Example text 1\")."
      ],
      "metadata": {
        "id": "Tb1oZHikGgw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Step 1: Load text\n",
        "\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# Read sample text, a poem\n",
        "URL = \"https://biologicslab.co/BIO1173/data/sonnet_18.txt\"\n",
        "f = urlopen(URL)\n",
        "EG_text_1 = f.read().decode(\"utf-8\")\n",
        "\n",
        "# Print out text\n",
        "print(EG_text_1)"
      ],
      "metadata": {
        "id": "XcLwqoeWtLS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image01A.png)"
      ],
      "metadata": {
        "id": "5obxgSSN6NLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually, you have to preprocess text into embeddings or other vector forms before presentation to a neural network. Hugging Face provides a pipeline that simplifies this process greatly. The pipeline allows you to pass regular Python strings to the transformers and return standard Python values.\n",
        "\n",
        "We begin by loading a text-classification model. We can specify which model to use, by passing the model parameter, such as:\n",
        "\n",
        "```\n",
        "pipe = pipeline(model=\"roberta-large-mnli\")\n",
        "```\n",
        "\n",
        "**RoBERTa-large-MNLI** is a fine-tuned version of the RoBERTa large model, which is a transformer-based language model developed by Facebook AI. RoBERTa itself is an optimized version of BERT (Bidirectional Encoder Representations from Transformers) and is trained on a large corpus of English text using a masked language modeling (MLM) objective.\n",
        "\n",
        "The code in Example 1 - Step 2 loads a model pipeline and a model for sentiment analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "mah5UesgtQRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1 - Step 2: Load Model Pipeline\n",
        "\n",
        "This code in the cell below uses the `transformers library` to create a pipeline for natural language inference (NLI) tasks. The pipeline function takes a model name as an argument, which in this case is \"roberta-large-mnli\". This specific model is a variant of the `RoBERTa model` that has been pre-trained on a large dataset of text and can perform NLI tasks such as question answering or natural language inference.\n",
        "\n",
        "The pipeline function returns an instance of a Pipeline class from the transformers library, which is a high-level API for interacting with NLP models. The returned pipeline object has methods for various NLI tasks such as question answering, natural language inference, and text classification.\n",
        "\n",
        "In this specific case, the code is creating a pipeline for natural language inference tasks using the \"roberta-large-mnli\" model. This means that it can be used to perform NLI tasks such as determining the meaning of a sentence or question given some context. The Pipeline object returned by the pipeline function will have methods for performing these tasks, such as predict and score, which can be used to make predictions on new input data and evaluate the performance of the model."
      ],
      "metadata": {
        "id": "yK4WlOW8JWSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Step 2: Load model pipeline\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Specify the model for the pipeline\n",
        "model_name = \"roberta-large-mnli\"\n",
        "\n",
        "# Create classification pipeline\n",
        "classifier = pipeline(\"text-classification\", model=model_name)\n",
        "\n",
        "# Verify the pipeline\n",
        "print(classifier)\n"
      ],
      "metadata": {
        "id": "-Jo1e4QX-w3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image03A.png)"
      ],
      "metadata": {
        "id": "TZXVtJNBJrK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now display the sentiment analysis results with a Pandas dataframe.\n"
      ],
      "metadata": {
        "id": "KSs2hJvjtax8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1 - Step 3: Run Sentiment Analysis on Text\n",
        "\n",
        "We can now display the sentiment analysis results with a Pandas dataframe."
      ],
      "metadata": {
        "id": "gpzyqVCYLZni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Step 3: Run sentiment analysis on text\n",
        "\n",
        "# Create variable to hold sentiment analysis\n",
        "sentiment_outputs = classifier(EG_text_1)\n",
        "\n",
        "# Display output in a Pandas DataFrame\n",
        "pd.DataFrame(sentiment_outputs)\n"
      ],
      "metadata": {
        "id": "iKQqnKlxtcNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image03F.png)"
      ],
      "metadata": {
        "id": "MDNJJ40MR4Qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, our sentiment analysis considered the tone of Sonnet 18 as being `NEUTRAL` with a `score` of 0.68.\n",
        "\n",
        "Shakespeare's Sonnet 18 is generally considered to have positive and admiring sentiment. The sonnet praises the subject, comparing them to a summer's day and highlighting their eternal beauty and the impermanence of natural beauty.\n",
        "\n",
        "So why did we get only a `NEUTRAL` rating?\n",
        "\n",
        "Sentiment analysis models like **`RoBERTa-large-MNLI`** work by analyzing the text and classifying it based on the presence of positive, negative, or neutral sentiments. Shakespeare's `Sonnet 18` (\"Shall I compare thee to a summer's day?\") is a complex and nuanced piece of literature, which can make it challenging for an AI model to accurately classify its sentiment.\n",
        "\n",
        "Several factors could have contributed to the neutral sentiment classification:\n",
        "\n",
        "1. **Language and Style:** Shakespeare's language and poetic style are intricate, with a mix of positive imagery and more neutral or descriptive language. This complexity might have led the model to classify the overall sentiment as neutral.\n",
        "\n",
        "2. **Ambiguity:** The sonnet contains elements of praise and admiration, but it also discusses the transience of beauty and the passage of time, which might have contributed to a more balanced or neutral sentiment score.\n",
        "\n",
        "3. **Contextual Understanding:** AI models might struggle with understanding the full context and emotional depth of the text. They might miss subtleties that human readers would easily pick up on.\n",
        "\n",
        "The `0.6795 score` indicates that the model is fairly confident in its neutral classification, but it doesn't mean the text lacks positive sentiment. It just means that the model detected a more balanced mix of sentiments overall."
      ],
      "metadata": {
        "id": "O9fRS1Dltjpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1 - Step 1: Load Text for Sentiment Analyis**\n",
        "\n",
        "For **Exercise 1**, use Shakespear's Sonnet 66.\n",
        "\n",
        "**Sonnet 66**\n",
        "\n",
        "> Tired with all these, for restful death I cry:  \n",
        "> As, to behold desert a beggar born,  \n",
        "> And needy nothing trimmed in jollity,  \n",
        "> And purest faith unhappily forsworn,  \n",
        "> And gilded honor shamefully misplaced,  \n",
        "> And maiden virtue rudely strumpeted,  \n",
        "> And right perfection wrongfully disgraced,  \n",
        "> And strength by limping sway disablèd,  \n",
        "> And art made tongue-tied by authority,  \n",
        "> And folly, doctor-like, controlling skill,   \n",
        "> And simple truth miscalled simplicity,  \n",
        "> And captive good attending captain ill.  \n",
        ">   Tired with all these, from these would I be gone,  \n",
        ">   Save that, to die, I leave my love alone.  \n",
        "\n",
        "\n",
        "Here is the `URL` to download this poem:\n",
        "\n",
        "```text\n",
        "URL = \"https://biologicslab.co/BIO1173/data/sonnet_66.txt\"\n",
        "```\n",
        "\n",
        "Call your text **`EX_text_1`** and print out `EX_test_1` at the end of the code cell.\n"
      ],
      "metadata": {
        "id": "Lx3GWspDz-FM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 - Step 1 here\n"
      ],
      "metadata": {
        "id": "WnFCZoeEz-FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct your should see the following output\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image02A.png)"
      ],
      "metadata": {
        "id": "j-e53kar6_0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1 - Step 2: Load Model Pipeline**\n",
        "\n",
        "In the cell below create the pipeline for your sentiment analysis. You can reuse the code in Example 1 - Step 2 with the following modification. Instead of using the `\"roberta-large-mnli\"` model, change your model to the following:\n",
        "\n",
        "```text\n",
        "\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "```\n",
        "\n",
        "The **`distilbert-base-uncased-finetuned-sst-2-english`** model is a lightweight, 66 M‑parameter transformer that was distilled from BERT‑base and fine‑tuned on the Stanford Sentiment Treebank (SST‑2) for binary sentiment classification. Using an uncased WordPiece tokenizer and a 512‑token limit, the model maps input text to a pair of logits (positive/negative) through a simple linear head on the [CLS] token; applying a softmax gives the sentiment probability. It achieves roughly 93 % accuracy on SST‑2, matching full BERT‑base while running about twice as fast and using half the memory, making it ideal for quick sentiment inference on English text in production settings."
      ],
      "metadata": {
        "id": "mhpsgipYz-FM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 - Step 2 here\n",
        "\n"
      ],
      "metadata": {
        "id": "MlGTY7_A_PAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image02F.png)"
      ],
      "metadata": {
        "id": "JIPcEWZeQleg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1 - Step 3: Run Sentiment Analysis on Text**\n",
        "\n",
        "In the cell below write the code to run and display the sentiment analysis on your **`EX_text_1`**."
      ],
      "metadata": {
        "id": "5o9W8dU2z-FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 - Step 3 here\n"
      ],
      "metadata": {
        "id": "a_TMdprsz-FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image04F.png)"
      ],
      "metadata": {
        "id": "stcQ0NbtVAeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shakespeare's Sonnet 66 indeed carries a much darker tone compared to Sonnet 18. Here are a few reasons why the sentiment analysis model might have given it a strong negative sentiment classification:\n",
        "\n",
        "1. **Tone and Themes:** Sonnet 66 delves into themes of despair, disillusionment, and a profound sense of weariness with the world's injustices. The recurring motifs of frustration and longing for rest likely contributed significantly to the negative sentiment score.\n",
        "\n",
        "2. **Language and Imagery:** The sonnet is filled with negative imagery and phrases expressing dissatisfaction and lamentation. Such language naturally skews towards a negative sentiment classification by the model.\n",
        "\n",
        "3. **Emotional Weight:** The emotional heaviness and the poet’s voice reflecting societal corruption and personal sorrow might resonate strongly with the AI model’s parameters for negative sentiment.\n",
        "\n",
        "The score of `0.995` indicates an almost complete certainty by the model that the sentiment is negative, which aligns well with the sonnet’s content and tone."
      ],
      "metadata": {
        "id": "59yGe0_gzrlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLP Entity Tagging**\n",
        "\n",
        "**Entity tagging**, also known as named entity recognition (NER), is a process in natural language processing (NLP) where entities such as names of people, organizations, locations, dates, and other specific items are identified and classified within a text. Here’s how it works:\n",
        "\n",
        "1. **Identification:** The model scans the text to find mentions of entities. For example, in the sentence \"Microsoft was founded by Bill Gates in 1975,\" the entities are \"Microsoft,\" \"Bill Gates,\" and \"1975.\"\n",
        "\n",
        "2. **Classification:** Once identified, the entities are classified into predefined categories, such as:\n",
        "\n",
        "- **Person:** Names of individuals (e.g., \"Bill Gates\")\n",
        "\n",
        "- **Organization:** Names of companies, institutions, etc. (e.g., \"Microsoft\")\n",
        "\n",
        "- **Location:** Names of places (e.g., \"Seattle\")\n",
        "\n",
        "- **Date/Time:** Specific dates or times (e.g., \"1975\")\n",
        "\n",
        "- **Others:** Such as events, quantities, monetary values, etc.\n",
        "\n",
        "Entity tagging is widely used in various applications like information extraction, search engines, and enhancing the accuracy of machine translation. It helps in structuring unstructured text data, making it easier to analyze and extract meaningful insights.\n",
        "\n",
        "Entity tagging is the process that takes source text and finds parts of that text that represent entities, such as one of the following:\n",
        "\n",
        "* Location (LOC)\n",
        "* Organizations (ORG)\n",
        "* Person (PER)\n",
        "* Miscellaneous (MISC)\n",
        "\n",
        "The code in Example 2 requests a \"named entity recognizer\" (ner) and processes the specified text."
      ],
      "metadata": {
        "id": "8sUPYLZktncz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2 - Step 1: Entity Tagging\n",
        "\n",
        "The code in the cell below creates a variable, `EG_text_2` with the name of a famous molecular biologist, `James Watson` in the United States."
      ],
      "metadata": {
        "id": "8xs1j0xLZjV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Step 1: Entity tagging\n",
        "\n",
        "# Specify text\n",
        "EG_text_2 = \"James Watson was a molecular biologist who lived in the United States.\"\n",
        "\n",
        "# Create one pipeline instance for each task\n",
        "classifier = pipeline(\"text-classification\", model=\"roberta-large-mnli\")\n",
        "ner_tagger   = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
        "\n",
        "# Classification\n",
        "print(classifier(EG_text_2))     # MNLI style entailment / contradiction / neutral\n",
        "\n",
        "# NER\n",
        "print(ner_tagger(EG_text_2))\n"
      ],
      "metadata": {
        "id": "ANvzEcwp-Lot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image03A.png)"
      ],
      "metadata": {
        "id": "y6Rupj35XQi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2 - Step 2: Review the Results\n",
        "\n",
        "The code in the next cell let us view the results stored in the `Pandas` DataFrame.  As you can see in the output, the person (`PER`) is `James Watson` and the location (`LOC`) is the `United States`."
      ],
      "metadata": {
        "id": "mPxC6XUets6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Step 2: Review results\n",
        "\n",
        "# Define outputs\n",
        "outputs = ner_tagger(EG_text_2)\n",
        "\n",
        "# Display Output DataFrame\n",
        "pd.DataFrame(outputs)\n"
      ],
      "metadata": {
        "id": "WjCl_Ak2twgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image06F.png)"
      ],
      "metadata": {
        "id": "pxspeNp4YEZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your entity tagging pipeline has successfully identified and classified \"James Watson\" as a person and \"United States\" as a location with very high confidence."
      ],
      "metadata": {
        "id": "ddJJCnyuZbm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2 - Step 1: Entity Tagging**\n",
        "\n",
        "In the cell below create a variable, `EG_text3` with the name of another famous molecular biologist, `Francis Crick` in the `United Kingdom`."
      ],
      "metadata": {
        "id": "bYxKAJB9aGAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 - Step 1 here\n",
        "\n"
      ],
      "metadata": {
        "id": "3Ao9ttxkaGAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image05A.png)"
      ],
      "metadata": {
        "id": "nF-xVBgoaGAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2 - Step 2: Review the Results**\n",
        "\n",
        "In the cell below write the code to define and review the results of your **`EX_text_2`**."
      ],
      "metadata": {
        "id": "_2C0y73aaGAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 - Step 2: here\n",
        "\n"
      ],
      "metadata": {
        "id": "wCBjXGXdaGAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image06A.png)"
      ],
      "metadata": {
        "id": "WOkmwOTYaGAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your entity tagging pipeline has successfully identified and classified \"Francis Crick\" as a person and \"United Kingdom\" as a location with very high confidence."
      ],
      "metadata": {
        "id": "U8uPVF8CaGAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLP Question Answering**\n",
        "\n",
        "**Question Answering (QA)** is a powerful NLP task that involves providing precise answers to questions based on a given reference text. This capability is incredibly useful in various applications, such as information retrieval, and educational tools.\n",
        "\n",
        "Here's a quick rundown of how QA works:\n",
        "\n",
        "1. **Reference Text:** A passage or document from which the answer can be extracted.\n",
        "\n",
        "2. **Question:** A specific query related to the reference text.\n",
        "\n",
        "3. **Answer Extraction:** The NLP model processes the reference text to find the exact segment that answers the question.\n",
        "\n",
        "#### **Use Cases for a Computational Biologist**\n",
        "\n",
        "For a computational biologist, “QA analysis” usually refers to **question‑answering** systems that can interrogate large biological data sets, literature corpora, and simulation outputs.  Below are practical ways the technology can be applied in a research or industrial setting.\n",
        "\n",
        "| Use Case | What the QA system does | Typical inputs | Typical outputs | Why it matters |\n",
        "|----------|------------------------|----------------|-----------------|----------------|\n",
        "| **Literature mining & summarization** | Reads PubMed, bioRxiv, and review articles to answer specific biological questions (e.g., “Which genes are co‑expressed with *TP53* in breast cancer?”). | Natural‑language query + optional PMID set | Short answer, evidence sentences, citation list | Speeds up hypothesis generation and keeps investigators up‑to‑date. |\n",
        "| **Protocol & pipeline verification** | Checks whether computational workflows (e.g., variant‑calling pipelines) meet best‑practice standards or regulatory requirements. | Pipeline description, code, test logs | Pass/fail verdict, compliance report | Helps avoid costly downstream errors and accelerates reproducibility. |\n",
        "| **Clinical data interrogation** | Answers clinical questions from electronic health records or multi‑omics datasets (“What is the probability that a patient with X mutation will respond to drug Y?”). | Structured patient data, genomic profiles | Risk score, confidence interval, relevant literature | Supports precision‑medicine decisions. |\n",
        "| **Drug‑target & repurposing discovery** | Queries databases (DrugBank, ChEMBL) to find potential new targets or off‑target effects. | Chemical structure or name, target protein | List of candidate targets, predicted affinity scores | Narrows experimental screening lists. |\n",
        "| **Ontology & annotation assistance** | Maps free‑text notes or sequence motifs to controlled vocabularies (GO, MeSH). | Raw text, protein FASTA | Curated annotations, suggested terms | Improves data standardization for downstream analysis. |\n",
        "| **Model validation & debugging** | Interrogates machine‑learning models to expose decision pathways (“Why did the model predict a splice‑site mutation is benign?”). | Model, input features, intermediate activations | Explanatory text, feature importance, counter‑factuals | Enhances trust and aids model refinement. |\n",
        "| **Educational & training support** | Answers students’ queries during bioinformatics courses (e.g., “Explain the difference between UTRs and introns”). | Student question | Concise, citation‑backed answer | Lowers the learning curve for new trainees. |\n",
        "\n",
        "#### **How it typically works**\n",
        "1. **Indexing** – Textual resources (papers, databases, logs) are pre‑processed and stored in a vector‑search index.  \n",
        "2. **Query processing** – The user’s natural‑language question is encoded into a vector and matched against the index.  \n",
        "3. **Answer generation** – The system extracts the most relevant passages, optionally runs a language‑model head to paraphrase or synthesize an answer, and tags the answer with evidence citations.  \n",
        "4. **Human‑in‑the‑loop** – Domain experts review the answer; feedback is fed back into the model for continual improvement.\n",
        "\n",
        "By integrating QA analysis into the computational biology workflow, researchers can dramatically reduce the time spent on literature reviews, data curation, and pipeline troubleshooting, allowing more focus on discovery and hypothesis testing.\n"
      ],
      "metadata": {
        "id": "d5rdGwmvt04K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build QA Pipeline\n",
        "\n",
        "The code in the cell below builds a **Questioning Answering (QA)** pipeline."
      ],
      "metadata": {
        "id": "QghFY-IXLuIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build pipeline\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Build the question‑answering pipeline (GPU is auto‑selected if available)\n",
        "qa_pipe = pipeline(\n",
        "    task=\"question-answering\",\n",
        "    model=\"distilbert-base-uncased-distilled-squad\",  # any QA‑ready model works\n",
        ")\n",
        "\n",
        "print(\" QA pipeline is ready!\")\n"
      ],
      "metadata": {
        "id": "r6cIl19QLu0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image15A.png)"
      ],
      "metadata": {
        "id": "bb2PCSKy5A3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Generate QA Result\n",
        "\n",
        "The code in the cell below uses our `QA Pipeline` to ask the question\n",
        "\n",
        "```text\n",
        "\"what shall fade\"\n",
        "```\n",
        "The `context` of this question is Shakespear's `Sonnet 18` stored in `EG_text_1` that we created above in `Example 1`."
      ],
      "metadata": {
        "id": "Xz5wnIkW0iod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Generate QA result\n",
        "\n",
        "# Define the context and the question you want answered\n",
        "context = EG_text_1  # Sonnet 18 from Example 1\n",
        "\n",
        "# Define the question\n",
        "question = \"what shall fade\"\n",
        "\n",
        "# Run the QA model\n",
        "result = qa_pipe(question=question, context=context)\n",
        "\n",
        "# Pretty‑print the answer\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer  : {result['answer']}\")\n",
        "print(f\"Score   : {result['score']:.4f}\")\n",
        "print(f\"Start   : {result['start']}  End: {result['end']}\")\n"
      ],
      "metadata": {
        "id": "d6ofbDKO0jQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image16A.png)\n",
        "\n",
        "With a `Score = 0.38`, `Hugging Face` is not especially confident that `external summer` is the correct answer to the question `what will fade`."
      ],
      "metadata": {
        "id": "b8H3VTUx3paM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Generate QA Result**\n",
        "\n",
        "The following is a famous quotation by the molecular geneticist James Watson as the forward to _Discovering the Brain_ by Sandra Ackerman.\n",
        "\n",
        "```type\n",
        "\"The brain is the last and grandest biological frontier,\"\n",
        "\"the most complex thing we have yet discovered in our universe.\"\n",
        "\"It contains hundreds of billions of cells interlinked through \"\n",
        "\"trillions of connections. The brain boggles the mind.\"\n",
        "```\n",
        "\n",
        "For **Exercise 3** you are to use Watson's quote as the `context`. The easiest way to do this is to `copy-and-paste` the quotation into space between the two parantheses as follows:\n",
        "\n",
        "```text\n",
        "Watson_quotation = (\n",
        "\"The brain is the last and grandest biological frontier,\"\n",
        "\"the most complex thing we have yet discovered in our universe.\"\n",
        "\"It contains hundreds of billions of cells interlinked through \"\n",
        "\"trillions of connections. The brain boggles the mind.\"\n",
        ")  \n",
        "```\n",
        "You will need to assign the `context` to your `Watson_quotation using this code:\n",
        "\n",
        "```text\n",
        "# Assign quotation to content\n",
        "content=Watson_quotation\n",
        "```\n",
        "Finally, for your question, use this text:\n",
        "\n",
        "```type\n",
        "\"how many connections in the brain\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "jb7TZNa5BN33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 3 here\n",
        "\n"
      ],
      "metadata": {
        "id": "ngvBTs726nxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image14A.png)\n"
      ],
      "metadata": {
        "id": "PpU_4gq26nxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------\n",
        "\n",
        "### **What does the `score` field mean in Hugging Face QA pipelines?**\n",
        "\n",
        "When a question‑answering model returns a span, it also returns a\n",
        "`score`.  The score is **not a hard probability** of correctness, but\n",
        "the model’s internal confidence that the chosen start‑and‑end tokens\n",
        "are the right answer.\n",
        "\n",
        "#### **How it is computed**\n",
        "1. The model outputs *start* and *end* logits for every token in the\n",
        "   context.  \n",
        "2. A softmax is applied to each set of logits → probabilities that sum\n",
        "   to 1.  \n",
        "3. The pipeline picks the span \\([s, e]\\) that maximises  \n",
        "   \\( P_{\\text{start}}(s) \\times P_{\\text{end}}(e) \\).  \n",
        "   This product is the `score`.\n",
        "\n",
        "#### **Interpreting a score**\n",
        "| Score range | Rough meaning | What to do |\n",
        "|-------------|---------------|------------|\n",
        "| < 0.3 | Very low confidence | Treat the answer as unreliable; re‑phrase the question or add more context. |\n",
        "| ≈ 0.5 | Moderate confidence | The answer is plausible but not guaranteed; double‑check if accuracy matters. |\n",
        "| > 0.8 | High confidence | The answer is likely correct; you can use it as is. |\n",
        "\n",
        "> **Caveat** – Hugging Face models are *not* calibrated to produce true\n",
        "> probabilities.  Use the score to **rank or filter** results, not as an\n",
        "> absolute correctness metric.\n",
        "\n",
        "-------------------------------\n"
      ],
      "metadata": {
        "id": "sfDLm-qyM716"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLP Language Translation**\n",
        "\n",
        "**Language Translation** is a fascinating and practical application of natural language processing (NLP). It involves converting text from one language into another while preserving the original meaning, context, and nuances.\n",
        "\n",
        "Hugging Face provides powerful pre-trained models for language translation. Here is a list of the popular models:\n",
        "\n",
        "#### **List of Translators Used by Hugging Face**\n",
        "\n",
        "Hugging Face offers a variety of translation models for different language pairs. Here are some popular ones:\n",
        "\n",
        "1. **MarianMT Models**: These models support translation between multiple language pairs. For example:\n",
        "   - `Helsinki-NLP/opus-mt-en-de`: English to German\n",
        "   - `Helsinki-NLP/opus-mt-en-fr`: English to French\n",
        "   - `Helsinki-NLP/opus-mt-en-es`: English to Spanish\n",
        "\n",
        "2. **T5 Models**: These models can be fine-tuned for translation tasks. For example:\n",
        "   - `t5-small`: A smaller version of the T5 model that can be fine-tuned for translation.\n",
        "   - `t5-large`: A larger version of the T5 model with more parameters for better performance.\n",
        "\n",
        "3. **mBART Models**: These models are designed for multilingual translation tasks. For example:\n",
        "   - `facebook/mbart-large-50-many-to-many-mmt`: A multilingual model that supports translation between 50 languages.\n",
        "\n",
        "4. **M2M100 Models**: These models are designed for many-to-many translation tasks. For example:\n",
        "   - `facebook/m2m100_418M`: A model that supports translation between 100 languages.\n",
        "\n",
        "You can find more translation models and explore their capabilities on the [Hugging Face Models page](https://huggingface.co/models?search=translate).\n"
      ],
      "metadata": {
        "id": "Kq2yoUGGuD1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `sacremoses` package\n",
        "\n",
        "Before we can start translating, we need to install the `sacremoses` package.\n",
        "The `sacremoses` package is a Python port of the `Moses tokenizer`, `truecaser`, and `normalizer`. It provides tools for text preprocessing, which are essential for various natural language processing (NLP) tasks. Here are some key features of sacremoses:\n",
        "\n",
        "**Tokenizer:** The Moses tokenizer splits text into tokens (words, punctuation, etc.) while handling special characters and preserving the original meaning. For example, it can tokenize sentences with unusual symbols and punctuation.\n",
        "\n",
        "**Detokenizer:** The Moses detokenizer reverses the tokenization process, converting tokens back into a coherent sentence.\n",
        "\n",
        "**Truecaser:** The Moses truecaser adjusts the casing of words in a text to match typical usage patterns. It can be trained on a large corpus to learn the correct casing for words.\n",
        "\n",
        "Google Colab doesn't normally include the `sacremoses` package but you can add it by running the following code cell."
      ],
      "metadata": {
        "id": "82F8ZG8ej7Fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install sacremoses package\n",
        "\n",
        "!pip install sacremoses # Problem?"
      ],
      "metadata": {
        "id": "tiKsGDwGgsit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image12A.png)"
      ],
      "metadata": {
        "id": "6rEo499NAT85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4 - Step 1: Select Translator\n",
        "\n",
        "In this example we will use the `\"Helsinki-NLP/opus-mt-en-de\"` model. The **`Helsinki-NLP/opus-mt-en-de`** model is a powerful tool for translating text from English to German. Developed by the Language Technology Research Group at the University of Helsinki, this model is part of the `OPUS-MT` project, which provides open translation services for various language pairs"
      ],
      "metadata": {
        "id": "rDkYgglxf8B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4 - Step 1: Setup translator\n",
        "\n",
        "# Select translator\n",
        "translator = pipeline(\"translation_en_to_de\",\n",
        "                      model=\"Helsinki-NLP/opus-mt-en-de\")\n"
      ],
      "metadata": {
        "id": "7JROV9iruGq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image17A.png)"
      ],
      "metadata": {
        "id": "1Dk7gaStAlni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4 - Step 2: Translate Text\n",
        "\n",
        "The following code translates Watson's quotation (`Watson_quotation`) that was used in **Exercise 3** into German.\n"
      ],
      "metadata": {
        "id": "fmWyexSduQf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4 - Step 2 Translate text\n",
        "\n",
        "import textwrap\n",
        "from transformers import pipeline\n",
        "\n",
        "# Normalize the text by removing extra spaces\n",
        "normalized_text = ' '.join(Watson_quotation.split())\n",
        "\n",
        "# Print the normalized text with proper wrapping\n",
        "wrapped_text = textwrap.fill(normalized_text, width=50)\n",
        "print(wrapped_text)\n",
        "\n",
        "# Print spacer\n",
        "print(\"\\n\")\n",
        "\n",
        "# Perform translation\n",
        "outputs = translator(Watson_quotation, clean_up_tokenization_spaces=True, min_length=100)\n",
        "\n",
        "# Get the translated text\n",
        "translated_text = outputs[0]['translation_text']\n",
        "\n",
        "# Remove excessive dots (e.g., ellipses)\n",
        "cleaned_text = translated_text.rstrip('.')\n",
        "\n",
        "# Break the cleaned text into separate lines\n",
        "wrapped_text = textwrap.fill(cleaned_text, width=50)\n",
        "\n",
        "print(wrapped_text)\n"
      ],
      "metadata": {
        "id": "WKg_Ivc4iZ4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image18A.png)"
      ],
      "metadata": {
        "id": "TWMWrvIZjCeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLP Summarization**\n",
        "\n",
        "**Summarization** is a key NLP task that involves condensing a longer text into a shorter version while preserving the main ideas and important details. There are two primary types of summarization:\n",
        "\n",
        "1. **Extractive Summarization:** This method involves selecting and extracting key sentences or phrases from the original text. The selected sentences are then combined to form a summary. It's like creating a highlight reel of the text.\n",
        "\n",
        "2. **Abstractive Summarization:** This method involves generating new sentences that capture the essence of the original text. It requires the model to understand the context and rephrase the content in a concise manner. This approach is more challenging but can produce more natural and coherent summaries.\n",
        "\n",
        "Summarization is an NLP task that summarizes a more lengthy text into just a few sentences.\n",
        "\n",
        "#### **Use Cases**\n",
        "\n",
        "**NLP summarization**—both extractive and the more recent abstractive methods powered by transformer architectures such as BERT, T5, and GPT—automatically condenses long documents into concise, coherent overviews while preserving key information. In computational biology, this capability is indispensable for navigating the deluge of biomedical literature and data reports. Researchers employ summarization to generate rapid literature reviews, extract essential findings from high‑throughput genomics, proteomics, and metabolomics papers, and produce concise summaries of experimental protocols or clinical trial outcomes. Additionally, summarization helps distill complex pathway and interaction network descriptions, create short reports of multi‑omics integration results, and aid in hypothesis generation by highlighting novel associations across datasets. By reducing manual curation effort, summarization accelerates discovery pipelines, improves reproducibility, and supports evidence‑based decision‑making in genomics, drug discovery, and personalized medicine."
      ],
      "metadata": {
        "id": "NjEAp1jguJvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Summarization\n",
        "\n",
        "The code in the cell below summarizes the text in the variable `EG_text3`."
      ],
      "metadata": {
        "id": "RySRVoR6oo-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5 - Step 1: Setup pipeline for summarizer\n",
        "\n",
        "EG_text3 = \"\"\"\n",
        "Hugging Face is a company and an open-source platform that focuses on Natural\n",
        "Language Processing (NLP) technologies and machine learning models.\n",
        "We’re on a journey to advance and democratize artificial intelligence\n",
        "through open source and open science. Our mission is to make AI accessible\n",
        "to everyone, enabling researchers, developers, and organizations to\n",
        "build and deploy state-of-the-art models with ease. By fostering a\n",
        "collaborative community and providing cutting-edge tools,\n",
        "we aim to accelerate the development and adoption of AI technologies\n",
        "for the benefit of all.\n",
        "\"\"\"\n",
        "\n",
        "summarizer = pipeline(\"summarization\")"
      ],
      "metadata": {
        "id": "em2NSdN2uIiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image11F.png)"
      ],
      "metadata": {
        "id": "jxIMRh_3qQv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5 - Step 2: Print Summary\n",
        "\n",
        "The following code summarizes the text in `EG_text3`"
      ],
      "metadata": {
        "id": "7jafTxh5uiUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5 - Step 2: Print output\n",
        "\n",
        "import textwrap\n",
        "from transformers import pipeline\n",
        "\n",
        "# Print original text\n",
        "print(\"Original Text:\")\n",
        "print(EG_text3)\n",
        "\n",
        "# Print spacer\n",
        "print(\"\\nSummary: \\n\")\n",
        "\n",
        "# Send text to summarizer\n",
        "outputs = summarizer(EG_text3, max_length=45, min_length=20,\n",
        "                     clean_up_tokenization_spaces=True)\n",
        "\n",
        "# Get the summary text\n",
        "summary_text = outputs[0]['summary_text']\n",
        "\n",
        "# Break the summary into separate lines\n",
        "wrapped_summary = textwrap.fill(summary_text, width=50)\n",
        "\n",
        "# Print the wrapped summary\n",
        "print(wrapped_summary)\n"
      ],
      "metadata": {
        "id": "kKErV4rrsYjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLP Text Generation**\n",
        "\n",
        "**Text generation** is a fundamental and powerful capability in natural language processing (NLP), and Hugging Face provides several models that excel in this area. Here's why text generation is important and how it is utilized:\n",
        "\n",
        "1. **Creative Writing:** Text generation models can help authors, poets, and scriptwriters generate new content, brainstorm ideas, or even complete unfinished pieces. This can significantly boost creativity and productivity.\n",
        "\n",
        "2. **Conversational Agents:** Text generation models are used to create chatbots and virtual assistants that can engage in natural and meaningful conversations with users. These models can provide customer support, answer queries, and even offer companionship.\n",
        "\n",
        "3. **Content Creation:** Businesses and content creators use text generation to produce articles, social media posts, product descriptions, and more. This helps in maintaining a consistent flow of content and saves time.\n",
        "\n",
        "4. **Language Translation:** Text generation plays a crucial role in machine translation, where models generate translated text from one language to another, ensuring that the meaning and nuances are preserved.\n",
        "\n",
        "5. **Summarization:** As mentioned earlier, text generation models can summarize lengthy documents into concise summaries, making it easier to digest large volumes of information quickly.\n",
        "\n",
        "6. **Code Generation:** Developers use text generation models to assist in coding by generating code snippets, documenting code, or even completing functions based on prompts.\n",
        "\n",
        "7. **Educational Tools:** Text generation can be used to create educational content, generate quizzes, provide explanations, and even tutor students in various subjects.\n",
        "\n",
        "Hugging Face offers several powerful models for text generation, such as GPT-3, BERT, T5, and more. These models leverage large-scale pre-training on diverse datasets to understand and generate human-like text."
      ],
      "metadata": {
        "id": "ptrnxK9auo_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6 - Step 1: Setup Generator Pipeline\n",
        "\n",
        "The code in the cell below setups up the generator pipeline."
      ],
      "metadata": {
        "id": "OtKI4PiNulDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6 - Step 1: Setup generator pipeline\n",
        "\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# Setup pipeline\n",
        "generator = pipeline(\"text-generation\")\n"
      ],
      "metadata": {
        "id": "-ynuPrL4usHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to the following output:\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image12F.png)"
      ],
      "metadata": {
        "id": "AfXlDQTUvEjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 6 - Step 2: Generate New Text\n",
        "\n",
        "The code in the cell below generates additional text after Sonnet 18.\n"
      ],
      "metadata": {
        "id": "MMHsEEF4uwQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6 - Step 2: Generate new text\n",
        "\n",
        "\n",
        "# Print original text\n",
        "print(\"\\nOriginal Text:---------------------- \\n\")\n",
        "print(EG_text)\n",
        "print(\"\\nGenerated Text:------------------- \\n\")\n",
        "\n",
        "outputs = generator(EG_text_1, max_length=400)\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "cU1Zh7hJufjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is correct you should see something similar to this output.\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image20A.png)\n",
        "\n",
        "NOTE: If your rerun the code several times, you will generate quite different outputs!"
      ],
      "metadata": {
        "id": "ShIXZvnKvCiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparison of Original vs. Generated Text**\n",
        "\n",
        "The following is an analysis of the Original Text (Sonnet 18) and the Generated Text (shown above) by `OpenAI's` **`ChatGPT-4o`** model.\n",
        "\n",
        "#### **1. Fidelity to Original**\n",
        "- **Original Text**: Maintains Shakespeare’s iambic pentameter, rhyme scheme (ABAB CDCD EFEF GG), and thematic focus on eternal beauty and poetic immortality.\n",
        "- **Generated Text**: The first 12 lines are identical to the original, but the continuation breaks from the sonnet structure and introduces new, loosely connected ideas.\n",
        "\n",
        "#### **2. Coherence and Structure**\n",
        "- **Original**: Cohesive and logically structured argument about the enduring nature of the subject’s beauty.\n",
        "- **Generated**: Becomes repetitive and structurally disorganized. Phrases like “the boy shall be called, as he is now” are repeated without clear poetic or thematic purpose.\n",
        "\n",
        "#### **3. Style and Language**\n",
        "- **Original**: Rich in metaphor (“eye of heaven,” “eternal summer”), elevated diction, and poetic devices.\n",
        "- **Generated**: Attempts poetic language but lacks the sophistication and precision of Shakespeare. The metaphors are simpler and less evocative (“sun and moon are the sweetest kisses of love”).\n",
        "\n",
        "#### **4. Thematic Depth**\n",
        "- **Original**: Explores themes of beauty, mortality, and the power of poetry to preserve.\n",
        "- **Generated**: Shifts toward themes of identity and romantic roles but without clear development or philosophical insight.\n",
        "\n",
        "#### **5. Creativity and Originality**\n",
        "- **Generated Text**: Shows an attempt to emulate poetic style and extend the theme, but the repetition and lack of clarity suggest limitations in the model’s understanding of poetic form and nuance.\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| Aspect              | Original Text                  | Generated Text                          |\n",
        "|---------------------|--------------------------------|------------------------------------------|\n",
        "| Structure           | Sonnet form, 14 lines          | Starts as sonnet, then diverges          |\n",
        "| Language            | Elevated, metaphorical         | Poetic attempt, but repetitive           |\n",
        "| Coherence           | Logical and thematic unity     | Fragmented and repetitive                |\n",
        "| Creativity          | Masterful and timeless         | Imitative, with limited poetic insight   |\n",
        "\n",
        "---\n",
        "\n",
        "It should be noted that some of the newest LLM models such as **`Gemini 2.5 Pro`**, **`ChatGPT-4o (OpenAI)`** and **`Claude 3.7 Sonnet (Anthropic)`** could probably do a much better job.\n"
      ],
      "metadata": {
        "id": "a7p6SSPuvo1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like the text generation output continued far beyond the original Sonnet 18, incorporating additional and somewhat repetitive lines. This can happen with generative models when they attempt to create content based on a given prompt."
      ],
      "metadata": {
        "id": "-y4t8mQZxzwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_05_1.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ],
      "metadata": {
        "id": "pwesIKw7THkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lizard Tail**\n",
        "\n",
        "## **Apple Mac Studio with M3 Ultra**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/Mac_Studio.jpg)\n",
        "\n",
        "The Mac Studio is a small-form-factor workstation computer developed and marketed by Apple Inc. It is one of four desktop computers in the Mac lineup, sitting above the consumer-range Mac Mini and iMac, and positioned below the Mac Pro. It is configurable with either the M4 Max or M3 Ultra system on a chip.\n",
        "\n",
        "## On-Line Purchase\n",
        "\n",
        "If you are looking for a new computer to do a little homework, surf the web or run Large Language Models locally, you can't go wrong buying the **Apple Mac Studio with M3 Ultra**\n",
        "\n",
        "![___](https://biologicslab.co/BIO1173/images/class_05/class_05_1_image21A.png)\n",
        "\n",
        "### **Why the Mac Studio Ultra with M3 Is Useful for Running LLMs Locally**\n",
        "\n",
        "#### 🔧 Key Hardware Advantages\n",
        "\n",
        "##### 1. M3 Ultra Chip Architecture\n",
        "- Combines two M3 Max chips, offering **up to 32 CPU cores and 80 GPU cores**.\n",
        "- Unified memory architecture with **up to 192GB of RAM**, crucial for loading large models entirely into memory.\n",
        "\n",
        "##### 2. High Memory Bandwidth\n",
        "- Apple Silicon chips have **extremely fast memory bandwidth**, aiding rapid data movement during inference and training.\n",
        "\n",
        "##### 3. Neural Engine\n",
        "- The **Apple Neural Engine (ANE)** can accelerate certain ML tasks.\n",
        "- While not fully utilized by all ML frameworks yet, it's promising for future optimization.\n",
        "\n",
        "##### 4. Energy Efficiency\n",
        "- Much more **power-efficient** than traditional workstations with discrete GPUs.\n",
        "- Ideal for long-running tasks without excessive heat or power draw.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🧠 Software Ecosystem for LLMs on macOS\n",
        "\n",
        "##### 1. Support for ML Frameworks\n",
        "- macOS supports **PyTorch**, **TensorFlow**, and **Hugging Face Transformers**.\n",
        "- Tools like **Llama.cpp**, **GGUF**, and **MLX (Apple’s ML framework)** are optimized for Apple Silicon.\n",
        "\n",
        "#####2. Running Quantized Models\n",
        "- Models like **LLaMA 2**, **Mistral**, or **Phi-3** can run in 4-bit or 8-bit quantized formats.\n",
        "- With 192GB RAM, even **larger models** like LLaMA 3 70B can be experimented with in quantized form.\n",
        "\n",
        "##### 3. MLX Framework\n",
        "- Apple’s MLX framework is designed to take full advantage of Apple Silicon.\n",
        "- Makes it easier to run and experiment with models locally.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🧪 Use Cases for Local LLMs\n",
        "\n",
        "- **Privacy-sensitive applications** (e.g., medical or educational data).\n",
        "- **Offline access** for fieldwork or remote teaching.\n",
        "- **Rapid prototyping** without relying on cloud APIs.\n",
        "- **Fine-tuning small models** for domain-specific tasks.\n",
        "\n",
        "---\n",
        "\n",
        "Would you like help setting up a local LLM on a Mac Studio Ultra, or exploring which models would run best on it?\n",
        "\n",
        "\n",
        "## Overview\n",
        "### Rear ports\n",
        "\n",
        "The Mac Studio is a desktop personal computer, designed to sit between the consumer-level Mac Mini and the professional-targeted Mac Pro. The Mac Studio has an identical width and depth to the contemporary Mac mini, 7.7 inches (200 mm), but it stands taller at 3.7 inches (94 mm).\n",
        "\n",
        "The Mac Studio was initially offered in two ARM-based SoC: the M1 Max or the M1 Ultra, which combines two M1 Max chips in one package. It has:\n",
        "\n",
        "- Four Thunderbolt 4 (USB 4) ports\n",
        "- Two USB 3.0 Type-A ports\n",
        "- HDMI (up to 4K @ 60 Hz)\n",
        "- 10Gb Ethernet with Lights Out Management\n",
        "- Headphone jack\n",
        "\n",
        "The front panel has:\n",
        "\n",
        "- Two USB-C ports (Thunderbolt 4 in M1 Ultra models)\n",
        "- SD card slot (supports SDXC cards and UHS-II bus)\n",
        "\n",
        "It is cooled by a pair of double-sided blowers and a mesh of holes on the bottom and back of the case, which helps reduce fan noise. Nevertheless, there have been reports of excessive fan noise.\n",
        "\n",
        "Mac Studio models with the Ultra SoC are heavier than the Max-equipped models, as they exchange the aluminum heat sink for one composed of copper. Apple says the Mac Studio performs 50% faster than a Mac Pro with a 16-core Intel Xeon processor.\n",
        "\n",
        "The Mac Studio was introduced alongside the Apple Studio Display, a 27-inch 5K monitor with:\n",
        "\n",
        "- Integrated 12 MP camera\n",
        "- Six-speaker sound system with spatial audio and Dolby Atmos support\n",
        "- Height adjustable stand\n",
        "\n",
        "Customers reported months-long shipping delays for the Mac Studio, attributed to a global chip shortage.\n",
        "\n",
        "## Updates\n",
        "- **June 5, 2023 (WWDC)**: Updated Mac Studio models with M2 Max and M2 Ultra chips.\n",
        "  - Bluetooth 5.3\n",
        "  - Wi-Fi 6E\n",
        "  - Support for up to six 6K monitors\n",
        "  - 8K display support over Thunderbolt and HDMI\n",
        "\n",
        "- **March 5, 2025**: Updated Mac Studio models with M4 Max and M3 Ultra chips (shipping began March 12).\n",
        "  - Thunderbolt 5\n",
        "  - Memory configurable up to 512 GB\n",
        "  - Storage configurable up to 16 TB (M3 Ultra)\n",
        "  - M3 Ultra included due to no existing Ultra chips in the M4 line\n",
        "\n",
        "## Repairability\n",
        "\n",
        "![Mac Studio with Studio Display, Magic Keyboard, and Magic Trackpad in an Apple Store](image-placeholder removable flash storage ports, with one or two in use depending on storage configuration. While swapping flash storage cards between same-size models is possible with Apple Configurator restore, upgrading is not officially supported.\n",
        "\n",
        "Criticism includes:\n",
        "\n",
        "- Limited upgradeability unfriendly to right to repair\n",
        "- SSD controller integrated into SoC for encryption\n",
        "- SSD placement beneath exposed power supply\n",
        "\n",
        "## Reception\n",
        "\n",
        "> This section needs expansion. You can help by adding to it. (March 2025)\n",
        "\n",
        "## Specifications\n",
        "| Model | 2022 | 2023 | 2025 |\n",
        "|-------|------|------|------|\n",
        "| **Announced** | Mar 8, 2022 | Jun 5, 2023 | Mar 5, 2025 |\n",
        "| **Released** | Mar 18, 2022 | Jun 13, 2023 | Mar 12, 2025 |\n",
        "| **Discontinued** | Jun 5, 2023 | Mar 5, 2025 | In production |\n",
        "\n",
        "### Chip Configurations\n",
        "\n",
        "- **2022**\n",
        "  - M1 Max: 10-core CPU, 24-core GPU, 16-core Neural Engine\n",
        "  - M1 Ultra: 20-core CPU, 48-core GPU, 32-core Neural Engine\n",
        "\n",
        "- **2023**\n",
        "  - M2 Max: 12-core CPU, 30-core GPU, 16-core Neural Engine\n",
        "  - M2 Ultra: 24-core CPU, 60-core GPU, 32-core Neural Engine\n",
        "\n",
        "- **2025**\n",
        "  - M4 Max: 14-core CPU, 32-core GPU, 16-core Neural Engine\n",
        "  - M3 Ultra: 28-core CPU, 60-core GPU, 32-core Neural Engine\n",
        "\n",
        "### Memory\n",
        "\n",
        "- 2022: 32 GB (up to 64 GB)\n",
        "- 2023: 64 GB (up to 128 GB)\n",
        "- 2025:\n",
        "  - M4 Max: 36 GB (up to 128 GB)\n",
        "  - M3 Ultra: 96 GB (up to 512 GB)\n",
        "\n",
        "### Storage\n",
        "\n",
        "- 2022 & 2023: 512 GB (Max) or 1 TB (Ultra), up to 8 TB\n",
        "- 2025: Up to 16 TB (Ultra)\n",
        "\n",
        "### Wireless\n",
        "\n",
        "- 2022: Wi-Fi 6, Bluetooth 5.0\n",
        "- 2023: Wi-Fi 6E, Bluetooth 5.3\n",
        "\n",
        "### Connectivity\n",
        "\n",
        "- Thunderbolt 4/5 USB-C ports\n",
        "- USB-A ports\n",
        "- HDMI 2.0/2.1\n",
        "- 10Gb Ethernet\n",
        "- 3.5 mm headphone jack\n",
        "- SDXC card slot\n",
        "### Power\n",
        "\n",
        "- 2022: 370 W\n",
        "- 2023: 480 W\n",
        "\n",
        "### Dimensions\n",
        "- 3.7 in × 7.7 in × 7.7 in\n",
        "\n",
        "### Weight\n",
        "\n",
        "- Max: ~5.9 lb\n",
        "- Ultra: ~7.9–8.0 lb\n",
        "\n",
        "### Emissions\n",
        "\n",
        "- Varies by model and configuration (e.g., 262–382 kg CO₂e)\n",
        "\n",
        "## Software and Operating Systems\n",
        "\n",
        "All Mac Studio models ship with macOS, starting with **macOS Monterey**.\n",
        "\n",
        "### Supported macOS Releases\n",
        "\n",
        "| OS Release | 2022 | 2023 | 2025 |\n",
        "|------------|------|------|------|\n",
        "| 12 Monterey | 12.2 | — | — |\n",
        "| 13 Ventura | Yes | 13.4 | — |\n",
        "| 14 Sonoma | Yes | Yes | — |\n",
        "| 15 Sequoia | Yes | Yes | 15.2 |\n",
        "| 26 Tahoe | Yes | Yes | Yes |\n",
        "\n",
        "---\n",
        "\n",
        "**See also**: [List of Mac models]\n"
      ],
      "metadata": {
        "id": "I7So4JT8SXy0"
      }
    }
  ]
}