{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNJWLJTaxWY3IzMqrwDS9lR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_DICOM_V12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b11DEz3IT9nn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Custom Function"
      ],
      "metadata": {
        "id": "El2kWL0SUMoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function to print out elapsed time\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "cfT0lL00UPNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and Extract Data"
      ],
      "metadata": {
        "id": "Ch-xquaNUMgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "import zipfile\n",
        "import sys\n",
        "import shutil\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Configuration – change only if you want a different URL / filename\n",
        "# ------------------------------------------------------------------\n",
        "URL = \"https://biologicslab.co/BIO1173/data/\"\n",
        "ZIP_FILENAME = \"pna_data.zip\"\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Download the zip file (streamed, so it works with large files)\n",
        "# ------------------------------------------------------------------\n",
        "def download_zip(url: str, dest: Path, chunk_size: int = 8192) -> None:\n",
        "    \"\"\"Download a file from `url` and write it to `dest`.\"\"\"\n",
        "    print(f\"Downloading {ZIP_FILENAME} to {dest}...\", end='')\n",
        "    with requests.get(url, stream=True, timeout=30) as r:\n",
        "        r.raise_for_status()           # will raise for 4xx/5xx\n",
        "        with dest.open(\"wb\") as f_out:\n",
        "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                if chunk:               # filter out keep‑alive new chunks\n",
        "                    f_out.write(chunk)\n",
        "    print(\"done\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Un‑zip the downloaded archive into a *named* directory\n",
        "# ------------------------------------------------------------------\n",
        "def unzip_file(zip_path: Path, extract_to: Path) -> None:\n",
        "    \"\"\"Extract all members of `zip_path` into `extract_to`.\"\"\"\n",
        "    print(f\"Unzipping {ZIP_FILENAME} to {extract_to}...\", end='')\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(extract_to)\n",
        "    print(\"done\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Optional – delete the zip after extraction\n",
        "# ------------------------------------------------------------------\n",
        "def clean_up_zip(zip_path: Path) -> None:\n",
        "    \"\"\"Delete the zip file – only if you no longer need it.\"\"\"\n",
        "    zip_path.unlink()\n",
        "    print(f\"Removed temporary archive: {zip_path}... done\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Main routine\n",
        "# ------------------------------------------------------------------\n",
        "def main() -> None:\n",
        "    cwd          = Path.cwd()            # current working directory\n",
        "    zip_path     = cwd / ZIP_FILENAME\n",
        "    extract_dir  = cwd / zip_path.stem   # e.g. /pna_data\n",
        "\n",
        "    # Ensure the extraction directory exists\n",
        "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download\n",
        "    download_zip(URL+ZIP_FILENAME, zip_path)\n",
        "\n",
        "    # Un‑zip\n",
        "    unzip_file(zip_path, extract_dir)\n",
        "\n",
        "    # Clean‑up the downloaded archive\n",
        "    clean_up_zip(zip_path)\n",
        "\n",
        "    print(f\"Files have been extracted to {extract_dir}\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOblK7KpUb63",
        "outputId": "47926ca0-ad53-491f-aa75-2e7c9c445fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading pna_data.zip to /content/pna_data.zip...done\n",
            "Unzipping pna_data.zip to /content/pna_data...done\n",
            "Removed temporary archive: /content/pna_data.zip... done\n",
            "Files have been extracted to /content/pna_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Packages"
      ],
      "metadata": {
        "id": "UNNLphVsU3un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydicom\n",
        "!pip install -q dropblock"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsai5uYBU4l-",
        "outputId": "c6bda0ec-3dc1-4518-947b-fa4248d5a293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process DICOM Files"
      ],
      "metadata": {
        "id": "M6Y1OAM6UMbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process DICOM Files\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import warnings\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Global settings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Helper: read a single DICOM file\n",
        "# -------------------------------------------------------------\n",
        "def read_dicom_file(file_path: str):\n",
        "    \"\"\"Read a DICOM file and extract image data + basic metadata.\"\"\"\n",
        "    ds = pydicom.dcmread(file_path)\n",
        "\n",
        "    # Basic metadata\n",
        "    metadata = {\n",
        "        'filename': os.path.basename(file_path),\n",
        "        'patient_name': getattr(ds, 'PatientName', 'Unknown'),\n",
        "        'patient_id': getattr(ds, 'PatientID', 'Unknown'),\n",
        "        'study_date': getattr(ds, 'StudyDate', 'Unknown'),\n",
        "        'study_time': getattr(ds, 'StudyTime', 'Unknown'),\n",
        "        'modality': getattr(ds, 'Modality', 'Unknown'),\n",
        "        'manufacturer': getattr(ds, 'Manufacturer', 'Unknown'),\n",
        "        'institution_name': getattr(ds, 'InstitutionName', 'Unknown'),\n",
        "        'series_description': getattr(ds, 'SeriesDescription', 'Unknown'),\n",
        "        'bits_allocated': getattr(ds, 'BitsAllocated', 'Unknown'),\n",
        "        'rows': getattr(ds, 'Rows', 'Unknown'),\n",
        "        'columns': getattr(ds, 'Columns', 'Unknown'),\n",
        "        'pixel_spacing': getattr(ds, 'PixelSpacing', 'Unknown')\n",
        "    }\n",
        "\n",
        "    # Image data\n",
        "    if hasattr(ds, 'pixel_array'):\n",
        "        image_array = ds.pixel_array\n",
        "\n",
        "        # Normalise to 0‑255 if needed\n",
        "        if image_array.dtype != np.uint8:\n",
        "            image_array = ((image_array - image_array.min()) /\n",
        "                           (image_array.max() - image_array.min()) * 255).astype(np.uint8)\n",
        "\n",
        "        metadata['image_available'] = True\n",
        "        metadata['image_shape'] = image_array.shape\n",
        "    else:\n",
        "        metadata['image_available'] = False\n",
        "        metadata['image_shape'] = 'No image data'\n",
        "\n",
        "    return ds, metadata\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Helper: fast drop‑check\n",
        "# -------------------------------------------------------------\n",
        "def is_file_dropped(file_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Quick guard that tells us whether a DICOM file is already\n",
        "    missing / unreadable.\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(file_path):\n",
        "        return True\n",
        "\n",
        "    if os.path.getsize(file_path) == 0:\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        pydicom.dcmread(file_path, stop_before_pixels=True)\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Load & preprocess – merge CSVs, keep only valid DICOM rows\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "\n",
        "# (Assumes you already have an `is_file_dropped` helper somewhere in your code)\n",
        "# from your_module import is_file_dropped   # <-- adjust import as needed\n",
        "\n",
        "def load_and_preprocess_data(\n",
        "    data_dir: str = '.',\n",
        "    log_dropped: bool = True,\n",
        "    max_files: int | None = None,\n",
        "    seed: int = 42\n",
        "):\n",
        "    \"\"\"\n",
        "    Load the two CSVs, merge on patient ID, filter out rows that don't have a\n",
        "    valid DICOM file and optionally subsample the result.\n",
        "\n",
        "    Returns:\n",
        "        image_df (pd.DataFrame):  DataFrame that contains a new column\n",
        "                                 `file_path` pointing to the DICOM file\n",
        "                                 and the original merged columns.\n",
        "        dropped_ids (list[str]):  Patient IDs that were dropped.\n",
        "    \"\"\"\n",
        "    # --- 1. Resolve the path & sanity‑check the CSVs ---------------------------------\n",
        "    data_path   = pathlib.Path(data_dir).expanduser().resolve()\n",
        "    info_csv    = data_path / 'pna_detailed_class_info.csv'\n",
        "    labels_csv  = data_path / 'pna_train_labels.csv'\n",
        "\n",
        "    for fp in (info_csv, labels_csv):\n",
        "        if not fp.is_file():\n",
        "            raise FileNotFoundError(\n",
        "                f\"Required file not found: {fp}\\n\"\n",
        "                f\"Ensure `data_dir` points to the folder containing both CSVs.\"\n",
        "            )\n",
        "\n",
        "    # --- 2. Load the CSVs ----------------------------------------------------------------\n",
        "    info_df   = pd.read_csv(info_csv)\n",
        "    labels_df = pd.read_csv(labels_csv)\n",
        "\n",
        "    # --- 3. Merge on patient ID ---------------------------------------------------------\n",
        "    info_id_col   = 'patientId'\n",
        "    labels_id_col = 'patientId'\n",
        "\n",
        "    merged_df = pd.merge(\n",
        "        info_df,\n",
        "        labels_df,\n",
        "        left_on=info_id_col,\n",
        "        right_on=labels_id_col,\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    # --- 4. Keep only rows that actually have a readable DICOM -------------------------\n",
        "    dicom_dir   = data_path / 'pna_train_images'\n",
        "    valid_rows  = []\n",
        "    dropped_ids = []\n",
        "\n",
        "    for idx, row in merged_df.iterrows():\n",
        "        patient_id = row[info_id_col]\n",
        "        dicom_file = dicom_dir / f\"{patient_id}.dcm\"\n",
        "\n",
        "        if is_file_dropped(dicom_file):\n",
        "            dropped_ids.append(patient_id)\n",
        "        else:\n",
        "            valid_rows.append(idx)\n",
        "\n",
        "    image_df = merged_df.loc[valid_rows].copy()\n",
        "    if log_dropped:\n",
        "        print(f\"Dropped {len(dropped_ids)} rows (no valid DICOM).\")\n",
        "\n",
        "    # --- 5. (Optional) Random subsample ------------------------------------------------\n",
        "    if max_files is not None and len(image_df) > max_files:\n",
        "        image_df = image_df.sample(n=max_files, random_state=seed).reset_index(drop=True)\n",
        "        if log_dropped:\n",
        "            print(f\"Randomly subsampled to {max_files} rows (seed={seed}).\")\n",
        "\n",
        "    # --- 6. Add `file_path` column ----------------------------------------------------\n",
        "    #   This mirrors the original behaviour – the rest of the code can still\n",
        "    #   reference `image_df['file_path']` exactly as before.\n",
        "    image_df['file_path'] = image_df['patientId'].apply(\n",
        "        lambda pid: str(dicom_dir / f\"{pid}.dcm\")\n",
        "    )\n",
        "\n",
        "    # print(f\"Filtered DataFrame shape (with valid DICOM files): {image_df.shape}\")\n",
        "    return image_df, dropped_ids\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "#  Update load_and_preprocess_data to return only file‑paths + labels\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "def load_file_paths_and_labels(image_df, max_samples=None):\n",
        "    \"\"\"\n",
        "    Return two lists: `file_paths` and `labels`.  The function keeps the\n",
        "    *original* logic that filtered out NaNs / duplicated rows – we simply\n",
        "    strip it down to the very few objects that the new lazy loader needs.\n",
        "    \"\"\"\n",
        "    file_paths = image_df[\"file_path\"].tolist()\n",
        "    labels = image_df[\"label\"].tolist()\n",
        "\n",
        "    if max_samples is not None:\n",
        "        file_paths = file_paths[:max_samples]\n",
        "        labels = labels[:max_samples]\n",
        "\n",
        "    return file_paths, labels\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Main block – run the whole pipeline\n",
        "# -------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the data root\n",
        "    data_root = os.path.join('.', 'pna_data')"
      ],
      "metadata": {
        "id": "qS6FnybtUmSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate img_df"
      ],
      "metadata": {
        "id": "lMFGfh0nUMVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example:  keep at most 10 000 random rows (reproducible)\n",
        "\n",
        "MAX_FILES=10000\n",
        "SET_SEED=586\n",
        "\n",
        "# generate image_df\n",
        "print(f\"Generating img_df with {MAX_FILES} DICOM images with seed {SET_SEED}...\")\n",
        "img_df, dropped = load_and_preprocess_data(\n",
        "    data_dir='./pna_data',\n",
        "    max_files=MAX_FILES,         # 10_000,\n",
        "    seed=SET_SEED,\n",
        "    log_dropped=True\n",
        ")\n",
        "img_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "7WQ2nOHUVHIt",
        "outputId": "60ffa5a7-8d3e-4b2d-8c44-959a87c320ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating img_df with 10000 DICOM images with seed 586...\n",
            "Dropped 28292 rows (no valid DICOM).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               patientId                         class      x  \\\n",
              "1   00313ee0-9eaa-42f4-b0ab-c148ed3241cd  No Lung Opacity / Not Normal    NaN   \n",
              "2   00322d4d-1c29-4943-afc9-b6754be640eb  No Lung Opacity / Not Normal    NaN   \n",
              "18  00aecb01-a116-45a2-956c-08d2fa55433f                  Lung Opacity  288.0   \n",
              "19  00aecb01-a116-45a2-956c-08d2fa55433f                  Lung Opacity  547.0   \n",
              "20  00aecb01-a116-45a2-956c-08d2fa55433f                  Lung Opacity  288.0   \n",
              "\n",
              "        y  width  height  Target  \\\n",
              "1     NaN    NaN     NaN       0   \n",
              "2     NaN    NaN     NaN       0   \n",
              "18  322.0   94.0   135.0       1   \n",
              "19  299.0  119.0   165.0       1   \n",
              "20  322.0   94.0   135.0       1   \n",
              "\n",
              "                                            file_path  \n",
              "1   /content/pna_data/pna_train_images/00313ee0-9e...  \n",
              "2   /content/pna_data/pna_train_images/00322d4d-1c...  \n",
              "18  /content/pna_data/pna_train_images/00aecb01-a1...  \n",
              "19  /content/pna_data/pna_train_images/00aecb01-a1...  \n",
              "20  /content/pna_data/pna_train_images/00aecb01-a1...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2a10878-710f-482a-8443-2e49a1d8840b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patientId</th>\n",
              "      <th>class</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>Target</th>\n",
              "      <th>file_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/pna_data/pna_train_images/00313ee0-9e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/pna_data/pna_train_images/00322d4d-1c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>00aecb01-a116-45a2-956c-08d2fa55433f</td>\n",
              "      <td>Lung Opacity</td>\n",
              "      <td>288.0</td>\n",
              "      <td>322.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>1</td>\n",
              "      <td>/content/pna_data/pna_train_images/00aecb01-a1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>00aecb01-a116-45a2-956c-08d2fa55433f</td>\n",
              "      <td>Lung Opacity</td>\n",
              "      <td>547.0</td>\n",
              "      <td>299.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>1</td>\n",
              "      <td>/content/pna_data/pna_train_images/00aecb01-a1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>00aecb01-a116-45a2-956c-08d2fa55433f</td>\n",
              "      <td>Lung Opacity</td>\n",
              "      <td>288.0</td>\n",
              "      <td>322.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>1</td>\n",
              "      <td>/content/pna_data/pna_train_images/00aecb01-a1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2a10878-710f-482a-8443-2e49a1d8840b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b2a10878-710f-482a-8443-2e49a1d8840b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b2a10878-710f-482a-8443-2e49a1d8840b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-554461b9-4b76-4351-b584-8b3866b1fb34\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-554461b9-4b76-4351-b584-8b3866b1fb34')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-554461b9-4b76-4351-b584-8b3866b1fb34 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "img_df",
              "summary": "{\n  \"name\": \"img_df\",\n  \"rows\": 9337,\n  \"fields\": [\n    {\n      \"column\": \"patientId\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6671,\n        \"samples\": [\n          \"0c8b3735-25e3-4628-bb53-8c24e10eec56\",\n          \"5567ddc7-4c8e-456a-9ef1-24128a0dd9c8\",\n          \"5f2a4520-aaf5-478d-91dd-03615b06e0a3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"No Lung Opacity / Not Normal\",\n          \"Lung Opacity\",\n          \"Normal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"x\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 205.52980597777244,\n        \"min\": 10.0,\n        \"max\": 790.0,\n        \"num_unique_values\": 596,\n        \"samples\": [\n          560.0,\n          355.0,\n          101.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 148.30640671724913,\n        \"min\": 2.0,\n        \"max\": 845.0,\n        \"num_unique_values\": 598,\n        \"samples\": [\n          572.0,\n          172.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 59.65889207510873,\n        \"min\": 58.0,\n        \"max\": 528.0,\n        \"num_unique_values\": 292,\n        \"samples\": [\n          335.0,\n          301.0,\n          289.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 158.7684902816107,\n        \"min\": 52.0,\n        \"max\": 817.0,\n        \"num_unique_values\": 615,\n        \"samples\": [\n          309.0,\n          485.0,\n          248.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6671,\n        \"samples\": [\n          \"/content/pna_data/pna_train_images/0c8b3735-25e3-4628-bb53-8c24e10eec56.dcm\",\n          \"/content/pna_data/pna_train_images/5567ddc7-4c8e-456a-9ef1-24128a0dd9c8.dcm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Lists"
      ],
      "metadata": {
        "id": "cEZZu4VTUMKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the two lists that the rest of your pipeline expects ----\n",
        "file_paths_list = img_df['file_path'].tolist()\n",
        "labels_list     = img_df['Target'].astype(int).tolist()\n",
        "\n",
        "# --- Optional: quick sanity check ----------------------------------\n",
        "print(f\"First file path: {file_paths_list[:1]}\")\n",
        "print(f\"First label:     {labels_list[:1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VWd3Xg1VV4k",
        "outputId": "adbff37f-cca0-49cf-b6c2-e3f0c143eaf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First file path: ['/content/pna_data/pna_train_images/00313ee0-9eaa-42f4-b0ab-c148ed3241cd.dcm']\n",
            "First label:     [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Plotting Functions\n",
        "\n"
      ],
      "metadata": {
        "id": "W-LWNRFmVRY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Plotting Functions\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Show a single DICOM image (for sanity checks)\n",
        "# -------------------------------------------------------------\n",
        "def display_dicom_image(file_path: str, figsize: tuple = (10, 10)):\n",
        "    \"\"\"Show a single DICOM image with proper orientation.\"\"\"\n",
        "    ds = pydicom.dcmread(file_path)\n",
        "    if hasattr(ds, 'pixel_array'):\n",
        "        img = ds.pixel_array\n",
        "        if getattr(ds, 'PhotometricInterpretation', None) == 'MONOCHROME1':\n",
        "            img = np.max(img) - img\n",
        "\n",
        "        plt.figure(figsize=figsize)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(ds.SOPClassUID)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"This DICOM has no pixel data.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Visualise class / target distributions\n",
        "# -------------------------------------------------------------\n",
        "def visualize_data_distribution(image_df: pd.DataFrame):\n",
        "    if image_df is None:\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Class distribution\n",
        "    plt.subplot(1, 2, 1)\n",
        "    if 'class' in image_df.columns:\n",
        "        class_counts = image_df['class'].value_counts()\n",
        "        class_counts.plot(kind='bar')\n",
        "        plt.title('Class Distribution')\n",
        "        plt.ylabel('Count')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No class column found', ha='center')\n",
        "        plt.title('Class Distribution')\n",
        "\n",
        "    # Target distribution\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if 'Target' in image_df.columns:\n",
        "        target_counts = image_df['Target'].value_counts()\n",
        "        target_counts.plot(kind='bar')\n",
        "        plt.title('Target Distribution')\n",
        "        plt.ylabel('Count')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No Target column found', ha='center')\n",
        "        plt.title('Target Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Visualise class / target distributions\n",
        "# -------------------------------------------------------------\n",
        "def plot_train_val(history: dict,\n",
        "                   num_epochs: int,\n",
        "                   batch_size: int,\n",
        "                   title: str | None = None) -> None:\n",
        "    \"\"\"\n",
        "    Plot a two‑panel figure with vertical line at the best epoch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    history : dict\n",
        "        Must contain the keys 'train_acc', 'val_acc',\n",
        "        'train_loss', 'val_loss'.\n",
        "    num_epochs : int\n",
        "        Total number of epochs that were run.\n",
        "    batch_size : int\n",
        "        Batch size that was used (kept for API compatibility).\n",
        "    title : str | None, optional\n",
        "        Figure title.  If None, a default title is used.\n",
        "    \"\"\"\n",
        "    epochs = np.arange(1, num_epochs + 1)\n",
        "\n",
        "    # ---- 2‑panel layout ---------------------------------------------\n",
        "    fig, (ax_acc, ax_loss) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # ---------- Left panel – accuracy --------------------------------\n",
        "    ax_acc.plot(epochs, history[\"train_acc\"], label=\"train acc\", color=\"C0\")\n",
        "    ax_acc.plot(epochs, history[\"val_acc\"],   label=\"val acc\",   color=\"C1\")\n",
        "    ax_acc.set_xlabel(\"Epoch\")\n",
        "    ax_acc.set_ylabel(\"Accuracy\")\n",
        "    ax_acc.set_ylim(0, 1)\n",
        "    ax_acc.legend(loc=\"lower right\")\n",
        "    ax_acc.set_title(\"Accuracy\")\n",
        "\n",
        "    # ---------- Right panel – loss -----------------------------------\n",
        "    ax_loss.plot(epochs, history[\"train_loss\"], label=\"train loss\", color=\"C2\")\n",
        "    ax_loss.plot(epochs, history[\"val_loss\"],   label=\"val loss\",   color=\"C3\")\n",
        "    ax_loss.set_xlabel(\"Epoch\")\n",
        "    ax_loss.set_ylabel(\"Loss\")\n",
        "    ax_loss.set_ylim(bottom=0)        # let matplotlib decide the top limit\n",
        "    ax_loss.legend(loc=\"upper right\")\n",
        "    ax_loss.set_title(\"Loss\")\n",
        "\n",
        "    # ---------- Vertical line at best epoch ------------------------\n",
        "    if global_best_epoch is not None:\n",
        "        # Draw the line on both axes\n",
        "        for ax in (ax_acc, ax_loss):\n",
        "            ax.axvline(\n",
        "                global_best_epoch, color=\"red\",\n",
        "                linestyle=\"--\", linewidth=1.5,\n",
        "                label=\"Best epoch\"\n",
        "                )\n",
        "        # Update legend to include the new label (only once)\n",
        "        # The 'best_epoch' label is already added in the loop; we simply\n",
        "        # force the legend to re‑compute.\n",
        "        ax_acc.legend(loc=\"lower right\")\n",
        "        ax_loss.legend(loc=\"upper right\")\n",
        "\n",
        "    # ---------- Figure title ----------------------------------------\n",
        "    if title is None:\n",
        "        title = \"Training History\"\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Keep the suptitle separate from the sub‑plots\n",
        "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "bEmPeku5Vg6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Data Functions"
      ],
      "metadata": {
        "id": "Y9qe072LVRRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Data Functions\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms, models\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from typing import Optional\n",
        "\n",
        "# Set Import Variables\n",
        "NUM_EPOCHS: int = 50\n",
        "BATCH_SIZE: int = 64  # Reduced for memory management\n",
        "IMG_SIZE: int = 512  # Standard size for ResNet101\n",
        "LEARNING_RATE: float = 0.0010\n",
        "WEIGHT_DECAY: float  = 0.0010\n",
        "VAL_SPLIT: float = 0.2\n",
        "PATIENCE = 5\n",
        "EARLY_STOPPING = True\n",
        "\n",
        "# ------------------------------------------\n",
        "# Custom Dataset Class with Transforms\n",
        "# ------------------------------------------\n",
        "class DicomImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Load a DICOM image from disk and return (image_tensor, label).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_paths : list[str]\n",
        "        Absolute paths to the DICOM files.\n",
        "    labels    : list[int]\n",
        "        Integer labels that align with `file_paths`.\n",
        "    transform : callable | None\n",
        "        Optional transform that receives the `pydicom.dataset.FileDataset`\n",
        "        and returns a torch tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        assert len(file_paths) == len(labels), \\\n",
        "            f\"file_paths ({len(file_paths)}) != labels ({len(labels)})\"\n",
        "        self.file_paths = file_paths\n",
        "        self.labels    = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path  = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            dicom_img = pydicom.dcmread(path, force=True)   # <-- fails on corrupt files\n",
        "        except Exception as exc:\n",
        "            # Raise a RuntimeError that will be caught by the caller.\n",
        "            raise RuntimeError(f\"Could not read DICOM at {path!r}: {exc}\")\n",
        "\n",
        "        # Convert DICOM pixel array to proper format for transforms\n",
        "        if hasattr(dicom_img, 'pixel_array'):\n",
        "            img_array = dicom_img.pixel_array\n",
        "\n",
        "            # Handle different data types and normalize\n",
        "            if img_array.dtype != np.uint8:\n",
        "                if img_array.max() > 1.0:\n",
        "                    img_array = ((img_array - img_array.min()) /\n",
        "                               (img_array.max() - img_array.min()) * 255).astype(np.uint8)\n",
        "                else:\n",
        "                    img_array = (img_array * 255).astype(np.uint8)\n",
        "\n",
        "            # Convert to PIL Image for transforms\n",
        "            from PIL import Image\n",
        "            if len(img_array.shape) == 2:\n",
        "                # Single channel - convert to RGB by duplicating\n",
        "                pil_img = Image.fromarray(img_array, mode='L')\n",
        "                # Convert grayscale to RGB\n",
        "                pil_img = pil_img.convert('RGB')\n",
        "            else:\n",
        "                # Already multi-channel, convert to RGB if needed\n",
        "                pil_img = Image.fromarray(img_array)\n",
        "                if pil_img.mode != 'RGB':\n",
        "                    pil_img = pil_img.convert('RGB')\n",
        "\n",
        "            if self.transform is not None:\n",
        "                img = self.transform(pil_img)\n",
        "            else:\n",
        "                # Convert to tensor if no transform\n",
        "                img = torch.tensor(img_array, dtype=torch.float32)\n",
        "                if len(img.shape) == 2:\n",
        "                    img = img.unsqueeze(0)  # Add channel dimension\n",
        "        else:\n",
        "            raise RuntimeError(f\"No pixel array in DICOM file {path!r}\")\n",
        "\n",
        "        return img, label\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Helper: Check file paths\n",
        "# ------------------------------------------\n",
        "def filter_valid_dicom(paths, labels, max_attempts=5):\n",
        "    \"\"\"\n",
        "    Returns a pair (valid_paths, valid_labels) that only contains files that\n",
        "    can be successfully read with pydicom.\n",
        "    \"\"\"\n",
        "    valid_paths, valid_labels = [], []\n",
        "    for p, l in zip(paths, labels):\n",
        "        try:\n",
        "            pydicom.dcmread(p, force=True)\n",
        "        except Exception as exc:\n",
        "            # Log and skip\n",
        "            print(f\"⚠️  Skipping {p!r}: {exc}\")\n",
        "            continue\n",
        "        valid_paths.append(p)\n",
        "        valid_labels.append(l)\n",
        "\n",
        "    if not valid_paths:\n",
        "        raise RuntimeError(\"All DICOM files failed to load – dataset is empty.\")\n",
        "    return valid_paths, valid_labels\n",
        "\n",
        "# ------------------------------------------\n",
        "# Helper: Build transforms\n",
        "# ------------------------------------------\n",
        "def get_transform(\n",
        "    img_size=IMG_SIZE,\n",
        "    is_train: bool = True,\n",
        "    crop_size=IMG_SIZE,\n",
        "    h_flip: bool = True,\n",
        "    augment: bool = False\n",
        ") -> transforms.Compose:\n",
        "    \"\"\"\n",
        "    Returns a torchvision transform chain.\n",
        "    \"\"\"\n",
        "    if is_train:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.RandomResizedCrop(crop_size) if augment else transforms.CenterCrop(crop_size),\n",
        "            transforms.RandomHorizontalFlip() if h_flip else transforms.Lambda(lambda x: x),\n",
        "            transforms.ToTensor(),  # This will convert to float and normalize [0,1]\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "    else:  # eval / test\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.CenterCrop(crop_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    return transform\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Helper: build loaders from lazy datasets\n",
        "# ------------------------------------------------------------------\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import ResNet101_Weights\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import gc\n",
        "\n",
        "def build_loaders(\n",
        "    file_paths, labels,\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "    val_split:  float = VAL_SPLIT,\n",
        "    num_workers: int = 0,\n",
        "    pin_memory: bool = False\n",
        "):\n",
        "    \"\"\"Create train/validation DataLoaders from lazy dataset.\"\"\"\n",
        "    transform_train = get_transform(is_train=True)\n",
        "    transform_val   = get_transform(is_train=False)\n",
        "\n",
        "    full_dataset = DicomImageDataset(file_paths, labels, transform=transform_train)\n",
        "\n",
        "    train_size = int(len(full_dataset) * (1.0 - val_split))\n",
        "    val_size   = len(full_dataset) - train_size\n",
        "    train_ds, val_ds = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    # Override validation transform\n",
        "    val_ds.dataset.transform = transform_val\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# ------------------------------------------\n",
        "# Training loop\n",
        "# ------------------------------------------\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    model_name: str | None = None,          # NEW\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Train a single epoch.  The new ``model_name`` argument is *only* for\n",
        "    bookkeeping – it is stored on the model as ``model.name``.\n",
        "    \"\"\"\n",
        "    if model_name is not None:\n",
        "        # Store the name for later inspection\n",
        "        model.name = model_name\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for imgs, targets in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    return epoch_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# Measure validation loss during training\n",
        "# --------------------------------------------\n",
        "def validate(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> tuple[float, float]:\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in tqdm(loader, desc=\"Validation\", leave=False):\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            epoch_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == targets).sum().item()\n",
        "\n",
        "    val_loss = epoch_loss / len(loader.dataset)\n",
        "    val_acc = correct / len(loader.dataset)\n",
        "    return val_loss, val_acc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from dropblock import DropBlock2D  # pip install dropblock\n",
        "\n",
        "def get_resnet101(\n",
        "    num_classes: int,\n",
        "    pretrained: bool = True,\n",
        "    device: torch.device | None = None,\n",
        "    name: str | None = None,\n",
        "    dropout_p: float = 0.5,\n",
        "    dropblock_size: int | None = 5,\n",
        "    dropblock_prob: float | None = 0.1,\n",
        "    add_norm: bool = False,\n",
        "    **kwargs,\n",
        ") -> nn.Module:\n",
        "    \"\"\"\n",
        "    Return a ResNet‑101 backbone followed by optional regularisers\n",
        "    and a final linear head.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_classes : int\n",
        "        Number of output classes.\n",
        "    pretrained : bool, default=True\n",
        "        Load ImageNet pre‑trained weights.\n",
        "    device : torch.device | None, default=None\n",
        "        Device to move the model onto.\n",
        "    name : str | None, default=None\n",
        "        Optional human‑friendly name (`model.name = name`).\n",
        "    dropout_p : float, default=0.5\n",
        "        Drop‑out probability *after* the backbone.\n",
        "    dropblock_size : int | None, default=5\n",
        "        Size of the spatial block to drop (used only if dropblock_prob > 0).\n",
        "    dropblock_prob : float | None, default=0.1\n",
        "        Probability that a block is dropped.\n",
        "    add_norm : bool, default=False\n",
        "        Add a 1‑D BatchNorm layer *after* the backbone.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    nn.Module\n",
        "        Sequential model: `backbone → (optional layers) → Linear`\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load the pre‑trained ResNet‑101\n",
        "    backbone = models.resnet101(\n",
        "        weights=models.ResNet101_Weights.DEFAULT if pretrained else None\n",
        "    )\n",
        "\n",
        "    # Insert DropBlock *before* the global avg‑pool\n",
        "    if dropblock_size is not None and dropblock_prob is not None:\n",
        "        # The final residual block produces a 4‑D tensor (b, 2048, 7, 7)\n",
        "        backbone.layer4 = nn.Sequential(\n",
        "            backbone.layer4,\n",
        "            DropBlock2D(block_size=dropblock_size, drop_prob=dropblock_prob)\n",
        "        )\n",
        "\n",
        "    # Make the backbone output a 1‑D vector (2048‑dim)\n",
        "    in_features = backbone.fc.in_features\n",
        "    backbone.fc = nn.Identity()\n",
        "\n",
        "    # Build the tail\n",
        "    layers: list[nn.Module] = [backbone]\n",
        "\n",
        "    if add_norm:\n",
        "        layers.append(nn.BatchNorm1d(in_features))\n",
        "\n",
        "    if dropout_p > 0.0:\n",
        "        layers.append(nn.Dropout(p=dropout_p))\n",
        "\n",
        "    # Final classification head\n",
        "    layers.append(nn.Linear(in_features, num_classes))\n",
        "\n",
        "    # Stack everything\n",
        "    model = nn.Sequential(*layers).to(device)\n",
        "\n",
        "    if name is not None:\n",
        "        model.name = name\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# Training routine\n",
        "# ------------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import copy, gc\n",
        "\n",
        "# 1. Declare a global variable at module level\n",
        "global_best_epoch = None\n",
        "\n",
        "def run_training_lazy(\n",
        "    file_paths: list[str],\n",
        "    labels:     list[int],\n",
        "    num_epochs: int   = NUM_EPOCHS,\n",
        "    batch_size: int   = BATCH_SIZE,\n",
        "    lr:          float = LEARNING_RATE,\n",
        "    weight_decay:float = WEIGHT_DECAY,\n",
        "    patience:    int   = PATIENCE,\n",
        "    val_split:   float = VAL_SPLIT,\n",
        "    early_stop:  bool  = EARLY_STOPPING,\n",
        "    device:      torch.device = None\n",
        "):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # ----- Loaders\n",
        "    train_loader, val_loader = build_loaders(\n",
        "        file_paths, labels,\n",
        "        batch_size=batch_size,\n",
        "        val_split=val_split,\n",
        "        num_workers=0,\n",
        "        pin_memory=False\n",
        "    )\n",
        "\n",
        "    # ----- Model\n",
        "    num_classes = max(labels) + 1\n",
        "    # model = get_resnet101(num_classes=num_classes, pretrained=True, device=device)\n",
        "    model = get_resnet101(\n",
        "    num_classes=10,\n",
        "    pretrained=True,\n",
        "    dropout_p=0.5,\n",
        "    dropblock_size=5,\n",
        "    dropblock_prob=0.1,\n",
        "    add_norm=True,\n",
        "    name=\"resnet101_reg\"\n",
        ")\n",
        "\n",
        "    # ----- Loss/optimiser/scheduler\n",
        "    #criterion = nn.CrossEntropyLoss()\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    #optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=num_epochs, eta_min=lr * 0.01\n",
        "    )\n",
        "\n",
        "    # ----- Early‑stopping bookkeeping\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_epoch = 0\n",
        "    best_train_acc = best_val_acc = None\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    epochs_without_improve = 0\n",
        "\n",
        "    # ----- History\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs,   val_accs   = [], []\n",
        "\n",
        "    # ----- Epoch loop\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        # training\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        n_train = len(train_loader.dataset)\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item() * xb.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct_train += (preds == yb).sum().item()\n",
        "\n",
        "        epoch_train_loss /= n_train\n",
        "        epoch_train_acc  = correct_train / n_train\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        n_val = len(val_loader.dataset)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "\n",
        "                epoch_val_loss += loss.item() * xb.size(0)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                correct_val += (preds == yb).sum().item()\n",
        "\n",
        "        epoch_val_loss /= n_val\n",
        "        epoch_val_acc  = correct_val / n_val\n",
        "\n",
        "        # scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        # early stopping\n",
        "        if early_stop:\n",
        "            if epoch_val_loss < best_val_loss - 1e-5:\n",
        "                best_val_loss = epoch_val_loss\n",
        "                best_epoch = epoch\n",
        "                best_train_acc = epoch_train_acc\n",
        "                best_val_acc   = epoch_val_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                epochs_without_improve = 0\n",
        "            else:\n",
        "                epochs_without_improve += 1\n",
        "\n",
        "            if epochs_without_improve >= patience:\n",
        "                print(\"\\nEarly stopping triggered.\")\n",
        "                print(f\"Best epoch (before stopping): {best_epoch}\")\n",
        "                print(f\"  Train Acc: {best_train_acc:.4f} | Train Loss: {epoch_train_loss:.4f}\")\n",
        "                print(f\"  Valid Acc: {best_val_acc:.4f} | Valid Loss: {epoch_val_loss:.4f}\\n\")\n",
        "                model.load_state_dict(best_model_wts)\n",
        "                break\n",
        "\n",
        "        # log\n",
        "        print(\n",
        "            f\"Epoch {epoch}/{num_epochs} | \"\n",
        "            f\"Train Acc:  {epoch_train_acc:.4f} | Train Loss: {epoch_train_loss:.4f} | \"\n",
        "            f\"Val Acc:    {epoch_val_acc:.4f} | Val Loss: {epoch_val_loss:.4f}\"\n",
        "        )\n",
        "\n",
        "        # history\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "\n",
        "    # final report\n",
        "    if not early_stop or epochs_without_improve < patience:\n",
        "        best_epoch = epoch\n",
        "        best_train_acc = epoch_train_acc\n",
        "        best_val_acc   = epoch_val_acc\n",
        "        best_val_loss  = epoch_val_loss\n",
        "\n",
        "    print(\"\\nTraining finished.\")\n",
        "    print(f\"Best epoch: {best_epoch}\")\n",
        "    print(f\"  Train Acc: {best_train_acc:.4f} | Train Loss: {epoch_train_loss:.4f}\")\n",
        "    print(f\"  Val   Acc: {best_val_acc:.4f}   | Val   Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Store best_epoch globally\n",
        "    global global_best_epoch\n",
        "    global_best_epoch = best_epoch\n",
        "\n",
        "    return {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"train_acc\":   train_accs,\n",
        "        \"val_loss\":    val_losses,\n",
        "        \"val_acc\":     val_accs,\n",
        "        \"best_epoch\":  best_epoch,\n",
        "        \"best_train_acc\": best_train_acc,\n",
        "        \"best_val_acc\":   best_val_acc,\n",
        "        \"best_train_loss\": epoch_train_loss,\n",
        "        \"best_val_loss\":   epoch_val_loss,\n",
        "    }"
      ],
      "metadata": {
        "id": "YTXmganaVo-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Device"
      ],
      "metadata": {
        "id": "JwqwLG3LVRJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLjEp3lwV1cD",
        "outputId": "6824f7db-960f-4fed-add7-12e6d6e2b837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get File Paths to Data"
      ],
      "metadata": {
        "id": "or9GVTHpVQ0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get file paths to data\n",
        "file_paths_list = img_df[\"file_path\"].tolist()\n",
        "labels_list     = img_df[\"Target\"].astype(int).tolist()\n",
        "\n",
        "print(file_paths_list[:1])   # e.g. ['path/to/patient001.dcm', 'path/to/patient002.dcm', ...]\n",
        "print(labels_list[:1])       # e.g. [0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsIWXL55WAZ6",
        "outputId": "9ea1f59d-5e4c-46d4-91ff-5dab0a3c4931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/pna_data/pna_train_images/00313ee0-9eaa-42f4-b0ab-c148ed3241cd.dcm']\n",
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Training"
      ],
      "metadata": {
        "id": "qXBP0g0FV7RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Training\n",
        "\n",
        "import time\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Start training\n",
        "# ------------------------------------------------------------------------\n",
        "print(f\"-- Training (classification) is starting for {NUM_EPOCHS} epochs----------------------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Run training\n",
        "history = run_training_lazy(\n",
        "    file_paths=file_paths_list,\n",
        "    labels=labels_list,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    patience=PATIENCE,\n",
        "    val_split=VAL_SPLIT,\n",
        "    early_stop=EARLY_STOPPING\n",
        ")\n",
        "\n",
        "# Record end time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Print elapsed time\n",
        "print(f\"Elapsed time: {hms_string(elapsed_time)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHm9uEmCWOxR",
        "outputId": "43cdf324-9b56-4bef-904d-3c2d5a80b158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Training (classification) is starting for 50 epochs----------------------------\n",
            "Epoch 1/50 | Train Acc:  0.7668 | Train Loss: 1.3391 | Val Acc:    0.7794 | Val Loss: 2.3417\n",
            "Epoch 2/50 | Train Acc:  0.8543 | Train Loss: 0.9376 | Val Acc:    0.8453 | Val Loss: 1.0855\n",
            "Epoch 3/50 | Train Acc:  0.8609 | Train Loss: 0.9110 | Val Acc:    0.8592 | Val Loss: 0.8931\n",
            "Epoch 4/50 | Train Acc:  0.9189 | Train Loss: 0.7552 | Val Acc:    0.8769 | Val Loss: 0.9417\n",
            "Epoch 5/50 | Train Acc:  0.9691 | Train Loss: 0.6313 | Val Acc:    0.9020 | Val Loss: 0.8228\n",
            "Epoch 6/50 | Train Acc:  0.9926 | Train Loss: 0.5783 | Val Acc:    0.9074 | Val Loss: 0.7736\n",
            "Epoch 7/50 | Train Acc:  0.9969 | Train Loss: 0.5559 | Val Acc:    0.9095 | Val Loss: 0.7601\n",
            "Epoch 8/50 | Train Acc:  0.9981 | Train Loss: 0.5493 | Val Acc:    0.9122 | Val Loss: 0.7566\n",
            "Epoch 9/50 | Train Acc:  0.9991 | Train Loss: 0.5388 | Val Acc:    0.9031 | Val Loss: 0.7540\n",
            "Epoch 10/50 | Train Acc:  0.9989 | Train Loss: 0.5324 | Val Acc:    0.9127 | Val Loss: 0.7375\n",
            "Epoch 11/50 | Train Acc:  0.9987 | Train Loss: 0.5316 | Val Acc:    0.9117 | Val Loss: 0.7402\n",
            "Epoch 12/50 | Train Acc:  0.9991 | Train Loss: 0.5290 | Val Acc:    0.9063 | Val Loss: 0.7390\n",
            "Epoch 13/50 | Train Acc:  0.9991 | Train Loss: 0.5271 | Val Acc:    0.9117 | Val Loss: 0.7299\n",
            "Epoch 14/50 | Train Acc:  0.9988 | Train Loss: 0.5246 | Val Acc:    0.9111 | Val Loss: 0.7346\n",
            "Epoch 15/50 | Train Acc:  0.9993 | Train Loss: 0.5230 | Val Acc:    0.9031 | Val Loss: 0.7391\n",
            "Epoch 16/50 | Train Acc:  0.9991 | Train Loss: 0.5240 | Val Acc:    0.8972 | Val Loss: 0.7450\n",
            "Epoch 17/50 | Train Acc:  0.9992 | Train Loss: 0.5205 | Val Acc:    0.9069 | Val Loss: 0.7408\n",
            "\n",
            "Early stopping triggered.\n",
            "Best epoch (before stopping): 13\n",
            "  Train Acc: 0.9991 | Train Loss: 0.5203\n",
            "  Valid Acc: 0.9117 | Valid Loss: 0.7315\n",
            "\n",
            "\n",
            "Training finished.\n",
            "Best epoch: 13\n",
            "  Train Acc: 0.9991 | Train Loss: 0.5203\n",
            "  Val   Acc: 0.9117   | Val   Loss: 0.7315\n",
            "Elapsed time: 1:07:12.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DuzvNhMK2OMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_epoch = run_training_lazy(...).get(\"best_epoch\")\n",
        "print(best_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "kvwWEPb92Omh",
        "outputId": "3a4a307c-9405-4627-9f51-3a435ccf51b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "run_training_lazy() missing 1 required positional argument: 'labels'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3170072883.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: run_training_lazy() missing 1 required positional argument: 'labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Train History"
      ],
      "metadata": {
        "id": "9tQMWwuPV7Kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_train_val(history, NUM_EPOCHS, BATCH_SIZE, title=\"Training and Validation\")\n",
        "plot_train_val(history=history,\n",
        "               num_epochs=NUM_EPOCHS,\n",
        "               batch_size=BATCH_SIZE,\n",
        "               #best_epoch=10,\n",
        "               title=\"Model Performance\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "nmVOTKaBWyZO",
        "outputId": "d8c549d1-e1c4-43ac-99dd-31b603d08f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (50,) and (17,)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1094734307.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plot_train_val(history, NUM_EPOCHS, BATCH_SIZE, title=\"Training and Validation\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m plot_train_val(history=history,\n\u001b[0m\u001b[1;32m      3\u001b[0m                \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                \u001b[0;31m#best_epoch=10,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1656020272.py\u001b[0m in \u001b[0;36mplot_train_val\u001b[0;34m(history, num_epochs, batch_size, title)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# ---------- Left panel – accuracy --------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0max_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0max_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val acc\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0max_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \"\"\"\n\u001b[1;32m   1776\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1777\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    495\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (50,) and (17,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "idPYp50NV7Di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dMPEzhHMV66i"
      }
    }
  ]
}