{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whjsJasuhstV"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_02_2_multi_prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euOZxlIMhstX"
      },
      "source": [
        "# T81-559: Applications of Generative Artificial Intelligence\n",
        "**Module 2: Code Generation**\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Yov72PhstY",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Module 2 Material\n",
        "\n",
        "* Part 2.1: Prompting for Code Generation [[Video]](https://www.youtube.com/watch?v=HVId6kYKKgQ) [[Notebook]](t81_559_class_02_1_dev.ipynb)\n",
        "* **Part 2.2: Handling Revision Prompts** [[Video]](https://www.youtube.com/watch?v=APpV46tplXA) [[Notebook]](t81_559_class_02_2_multi_prompt.ipynb)\n",
        "* Part 2.3: Using a LLM to Help Debug [[Video]](https://www.youtube.com/watch?v=VPqSNb38QK0) [[Notebook]](t81_559_class_02_3_llm_debug.ipynb)\n",
        "* Part 2.4: Tracking Prompts in Software Development [[Video]](https://www.youtube.com/watch?v=oUFUuYfvXZU) [[Notebook]](t81_559_class_02_4_software_eng.ipynb)\n",
        "* Part 2.5: Limits of LLM Code Generation [[Video]](https://www.youtube.com/watch?v=dKtRI0LZSyY) [[Notebook]](t81_559_class_02_5_code_gen_limits.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcAUP0c3hstY"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running and maps Google Drive if needed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set COLAB to True\n",
        "COLAB = True\n",
        "print(\"Note: using Google CoLab\")\n",
        "\n",
        "# OpenAI Secrets\n",
        "if COLAB:\n",
        "    # Path to the file containing the API key\n",
        "    api_key_path = '/content/drive/My Drive/openai_api_key.txt'\n",
        "\n",
        "    # Read the API key from the file\n",
        "    with open(api_key_path, 'r') as file:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = file.read().strip()\n",
        "\n",
        "# Print the API key to verify (optional, for debugging purposes)\n",
        "print(os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# Install needed libraries in CoLab\n",
        "if COLAB:\n",
        "    !pip install langchain langchain_openai"
      ],
      "metadata": {
        "id": "NJdcgQcKVr7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsI496h5hstZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "# OpenAI Secrets\n",
        "if COLAB:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Install needed libraries in CoLab\n",
        "if COLAB:\n",
        "    !pip install langchain langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC9A-LaYhsta"
      },
      "source": [
        "# 2.2: Handling Revision Prompts\n",
        "\n",
        "Previously, we just sent one prompt to the LLM, which generated code. It is possible to perform this code more conversationally. In this module, we will see how to converse with the LLM to request changes to outputted code and even help the LLM to produce a more accurate model.\n",
        "\n",
        "We will also see that it might be beneficial to recreate your conversation as one single prompt that generates the final result. Keeping track of one prompt, rather than a conversation, that created your final code is more maintainable.\n",
        "\n",
        "## Conversational Code Generation\n",
        "\n",
        "We will introduce a more advanced code generation function that allows you to start the conversation to generate code and follow up with additional prompts if needed.\n",
        "\n",
        "In future modules, we will see how to create chatbots similar to this one. We will use the code I provided to generate your code for now. This generator uses a system prompt that requests that the generated code conform to the following:\n",
        "\n",
        "* Imports should be sorted\n",
        "* Code should conform to PEP-8 formatting\n",
        "* Do not mix uncompilable notes with code\n",
        "* Add comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TMF-rtxgRAea"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts.chat import PromptTemplate\n",
        "from IPython.display import display_markdown\n",
        "\n",
        "MODEL = 'gpt-4o-mini'\n",
        "TEMPLATE = \"\"\"The following is a friendly conversation between a human and an\n",
        "AI to generate Python code. If you have notes about the code, place them before\n",
        "the code. Any nots about execution should follow the code. If you do mix any\n",
        "notes with the code, make them comments. Add proper comments to the code.\n",
        "Sort imports and follow PEP-8 formatting.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "Code Assistant:\"\"\"\n",
        "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"history\", \"input\"], template=TEMPLATE)\n",
        "\n",
        "def start_conversation():\n",
        "    # Initialize the OpenAI LLM with your API key\n",
        "    llm = ChatOpenAI(\n",
        "        model=MODEL,\n",
        "        temperature=0.0,\n",
        "        n=1\n",
        "    )\n",
        "\n",
        "    # Initialize memory and conversation\n",
        "    memory = ConversationBufferWindowMemory()\n",
        "    conversation = ConversationChain(\n",
        "        prompt=PROMPT_TEMPLATE,\n",
        "        llm=llm,\n",
        "        memory=memory,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    return conversation\n",
        "\n",
        "def generate_code(conversation, prompt):\n",
        "    print(\"Model response:\")\n",
        "    output = conversation.invoke(prompt)\n",
        "    display_markdown(output['response'], raw=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClPhLkGldhPt"
      },
      "source": [
        "## First Attempt at an XOR Approximator\n",
        "\n",
        "We will construct a prompt that requests the LLM to generate a PyTorch neural network to approximate the [Exclusive Or](https://en.wikipedia.org/wiki/Exclusive_or). The truth table for the Exclusive Or (XOR) function is provided here:\n",
        "\n",
        "```\n",
        "0 XOR 0 = 0\n",
        "1 XOR 0 = 1\n",
        "0 XOR 1 = 1\n",
        "1 XOR 1 = 0\n",
        "```\n",
        "\n",
        "If given data, neural networks can learn to approximate functions, so let's create a PyTorch neural network to approximate the XOR function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ydaqwgRiH4D6",
        "outputId": "70d98251-13d5-4f89-b8cc-0628f3db387f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-860a868a87f1>:29: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory()\n",
            "<ipython-input-2-860a868a87f1>:30: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Sure! Below is a Python code snippet that demonstrates how to learn the XOR function using PyTorch. The code includes comments to explain each part of the process.\n\n```python\n# Import necessary libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the XOR dataset\n# Input: 2 features (x1, x2), Output: 1 target (y)\ndata = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\nlabels = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n\n# Define a simple feedforward neural network\nclass XORModel(nn.Module):\n    def __init__(self):\n        super(XORModel, self).__init__()\n        # Input layer to hidden layer\n        self.hidden = nn.Linear(2, 2)\n        # Hidden layer to output layer\n        self.output = nn.Linear(2, 1)\n        # Activation function\n        self.activation = nn.Sigmoid()\n\n    def forward(self, x):\n        # Pass input through the hidden layer\n        hidden_output = self.activation(self.hidden(x))\n        # Pass hidden layer output to the output layer\n        output = self.output(hidden_output)\n        return self.activation(output)\n\n# Initialize the model, loss function, and optimizer\nmodel = XORModel()\ncriterion = nn.BCELoss()  # Binary Cross Entropy Loss\noptimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent\n\n# Training the model\nnum_epochs = 10000  # Number of epochs for training\nfor epoch in range(num_epochs):\n    # Zero the gradients\n    optimizer.zero_grad()\n    # Forward pass: compute predicted outputs\n    outputs = model(data)\n    # Compute the loss\n    loss = criterion(outputs, labels)\n    # Backward pass: compute gradients\n    loss.backward()\n    # Update weights\n    optimizer.step()\n\n    # Print loss every 1000 epochs\n    if (epoch + 1) % 1000 == 0:\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Test the model\nwith torch.no_grad():\n    test_data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n    predictions = model(test_data)\n    print(\"Predictions:\")\n    print(predictions.round())  # Round the predictions to get binary output\n```\n\n### Notes on Execution:\n- Ensure you have PyTorch installed in your Python environment. You can install it using pip:\n  ```bash\n  pip install torch\n  ```\n- The model is trained for 10,000 epochs, which should be sufficient for it to learn the XOR function. You can adjust the number of epochs or the learning rate if needed.\n- The output predictions are rounded to get binary values (0 or 1) for the XOR function."
          },
          "metadata": {}
        }
      ],
      "source": [
        "conversation = start_conversation()\n",
        "generate_code(conversation, \"\"\"Write Python code to learn the XOR function with PyTorch.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPqM7BslmM7"
      },
      "source": [
        "# Requesting a Change to Generated Code\n",
        "\n",
        "If you've taken my other course, you will know I prefer PyTorch sequences over extending the nn.Module class, at least for simple neural networks like an XOR approximator. LLMs do not share this opinion. However, the LLM will gladly humor me and generate a sequence. Here, I provide an additional prompt to request this rather than resubmitting a modified version of my first prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lqnDZhc4OVU6",
        "outputId": "674fdbd7-d1fc-4ee5-8c03-b28f63f49aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Certainly! Below is the modified Python code that uses a PyTorch `nn.Sequential` to define the model instead of creating a custom `nn.Module` class. The code includes comments to explain each part of the process.\n\n```python\n# Import necessary libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the XOR dataset\n# Input: 2 features (x1, x2), Output: 1 target (y)\ndata = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\nlabels = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n\n# Define a simple feedforward neural network using nn.Sequential\nmodel = nn.Sequential(\n    nn.Linear(2, 2),  # Input layer to hidden layer\n    nn.Sigmoid(),     # Activation function for hidden layer\n    nn.Linear(2, 1),  # Hidden layer to output layer\n    nn.Sigmoid()      # Activation function for output layer\n)\n\n# Initialize the loss function and optimizer\ncriterion = nn.BCELoss()  # Binary Cross Entropy Loss\noptimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent\n\n# Training the model\nnum_epochs = 10000  # Number of epochs for training\nfor epoch in range(num_epochs):\n    # Zero the gradients\n    optimizer.zero_grad()\n    # Forward pass: compute predicted outputs\n    outputs = model(data)\n    # Compute the loss\n    loss = criterion(outputs, labels)\n    # Backward pass: compute gradients\n    loss.backward()\n    # Update weights\n    optimizer.step()\n\n    # Print loss every 1000 epochs\n    if (epoch + 1) % 1000 == 0:\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Test the model\nwith torch.no_grad():\n    test_data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n    predictions = model(test_data)\n    print(\"Predictions:\")\n    print(predictions.round())  # Round the predictions to get binary output\n```\n\n### Notes on Execution:\n- Ensure you have PyTorch installed in your Python environment. You can install it using pip:\n  ```bash\n  pip install torch\n  ```\n- The model is trained for 10,000 epochs, which should be sufficient for it to learn the XOR function. You can adjust the number of epochs or the learning rate if needed.\n- The output predictions are rounded to get binary values (0 or 1) for the XOR function."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate_code(conversation, \"\"\"Could you make use of a PyTorch sequence rather than a nn.Module class?\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7U1Bx3Wbje1"
      },
      "source": [
        "# Testing the Generated Code\n",
        "\n",
        "LLMs are not overachievers; they will implement the code you ask for and not provide much more. When we run the XOR approximator's first version, the results are only sometimes accurate, especially if we run the program multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdJNKLVhRcD3",
        "outputId": "0459a6b5-2e80-4b9d-c3b7-7b1c68716ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 0.25302624702453613\n",
            "Epoch 1000 Loss: 0.24863247573375702\n",
            "Epoch 2000 Loss: 0.24629078805446625\n",
            "Epoch 3000 Loss: 0.2398288995027542\n",
            "Epoch 4000 Loss: 0.22486700117588043\n",
            "Epoch 5000 Loss: 0.20344685018062592\n",
            "Epoch 6000 Loss: 0.18549028038978577\n",
            "Epoch 7000 Loss: 0.1696184277534485\n",
            "Epoch 8000 Loss: 0.1411036103963852\n",
            "Epoch 9000 Loss: 0.08170422911643982\n",
            "Predicted values:\n",
            "tensor([[0.1506],\n",
            "        [0.8082],\n",
            "        [0.8080],\n",
            "        [0.2405]])\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the XOR network using a sequential container\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(2, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Initialize the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Training data for XOR\n",
        "data = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
        "labels = torch.tensor([[0.0], [1.0], [1.0], [0.0]])\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10000):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    pred = model(data)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(pred, labels)\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch} Loss: {loss.item()}')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    test_pred = model(data)\n",
        "    print(\"Predicted values:\")\n",
        "    print(test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY-NmeRAoPVr"
      },
      "source": [
        "If you receive an error or the output is not exactly what you like, it is effective to provide that output and any errors to the LLM. Here, we provide the output and ask the LLM if that seems correct. Sometimes, the LLM may insist that the output is correct, so you must \"debate\" the LLM, providing additional details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bzkCRwx_RxxI",
        "outputId": "7d23e45c-3421-4f69-e88c-ce557c0657fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "It seems that the model may not have fully learned the XOR function, which is a common issue when training neural networks, especially with simple architectures. The XOR function is not linearly separable, and sometimes a more complex model or different training parameters are needed to achieve better results.\n\nLet's ensure that the model is correctly set up and consider a few adjustments. Below is the revised code with a slightly more complex architecture (adding an additional hidden layer) to improve the learning capability of the model.\n\n```python\n# Import necessary libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the XOR dataset\n# Input: 2 features (x1, x2), Output: 1 target (y)\ndata = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\nlabels = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n\n# Define a simple feedforward neural network using nn.Sequential\nmodel = nn.Sequential(\n    nn.Linear(2, 4),  # Input layer to first hidden layer\n    nn.Sigmoid(),     # Activation function for first hidden layer\n    nn.Linear(4, 2),  # First hidden layer to second hidden layer\n    nn.Sigmoid(),     # Activation function for second hidden layer\n    nn.Linear(2, 1),  # Second hidden layer to output layer\n    nn.Sigmoid()      # Activation function for output layer\n)\n\n# Initialize the loss function and optimizer\ncriterion = nn.BCELoss()  # Binary Cross Entropy Loss\noptimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent\n\n# Training the model\nnum_epochs = 10000  # Number of epochs for training\nfor epoch in range(num_epochs):\n    # Zero the gradients\n    optimizer.zero_grad()\n    # Forward pass: compute predicted outputs\n    outputs = model(data)\n    # Compute the loss\n    loss = criterion(outputs, labels)\n    # Backward pass: compute gradients\n    loss.backward()\n    # Update weights\n    optimizer.step()\n\n    # Print loss every 1000 epochs\n    if (epoch + 1) % 1000 == 0:\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Test the model\nwith torch.no_grad():\n    test_data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n    predictions = model(test_data)\n    print(\"Predictions:\")\n    print(predictions.round())  # Round the predictions to get binary output\n```\n\n### Notes on Execution:\n- Ensure you have PyTorch installed in your Python environment. You can install it using pip:\n  ```bash\n  pip install torch\n  ```\n- The model architecture has been modified to include an additional hidden layer with 4 neurons, which should help the model learn the XOR function more effectively.\n- The model is still trained for 10,000 epochs, which should be sufficient. You can adjust the number of epochs or the learning rate if needed.\n- The output predictions are rounded to get binary values (0 or 1) for the XOR function. After running this code, you should see predictions closer to the expected output of the XOR function."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate_code(conversation, \"\"\"The output was:\n",
        "\n",
        "Predicted values:\n",
        "tensor([[0.4843],\n",
        "        [0.5800],\n",
        "        [0.4278],\n",
        "        [0.4623]])\n",
        "\n",
        "Are you sure that is correct?\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r2Qpe11cWwm"
      },
      "source": [
        "## Test the Improved Version\n",
        "\n",
        "We now receive much more accurate output when we test the neural network provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V19QyaROSUP1",
        "outputId": "6e93e40d-d75a-42c4-ecec-da1ccaad79c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 0.2580437660217285\n",
            "Epoch 1000 Loss: 5.0754984840750694e-05\n",
            "Epoch 2000 Loss: 1.4872541214572266e-05\n",
            "Epoch 3000 Loss: 6.542353730765171e-06\n",
            "Epoch 4000 Loss: 3.3518126656417735e-06\n",
            "Epoch 5000 Loss: 1.848135752879898e-06\n",
            "Epoch 6000 Loss: 1.0599346751405392e-06\n",
            "Epoch 7000 Loss: 6.216667429725931e-07\n",
            "Epoch 8000 Loss: 3.69443455383589e-07\n",
            "Epoch 9000 Loss: 2.212603931184276e-07\n",
            "Epoch 10000 Loss: 1.331342218691134e-07\n",
            "Epoch 11000 Loss: 8.035284082552607e-08\n",
            "Epoch 12000 Loss: 4.859315794192298e-08\n",
            "Epoch 13000 Loss: 2.94252586741095e-08\n",
            "Epoch 14000 Loss: 1.784731296083919e-08\n",
            "Epoch 15000 Loss: 1.083044054439597e-08\n",
            "Epoch 16000 Loss: 6.579485400948215e-09\n",
            "Epoch 17000 Loss: 4.005686449204404e-09\n",
            "Epoch 18000 Loss: 2.4488615579798534e-09\n",
            "Epoch 19000 Loss: 1.5028531752392382e-09\n",
            "Predicted values:\n",
            "tensor([[3.1293e-05],\n",
            "        [9.9997e-01],\n",
            "        [9.9997e-01],\n",
            "        [3.6038e-05]])\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the XOR network using a sequential container\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 4),  # Increased the number of neurons in the hidden layer\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(4, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Initialize the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.1)  # Changed to Adam optimizer\n",
        "\n",
        "# Training data for XOR\n",
        "data = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
        "labels = torch.tensor([[0.0], [1.0], [1.0], [0.0]])\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(20000):  # Increased the number of epochs\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    pred = model(data)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(pred, labels)\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch} Loss: {loss.item()}')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    test_pred = model(data)\n",
        "    print(\"Predicted values:\")\n",
        "    print(test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCAmOVHEpAjf"
      },
      "source": [
        "## Combining the Conversation into a Single Prompt\n",
        "\n",
        "We should combine this entire conversation into a single prompt, especially if we wish to save the prompt along with the code. We can request the LLM to create this combined prompt for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "MKMcmXTNSiFw",
        "outputId": "cccb0e97-bee1-4140-ef94-a6c4718d64b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Certainly! A single prompt that would have led to the last code output could be:\n\n\"Write Python code using PyTorch to learn the XOR function with a feedforward neural network that includes two hidden layers. Ensure to use `nn.Sequential` for model definition, and include comments to explain each part of the code. The model should be trained for 10,000 epochs, and the output predictions should be rounded to get binary values (0 or 1).\"\n\nThis prompt clearly specifies the requirements for the model architecture, the use of `nn.Sequential`, the training duration, and the need for comments, which would guide the generation of the desired code."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate_code(conversation, \"\"\"Okay, that is great, can you suggest a single\n",
        "prompt that would have resulted in this last code output?\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw-QDu7Rpo_T"
      },
      "source": [
        "The LLM's attempt at a consoldated prompt is incomplete. It skips several important details and does not provide precise requirements. I will manually make some improvements, which you can see here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1TzyiT6aS7ej",
        "outputId": "a1410e90-e1c7-4083-ec45-1d0508700902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Sure! Below is the Python code that uses PyTorch to learn the XOR function with 4 hidden neurons, the Adam optimizer, and 20,000 training epochs. The code is structured as a sequence rather than using a `nn.Module` class.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Set the random seed for reproducibility\ntorch.manual_seed(0)\n\n# Define the XOR input and output\nX = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\ny = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n\n# Create a sequential model\nmodel = nn.Sequential(\n    nn.Linear(2, 4),  # Input layer to hidden layer with 4 neurons\n    nn.ReLU(),        # Activation function\n    nn.Linear(4, 1),  # Hidden layer to output layer\n    nn.Sigmoid()      # Activation function for output\n)\n\n# Define the loss function and optimizer\ncriterion = nn.BCELoss()  # Binary Cross Entropy Loss\noptimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer\n\n# Training loop\nnum_epochs = 20000\nfor epoch in range(num_epochs):\n    # Forward pass: compute predicted y by passing x to the model\n    y_pred = model(X)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    \n    # Zero gradients, backward pass, and optimize\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Backpropagation\n    optimizer.step()       # Update weights\n\n    # Print loss every 2000 epochs\n    if (epoch + 1) % 2000 == 0:\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Test the model\nwith torch.no_grad():\n    test_output = model(X)\n    predicted = (test_output > 0.5).float()  # Convert probabilities to binary output\n    print(\"Predicted outputs:\")\n    print(predicted)\n```\n\n### Notes on Execution:\n- Ensure you have PyTorch installed in your Python environment. You can install it using pip if you haven't done so already: `pip install torch`.\n- The model is trained for 20,000 epochs, which may take some time depending on your hardware.\n- The output will show the loss at every 2000 epochs, and finally, it will print the predicted outputs for the XOR function.\n- The predicted outputs will be in binary format (0 or 1) based on the learned model."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Start a new conversation\n",
        "conversation = start_conversation()\n",
        "generate_code(conversation, \"\"\"\n",
        "Can you provide Python code using PyTorch to effectively learn the XOR function\n",
        "with 4 hidden neurons, using the Adam optimizer, and 20K training epochs?\n",
        "Use a sequence not a nn.Module class.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EiMd01ATosU"
      },
      "source": [
        "## Test the Final Prompt\n",
        "\n",
        "Now, we test the final prompt. My prompt produces an acceptable result, but there are some opportunities for improvement. You can specify the exact format for the output. For example, sometimes code is generated to round the results, but other times it is not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx0_Al1kTmh7",
        "outputId": "cd7064d3-f2fb-4088-cb0d-e8ae6e132b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1000/20000], Loss: 0.0019\n",
            "Epoch [2000/20000], Loss: 0.0004\n",
            "Epoch [3000/20000], Loss: 0.0002\n",
            "Epoch [4000/20000], Loss: 0.0001\n",
            "Epoch [5000/20000], Loss: 0.0000\n",
            "Epoch [6000/20000], Loss: 0.0000\n",
            "Epoch [7000/20000], Loss: 0.0000\n",
            "Epoch [8000/20000], Loss: 0.0000\n",
            "Epoch [9000/20000], Loss: 0.0000\n",
            "Epoch [10000/20000], Loss: 0.0000\n",
            "Epoch [11000/20000], Loss: 0.0000\n",
            "Epoch [12000/20000], Loss: 0.0000\n",
            "Epoch [13000/20000], Loss: 0.0000\n",
            "Epoch [14000/20000], Loss: 0.0000\n",
            "Epoch [15000/20000], Loss: 0.0000\n",
            "Epoch [16000/20000], Loss: 0.0000\n",
            "Epoch [17000/20000], Loss: 0.0000\n",
            "Epoch [18000/20000], Loss: 0.0000\n",
            "Epoch [19000/20000], Loss: 0.0000\n",
            "Epoch [20000/20000], Loss: 0.0000\n",
            "Predicted tensor: tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "Actual tensor: tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the XOR inputs and outputs\n",
        "inputs = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float)\n",
        "targets = torch.tensor([[0], [1], [1], [0]], dtype=torch.float)\n",
        "\n",
        "# Define the model using a sequential container\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 4),  # Input layer to hidden layer with 4 neurons\n",
        "    nn.ReLU(),        # ReLU activation function\n",
        "    nn.Linear(4, 1),  # Hidden layer to output layer\n",
        "    nn.Sigmoid()      # Sigmoid activation function for binary output\n",
        ")\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer with learning rate of 0.01\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(20000):  # 20,000 training epochs\n",
        "    optimizer.zero_grad()   # Clear gradients for each training step\n",
        "    outputs = model(inputs)  # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "    loss = criterion(outputs, targets)  # Compute loss\n",
        "    loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "    optimizer.step()  # Perform a single optimization step (parameter update)\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/20000], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing the model\n",
        "with torch.no_grad():  # Context-manager that disabled gradient calculation\n",
        "    predicted = model(inputs).round()  # Forward pass and rounding off to get predictions\n",
        "    print(f'Predicted tensor: {predicted}')\n",
        "    print(f'Actual tensor: {targets}')"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11 (genai)",
      "language": "python",
      "name": "genai"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}