{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSenseman/BIO1173/blob/main/Class_05_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqcED_3hRZSX"
      },
      "source": [
        "---------------------------\n",
        "**COPYRIGHT NOTICE:** This Jupyterlab Notebook is a Derivative work of [Jeff Heaton](https://github.com/jeffheaton) licensed under the Apache License, Version 2.0 (the \"License\"); You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SpTP1ToRZSX"
      },
      "source": [
        "# **BIO 1173: Intro Computational Biology**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAnUxa1ARZSX"
      },
      "source": [
        "**Module 5: Regularization and Dropout**\n",
        "\n",
        "* Instructor: [David Senseman](mailto:David.Senseman@utsa.edu), [Department of Biology, Health and the Environment](https://sciences.utsa.edu/bhe/), [UTSA](https://www.utsa.edu/)\n",
        "\n",
        "### Module 5 Material\n",
        "\n",
        "* Part 5.1: Part 5.1: Introduction to Regularization: Ridge and Lasso\n",
        "* Part 5.2: Using K-Fold Cross Validation with Keras\n",
        "* **Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting**\n",
        "* Part 5.4: Drop Out for Keras to Decrease Overfitting\n",
        "* Part 5.5: Benchmarking Keras Deep Learning Regularization Techniques\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEFQv3EDRZSY"
      },
      "source": [
        "## Google CoLab Instructions\n",
        "\n",
        "You MUST run the following code cell to get credit for this class lesson. By running this code cell, you will map your GDrive to /content/drive and print out your Google GMAIL address. Your Instructor will use your GMAIL address to verify the author of this class lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXFCYH4LDUM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# You must run this cell first\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    COLAB = True\n",
        "    print(\"Note: Using Google CoLab\")\n",
        "    import requests\n",
        "    gcloud_token = !gcloud auth print-access-token\n",
        "    gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "    print(gcloud_tokeninfo['email'])\n",
        "except:\n",
        "    print(\"**WARNING**: Your GMAIL address was **not** printed in the output below.\")\n",
        "    print(\"**WARNING**: You will NOT receive credit for this lesson.\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seYoGoRPRZSZ"
      },
      "source": [
        "## **Datasets for Class_05_3**\n",
        "\n",
        "For this lesson we will be using the [Obesity Prediction Dataset](https://www.kaggle.com/datasets/mrsimple07/obesity-prediction) for the Examples and the [Body Performance Dataset](https://www.kaggle.com/datasets/kukuroo3/body-performance-data) for the **Exercises**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLqDbjOMRZSZ"
      },
      "source": [
        "### **Obesity Prediction Dataset**\n",
        "\n",
        "The **_Obesity Prediction dataset_** provides comprehensive information on individuals' demographic characteristics, physical attributes, and lifestyle habits, aiming to facilitate the analysis and prediction of obesity prevalence.\n",
        "\n",
        "The 7 categories of obesity/demographics measurements are:\n",
        "\n",
        "* **Age:** The age of the individual, expressed in years (Mean=49.9 yrs +/- 18.1)\n",
        "* **Gender:** The gender of the individual coded `Male` and `Female`\n",
        "* **Height:** The height of the individual measured in centimeters (Mean=170 cm +/- 10.3)\n",
        "* **Weight:** The weight of the individual measured in kilograms (Mean=71.2 kg +/- 15.5)\n",
        "* **BMI:** Body mass index, a calculated metric derived from the individual's weight and height (Mean=24.9 +/- 6.19)\n",
        "* **PhysicalActivityLevel:** This variable quantifies the individual's level of physical activity (Mean=2.53 +/- 1.12)\n",
        "* **ObesityCategory:** Categorization of individuals based on their BMI into different obesity categories\n",
        "\n",
        "The output from `opDF.info` is:\n",
        "~~~text\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 1000 entries, 0 to 999\n",
        "Data columns (total 7 columns):\n",
        " #   Column                 Non-Null Count  Dtype  \n",
        "---  ------                 --------------  -----  \n",
        " 0   Age                    1000 non-null   int64  \n",
        " 1   Gender                 1000 non-null   object\n",
        " 2   Height                 1000 non-null   float64\n",
        " 3   Weight                 1000 non-null   float64\n",
        " 4   BMI                    1000 non-null   float64\n",
        " 5   PhysicalActivityLevel  1000 non-null   int64  \n",
        " 6   ObesityCategory        1000 non-null   object\n",
        "dtypes: float64(3), int64(2), object(2)\n",
        "memory usage: 54.8+ KB\n",
        "~~~\n",
        "As you can see, two columns, `Age` and `ObesityCategory`, are non-numeric and will need to be converted into numeric values. Since all columns have the same `Non-Null Count` (_n_=1000) there is no missing data.\n",
        "\n",
        "The output from `opDF['ObesityCategory'].value_counts()` is as follows:\n",
        "~~~text\n",
        "ObesityCategory\n",
        "Normal weight    371\n",
        "Overweight       295\n",
        "Obese            191\n",
        "Underweight      143\n",
        "Name: count, dtype: int64\n",
        "~~~\n",
        "As you can see, the column `ObesityCategory` has four categorical values.\n",
        "\n",
        "The output from `opDF['PhysicalActivityLevel'].value_counts()` is as follows:\n",
        "~~~text\n",
        "PhysicalActivityLevel\n",
        "4    259\n",
        "3    255\n",
        "2    247\n",
        "1    239\n",
        "Name: count, dtype: int64\n",
        "~~~\n",
        "\n",
        "There are four different activity levels, ranging from 1 to 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TJIS3T_RZSZ"
      },
      "source": [
        "### **Body Performance dataset**\n",
        "\n",
        "[Body Performance](https://www.kaggle.com/datasets/kukuroo3/body-performance-data)\n",
        "\n",
        "For the Examples in this lesson, we will be using the [Body Performance dataset](https://www.kaggle.com/datasets/kukuroo3/body-performance-data) provided by the [Seoul Olympic Games Korea Sports Promotion Foundation](https://www.bigdata-culture.kr/bigdata/user/data_market/detail.do?id=ace0aea7-5eee-48b9-b616-637365d665c1).\n",
        "\n",
        "This dataset has 12 categories of body performance for a relatively large number of men and women (_n_=13,303). To speed-up the training of neural networks in the Examples, we will only use a fraction of the total number.\n",
        "\n",
        "The 12 categories of fitness measurements are:\n",
        "* **age:** 20 ~64\n",
        "* **gender:** M,F\n",
        "* **height_cm:** (If you want to convert to feet, divide by 30.48)\n",
        "* **weight_kg:**\n",
        "* **body fat_%:**\n",
        "* **diastolic:** diastolic blood pressure (min)\n",
        "* **systolic:** systolic blood pressure (min)\n",
        "* **gripForce:**\n",
        "* **sit and bend forward_cm:**\n",
        "* **sit-ups counts:**\n",
        "* **broad jump_cm:**\n",
        "* **class:** A,B,C,D ( A: best) / stratified\n",
        "\n",
        "The output for the command `df.info()` is as follows:\n",
        "~~~text\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 13393 entries, 0 to 13392\n",
        "Data columns (total 12 columns):\n",
        " #   Column                   Non-Null Count  Dtype  \n",
        "---  ------                   --------------  -----  \n",
        " 0   age                      13393 non-null  float64\n",
        " 1   gender                   13393 non-null  object\n",
        " 2   height_cm                13393 non-null  float64\n",
        " 3   weight_kg                13393 non-null  float64\n",
        " 4   body fat_%               13393 non-null  float64\n",
        " 5   diastolic                13393 non-null  float64\n",
        " 6   systolic                 13393 non-null  float64\n",
        " 7   gripForce                13393 non-null  float64\n",
        " 8   sit and bend forward_cm  13393 non-null  float64\n",
        " 9   sit-ups counts           13393 non-null  float64\n",
        " 10  broad jump_cm            13393 non-null  float64\n",
        " 11  class                    13393 non-null  object\n",
        "dtypes: float64(10), object(2)\n",
        "~~~\n",
        "\n",
        "As you can see, all but two columns, `age` and `class`, are numeric. Since all columns have the same `Non-Null Count` there is no missing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syyWcUabRZSZ"
      },
      "source": [
        "## Create functions for this lesson\n",
        "\n",
        "The code in the cell below creates 2 useful functions for this lesson, `elaspedTime(start,stop)` and `rename_col_by_index(dataframe, index_mapping)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9A1H2asuRZSZ"
      },
      "outputs": [],
      "source": [
        "# Create functions\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Simple function to print out elasped time\n",
        "def elaspedTime(start,end):\n",
        "    # Print out time\n",
        "    seconds = int((end-start))\n",
        "    seconds = seconds % (24 * 3600)\n",
        "    hour = seconds // 3600\n",
        "    seconds %= 3600\n",
        "    minutes = seconds // 60\n",
        "    seconds %= 60\n",
        "    print(\"Elapsed time = %d:%02d:%02d\" % (hour, minutes, seconds))\n",
        "    print()\n",
        "\n",
        "# Simple function to change column name in a dataframe\n",
        "def rename_col_by_index(dataframe, index_mapping):\n",
        "    dataframe.columns = [index_mapping.get(i, col) for i, col in enumerate(dataframe.columns)]\n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIEm8MuCRZSa"
      },
      "source": [
        "# **L1 and L2 Regularization to Decrease Overfitting**\n",
        "\n",
        "L1 and L2 regularization are two common regularization techniques that can reduce the effects of overfitting [[Cite:ng2004feature]](http://cseweb.ucsd.edu/~elkan/254spring05/Hammon.pdf). These algorithms can either work with an objective function or as a part of the backpropagation algorithm. In both cases, the regularization algorithm is attached to the training algorithm by adding an objective.\n",
        "\n",
        "## **L1 Regularization**\n",
        "In the context of a neural network with L1 regularization, the objective function typically consists of two main components: the original loss function and the L1 regularization term. The objective function serves as a measure that the optimization algorithm aims to minimize during the training process.\n",
        "\n",
        "The objective function with L1 regularization can be represented as:\n",
        "\n",
        "`Objective function = Loss function + λ * L1 regularization term`\n",
        "\n",
        "where:\n",
        "\n",
        "* **Loss function:** The original loss function used to evaluate the performance of the neural network on the training data, such as the cross-entropy loss or mean squared error.\n",
        "* **λ (lambda):** The regularization parameter that controls the strength of the L1 regularization penalty.\n",
        "* **L1 regularization term:** The sum of absolute values of the weights in the neural network.\n",
        "\n",
        "The addition of the L1 regularization term to the objective function encourages sparsity in the weights of the neural network by penalizing large weights. This helps prevent overfitting and can lead to a simpler and more interpretable model. The trade-off between minimizing the loss function and reducing the magnitude of the weights is controlled by the regularization parameter λ.\n",
        "\n",
        "During the training process, the neural network adjusts its weights by minimizing the composite objective function, striking a balance between fitting the training data well (minimizing the loss function) and reducing model complexity (L1 regularization).\n",
        "\n",
        "These algorithms work by adding a weight penalty to the neural network training. This penalty encourages the neural network to keep the weights to small values. Both L1 and L2 calculate this penalty differently. You can add this penalty calculation to the calculated gradients for gradient-descent-based algorithms, such as backpropagation. The penalty is negatively combined with the objective score for objective-function-based training, such as simulated annealing.\n",
        "\n",
        "\n",
        "## **L1 vs L2 Regularization**\n",
        "Both L1 and L2 work similarily in that they penalize the size of the weight, but in significantly different ways. L2 will force the weights into a pattern similar to a Gaussian distribution while the L1 will force the weights into a pattern similar to a Laplace distribution, as demonstrated in the following figure.\n",
        "\n",
        "![L1 vs L2](https://biologicslab.co/BIO1173/images/class_9_l1_l2.png \"L1 vs L2\")\n",
        "\n",
        "As you can see, L1 algorithm is more tolerant of weights further from 0, whereas the L2 algorithm is less tolerant. We will highlight other important differences between L1 and L2 in the following sections. You also need to note that both L1 and L2 count their penalties based only on weights; they do not count penalties on bias values. Keras allows [l1/l2 to be directly added to your network](http://tensorlayer.readthedocs.io/en/stable/modules/cost.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihAgwQiORZSa"
      },
      "source": [
        "## Example 1: L1 Regularization of a classification neural network\n",
        "\n",
        "In Example 1, L1 regularization will be demonstrated using the Obesity Prediction dataset and a classification neural network that will predict the Obesity Category of individuals. To make coding easier to follow, Example 1 has been broken down into 3 steps labeled \"A\", \"B\" and \"C\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmre6CkmRZSa"
      },
      "source": [
        "### Example 1 - Step 1: Create feature vector\n",
        "\n",
        "The code in the cell below reads the Obesity Prediction dataset, `obesity_prediction.csv` from the course HTTPS server and and creates a new DataFrame called `opDF`. The column `Gender` is mapped with the string `Male` being mapped to `0` and `Female` mapped to `1`. The columns `Age`, `Height`, `Weight` and `BMI` are standardized to their Zscores. Since `ObesityCategory` is the y-value for this neural network, it is dropped when generating the list with the names of the columns to be used for the x-values (`opX_columns`).\n",
        "\n",
        "Since we will be building a neural network for **_classification_** we need to One-Hot encode the column `ObesityCategory` using the following code chunk:\n",
        "~~~text\n",
        "# One-Hot encode the column containing the y-values\n",
        "dummies = pd.get_dummies(opDF['ObesityCategory']) # Classification\n",
        "ObCategories = dummies.columns\n",
        "opY = dummies.values\n",
        "opY = np.asarray(opY).astype('float32')\n",
        "~~~\n",
        "\n",
        "Finally, the cell prints out the categorical values (names) that were One-Hot encoded using the \"starred\" print statement:\n",
        "~~~text\n",
        "# Print y categorical names\n",
        "print(*ObCategories)\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "rztIq6gXRZSa"
      },
      "outputs": [],
      "source": [
        "# Example 1 - Step 1: Create feature vector\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Read the data set\n",
        "opDF = pd.read_csv(\n",
        "    \"https://biologicslab.co/BIO1173/data/obesity_prediction.csv\",\n",
        "#   index_col=0,\n",
        "    sep=',',\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "# Map Gender\n",
        "mapping =  {'Male': 0,\n",
        "            'Female': 1}\n",
        "opDF['Gender'] = opDF['Gender'].map(mapping)\n",
        "\n",
        "# Standardize ranges\n",
        "opDF['Age'] = zscore(opDF['Age'])\n",
        "opDF['Height'] = zscore(opDF['Height'])\n",
        "opDF['Weight'] = zscore(opDF['Weight'])\n",
        "opDF['BMI'] = zscore(opDF['BMI'])\n",
        "\n",
        "# Generate list of columns for x\n",
        "opX_columns = opDF.columns.drop('ObesityCategory')  #\n",
        "\n",
        "# Generate x-values as numpy array\n",
        "opX = opDF[opX_columns].values\n",
        "opX = np.asarray(opX).astype('float32')\n",
        "\n",
        "# One-Hot encode the column containing the y-values\n",
        "dummies = pd.get_dummies(opDF['ObesityCategory']) # Classification\n",
        "obCategories = dummies.columns\n",
        "opY = dummies.values\n",
        "opY = np.asarray(opY).astype('float32')\n",
        "\n",
        "# Print y categorical names\n",
        "print(*obCategories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuwZChYIRZSa"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "~~~text\n",
        "Normal weight Obese Overweight Underweight\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yrO19wKRZSb"
      },
      "source": [
        "### Example 1 - Step 2: L1 Regularization to Decrease Overfitting\n",
        "\n",
        "We now create a Keras network with L1 regression.\n",
        "\n",
        "The specific Keras code chunk that adds L1 regularization is the following:\n",
        "~~~text\n",
        "activity_regularizer=regularizers.l1(1e-4)\n",
        "~~~\n",
        "It should be noted that the L1 regularizer is added to each hidden layer, but **not** to the output layer:\n",
        "~~~text\n",
        "def create_and_compile_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))  # Input\n",
        "    model.add(Dense(50, activation='relu', activity_regularizer=regularizers.l1(1e-4)))\n",
        "    model.add(Dense(25, activation='relu', activity_regularizer=regularizers.l1(1e-4)))\n",
        "    model.add(Dense(opY.shape[1], activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Step 2: L1 Regularization to Decrease Overfitting\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras import regularizers\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "# Set variables\n",
        "EPOCHS=100 # number of epochs for each loop\n",
        "numK=5     # Set number of K-folds\n",
        "\n",
        "def create_and_compile_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))  # Input\n",
        "    model.add(Dense(50, activation='relu', activity_regularizer=regularizers.l1(1e-4)))\n",
        "    model.add(Dense(25, activation='relu', activity_regularizer=regularizers.l1(1e-4)))\n",
        "    model.add(Dense(opY.shape[1], activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "fold = 0\n",
        "kf = KFold(n_splits=5)\n",
        "oos_y = []\n",
        "oos_pred = []\n",
        "\n",
        "# Record the start time in T_start\n",
        "T_start = time.time()\n",
        "\n",
        "print(\"STAND BY: TRAINING IS STARTING\")\n",
        "\n",
        "# Start Loop ------------------------------------------------------------#\n",
        "for train, test in kf.split(opX):\n",
        "    fold += 1\n",
        "    print(f\"Starting Fold #{fold}...\")\n",
        "\n",
        "    x_train = opX[train]\n",
        "    y_train = opY[train]\n",
        "    x_test = opX[test]\n",
        "    y_test = opY[test]\n",
        "\n",
        "    # Create and compile the model for this fold\n",
        "    model = create_and_compile_model(opX.shape[1])\n",
        "\n",
        "    # Train model for this fold\n",
        "    model.fit(x_train, y_train, validation_data=(x_test, y_test), verbose=0, epochs=EPOCHS)\n",
        "\n",
        "    # Use model to make predictions\n",
        "    pred = model.predict(x_test)\n",
        "\n",
        "    # Add actual y-values for the data used this fold\n",
        "    oos_y.append(y_test)\n",
        "\n",
        "    # Raw probabilities to chosen class (highest probability)\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "    oos_pred.append(pred)\n",
        "\n",
        "    # Measure this fold's accuracy\n",
        "    y_compare = np.argmax(y_test, axis=1)  # For accuracy calculation\n",
        "    score = metrics.accuracy_score(y_compare, pred)\n",
        "    print(f\"Fold score (accuracy): {score:.3f}\")\n",
        "\n",
        "# End Loop ------------------------------------------------------------#\n",
        "\n",
        "# Build the oos prediction list and calculate the error.\n",
        "oos_y = np.concatenate(oos_y)\n",
        "oos_pred = np.concatenate(oos_pred)\n",
        "oos_y_compare = np.argmax(oos_y,axis=1) # For accuracy calculation\n",
        "\n",
        "score = metrics.accuracy_score(oos_y_compare, oos_pred)\n",
        "print(f\"Final score (accuracy): {score:.3f}\")\n",
        "\n",
        "# Write the cross-validated prediction\n",
        "oos_y = pd.DataFrame(oos_y)\n",
        "oos_pred = pd.DataFrame(oos_pred)\n",
        "oosDF = pd.concat( [oos_pred, oos_y],axis=1 )\n",
        "\n",
        "# Record the end time in T_end\n",
        "T_end = time.time()\n",
        "\n",
        "# Print out elapsed time\n",
        "elaspedTime(T_start,T_end)\n"
      ],
      "metadata": {
        "id": "gezw40WEt_rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcZ0xkv5RZSb"
      },
      "source": [
        "If your code is correct, you should see something similar to following output:\n",
        "~~~text\n",
        "STAND BY: TRAINING IS STARTING\n",
        "Starting Fold #1...\n",
        "7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step\n",
        "Fold score (accuracy): 0.975\n",
        "Starting Fold #2...\n",
        "7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
        "Fold score (accuracy): 0.970\n",
        "Starting Fold #3...\n",
        "7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n",
        "Fold score (accuracy): 0.960\n",
        "Starting Fold #4...\n",
        "7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step\n",
        "Fold score (accuracy): 0.970\n",
        "Starting Fold #5...\n",
        "7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n",
        "Fold score (accuracy): 0.990\n",
        "Final score (accuracy): 0.973\n",
        "Elapsed time = 0:01:30\n",
        "\n",
        "~~~\n",
        "The `Final score (accuracy): 0.973` is very high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CphonQ79RZSb"
      },
      "source": [
        "### Example 1 - Step 3: Print out actual and predicted y-values\n",
        "\n",
        "The code in the cell below prints out the predicted and the actual Obesity Categories for the \"out-of-sample\" individuals. As mentioned previously, \"out-of-sample\" refers to data that was _not_ used in the process of developing the neural network model. It is only used to evaluate the accuracy and performance of the model on new, unseen data to assess its generalizability and potential for predicting future outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06elfGW-RZSb"
      },
      "outputs": [],
      "source": [
        "# Example 1 - Step 3: Print out actual and predicted y-values\n",
        "\n",
        "# Rename columns\n",
        "new_column_mapping = {0: 'Predicted Ob Class', 1: 'Actual: 0'}\n",
        "oosDF = rename_col_by_index(oosDF, new_column_mapping)\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_rows', 8)\n",
        "pd.set_option('display.max_columns', 8)\n",
        "\n",
        "# Display DataFrame\n",
        "display(oosDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yCeyNCZRZSb"
      },
      "source": [
        "If your code is correct you should see something similar to the following table:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_05_3_Exm1C.png)\n",
        "\n",
        "By inspection of the output above, you can see the model's predictions of the Obesity Category are very good for the out-of-sample individuals as would be expected with a `Final score (accuracy): 0.97`. In the output shown above, there were no errors in the model's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaxKBkukRZSb"
      },
      "source": [
        "## **Exercise 1: L1 Regularization of a classification neural network**\n",
        "\n",
        "For **Exercise 1**, you are to use the Body Performance dataset and build a classification neural network that will predict the fitness `class` of individuals. To make coding easier, **Exercise 1** has been broken down into 3 steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0T_P1m5RZSb"
      },
      "source": [
        "### **Exercise 1 - Step 1: Create feature vector**\n",
        "\n",
        "In the cell below, read the Body Performance dataset, `bodyPerformance.csv` from the course HTTPS server and and create a new DataFrame called `bpBigDF`. Since this is a fairly large dataset, you can speed up training time by only using a part of it. Use this code chunk to create a DataFrame with only 20% of the samples:\n",
        "~~~text\n",
        "bpDF=bpBigDF.sample(frac=0.20)\n",
        "~~~\n",
        "\n",
        "You need to map the column `gender` with the categorical values `M` and `F` to integers.  \n",
        "\n",
        "You should also standardize some, but not all of the other numeric values using this code chunk:\n",
        "~~~text\n",
        "# Standardize ranges\n",
        "bpDF['age'] = zscore(bpDF['age'])\n",
        "bpDF['height_cm'] = zscore(bpDF['height_cm'])\n",
        "bpDF['weight_kg'] = zscore(bpDF['weight_kg'])\n",
        "bpDF['diastolic'] = zscore(bpDF['diastolic'])\n",
        "bpDF['systolic'] = zscore(bpDF['systolic'])\n",
        "bpDF['gripForce'] = zscore(bpDF['gripForce'])\n",
        "~~~\n",
        "\n",
        "Since you are building a classification neural network to predict `class`, you will need to drop that column when creating your list of X columns:\n",
        "~~~text\n",
        "# Generate list of columns for x\n",
        "bpX_columns = bpDF.columns.drop('class')  # class is y-value\n",
        "~~~\n",
        "Using this list, you can generate the x values for your model using the following code chunk:\n",
        "~~~text\n",
        "# Generate x-values as numpy array\n",
        "bpX = bpDF[bpX_columns].values\n",
        "bpX = np.asarray(bpX).astype('float32')\n",
        "~~~\n",
        "\n",
        "Since this is a classification neural network, you will also need to One-Hot encode the column `class` using the following code chunk:\n",
        "~~~text\n",
        "# One-Hot encode the column containing the y-values\n",
        "dummies = pd.get_dummies(bpDF['class']) # Classification\n",
        "bpCategories = dummies.columns\n",
        "bpY = dummies.values\n",
        "bpY = np.asarray(bpY).astype('float32')\n",
        "~~~\n",
        "\n",
        "Finally, prints out the categorical values (names), `bpCategories` that were One-Hot encoded, using the \"starred\" print statement:\n",
        "~~~text\n",
        "# Print y categorical names\n",
        "print(*bpCategories)\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Aa12Y4jPRZSc"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1 - Step 1 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLflioppRZSc"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "~~~text\n",
        "A B C D\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef8RWNSmRZSc"
      },
      "source": [
        "### **Exercise 1 - Step 2: L1 Regularization to Decrease Overfitting**\n",
        "\n",
        "In the cell below, create a Keras network with L1 regression to predict the fitness `class` in the Body Performance Dataset using the feature vector that you prepared in **Exercise 1 - Step 1**. The code in Example 1 - Step 2 should act as your template."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 1 - Step 2 here\n",
        "\n"
      ],
      "metadata": {
        "id": "pyYmxlovw1fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJXdLkH9RZSc"
      },
      "source": [
        "If your code is correct, you should see something similar to following output:\n",
        "~~~text\n",
        "STAND BY: TRAINING IS STARTING\n",
        "Starting Fold #1...\n",
        "17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n",
        "Fold score (accuracy): 0.629\n",
        "Starting Fold #2...\n",
        "17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n",
        "Fold score (accuracy): 0.692\n",
        "Starting Fold #3...\n",
        "17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n",
        "Fold score (accuracy): 0.653\n",
        "Starting Fold #4...\n",
        "17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n",
        "Fold score (accuracy): 0.632\n",
        "Starting Fold #5...\n",
        "17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n",
        "Fold score (accuracy): 0.639\n",
        "Final score (accuracy): 0.649\n",
        "Elapsed time = 0:02:30\n",
        "~~~\n",
        "The `Final score (accuracy): 0.649` is not especially accurate. We'll see how well it can classify fitness classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZlE4AnwRZSc"
      },
      "source": [
        "### **Exercise 1 - Step 3: Print out actual and predicted y-values**\n",
        "\n",
        "In the cell below, print out the predicted and the actual Fitness `class` for the out-of-sample individuals. Label your columns using the following code chunk:\n",
        "~~~text\n",
        "# Rename columns\n",
        "new_column_mapping = {0: 'Predicted Fitness Class', 1: 'Actual: 0'}\n",
        "oosDF = rename_col_by_index(oosDF, new_column_mapping)\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iSYPvKTRZSc"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 1 - Step 3 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6_xeBWORZSc"
      },
      "source": [
        "If your code is correct you should see something similar to the following table:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_05/class_05_3_image01.png)\n",
        "\n",
        "By inspection of the output above, you can see the model's predictions of the fitness `class` is not perfect for the out-of-sample individuals. This should not come as a big surprise given a `Final score (accuracy): 0.649`.\n",
        "\n",
        "You should be able to look at this output and recognize which subjects were correctly classified and which one were incorrectly classified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkPsADPCRZSe"
      },
      "source": [
        "## **Exercise 2: L2 Regularization of a classification neural network**\n",
        "\n",
        "L2 regularization might generate better results than L1 regularization under the following conditions:\n",
        "\n",
        "* **Feature Correlation:** When features are highly correlated, L2 regularization tends to perform better than L1 regularization. L2 regularization encourages sparse coefficients but does not force them to zero, allowing correlated features to share the regularization penalty more evenly.\n",
        "* **Data Noise:** In the presence of noisy data, L2 regularization can be more effective at smoothing out the noise due to its tendency to distribute the penalty more uniformly across all weights. L2 regularization helps prevent individual noisy data points from disproportionately influencing the model.\n",
        "* **Model Stability:** L2 regularization often leads to more stable and well-conditioned models compared to L1 regularization. L2 regularization prevents the weights from growing excessively large, which can improve the numerical stability of the optimization process and enhance generalization performance.\n",
        "* **Small Dataset:** When working with a small dataset, L2 regularization can help prevent overfitting by providing smoother and more continuous solutions. L2 regularization penalizes large weights more gently than L1 regularization, making it more suitable for avoiding overfitting in smaller datasets.\n",
        "* **Uniform Impact on All Weights:** If the goal is to ensure that all weights have some level of regularization, rather than inducing sparsity, L2 regularization is preferred. L2 regularization treats all weights equally, promoting a more balanced impact on the model parameters.\n",
        "\n",
        "In summary, L2 regularization often outperforms L1 regularization in scenarios where feature correlation, data noise, model stability, small dataset size, or a uniform impact on all weights are important considerations for achieving better results and improved generalization performance.\n",
        "\n",
        "For **Exercise 2**, you are to again use the Body Performance dataset and build a classification neural network that will predict the fitness `class` of individuals. As before, **Exercise 2** has been broken down into 3 steps labeled \"A\", \"B\" and \"C\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk0KhcTWRZSe"
      },
      "source": [
        "### **Exercise 2 - Step 1: Create feature vector**\n",
        "\n",
        "In the cell below create a feature vector for the Body Performance dataset. You should use **exactly** the same code that you wrote for **Exercise 1 - Step 1**. As above, only use 20% of the dataset for your neural network model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "AAX1XzVbRZSe"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 2 - Step 1 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6pzG86xRZSe"
      },
      "source": [
        "If your code is correct, you should see the following output:\n",
        "~~~text\n",
        "A B C D\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqh7G5NQRZSe"
      },
      "source": [
        "### **Exercise 2 - Step 2: L2 Regularization to Decrease Overfitting**\n",
        "\n",
        "In the cell below, create a Keras network with L2 regression. The code for **Exercise 2 - Step 2** should be _identical_ to the code you wrote for **Exercise 1 - Step 2** with only one difference. To enable L2 regularization you will need to make the following code changes.\n",
        "\n",
        "Change the line:\n",
        "~~~text\n",
        "activity_regularizer=regularizers.l1(1e-4)\n",
        "~~~\n",
        "to read:\n",
        "~~~text\n",
        "kernel_regularizer=regularizers.l2(0.01)\n",
        "~~~\n",
        "It is somewhat hard to spot the difference. Can you see what is different?\n",
        "\n",
        "As above, the L2 regularizer must be added to each hidden layer, but not to the output layer:\n",
        "~~~text\n",
        "def create_and_compile_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))  # Input\n",
        "    model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(Dense(25, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(Dense(bpY.shape[1], activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your code for Exercise 2 - Step 2 here\n",
        "\n"
      ],
      "metadata": {
        "id": "gX-AHQC3ycjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Xg_HTERZSi"
      },
      "source": [
        "If your code is correct, you should see something similar to following output:\n",
        "~~~text\n",
        "STAND BY: TRAINING IS STARTING\n",
        "Starting Fold #1...\n",
        "17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n",
        "Fold score (accuracy): 0.560\n",
        "Starting Fold #2...\n",
        "17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n",
        "Fold score (accuracy): 0.595\n",
        "Starting Fold #3...\n",
        "17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n",
        "Fold score (accuracy): 0.614\n",
        "Starting Fold #4...\n",
        "17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step\n",
        "Fold score (accuracy): 0.610\n",
        "Starting Fold #5...\n",
        "17/17 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n",
        "Fold score (accuracy): 0.581\n",
        "Final score (accuracy): 0.592\n",
        "Elapsed time = 0:03:41\n",
        "\n",
        "~~~\n",
        "The `Final score (accuracy):  0.592` is even worse than the `0.649` score obtained with L1 Regularization in **Exercise 1 - Step 2**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q0wT51aRZSi"
      },
      "source": [
        "### **Exercise 2 - Step 3: Print out actual and predicted y-values**\n",
        "\n",
        "In the cell below, print out the predicted and the actual Fitness `class` for the \"out-of-sample\" individuals. Label your columns using the following code chunk:\n",
        "~~~text\n",
        "# Rename columns\n",
        "new_column_mapping = {0: 'Predicted Fitness Class', 1: 'Actual: 0'}\n",
        "oosDF = rename_col_by_index(oosDF, new_column_mapping)\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_8F12HJRZSi"
      },
      "outputs": [],
      "source": [
        "# Insert your code for Exercise 2 - Step 3 here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LjzjAcTRZSi"
      },
      "source": [
        "If your code is correct you should see something similar to the following table:\n",
        "\n",
        "![__](https://biologicslab.co/BIO1173/images/class_05/class_05_3_image02.png)\n",
        "\n",
        "By inspection of the output above, you can see the model's predictions of the fitness `class` is not perfect for the out-of-sample individuals. This should not come as a big surprise given a `Final score (accuracy): 0.592`.\n",
        "\n",
        "As above, the output shows that there were some errors in the model's predictions. Again, you should be above to look at this kind of output and be able to spot the correct and the incorrect predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJtLiLS-RZSi"
      },
      "source": [
        "## **Lesson Turn-in**\n",
        "\n",
        "When you have completed and run all of the code cells, use the **File --> Print.. --> Save to PDF** to generate a PDF of your Colab notebook. Save your PDF as `Class_05_3.lastname.pdf` where _lastname_ is your last name, and upload the file to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Poly-A Tail**\n",
        "\n",
        "## **Sol-20**\n",
        "\n",
        "![__](https://upload.wikimedia.org/wikipedia/commons/5/5e/Processor_Technology_SOL_20_Computer.jpg)\n",
        "\n",
        "\n",
        "The **Sol-20** was the first fully assembled microcomputer with a built-in keyboard and television output, what would later be known as a home computer. The design was the integration of an Intel 8080-based motherboard, a VDM-1 graphics card, the 3P+S I/O card to drive a keyboard, and circuitry to connect to a cassette deck for program storage. Additional expansion was available via five S-100 bus slots inside the machine. It also included swappable ROMs that the manufacturer called 'personality modules', containing a rudimentary operating system.\n",
        "\n",
        "The design was originally suggested by Les Solomon, the editor of Popular Electronics. He asked Bob Marsh of Processor Technology if he could design a smart terminal for use with the Altair 8800. Lee Felsenstein, who shared a garage working space with Marsh, had previously designed such a terminal but never built it. Reconsidering the design using modern electronics, they agreed the best solution was to build a complete computer with a terminal program in ROM. Felsenstein suggested the name \"Sol\" because they were including \"the wisdom of Solomon\" in the box.\n",
        "\n",
        "The Sol appeared on the cover of the July 1976 issue of Popular Electronics as a \"high-quality intelligent terminal\". It was initially offered in three versions; the Sol-PC motherboard in kit form, the Sol-10 without expansion slots, and the Sol-20 with five slots.\n",
        "\n",
        "A Sol-20 was taken to the Personal Computing Show in Atlantic City in August 1976 where it was a hit, building an order backlog that took a year to fill. Systems began shipping late that year and were dominated by the expandable Sol-20, which sold for $1,495 in its most basic fully-assembled form. The company also offered schematics for the system for free for those interested in building their own.\n",
        "\n",
        "The Sol-20 remained in production until 1979, by which point about 12,000 machines had been sold. By that time, the \"1977 trinity\" —the Apple II, Commodore PET and TRS-80— had begun to take over the market, and a series of failed new product introductions drove Processor Technology into bankruptcy. Felsenstein later developed the successful Osborne 1 computer, using much the same underlying design in a portable format.\n",
        "\n",
        "### **History**\n",
        "\n",
        "**Tom Swift Terminal**\n",
        "\n",
        "Lee Felsenstein was one of the sysops of Community Memory, the first public bulletin board system. Community Memory opened in 1973, running on a SDS 940 mainframe that was accessed through a Teletype Model 33, essentially a computer printer and keyboard, in a record store in Berkeley, California. The cost of running the system was untenable; the teletype normally cost $1,500 (their first example was donated from Tymshare as junk), the modem another $300, and time on the SDS was expensive – in 1968, Tymshare charged $13 per hour (equivalent to $114 in 2023). Even the reams of paper output from the terminal were too expensive to be practical and the system jammed all the time. The replacement of the Model 33 with a Hazeltine glass terminal helped, but it required constant repairs.\n",
        "\n",
        "Since 1973, Felsenstein had been looking for ways to lower the cost. One of his earliest designs in the computer field was the Pennywhistle modem, a 300 bits per second acoustic coupler that was the cost of commercial models. When he saw Don Lancaster's TV Typewriter on the cover of the September 1973 Radio Electronics, he began adapting its circuitry as the basis for a design he called the Tom Swift Terminal. The terminal was deliberately designed to allow it to be easily repaired. Combined with the Pennywhistle, users would have a cost-effective way to access Community Memory.\n",
        "\n",
        "In January 1975, Felsenstein saw a post on Community Memory by Bob Marsh asking if anyone would like to share a garage. Marsh was designing a fancy wood-cased digital clock and needed space to work on it. Felsenstein had previously met Marsh at school and agreed to split the $175 rent on a garage in Berkeley. Shortly after, Community Memory shut down for the last time, having burned out the relationship with its primary funding source, Project One, as well the energy of its founding members.\n",
        "\n",
        "**Processor Technology**\n",
        "\n",
        "January 1975 was also the month that the Altair 8800 appeared on the front page of Popular Electronics, sparking off intense interest among the engineers of the rapidly growing Silicon Valley. Shortly thereafter, on 5 March 1975, Gordon French and Fred Moore held the first meeting of what would become the Homebrew Computer Club. Felsenstein took Marsh to one of the meetings, Marsh saw an opportunity supplying add-on cards for the Altair, and in April, he formed Processor Technology with his friend Gary Ingram.\n",
        "\n",
        "The new company's first product was a 4 kB DRAM memory card for the Altair. A similar card was already available from the Altair's designers, MITS, but it was almost impossible to get working properly. Marsh began offering Felsenstein contracts to draw schematics or write manuals for the products they planned to introduce. Felsenstein was still working on the terminal as well, and in July, Marsh offered to pay him to develop the video portion. This was essentially a version of the terminal where the data would be supplied by the main memory of the Altair rather than a serial port.\n",
        "\n",
        "The result was the VDM-1, the first graphics card. The VDM-1 could display 16 lines of 64 characters per line, and included the complete ASCII character set with upper- and lower-case characters and a number of graphics characters like arrows and basic math symbols. An Altair equipped with a VDM-1 for output and Processor Technology's 3P+S card running a keyboard for input removed the need for a terminal, yet cost less than dedicated smart terminals like the Hazeltine.\n",
        "\n",
        "**Intelligent terminal concept**\n",
        "\n",
        "Before the VDM-1 was launched in late 1975, the only way to program the Altair was through its front-panel switches and LED lamps, or by purchasing a serial card and using a terminal of some sort. This was typically a Model 33, which still cost $1,500 if available. Normally the teletypes were not available – Teletype Corporation typically sold them only to large commercial customers, which led to a thriving market for broken-down machines that could be repaired and sold into the microcomputer market. Ed Roberts, who had developed the Altair, eventually arranged a deal with Teletype to supply refurbished Model 33s to MITS customers who had bought an Altair.\n",
        "\n",
        "Les Solomon, whose Popular Electronics magazine launched the Altair, felt a low-cost smart terminal would be highly desirable in the rapidly expanding microcomputer market. In December 1975, Solomon traveled to Phoenix to meet with Don Lancaster to ask about using his TV Typewriter as a video display in a terminal. Lancaster seemed interested, so Solomon took him to Albuquerque to meet Roberts. The two immediately began arguing when Lancaster criticized the design of the Altair and suggested changes to better support expansion cards, demands that Roberts flatly refused. Any hopes of a partnership disappeared.\n"
      ],
      "metadata": {
        "id": "_n8KC28jZx1x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fyv5JcC_blCQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}